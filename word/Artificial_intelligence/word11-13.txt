one drawback of hierarchical task network htn planning is the difficulty of providing complete domain knowledge i e a complete and correct set of htn method for every task to provide a principled way to overcome this difficulty we define a simple formalism that extends classical planning to include problem decomposition using method and a planning algorithm based on this formalism in our formalism the method specify way to achieve goal rather than task a in conventional htn planning and goal may be achieved even when no method are available our planning algorithm godel goal decomposition with landmark is sound and complete irrespective of whether the domain knowledge i e the set of method given to the planner is complete by comparing godel s performance with varying amount of domain knowledge across three benchmark planning domain we show experimentally that godel work correctly with partial planning knowledge godel s performance improves a more planning knowledge is given and when given full domain knowledge godel match the performance of a state of the art hierarchical planner 
by always mapping data from lower dimensional space into higher or even infinite dimensional space kernel k mean is able to organize data into group when data of different cluster are not linearly separable however kernel k mean incurs the large scale computation due to the representation theorem i e keeping an extremely large kernel matrix in memory when using popular gaussian and spatial pyramid matching kernel which largely limit it use for processing large scale data also existing kernel clustering can be overfitted by outlier a well in this paper we introduce an euler clustering which can not only maintain the benefit of nonlinear modeling using kernel function but also significantly solve the large scale computational problem in kernel based clustering this is realized by incorporating euler kernel euler kernel is relying on a nonlinear and robust cosine metric that is le sensitive to outlier more important it intrinsically induces an empirical map which map data onto a complex space of the same dimension euler clustering take these advantage to measure the similarity between data in a robust way without increasing the dimensionality of data and thus solves the large scale problem in kernel k mean we evaluate euler clustering and show it superiority against related method on five publicly available datasets 
the ability to predict or at least recognize the state of the world that an action brings about is a central feature of autonomous agent we propose herein a formal framework within which we investigate whether this ability can be autonomously learned the framework make explicit certain premise that we contend are central in such a learning task i slow sensor may prevent the sensing of an action s direct effect during learning ii prediction need to be made reliably in future and novel situation we initiate in this work a thorough investigation of the condition under which learning is or is not feasible despite the very strong negative learnability result that we obtain we also identify interesting special case where learning is feasible and useful 
dimensionality reduction play a vital role in pattern recognition however for normalized vector data existing method do not utilize the fact that the data is normalized in this paper we propose to employ an angular decomposition of the normalized vector data which corresponds to embedding them on a unit surface on graph data for similarity kernel matrix with constant diagonal element we propose the angular decomposition of the similarity matrix which corresponds to embedding object on a unit sphere in these angular embeddings the euclidean distance is equivalent to the cosine similarity thus data structure best described in the cosine similarity and data structure best captured by the euclidean distance can both be effectively detected in our angular embedding we provide the theoretical analysis derive the computational algorithm and evaluate the angular embedding on several datasets experiment on data clustering demonstrate that our method can provide a more discriminative subspace 
hash function learning ha been recently received more and more attention in fast search for large scale data however existing popular learning based hashing method are batch based learning model and thus incur large scale computational problem for learning an optimal model on a large scale of labelled data and cannot handle data which come sequentially in this paper we address the problem by developing an online hashing learning algorithm to get hashing model accommodate to each new pair of data at the same time the new updated hash model is penalized by the last learned model in order to retain important information learned in previous round we also derive a tight bound for the cumulative loss of our proposed online learning algorithm the experimental result demonstrate superiority of the proposed online hashing model on searching both metric distance neighbor and semantical similar neighbor in the experiment 
hashing based fast nearest neighbor search technique ha attracted great attention in both research and industry area recently many existing hashing approach encode data with projection based hash function and represent each projected dimension by bit however the dimension with high variance hold large energy or information of data but treated equivalently a dimension with low variance which lead to a serious information loss in this paper we introduce a novel hashing algorithm called harmonious hashing which aim at learning hash function with low information loss specifically we learn a set of optimized projection to preserve the maximum cumulative energy and meet the constraint of equivalent variance on each dimension a much a possible in this way we could minimize the information loss after binarization despite the extreme simplicity our method outperforms superiorly to many state of the art hashing method in large scale and high dimensional nearest neighbor search experiment 
the invited talk are how to compare alternative architecture by radia perlman of intel portal enabling application architecture co design for high performance interconnects by ron brightwell of sandia national laboratory and electronic photonic integration within switch and router by mike watt of mit brief author biography are also included 
lasso type variable selection ha increasingly expanded it machine learning application in this paper uncorrelated lasso is proposed for variable selection where variable de correlation is considered simultaneously with variable selection so that selected variable are uncorrelated a much a possible an effective iterative algorithm with the proof of convergence is presented to solve the sparse optimization problem experiment on benchmark data set show that the proposed method ha better classification performance than many state of the art variable selection method copyright association for the advancement of artificial intelligence www aaai org all right reserved 
we propose a novel approach for solving unary sa planning problem this approach extends an sa instance with new state variable representing intention about how each original state variable will be used or changed next and split the original action into several stage of intention followed by eventual execution the result is a new sa instance with the same basic solution a the original while the transformed problem is larger it ha additional structure that can be exploited to reduce the branching factor leading to reachable state space that are many order of magnitude smaller and hence much faster planning in several test domain with acyclic causal graph 
the behavior composition problem involves the automatic synthesis of a controller that is able to realize i e implement a desired target behavior specification by suitably coordinating a set of already available behavior while the problem ha been thoroughly studied one open issue ha resisted a principled solution if the target specification is not fully realizable is there a way to realize it at best in this doctoral work we look at quantitative and qualitative way to address this question 
iterated game are well known in the game theory literature we study iterated boolean game these are game in which player repeatedly choose truth value for boolean variable they have control over our model of iterated boolean game assumes that player have goal given by formula of linear temporal logic ltl a formalism for expressing property of state sequence in order to model the strategy that player use in such game we use a finite state machine model after introducing and formally defining iterated boolean game we investigate the computational complexity of their associated game theoretic decision problem a well a semantic condition characterising class of ltl property that are preserved by pure strategy nash equilibrium whenever they exist 
tree matching problem arise in many computational domain the literature provides several method for creating correspondence between labeled tree however by definition tree matching algorithm rigidly preserve ancestry that is once two node have been placed in correspondence their descendant must be matched a well we introduce flexible tree matching which relaxes this rigid requirement in favor of a tunable formulation in which the role of hierarchy can be controlled we show that flexible tree matching is strongly np complete give a stochastic approximation algorithm for the problem and demonstrate how structured prediction technique can learn the algorithm s parameter from a set of example matchings finally we present result from applying the method to task in web design 
conceptualization seek to map a short text i e a word or a phrase to a set of concept a a mechanism of understanding text most of prior research in conceptualization us human crafted knowledge base that map instance to concept such approach to conceptualization have the limitation that the mapping are not context sensitive to overcome this limitation we propose a framework in which we harness the power of a probabilistic topic model which inherently capture the semantic relation between word by combining latent dirichlet allocation a widely used topic model with probase a large scale probabilistic knowledge base we develop a corpus based framework for context dependent conceptualization through this simple but powerful framework we improve conceptualization and enable a wide range of application that rely on semantic understanding of short text including frame element prediction word similarity in context ad query similarity and query similarity 
we present constituent grammatical evolution cge a new evolutionary automatic programming algorithm that extends the standard grammatical evolution algorithm by incorporating the concept of constituent gene and conditional behaviour switching cge build from elementary and more complex building block a control program which dictate the behaviour of an agent and it is applicable to the class of problem where the subject of search is the behaviour of an agent in a given environment it take advantage of the powerful grammatical evolution feature of using a bnf grammar definition a a plug in component to describe the output language to be produced by the system the main benchmark problem in which cge is evaluated is the santa fe trail problem using a bnf grammar definition which defines a search space semantically equivalent with that of the original definition of the problem by koza furthermore cge is evaluated on two additional problem the loss alto hill and the hampton court maze the experimental result demonstrate that constituent grammatical evolution outperforms the standard grammatical evolution algorithm in these problem in term of both efficiency percent of solution found and effectiveness number of required step of solution found 
in boolean game player exercise control over propositional variable and strive to achieve a goal formula whose realization might require the opponent cooperation recently a theory of incentive engineering for such game ha been devised where an external authority steer the outcome of the game towards certain desirable property consistent with player goal by imposing a taxation mechanism on the player that make the outcome that do not comply with those property le appealing to them the present contribution stem from a complementary perspective and study instead how boolean game can be transformed from inside rather than from outside by endowing player with the possibility of sacrificing a part of their payoff received at a certain outcome in order to convince other player to play a certain strategy what we call here endogenous boolean game ebgs boil down to enriching the framework of boolean game with the machinery of side payment coming from game theory we analyze equilibrium in ebgs showing the precondition needed for desirable outcome to be achieved without external intervention finally making use of taxation mechanism we show how to transform an ebg in such a way that desirable outcome can be realized independently of side payment 
ensemble method which train multiple learner for a task are among the state of the art learning approach the diversity of the component learner ha been recognized a a key to a good ensemble and existing ensemble method try different way to encourage diversity mostly by heuristic in this paper we propose the diversity regularized machine drm in a mathematical programming framework which efficiently generates an ensemble of diverse support vector machine svms theoretical analysis discloses that the diversity constraint used in drm can lead to an effective reduction on it hypothesis space complexity implying that the diversity control in ensemble method indeed play a role of regularization a in popular statistical learning approach experiment show that drm can significantly improve generalization ability and is superior to some state of the art svm ensemble method 
we describe theoretical result and empirical study of context sensitive restart policy for randomized search procedure the method generalize previous result on optimal restart policy by exploiting dynamically updated belief about the probability distribution for run time rather than assuming complete knowledge or zero knowledge about the run time distribution we formulate restart policy that consider real time observation about property of instance and the solver s activity we describe background work on the application of bayesian method to build predictive model for run time introduce an optimal policy for dynamic restarts that considers prediction about run time and perform a comparative study of traditional fixed versus dynamic restart policy 
this paper investigates belief revision where the underlying logic is that governing horn clause it prof to be the case that classical agm belief revision doesn t immediately generalise to the horn case in particular a standard construction based on a total preorder over possible world may violate the accepted agm postulate conversely horn revision function in the obvious extension to the agm approach are not captured by total preorders over possible world we address these difficulty by first restricting the semantic construction to well behaved ordering and second by augmenting the revision postulate by an additional postulate this additional postulate is redundant in the agm approach but not in the horn case in a representation result we show that these two approach coincide arguably this work is interesting for several reason it extends agm revision to inferentially weaker horn theory hence it shed light on the theoretical underpinnings of belief change a well a generalising the agm paradigm thus this work is relevant to revision in area that employ horn clause such a deductive database and logic programming a well a area in which inference is weaker than classical logic such a in description logic 
in this paper we take a step towards using argumentation in social network and introduce social abstract argumentation framework an extension of dung s abstract argumentation framework that incorporates social voting we propose a class of semantics for these new social abstract argumentation framework and prove some important non trivial property which are crucial for their applicability in social network 
nominal schema have recently been introduced a a new approach for the integration of dl safe rule into the description logic framework the efficient processing of knowledge base with nominal schema remains however challenging we address this by extending the well known optimisation of absorption a well a the standard tableau calculus to directly handle the absorbed nominal schema axiom we implement the resulting extension of standard tableau calculus in a novel reasoning system and we integrate further optimisation in our empirical evaluation we show the effect of these optimisation and we find that the proposed approach performs well even when compared to other dl reasoner with dedicated rule support 
k flat is a model based linear manifold clustering algorithm which ha been successfully applied in many real world scenario though some previous work have shown that k flat doesn t always provide good performance little effort ha been devoted to analyze it inherent deficiency in this paper we address this challenge by showing that the deteriorative performance of k flat can be attributed to the usual reconstruction error measure and the infinitely extending representation of linear model then we propose localized k flat algorithm lkf which introduces localized representation of linear model and a new distortion measure to remove confusion among different cluster experiment on both synthetic and real world data set demonstrate the efficiency of the proposed algorithm moreover preliminary experiment show that lkf ha the potential to group manifold with nonlinear structure copyright association for the advancement of artificial intelligence all right reserved 
recent research ha highlighted the benefit of completeness a a retrieval criterion in recommender system in complete retrieval any subset of the constraint in a given query that can be satisfied must be satisfied by at least one of the retrieved product minimal completeness i e always retrieving the smallest set of product needed for completeness is also beginning to attract research interest a a way to minimize cognitive load in the approach other important feature of a retrieval algorithm s behavior include the diversity of the retrieved product and the order in which they are presented to the user in this paper we present a new algorithm for minimally complete retrieval mcr in which the ranking of retrieved product is primarily based on the number of constraint that they satisfy but other measure such a similarity or utility can also be used to inform the retrieval process we also present theoretical and empirical result that demonstrate our algorithm s ability to minimize cognitive load while ensuring the completeness and diversity of the retrieved product 
when doing learning from demonstration it is often the case that the demonstrator provides corrective example to fix errant behavior by the agent or robot we present a set of algorithm which use this corrective data to identify and remove noisy example in datasets which caused errant classification and ultimately errant behavior the objective is to actually modify the source datasets rather than solely rely on the noise insensitivity of the classification algorithm this is particularly useful in the sparse datasets often found in learning from demonstration experiment our approach try to distinguish between noisy misclassification and mere undersampling of the learning space if error are a result of misclassification we potentially remove the responsible point and update the classifier we demonstrate our method on uci machine learning datasets at different level of sparsity and noise using decision tree k nearest neighbor and support vector machine 
many fundamental problem in artificial intelligence knowledge representation and verification involve reasoning about set and relation between set and can be modeled a set constraint satisfaction problem set csps such problem are frequently intractable but there are several important set csps that are known to be polynomial time tractable we introduce a large class of set csps that can be solved in quadratic time our class which we call ei contains all previously known tractable set csps but also some new one that are of crucial importance for example in description logic the class of ei set constraint ha an elegant universal algebraic characterization which we use to show that every set constraint language that properly contains all ei set constraint already ha a finite sublanguage with an np hard constraint satisfaction problem 
we study the problem of designing efficient auction where bidder have interdependent value i e value that depend on the signal of other agent we consider a contingent bid model in which agent can explicitly condition the value of their bid on the bid submitted by others in particular we adopt a linear contingent bidding model for single minded combinatorial auction ca in which submitted bid are linear combination of bid received from others we extend the existing state of the art by identifying constraint on the interesting bundle and contingency weight reported by the agent which allow the efficient second priced fixed point bid auction to be implemented in single minded ca moreover for domain in which the required single crossing condition fails which characterizes when efficient ic auction are possible we design a two stage mechanism in which a subset of agent expert are allocated first using their report to allocate the remaining item to the other agent 
counting the model of a propositional formula is a key issue for a number of ai problem but few propositional language offer the possibility to count model efficiently in order to fill the gap we introduce the language eadt of extended affine decision tree an extended affine decision tree simply is a tree with affine decision node and some specific decomposable conjunction or disjunction node unlike standard decision tree the decision node of an eadt formula are not labeled by variable but by affine clause we study eadt and several subset of it along the line of the knowledge compilation map we also describe a cnf to eadt compiler and present some experimental result those result show that the eadt compilation based approach is competitive with and in some case is able to outperform the model counter cachet and the d dnnf compilation based approach to model counting 
various semi supervised learning method have been proposed recently to solve the long standing shortage problem of manually labeled data in sentiment classification however most existing study assume the balance between negative and positive sample in both the labeled and unlabeled data which may not be true in reality in this paper we investigate a more common case of semi supervised learning for imbalanced sentiment classification in particular various random subspace are dynamically generated to deal with the imbalanced class distribution problem evaluation across four domain show the effectiveness of our approach 
a agent technology becomes increasing more prevalent coordination in mixed agent human environment becomes a key issue agent human coordination is becoming even more important in real life situation where uncertainty and incomplete information exists and communication is costly while abundant research ha focused on aspect of computerized teamwork little attention ha been given to the issue raised in team that consist of both computerized agent and people in this paper we focus on teamwork between an agent and a human counterpart and present a novel agent designed to interact proficiently with people in extensive simulation we matched our agent with people and compared it with another state of the art agent our result demonstrate the significant improvement in coordination when our agent is involved 
computing the marketmaker price of a security in a combinatorial prediction market is p hard we devise a fully polynomial randomized approximation scheme fpras that computes the price of any security in disjunctive normal form dnf within an multiplicative error factor in time polynomial in and the size of the input with high probability and under reasonable assumption our algorithm is a monte carlo technique based on importance sampling the algorithm can also approximately price security represented in conjunctive normal form cnf with additive error bound to illustrate the applicability of our algorithm we show that many security in yahoo s popular combinatorial prediction market game called predictalot can be represented by dnf formula of polynomial size 
a novel proposal for object recognition based on relational grammar and bayesian network is presented based on a symbol relation grammar an object is represented a a hierarchy of feature and spatial relation this representation is transformed to a bayesian network structure which parameter are learned from example thus recognition is based on probabilistic inference in the bayesian network representation preliminary result in modeling natural object are presented 
in this paper we present a graph based approach aimed at learning a lexical taxonomy automatically starting from a domain corpus and the web unlike many taxonomy learning approach in the literature our novel algorithm learns both concept and relation entirely from scratch via the automated extraction of term definition and hypernym this result in a very dense cyclic and possibly disconnected hypernym graph the algorithm then induces a taxonomy from the graph our experiment show that we obtain high quality result both when building brand new taxonomy and when reconstructing wordnet sub hierarchy 
keyword extraction attracts much attention for it significant role in various natural language processing task while some existing method for keyword extraction have considered using single type of semantic relatedness between word or inherent attribute of word almost all of them ignore two important issue how to fuse multiple type of semantic relation between word into a uniform semantic measurement and automatically learn the weight of the edge between the word in the word graph of each document and how to integrate the relation between word and word intrinsic feature into a unified model in this work we tackle the two issue based on the supervised random walk model we propose a supervised ranking based method for keyword extraction which is called seafarer it can not only automatically learn the weight of the edge in the unified graph of each document which includes multiple semantic relation but also combine the merit of semantic relation of edge and intrinsic attribute of node together we conducted extensive experimental study on an established benchmark and the experimental result demonstrate that seafarer outperforms the state of the art supervised and unsupervised method 
mining opinion target from online review is an important and challenging task in opinion mining this paper proposes a novel approach to extract opinion target by using partially supervised word alignment model pswam at first we apply pswam in a monolingual scenario to mine opinion relation in sentence and estimate the association between word then a graph based algorithm is exploited to estimate the confidence of each candidate and the candidate with higher confidence will be extracted a the opinion target compared with existing syntax based method pswam can effectively avoid parsing error when dealing with informal sentence in online review compared with the method using alignment model pswam can capture opinion relation more precisely through partial supervision from partial alignment link moreover when estimating candidate confidence we make penalty on higher degree vertex in our graph based algorithm in order to decrease the probability of the random walk running into the unrelated region in the graph a a result some error can be avoided the experimental result on three data set with different size and language show that our approach outperforms state of the art method 
we propose to measure statistical dependence between two random variable by the mutual information dimension mid and present a scalable parameter free estimation method for this task supported by sound dimension theory our method give an effective solution to the problem of detecting interesting relationship of variable in massive data which is nowadays a heavily studied topic in many scientific discipline different from classical pearson s correlation coefficient mid is zero if and only if two random variable are statistically independent and is translation and scaling invariant we experimentally show superior performance of mid in detecting various type of relationship in the presence of noise data moreover we illustrate that mid can be effectively used for feature selection in regression 
in many multi agent system the emergence of norm is the primary factor that determines over all behavior and utility agent simulation can be used to predict and study the development of these norm however a large number of simulation is usually required to provide an accurate depiction of the agent behavior and some rare contingency may still be overlooked completely the cost and risk involved with agent simulation can be reduced by analyzing a system theoretically and producing model of it behavior we use such a theoretical approach to examine the dynamic of a population of agent playing a coordination game to determine all the norm to which the society can converge and develop a system of linear recurrence relation that predict how frequently each of these norm will be reached a well a the average convergence time this analysis produce certain guarantee about system behavior that canot be provided by a purely empirical approach and can be used to make prediction about the emergence of norm that numerically match those obtained through large scale simulation 
bayesian treatment of matrix factorization ha been successfully applied to the problem of collaborative prediction where unknown rating are determined by the predictive distribution inferring posterior distribution over user and item factor matrix that are used to approximate the user item matrix a their product in practice however bayesian matrix factorization suffers from cold start problem where inference are required for user or item about which a sufficient number of rating are not gathered in this paper we present a method for bayesian matrix factorization with side information to handle cold start problem to this end we place gaussian wishart prior on mean vector and precision matrix of gaussian user and item factor matrix such that mean of each prior distribution is regressed on corresponding side information we develop variational inference algorithm to approximately compute posterior distribution over user and item factor matrix in addition we provide bayesian cram r rao bound for our model showing that the hierarchical bayesian matrix factorization with side information improves the reconstruction over the standard bayesian matrix factorization where the side information is not used experiment on movielens data demonstrate the useful behavior of our model in the case of cold start problem 
bilingual machine readable dictionary are knowledge resource useful in many automatic task however compared to monolingual computational lexicon like wordnet bilingual dictionary typically provide a lower amount of structured information such a lexical and semantic relation and often do not cover the entire range of possible translation for a word of interest in this paper we present cycle and quasi cycle cqc a novel algorithm for the automated disambiguation of ambiguous translation in the lexical entry of a bilingual machine readable dictionary 
a notion of quantified conditional logic is provided that includes quantification over individual and propositional variable the former is supported with respect to constant and variable domain semantics in addition a sound and complete embedding of this framework in classical higher order logic is presented using prominent example from the literature it is demonstrated how this embedding enables effective automation of reasoning within object level and about meta level quantified conditional logic with off the shelf higher order theorem provers and model finder 
the ability to understand the goal and plan of other agent is an important characteristic of intelligent behaviour in many context one of the approach used to endow agent with this capability is the weighted model counting approach given a plan library and a sequence of observation this approach exhaustively enumerates plan execution model that are consistent with the observed behaviour the probability that the agent might be pursuing a particular goal is then computed a a proportion of plan execution model satisfying the goal the approach allows to recognize multiple interleaved plan but suffers from a combinatorial explosion of plan execution model which impedes it application to real world domain this paper present a heuristic weighted model counting algorithm that limit the number of generated plan execution model in order to recognize goal quickly by computing their lower and upper bound likelihood 
consider the following problem in game manipulation a tournament designer who ha full knowledge of the match outcome between any possible pair of player would like to create a bracket for a balanced single elimination tournament so that their favorite player will win although this problem ha been studied in the area of voting and tournament manipulation it is still unknown whether it can be solved in polynomial time we focus on identifying several general case for which the tournament can always be rigged efficiently so that the given player win we give constructive proof that under some natural assumption if a player is ranked among the top k player then one can efficiently rig the tournament for the given player even when k is a large a of the player 
this paper study the recovery guarantee of the model of minimizing f where is a tensor and and f are the trace and frobenius norm of respectively we show that they can efficiently recover low rank tensor in particular they enjoy exact guarantee similar to those known for minimizing under the condition on the sensing operator such a it null space property restricted isometry property or spherical section property to recover a low rank tensor minimizing f return the same solution a minimizing almost whenever max i x i 
different from laplacian or normal matrix the property of the adjacency eigenspace received much le attention recent work showed that node projected into the adjacency eigenspace exhibit an orthogonal line pattern and node from the same community locate along the same line in this paper we conduct theoretical study based on graph perturbation to demonstrate why this line orthogonality property hold in the adjacency eigenspace and why it generally disappears in the laplacian and normal eigenspaces using the orthogonality property in the adjacency eigenspace we present a graph partition algorithm adjcluster which first project node coordinate to the unit sphere and then applies the classic k mean to find cluster empirical evaluation on synthetic data and real world social network validate our theoretical finding and show the effectiveness of our graph partition algorithm 
we study bayesian reinforcement learning rl a a solution of the exploration exploitation dilemma a full bayesian planning is intractable except for special case previous work ha proposed several approximation method however these were often computationally expensive or limited to dirichlet prior in this paper we propose a new algorithm that is fast and of polynomial time for near bayesian optimal policy with any prior distribution that are not greatly misspecified perhaps even more interestingly the proposed algorithm can naturally avoid being misled by incorrect belief while effectively utilizing useful part of prior information it can work well even when an utterly misspecified prior is assigned in that case the algorithm will follow pac mdp behavior instead if an existing pacmdp algorithm doe so the proposed algorithm naturally outperformed other algorithm compared with it on a standard benchmark problem 
argument extraction is a challenging task in event extraction however most of previous study focused on intra sentence information and failed to extract inter sentence argument this paper proposes a discourse level joint model of argument identification and role determination to infer those inter sentence argument in a discourse moreover to better represent the relationship among relevant event mention and the relationship between an event mention and it argument in a discourse this paper introduces various kind of corpus based and discourse based constraint in the joint model either automatically learned or linguistically motivated evaluation on the ace chinese corpus justifies the effectiveness of our joint model over a strong baseline in chinese argument extraction in particular argument identification 
in this paper we propose a locality constrained and sparsity encouraged manifold fitting approach aiming at capturing the locally sparse manifold structure into neighborhood graph construction by exploiting a principled optimization model the proposed model formulates neighborhood graph construction a a sparse coding problem with the locality constraint therefore achieving simultaneous neighbor selection and edge weight optimization the core idea underlying our model is to perform a sparse manifold fitting task for each data point so that close by point lying on the same local manifold are automatically chosen to connect and meanwhile the connection weight are acquired by simple geometric reconstruction we term the novel neighborhood graph generated by our proposed optimization model m fitted graph since such a graph stem from sparse manifold fitting to evaluate the robustness and effectiveness of m fitted graph we leverage graph based semi supervised learning a the testbed extensive experiment carried out on six benchmark datasets validate that the proposed m fitted graph is superior to state of the art neighborhood graph in term of classification accuracy using popular graph based semi supervised learning method 
this paper target at automatically detecting and classifying user s suggestion from tweet the short and informal nature of tweet along with the imbalanced characteristic of suggestion tweet make the task extremely challenging to this end we develop a classification framework on factorization machine which is effective and efficient especially in classification task with feature sparsity setting moreover we tackle the imbalance problem by introducing cost sensitive learning technique in factorization machine extensively experimental study on a manually annotated real life data set show that the proposed approach significantly improves the baseline approach and yield the precision of and recall of we also investigate the reason why factorization machine perform better finally we introduce the first manually annotated dataset for suggestion classification copyright association for the advancement of artificial intelligence www aaai org all right reserved 
to support more precise query translation for english chinese bi directional cross language information retrieval clir we have developed a novel framework by integrating a semantic network to characterize the correlation between multiple inter related text term of interest and learn their inter related statistical query translation model first a semantic network is automatically generated from large scale english chinese bilingual parallel corpus to characterize the correlation between a large number of text term of interest second the semantic network is exploited to learn the statistical query translation model for such text term of interest finally these inter related query translation model are used to translate the query more precisely and achieve more effective clir our experiment on a large number of official public data have obtained very positive result 
there are not very many existing logic of belief which have both a perspicuous semantics and are computationally attractive an exception is the logic sl proposed by liu lakemeyer and levesque which allows for a decidable and often even tractable form of reasoning while the language is first order and hence quite expressive it still ha a number of shortcoming for one belief about belief are not addressed at all for another the name of individual are rigid that is their identity is assumed to be known in this paper we show how both shortcoming can be overcome by suitably extending the language and it semantics among other thing we show that determining the belief of a certain kind of fully introspective knowledge base is decidable and that unknown individual in the knowledge base can be accommodated in a decidable manner a well 
cooperation among automated agent is becoming increasingly important in various artificial intelligence application coalitional i e cooperative game theory supply conceptual and mathematical tool useful in the analysis of such interaction and in particular in the achievement of stable outcome among self interested agent here we study the minimal external subsidy required to stabilize the core of a coalitional game following the cost of stability co model introduced by bachrach et al a we give tight bound on the required subsidy under various restriction on the social structure of the game we then compare the extended core induced by subsidy with the least core of the game proving tight bound on the ratio between the minimal subsidy and the minimal demand relaxation that each lead to stability 
transposition table are a powerful tool in search domain for avoiding duplicate effort and for guiding node expansion traditionally however they have only been applicable when the current state is exactly the same a a previously explored state we consider a generalized transposition table whereby a similarity metric that exploit local structure is used to compare the current state with a neighbourhood of previously seen state we illustrate this concept and forward pruning based on function approximation in the domain of skat and show that we can achieve speedup of over standard method 
different solution approach for combinatorial problem often exhibit incomparable performance that depends on the concrete problem instance to be solved algorithm portfolio aim to combine the strength of multiple algorithmic approach by training a classifier that selects or schedule solver dependent on the given instance we devise a new classifier that selects solver based on a cost sensitive hierarchical clustering model experimental result on sat and maxsat show that the new method outperforms the most effective portfolio builder to date 
plan recognition is the problem of inferring the goal and plan of an agent from partial observation of her behavior recently it ha been shown that the problem can be formulated and solved using planner reducing plan recognition to plan generation in this work we extend this model based approach to plan recognition to the pomdp setting where action are stochastic and state are partially observable the task is to infer a probability distribution over the possible goal of an agent whose behavior result from a pomdp model the pomdp model is shared between agent and observer except for the true goal of the agent that is hidden to the observer the observation are action sequence o that may contain gap a some or even most of the action done by the agent may not be observed we show that the posterior goal distribution p g o can be computed from the value function vg b over belief b generated by the pomdp planner for each possible goal g some extension of the basic framework are discussed and a number of experiment are reported 
in this paper we study a dynamic problem of ridesharing and taxi sharing with time window we consider a scenario where people needing a taxi or interested in getting a ride use a phone app to designate their source and destination point in a city a well others restriction such a maximum allowable time to be at the destination on the other hand we have taxi and people interested in giving a ride with their current position and also some constraint vehicle capacity destination maximum time to destination we want to maximize the number of shared trip in the case of taxi people going to close location can share the cost of the trip and in case of ride the driver and passenger can share cost a well this problem is dynamic since new call for taxi or call for ride arrive on demand this give rise to an optimization problem which we prove to be np hard we then propose heuristic to deal with it we focus on the taxi sharing problem but we show that our model is easily extendable to model the ridesharing situation or even a situation where there are both taxi and car owner in addition we present a framework that consists basically of a client application and a server the last one process all incoming information in order to match vehicle to passenger request the entire system can be used by taxi company and rider in a way to reduce the traffic in the city and to reduce the emission of greenhouse gas 
one drawback of hierarchical task network htn planning is the difficulty of providing complete domain knowledge i e a complete and correct set of htn method for every task to provide a principled way to overcome this difficulty we define a simple formalism that extends classical planning to include problem decomposition using method and a planning algorithm based on this formalism in our formalism the method specify way to achieve goal rather than task a in conventional htn planning and goal may be achieved even when no method are available our planning algorithm godel goal decomposition with landmark is sound and complete irrespective of whether the domain knowledge i e the set of method given to the planner is complete by comparing godel s performance with varying amount of domain knowledge across three benchmark planning domain we show experimentally that godel work correctly with partial planning knowledge godel s performance improves a more planning knowledge is given and when given full domain knowledge godel match the performance of a state of the art hierarchical planner 
by always mapping data from lower dimensional space into higher or even infinite dimensional space kernel k mean is able to organize data into group when data of different cluster are not linearly separable however kernel k mean incurs the large scale computation due to the representation theorem i e keeping an extremely large kernel matrix in memory when using popular gaussian and spatial pyramid matching kernel which largely limit it use for processing large scale data also existing kernel clustering can be overfitted by outlier a well in this paper we introduce an euler clustering which can not only maintain the benefit of nonlinear modeling using kernel function but also significantly solve the large scale computational problem in kernel based clustering this is realized by incorporating euler kernel euler kernel is relying on a nonlinear and robust cosine metric that is le sensitive to outlier more important it intrinsically induces an empirical map which map data onto a complex space of the same dimension euler clustering take these advantage to measure the similarity between data in a robust way without increasing the dimensionality of data and thus solves the large scale problem in kernel k mean we evaluate euler clustering and show it superiority against related method on five publicly available datasets 
the ability to predict or at least recognize the state of the world that an action brings about is a central feature of autonomous agent we propose herein a formal framework within which we investigate whether this ability can be autonomously learned the framework make explicit certain premise that we contend are central in such a learning task i slow sensor may prevent the sensing of an action s direct effect during learning ii prediction need to be made reliably in future and novel situation we initiate in this work a thorough investigation of the condition under which learning is or is not feasible despite the very strong negative learnability result that we obtain we also identify interesting special case where learning is feasible and useful 
dimensionality reduction play a vital role in pattern recognition however for normalized vector data existing method do not utilize the fact that the data is normalized in this paper we propose to employ an angular decomposition of the normalized vector data which corresponds to embedding them on a unit surface on graph data for similarity kernel matrix with constant diagonal element we propose the angular decomposition of the similarity matrix which corresponds to embedding object on a unit sphere in these angular embeddings the euclidean distance is equivalent to the cosine similarity thus data structure best described in the cosine similarity and data structure best captured by the euclidean distance can both be effectively detected in our angular embedding we provide the theoretical analysis derive the computational algorithm and evaluate the angular embedding on several datasets experiment on data clustering demonstrate that our method can provide a more discriminative subspace 
hash function learning ha been recently received more and more attention in fast search for large scale data however existing popular learning based hashing method are batch based learning model and thus incur large scale computational problem for learning an optimal model on a large scale of labelled data and cannot handle data which come sequentially in this paper we address the problem by developing an online hashing learning algorithm to get hashing model accommodate to each new pair of data at the same time the new updated hash model is penalized by the last learned model in order to retain important information learned in previous round we also derive a tight bound for the cumulative loss of our proposed online learning algorithm the experimental result demonstrate superiority of the proposed online hashing model on searching both metric distance neighbor and semantical similar neighbor in the experiment 
hashing based fast nearest neighbor search technique ha attracted great attention in both research and industry area recently many existing hashing approach encode data with projection based hash function and represent each projected dimension by bit however the dimension with high variance hold large energy or information of data but treated equivalently a dimension with low variance which lead to a serious information loss in this paper we introduce a novel hashing algorithm called harmonious hashing which aim at learning hash function with low information loss specifically we learn a set of optimized projection to preserve the maximum cumulative energy and meet the constraint of equivalent variance on each dimension a much a possible in this way we could minimize the information loss after binarization despite the extreme simplicity our method outperforms superiorly to many state of the art hashing method in large scale and high dimensional nearest neighbor search experiment 
the invited talk are how to compare alternative architecture by radia perlman of intel portal enabling application architecture co design for high performance interconnects by ron brightwell of sandia national laboratory and electronic photonic integration within switch and router by mike watt of mit brief author biography are also included 
lasso type variable selection ha increasingly expanded it machine learning application in this paper uncorrelated lasso is proposed for variable selection where variable de correlation is considered simultaneously with variable selection so that selected variable are uncorrelated a much a possible an effective iterative algorithm with the proof of convergence is presented to solve the sparse optimization problem experiment on benchmark data set show that the proposed method ha better classification performance than many state of the art variable selection method copyright association for the advancement of artificial intelligence www aaai org all right reserved 
we propose a novel approach for solving unary sa planning problem this approach extends an sa instance with new state variable representing intention about how each original state variable will be used or changed next and split the original action into several stage of intention followed by eventual execution the result is a new sa instance with the same basic solution a the original while the transformed problem is larger it ha additional structure that can be exploited to reduce the branching factor leading to reachable state space that are many order of magnitude smaller and hence much faster planning in several test domain with acyclic causal graph 
the behavior composition problem involves the automatic synthesis of a controller that is able to realize i e implement a desired target behavior specification by suitably coordinating a set of already available behavior while the problem ha been thoroughly studied one open issue ha resisted a principled solution if the target specification is not fully realizable is there a way to realize it at best in this doctoral work we look at quantitative and qualitative way to address this question 
iterated game are well known in the game theory literature we study iterated boolean game these are game in which player repeatedly choose truth value for boolean variable they have control over our model of iterated boolean game assumes that player have goal given by formula of linear temporal logic ltl a formalism for expressing property of state sequence in order to model the strategy that player use in such game we use a finite state machine model after introducing and formally defining iterated boolean game we investigate the computational complexity of their associated game theoretic decision problem a well a semantic condition characterising class of ltl property that are preserved by pure strategy nash equilibrium whenever they exist 
tree matching problem arise in many computational domain the literature provides several method for creating correspondence between labeled tree however by definition tree matching algorithm rigidly preserve ancestry that is once two node have been placed in correspondence their descendant must be matched a well we introduce flexible tree matching which relaxes this rigid requirement in favor of a tunable formulation in which the role of hierarchy can be controlled we show that flexible tree matching is strongly np complete give a stochastic approximation algorithm for the problem and demonstrate how structured prediction technique can learn the algorithm s parameter from a set of example matchings finally we present result from applying the method to task in web design 
conceptualization seek to map a short text i e a word or a phrase to a set of concept a a mechanism of understanding text most of prior research in conceptualization us human crafted knowledge base that map instance to concept such approach to conceptualization have the limitation that the mapping are not context sensitive to overcome this limitation we propose a framework in which we harness the power of a probabilistic topic model which inherently capture the semantic relation between word by combining latent dirichlet allocation a widely used topic model with probase a large scale probabilistic knowledge base we develop a corpus based framework for context dependent conceptualization through this simple but powerful framework we improve conceptualization and enable a wide range of application that rely on semantic understanding of short text including frame element prediction word similarity in context ad query similarity and query similarity 
we present constituent grammatical evolution cge a new evolutionary automatic programming algorithm that extends the standard grammatical evolution algorithm by incorporating the concept of constituent gene and conditional behaviour switching cge build from elementary and more complex building block a control program which dictate the behaviour of an agent and it is applicable to the class of problem where the subject of search is the behaviour of an agent in a given environment it take advantage of the powerful grammatical evolution feature of using a bnf grammar definition a a plug in component to describe the output language to be produced by the system the main benchmark problem in which cge is evaluated is the santa fe trail problem using a bnf grammar definition which defines a search space semantically equivalent with that of the original definition of the problem by koza furthermore cge is evaluated on two additional problem the loss alto hill and the hampton court maze the experimental result demonstrate that constituent grammatical evolution outperforms the standard grammatical evolution algorithm in these problem in term of both efficiency percent of solution found and effectiveness number of required step of solution found 
in boolean game player exercise control over propositional variable and strive to achieve a goal formula whose realization might require the opponent cooperation recently a theory of incentive engineering for such game ha been devised where an external authority steer the outcome of the game towards certain desirable property consistent with player goal by imposing a taxation mechanism on the player that make the outcome that do not comply with those property le appealing to them the present contribution stem from a complementary perspective and study instead how boolean game can be transformed from inside rather than from outside by endowing player with the possibility of sacrificing a part of their payoff received at a certain outcome in order to convince other player to play a certain strategy what we call here endogenous boolean game ebgs boil down to enriching the framework of boolean game with the machinery of side payment coming from game theory we analyze equilibrium in ebgs showing the precondition needed for desirable outcome to be achieved without external intervention finally making use of taxation mechanism we show how to transform an ebg in such a way that desirable outcome can be realized independently of side payment 
ensemble method which train multiple learner for a task are among the state of the art learning approach the diversity of the component learner ha been recognized a a key to a good ensemble and existing ensemble method try different way to encourage diversity mostly by heuristic in this paper we propose the diversity regularized machine drm in a mathematical programming framework which efficiently generates an ensemble of diverse support vector machine svms theoretical analysis discloses that the diversity constraint used in drm can lead to an effective reduction on it hypothesis space complexity implying that the diversity control in ensemble method indeed play a role of regularization a in popular statistical learning approach experiment show that drm can significantly improve generalization ability and is superior to some state of the art svm ensemble method 
we describe theoretical result and empirical study of context sensitive restart policy for randomized search procedure the method generalize previous result on optimal restart policy by exploiting dynamically updated belief about the probability distribution for run time rather than assuming complete knowledge or zero knowledge about the run time distribution we formulate restart policy that consider real time observation about property of instance and the solver s activity we describe background work on the application of bayesian method to build predictive model for run time introduce an optimal policy for dynamic restarts that considers prediction about run time and perform a comparative study of traditional fixed versus dynamic restart policy 
this paper investigates belief revision where the underlying logic is that governing horn clause it prof to be the case that classical agm belief revision doesn t immediately generalise to the horn case in particular a standard construction based on a total preorder over possible world may violate the accepted agm postulate conversely horn revision function in the obvious extension to the agm approach are not captured by total preorders over possible world we address these difficulty by first restricting the semantic construction to well behaved ordering and second by augmenting the revision postulate by an additional postulate this additional postulate is redundant in the agm approach but not in the horn case in a representation result we show that these two approach coincide arguably this work is interesting for several reason it extends agm revision to inferentially weaker horn theory hence it shed light on the theoretical underpinnings of belief change a well a generalising the agm paradigm thus this work is relevant to revision in area that employ horn clause such a deductive database and logic programming a well a area in which inference is weaker than classical logic such a in description logic 
in this paper we take a step towards using argumentation in social network and introduce social abstract argumentation framework an extension of dung s abstract argumentation framework that incorporates social voting we propose a class of semantics for these new social abstract argumentation framework and prove some important non trivial property which are crucial for their applicability in social network 
nominal schema have recently been introduced a a new approach for the integration of dl safe rule into the description logic framework the efficient processing of knowledge base with nominal schema remains however challenging we address this by extending the well known optimisation of absorption a well a the standard tableau calculus to directly handle the absorbed nominal schema axiom we implement the resulting extension of standard tableau calculus in a novel reasoning system and we integrate further optimisation in our empirical evaluation we show the effect of these optimisation and we find that the proposed approach performs well even when compared to other dl reasoner with dedicated rule support 
k flat is a model based linear manifold clustering algorithm which ha been successfully applied in many real world scenario though some previous work have shown that k flat doesn t always provide good performance little effort ha been devoted to analyze it inherent deficiency in this paper we address this challenge by showing that the deteriorative performance of k flat can be attributed to the usual reconstruction error measure and the infinitely extending representation of linear model then we propose localized k flat algorithm lkf which introduces localized representation of linear model and a new distortion measure to remove confusion among different cluster experiment on both synthetic and real world data set demonstrate the efficiency of the proposed algorithm moreover preliminary experiment show that lkf ha the potential to group manifold with nonlinear structure copyright association for the advancement of artificial intelligence all right reserved 
recent research ha highlighted the benefit of completeness a a retrieval criterion in recommender system in complete retrieval any subset of the constraint in a given query that can be satisfied must be satisfied by at least one of the retrieved product minimal completeness i e always retrieving the smallest set of product needed for completeness is also beginning to attract research interest a a way to minimize cognitive load in the approach other important feature of a retrieval algorithm s behavior include the diversity of the retrieved product and the order in which they are presented to the user in this paper we present a new algorithm for minimally complete retrieval mcr in which the ranking of retrieved product is primarily based on the number of constraint that they satisfy but other measure such a similarity or utility can also be used to inform the retrieval process we also present theoretical and empirical result that demonstrate our algorithm s ability to minimize cognitive load while ensuring the completeness and diversity of the retrieved product 
when doing learning from demonstration it is often the case that the demonstrator provides corrective example to fix errant behavior by the agent or robot we present a set of algorithm which use this corrective data to identify and remove noisy example in datasets which caused errant classification and ultimately errant behavior the objective is to actually modify the source datasets rather than solely rely on the noise insensitivity of the classification algorithm this is particularly useful in the sparse datasets often found in learning from demonstration experiment our approach try to distinguish between noisy misclassification and mere undersampling of the learning space if error are a result of misclassification we potentially remove the responsible point and update the classifier we demonstrate our method on uci machine learning datasets at different level of sparsity and noise using decision tree k nearest neighbor and support vector machine 
many fundamental problem in artificial intelligence knowledge representation and verification involve reasoning about set and relation between set and can be modeled a set constraint satisfaction problem set csps such problem are frequently intractable but there are several important set csps that are known to be polynomial time tractable we introduce a large class of set csps that can be solved in quadratic time our class which we call ei contains all previously known tractable set csps but also some new one that are of crucial importance for example in description logic the class of ei set constraint ha an elegant universal algebraic characterization which we use to show that every set constraint language that properly contains all ei set constraint already ha a finite sublanguage with an np hard constraint satisfaction problem 
we study the problem of designing efficient auction where bidder have interdependent value i e value that depend on the signal of other agent we consider a contingent bid model in which agent can explicitly condition the value of their bid on the bid submitted by others in particular we adopt a linear contingent bidding model for single minded combinatorial auction ca in which submitted bid are linear combination of bid received from others we extend the existing state of the art by identifying constraint on the interesting bundle and contingency weight reported by the agent which allow the efficient second priced fixed point bid auction to be implemented in single minded ca moreover for domain in which the required single crossing condition fails which characterizes when efficient ic auction are possible we design a two stage mechanism in which a subset of agent expert are allocated first using their report to allocate the remaining item to the other agent 
counting the model of a propositional formula is a key issue for a number of ai problem but few propositional language offer the possibility to count model efficiently in order to fill the gap we introduce the language eadt of extended affine decision tree an extended affine decision tree simply is a tree with affine decision node and some specific decomposable conjunction or disjunction node unlike standard decision tree the decision node of an eadt formula are not labeled by variable but by affine clause we study eadt and several subset of it along the line of the knowledge compilation map we also describe a cnf to eadt compiler and present some experimental result those result show that the eadt compilation based approach is competitive with and in some case is able to outperform the model counter cachet and the d dnnf compilation based approach to model counting 
various semi supervised learning method have been proposed recently to solve the long standing shortage problem of manually labeled data in sentiment classification however most existing study assume the balance between negative and positive sample in both the labeled and unlabeled data which may not be true in reality in this paper we investigate a more common case of semi supervised learning for imbalanced sentiment classification in particular various random subspace are dynamically generated to deal with the imbalanced class distribution problem evaluation across four domain show the effectiveness of our approach 
a agent technology becomes increasing more prevalent coordination in mixed agent human environment becomes a key issue agent human coordination is becoming even more important in real life situation where uncertainty and incomplete information exists and communication is costly while abundant research ha focused on aspect of computerized teamwork little attention ha been given to the issue raised in team that consist of both computerized agent and people in this paper we focus on teamwork between an agent and a human counterpart and present a novel agent designed to interact proficiently with people in extensive simulation we matched our agent with people and compared it with another state of the art agent our result demonstrate the significant improvement in coordination when our agent is involved 
computing the marketmaker price of a security in a combinatorial prediction market is p hard we devise a fully polynomial randomized approximation scheme fpras that computes the price of any security in disjunctive normal form dnf within an multiplicative error factor in time polynomial in and the size of the input with high probability and under reasonable assumption our algorithm is a monte carlo technique based on importance sampling the algorithm can also approximately price security represented in conjunctive normal form cnf with additive error bound to illustrate the applicability of our algorithm we show that many security in yahoo s popular combinatorial prediction market game called predictalot can be represented by dnf formula of polynomial size 
a novel proposal for object recognition based on relational grammar and bayesian network is presented based on a symbol relation grammar an object is represented a a hierarchy of feature and spatial relation this representation is transformed to a bayesian network structure which parameter are learned from example thus recognition is based on probabilistic inference in the bayesian network representation preliminary result in modeling natural object are presented 
in this paper we present a graph based approach aimed at learning a lexical taxonomy automatically starting from a domain corpus and the web unlike many taxonomy learning approach in the literature our novel algorithm learns both concept and relation entirely from scratch via the automated extraction of term definition and hypernym this result in a very dense cyclic and possibly disconnected hypernym graph the algorithm then induces a taxonomy from the graph our experiment show that we obtain high quality result both when building brand new taxonomy and when reconstructing wordnet sub hierarchy 
keyword extraction attracts much attention for it significant role in various natural language processing task while some existing method for keyword extraction have considered using single type of semantic relatedness between word or inherent attribute of word almost all of them ignore two important issue how to fuse multiple type of semantic relation between word into a uniform semantic measurement and automatically learn the weight of the edge between the word in the word graph of each document and how to integrate the relation between word and word intrinsic feature into a unified model in this work we tackle the two issue based on the supervised random walk model we propose a supervised ranking based method for keyword extraction which is called seafarer it can not only automatically learn the weight of the edge in the unified graph of each document which includes multiple semantic relation but also combine the merit of semantic relation of edge and intrinsic attribute of node together we conducted extensive experimental study on an established benchmark and the experimental result demonstrate that seafarer outperforms the state of the art supervised and unsupervised method 
mining opinion target from online review is an important and challenging task in opinion mining this paper proposes a novel approach to extract opinion target by using partially supervised word alignment model pswam at first we apply pswam in a monolingual scenario to mine opinion relation in sentence and estimate the association between word then a graph based algorithm is exploited to estimate the confidence of each candidate and the candidate with higher confidence will be extracted a the opinion target compared with existing syntax based method pswam can effectively avoid parsing error when dealing with informal sentence in online review compared with the method using alignment model pswam can capture opinion relation more precisely through partial supervision from partial alignment link moreover when estimating candidate confidence we make penalty on higher degree vertex in our graph based algorithm in order to decrease the probability of the random walk running into the unrelated region in the graph a a result some error can be avoided the experimental result on three data set with different size and language show that our approach outperforms state of the art method 
we propose to measure statistical dependence between two random variable by the mutual information dimension mid and present a scalable parameter free estimation method for this task supported by sound dimension theory our method give an effective solution to the problem of detecting interesting relationship of variable in massive data which is nowadays a heavily studied topic in many scientific discipline different from classical pearson s correlation coefficient mid is zero if and only if two random variable are statistically independent and is translation and scaling invariant we experimentally show superior performance of mid in detecting various type of relationship in the presence of noise data moreover we illustrate that mid can be effectively used for feature selection in regression 
in many multi agent system the emergence of norm is the primary factor that determines over all behavior and utility agent simulation can be used to predict and study the development of these norm however a large number of simulation is usually required to provide an accurate depiction of the agent behavior and some rare contingency may still be overlooked completely the cost and risk involved with agent simulation can be reduced by analyzing a system theoretically and producing model of it behavior we use such a theoretical approach to examine the dynamic of a population of agent playing a coordination game to determine all the norm to which the society can converge and develop a system of linear recurrence relation that predict how frequently each of these norm will be reached a well a the average convergence time this analysis produce certain guarantee about system behavior that canot be provided by a purely empirical approach and can be used to make prediction about the emergence of norm that numerically match those obtained through large scale simulation 
bayesian treatment of matrix factorization ha been successfully applied to the problem of collaborative prediction where unknown rating are determined by the predictive distribution inferring posterior distribution over user and item factor matrix that are used to approximate the user item matrix a their product in practice however bayesian matrix factorization suffers from cold start problem where inference are required for user or item about which a sufficient number of rating are not gathered in this paper we present a method for bayesian matrix factorization with side information to handle cold start problem to this end we place gaussian wishart prior on mean vector and precision matrix of gaussian user and item factor matrix such that mean of each prior distribution is regressed on corresponding side information we develop variational inference algorithm to approximately compute posterior distribution over user and item factor matrix in addition we provide bayesian cram r rao bound for our model showing that the hierarchical bayesian matrix factorization with side information improves the reconstruction over the standard bayesian matrix factorization where the side information is not used experiment on movielens data demonstrate the useful behavior of our model in the case of cold start problem 
bilingual machine readable dictionary are knowledge resource useful in many automatic task however compared to monolingual computational lexicon like wordnet bilingual dictionary typically provide a lower amount of structured information such a lexical and semantic relation and often do not cover the entire range of possible translation for a word of interest in this paper we present cycle and quasi cycle cqc a novel algorithm for the automated disambiguation of ambiguous translation in the lexical entry of a bilingual machine readable dictionary 
a notion of quantified conditional logic is provided that includes quantification over individual and propositional variable the former is supported with respect to constant and variable domain semantics in addition a sound and complete embedding of this framework in classical higher order logic is presented using prominent example from the literature it is demonstrated how this embedding enables effective automation of reasoning within object level and about meta level quantified conditional logic with off the shelf higher order theorem provers and model finder 
the ability to understand the goal and plan of other agent is an important characteristic of intelligent behaviour in many context one of the approach used to endow agent with this capability is the weighted model counting approach given a plan library and a sequence of observation this approach exhaustively enumerates plan execution model that are consistent with the observed behaviour the probability that the agent might be pursuing a particular goal is then computed a a proportion of plan execution model satisfying the goal the approach allows to recognize multiple interleaved plan but suffers from a combinatorial explosion of plan execution model which impedes it application to real world domain this paper present a heuristic weighted model counting algorithm that limit the number of generated plan execution model in order to recognize goal quickly by computing their lower and upper bound likelihood 
consider the following problem in game manipulation a tournament designer who ha full knowledge of the match outcome between any possible pair of player would like to create a bracket for a balanced single elimination tournament so that their favorite player will win although this problem ha been studied in the area of voting and tournament manipulation it is still unknown whether it can be solved in polynomial time we focus on identifying several general case for which the tournament can always be rigged efficiently so that the given player win we give constructive proof that under some natural assumption if a player is ranked among the top k player then one can efficiently rig the tournament for the given player even when k is a large a of the player 
this paper study the recovery guarantee of the model of minimizing f where is a tensor and and f are the trace and frobenius norm of respectively we show that they can efficiently recover low rank tensor in particular they enjoy exact guarantee similar to those known for minimizing under the condition on the sensing operator such a it null space property restricted isometry property or spherical section property to recover a low rank tensor minimizing f return the same solution a minimizing almost whenever max i x i 
different from laplacian or normal matrix the property of the adjacency eigenspace received much le attention recent work showed that node projected into the adjacency eigenspace exhibit an orthogonal line pattern and node from the same community locate along the same line in this paper we conduct theoretical study based on graph perturbation to demonstrate why this line orthogonality property hold in the adjacency eigenspace and why it generally disappears in the laplacian and normal eigenspaces using the orthogonality property in the adjacency eigenspace we present a graph partition algorithm adjcluster which first project node coordinate to the unit sphere and then applies the classic k mean to find cluster empirical evaluation on synthetic data and real world social network validate our theoretical finding and show the effectiveness of our graph partition algorithm 
we study bayesian reinforcement learning rl a a solution of the exploration exploitation dilemma a full bayesian planning is intractable except for special case previous work ha proposed several approximation method however these were often computationally expensive or limited to dirichlet prior in this paper we propose a new algorithm that is fast and of polynomial time for near bayesian optimal policy with any prior distribution that are not greatly misspecified perhaps even more interestingly the proposed algorithm can naturally avoid being misled by incorrect belief while effectively utilizing useful part of prior information it can work well even when an utterly misspecified prior is assigned in that case the algorithm will follow pac mdp behavior instead if an existing pacmdp algorithm doe so the proposed algorithm naturally outperformed other algorithm compared with it on a standard benchmark problem 
argument extraction is a challenging task in event extraction however most of previous study focused on intra sentence information and failed to extract inter sentence argument this paper proposes a discourse level joint model of argument identification and role determination to infer those inter sentence argument in a discourse moreover to better represent the relationship among relevant event mention and the relationship between an event mention and it argument in a discourse this paper introduces various kind of corpus based and discourse based constraint in the joint model either automatically learned or linguistically motivated evaluation on the ace chinese corpus justifies the effectiveness of our joint model over a strong baseline in chinese argument extraction in particular argument identification 
in this paper we propose a locality constrained and sparsity encouraged manifold fitting approach aiming at capturing the locally sparse manifold structure into neighborhood graph construction by exploiting a principled optimization model the proposed model formulates neighborhood graph construction a a sparse coding problem with the locality constraint therefore achieving simultaneous neighbor selection and edge weight optimization the core idea underlying our model is to perform a sparse manifold fitting task for each data point so that close by point lying on the same local manifold are automatically chosen to connect and meanwhile the connection weight are acquired by simple geometric reconstruction we term the novel neighborhood graph generated by our proposed optimization model m fitted graph since such a graph stem from sparse manifold fitting to evaluate the robustness and effectiveness of m fitted graph we leverage graph based semi supervised learning a the testbed extensive experiment carried out on six benchmark datasets validate that the proposed m fitted graph is superior to state of the art neighborhood graph in term of classification accuracy using popular graph based semi supervised learning method 
this paper target at automatically detecting and classifying user s suggestion from tweet the short and informal nature of tweet along with the imbalanced characteristic of suggestion tweet make the task extremely challenging to this end we develop a classification framework on factorization machine which is effective and efficient especially in classification task with feature sparsity setting moreover we tackle the imbalance problem by introducing cost sensitive learning technique in factorization machine extensively experimental study on a manually annotated real life data set show that the proposed approach significantly improves the baseline approach and yield the precision of and recall of we also investigate the reason why factorization machine perform better finally we introduce the first manually annotated dataset for suggestion classification copyright association for the advancement of artificial intelligence www aaai org all right reserved 
to support more precise query translation for english chinese bi directional cross language information retrieval clir we have developed a novel framework by integrating a semantic network to characterize the correlation between multiple inter related text term of interest and learn their inter related statistical query translation model first a semantic network is automatically generated from large scale english chinese bilingual parallel corpus to characterize the correlation between a large number of text term of interest second the semantic network is exploited to learn the statistical query translation model for such text term of interest finally these inter related query translation model are used to translate the query more precisely and achieve more effective clir our experiment on a large number of official public data have obtained very positive result 
there are not very many existing logic of belief which have both a perspicuous semantics and are computationally attractive an exception is the logic sl proposed by liu lakemeyer and levesque which allows for a decidable and often even tractable form of reasoning while the language is first order and hence quite expressive it still ha a number of shortcoming for one belief about belief are not addressed at all for another the name of individual are rigid that is their identity is assumed to be known in this paper we show how both shortcoming can be overcome by suitably extending the language and it semantics among other thing we show that determining the belief of a certain kind of fully introspective knowledge base is decidable and that unknown individual in the knowledge base can be accommodated in a decidable manner a well 
cooperation among automated agent is becoming increasingly important in various artificial intelligence application coalitional i e cooperative game theory supply conceptual and mathematical tool useful in the analysis of such interaction and in particular in the achievement of stable outcome among self interested agent here we study the minimal external subsidy required to stabilize the core of a coalitional game following the cost of stability co model introduced by bachrach et al a we give tight bound on the required subsidy under various restriction on the social structure of the game we then compare the extended core induced by subsidy with the least core of the game proving tight bound on the ratio between the minimal subsidy and the minimal demand relaxation that each lead to stability 
transposition table are a powerful tool in search domain for avoiding duplicate effort and for guiding node expansion traditionally however they have only been applicable when the current state is exactly the same a a previously explored state we consider a generalized transposition table whereby a similarity metric that exploit local structure is used to compare the current state with a neighbourhood of previously seen state we illustrate this concept and forward pruning based on function approximation in the domain of skat and show that we can achieve speedup of over standard method 
different solution approach for combinatorial problem often exhibit incomparable performance that depends on the concrete problem instance to be solved algorithm portfolio aim to combine the strength of multiple algorithmic approach by training a classifier that selects or schedule solver dependent on the given instance we devise a new classifier that selects solver based on a cost sensitive hierarchical clustering model experimental result on sat and maxsat show that the new method outperforms the most effective portfolio builder to date 
plan recognition is the problem of inferring the goal and plan of an agent from partial observation of her behavior recently it ha been shown that the problem can be formulated and solved using planner reducing plan recognition to plan generation in this work we extend this model based approach to plan recognition to the pomdp setting where action are stochastic and state are partially observable the task is to infer a probability distribution over the possible goal of an agent whose behavior result from a pomdp model the pomdp model is shared between agent and observer except for the true goal of the agent that is hidden to the observer the observation are action sequence o that may contain gap a some or even most of the action done by the agent may not be observed we show that the posterior goal distribution p g o can be computed from the value function vg b over belief b generated by the pomdp planner for each possible goal g some extension of the basic framework are discussed and a number of experiment are reported 
in this paper we study a dynamic problem of ridesharing and taxi sharing with time window we consider a scenario where people needing a taxi or interested in getting a ride use a phone app to designate their source and destination point in a city a well others restriction such a maximum allowable time to be at the destination on the other hand we have taxi and people interested in giving a ride with their current position and also some constraint vehicle capacity destination maximum time to destination we want to maximize the number of shared trip in the case of taxi people going to close location can share the cost of the trip and in case of ride the driver and passenger can share cost a well this problem is dynamic since new call for taxi or call for ride arrive on demand this give rise to an optimization problem which we prove to be np hard we then propose heuristic to deal with it we focus on the taxi sharing problem but we show that our model is easily extendable to model the ridesharing situation or even a situation where there are both taxi and car owner in addition we present a framework that consists basically of a client application and a server the last one process all incoming information in order to match vehicle to passenger request the entire system can be used by taxi company and rider in a way to reduce the traffic in the city and to reduce the emission of greenhouse gas 
the increasing demand on drinkable water along with population growth water intensive agriculture and economic development pose critical challenge to water sustainability new technique to long term water conservation that incorporate principle of sustainability are expected recent study have shown that providing customer with usage information of fixture could help them save a considerable amount of water existing disaggregation technique focus on learning consumption pattern for individual device little attention ha been given to the hierarchical decomposition structure of the aggregated consumption in this paper a deep sparse coding based recursive disaggregation model dscrdm is proposed for water conservation we design a recursive decomposition structure to perform the disaggregation task and introduce sequential set to capture it characteristic an efficient and effective algorithm deep sparse coding is developed to automatically learn the disaggregation architecture along with discriminative and reconstruction dictionary for each layer we demonstrated that our proposed approach significantly improved the performance of the benchmark method on a large scale disaggregation task and illustrated how our model could provide practical feedback to customer for water conservation 
although chinese and spanish are two of the most spoken language in the world not much research ha been done in machine translation for this language pair this paper focus on investigating the state of the art of chinese to spanish statistical machine translation smt which nowadays is one of the most popular approach to machine translation we conduct experimental work with the largest of these three corpus to explore alternative smt strategy by mean of using a pivot language three alternative are considered for pivoting cascading pseudo corpus and triangulation a pivot language we use either english arabic or french result show that for a phrase based smt system english is the best pivot language between chinese and spanish we propose a system output combination using the pivot strategy which is capable of outperforming the direct translation strategy the main objective of this work is motivating and involving the research community to work in this important pair of language given their demographic impact 
in binary utility game an agent can have only two possible utility value for final state win and lose an adversarial binary utility game is one where for each final state there must be at least one winning and one losing agent we define an unbiased rational agent a one that seek to maximize it utility value but is equally likely to choose between state with the same utility value this induces a probability distribution over the outcome of the game from which an agent can infer it probability to win a single adversary binary game is one where there are only two possible outcome so that the winning probability remain binary value in this case the rational action for an agent is to play minimax in this work we focus on the more complex multiple adversary environment we propose a new algorithmic framework where agent try to maximize their winning probability we begin by theoretically analyzing why an unbiased rational agent should take our approach in an unbounded environment and not that of the existing paranoid or maxn algorithm we then expand our framework to a resource bounded environment where winning probability are estimated and show empirical result supporting our claim 
in this paper we consider a possibly continuous space of bernoulli experiment we assume that the bernoulli distribution are correlated all evidence data come in the form of successful or failed experiment at different point current state of the art method for expressing a distribution over a continuum of bernoulli distribution use logistic gaussian process or gaussian copula process however both of these require computationally expensive matrix operation cubic in the general case we introduce a more intuitive approach directly correlating beta distribution by sharing evidence between them according to a kernel function an approach which ha linear time complexity the approach can easily be extended to multiple outcome giving a continuous correlated dirichlet process and can be used for both classification and learning the actual probability of the bernoulli distribution we show result for a number of data set a well a a case study where a mixture of continuous beta process is used a part of an automated stroke rehabilitation system 
we propose a family of passive aggressive mahalanobis pam algorithm which are incremental online binary classifier that consider the distribution of data pam is in fact a generalization of the passive aggressive pa algorithm to handle data distribution that can be represented by a covariance matrix the update equation for pam are derived and theoretical error loss bound computed we benchmarked pam against the original pa i pa ii and confidenceweighted cw learning although pam somewhat resembles cwin it update equation pa minimizes difference in the weight while cwminimizes difference in weight distribution result on classification datasets which include a real lifemicro blog sentiment classification task show that pam consistently outperformed it competitor most notably cw this show that a simple approach like pam is more practical in real life classification task compared to more sophisticated approach like cw 
human have developed jurisprudence a a mechanism to solve conflictive situation by using past experience following this principle we propose an approach to enhance a multi agent system by adding an authority which is able to generate new regulation whenever conflict arise regulation are generated by learning from previous similar situation using a machine learning technique based on case based reasoning that solves new problem using previous experience this approach requires to be able to gather and evaluate experience and to be described in such a way that similar social situation require similar regulation a a scenario to evaluate our proposal we use a simplified version of a traffic scenario where agent are traveling car our goal are to avoid collision between car and to avoid heavy traffic these situation when happen lead to the synthesis of new regulation at each simulation step applicable regulation are evaluated in term of their effectiveness and necessity overtime the system generates a set of regulation that if followed improve system performance i e goal achievement 
we present a novel methodology for decision making by computer agent that leverage a computational concept of emotion it is believed that emotion help living organism perform well in complex environment can we use them to improve the decision making performance of computer agent we explore this possibility by formulating emotion a mathematical operator that serve to update the relative priority of the agent s goal the agent us rudimentary domain knowledge to monitor the expectation that it goal are going to be accomplished in the future and reacts to change in this expectation by experiencing emotion the end result is a projection of the agent s long run utility function which might be too complex to optimize or even represent to a time varying valuation function that is being myopically maximized by selecting appropriate action our methodology provides a systematic way to incorporate emotion into a decision theoretic framework and also provides a principled domain independent methodology for generating heuristic in novel situation we test our agent in simulation in two domain restless bandit and a simple foraging environment our result indicate that emotion based agent outperform other reasonable heuristic for such difficult domain and closely approach computationally expensive near optimal solution whenever these are computable yet requiring only a fraction of the cost 
predictive method are becoming increasingly popular for representing world knowledge in autonomous agent a recently introduced predictive method that show particular promise is the general value function gvf which is more flexible than previous predictive method and can more readily capture regularity in the agent s sensorimotor stream the goal of the current paper is to investigate the ability of these gvfs also called forecast to capture such regularity we generate focused set of forecast and measure their capacity for generalization we then compare the result with a closely related predictive method psrs already shown to have good generalization ability our result indicate that forecast provide a substantial improvement in generalization producing feature that lead to better value function approximation when computed with linear function approximators than psrs and better generalization to a yet unseen part of the state space 
the cake cutting problem model the fair division of a heterogeneous good between multiple agent previous work assumes that each agent derives value only from it own piece however agent may also care about the piece assigned to other agent such externality naturally arise in fair division setting we extend the classical model to capture externality and generalize the classical fairness notion of proportionality and envyfreeness our technical result characterize the relationship between these generalized property establish the existence or nonexistence of fair allocation and explore the computational feasibility of fairness in the face of externality 
this paper study a computational logic for dishonest reasoning we introduce logic program with disinformation to represent and reason with dishonesty we then consider two different case of dishonesty deductive dishonesty and abductive dishonesty the former misleads another agent to deduce wrong conclusion while the latter interrupt another agent to abduce correct explanation in deductive or abductive dishonesty an agent can perform different type of dishonest reasoning such a lying bullshitting and withholding information we show that these different type of dishonest reasoning are characterized by extended abduction and address their computational method using abductive logic programming 
my thesis work aim to study change operation for argumentation system especially for abstract argumentation system la dung this paper present a first study of the agm revision adapted to the case of argumentation we also sketch future research work planned to complete the one already achieved 
we investigate new method for creating and applying ensemble for coreference resolution while existing ensemble for coreference resolution are typically created using different learning algorithm clustering algorithm or training set we harness recent advance in coreference modeling and propose to create our ensemble from a variety of supervised coreference model however the presence of pairwise and non pairwise coreference model in our ensemble present a challenge to it application it is not immediately clear how to combine the coreference decision made by these model we investigate different method for applying a model heterogeneous ensemble for coreference resolution empirical result on the ace data set demonstrate the promise of ensemble approach all ensemble based system significantly outperform the best member of the ensemble 
many unsupervised learning method for recognising pattern in data stream are based on fixed length data sequence which make them unsuitable for application where the data sequence are of variable length such a in speech recognition behaviour recognition and text classification in order to use these method on variable length data sequence a pre processing step is required to manually segment the data and select the appropriate feature which is often not practical in real world application in this paper we suggest an unsupervised learning method that handle variable length data sequence by identifying structure in the data stream using text compression and the edit distance between word we demonstrate that using this method we can automatically cluster unlabelled data in a data stream and perform segmentation we evaluate the effectiveness of our proposed method using both fixed length and variable length benchmark datasets comparing it to the self organising map in the first case the result show a promising improvement over baseline recognition system 
we propose a formal model for argumentation based dialogue between agent using assumption based argumentation aba the model is given in term of aba specific utterance tree drawn from dialogue and legal move and outcome function we prove a formal connection between these dialogue and argumentation semantics we illustrate persuasion a an application of the dialogue model 
agent based negotiation team are negotiation party formed by more than a single individual individual unite a a single negotiation party because they share a common goal that is related to a negotiation with one or several opponent my research goal is providing agent based computational model for negotiation team in multi agent system 
we present various new concept and result related to abstract dialectical framework adfs a powerful generalization of dung s argumentation framework afs in particular we show how the existing definition of stable and preferred semantics which are restricted to the subcase of so called bipolar adfs can be improved and generalized to arbitrary framework furthermore we introduce preference handling method for adfs allowing for both reasoning with and about preference finally we present an implementation based on an encoding in answer set programming 
alternating time temporal logic atl is a well known logic for reasoning about strategic ability of agent an important feature that distinguishes variant of atl for imperfect information scenario is that the standard fixed point characterization of temporal modality do not hold anymore in this paper we show that adding explicit fixed point operator to the next time fragment of atl already allows to capture ability that could not be expressed in atl we also illustrate that the new language allows to specify important kind of ability namely one where the agent can always recompute their strategy while executing it thus the agent are not assumed to remember their strategy by definition like in the existing variant of atl last but not least we show that verification of such ability can be cheaper than for all the variant of atl with imperfect information considered so far 
the classic gibbard satterthwaite theorem establishes that only dictatorial voting rule are strategy proof under any other voting rule player have an incentive to lie about their true preference we consider a new approach for circumventing this result we consider randomized voting rule that only approximate a deterministic voting rule and only are approximately strategy proof we show that any deterministic voting rule can be approximated by an approximately strategy proof randomized voting rule and we provide asymptotically tight lower bound on the parameter required by such voting rule 
many application in multilingual and multimodal information access involve searching large database of high dimensional data object with multiple conditionally independent view in this work we consider the problem of learning hash function for similarity search across the view for such application we propose a principled method for learning a hash function for each view given a set of multiview training data object the hash function map similar object to similar code across the view thus enabling cross view similarity search we present result from an extensive empirical study of the proposed approach which demonstrate it effectiveness on japanese language people search and multilingual people search problem 
we propose a method for learning causal relation within high dimensional tensor data a they are typically recorded in non experimental database the method allows the simultaneous inclusion of numerous dimension within the data analysis such a sample time and domain variable construed a tensor in such tensor data we exploit and integrate non gaussian model and tensor analytic algorithm in a novel way we prove that we can determine simple causal relation independently of how complex the dimensionality of the data is we rely on a statistical decomposition that flattens higher dimensional data tensor into matrix this decomposition preserve the causal information and is therefore suitable for structure learning of causal graphical model where a causal relation can be generalised beyond dimension for example over all time point related method either focus on a set of sample for instantaneous effect or look at one sample for effect at certain time point we evaluate the resulting algorithm and discus it performance both with synthetic and real world data 
this paper introduces monte carlo minimax search mcms a monte carlo search algorithm for turned based stochastic two player zero sum game of perfect information the algorithm is designed for the class of densely stochastic game that is game where one would rarely expect to sample the same successor state multiple time at any particular chance node our approach combine sparse sampling technique from mdp planning with classic pruning technique developed for adversarial expectimax planning we compare and contrast our algorithm to the traditional minimax approach a well a mcts enhanced with the double progressive widening on four game pig einstein w rfelt nicht can t stop and ra our result show that mcms can be competitive with enhanced mcts variant in some domain while consistently outperforming the equivalent classic approach given the same amount of thinking time 
qualitative reasoning about commonsense space often involves entity of different dimension we present a weak axiomatization of multidimensional qualitative space based on relative dimension and dimension independent containment which suffice to define basic dimension dependent mereotopological relation we show the relationship to other meoreotopologies and to incidence geometry the extension with betweenness a primitive of relative position result in a first order theory that qualitatively abstract ordered incidence geometry 
multi context system mc are a powerful framework for interlinking heterogeneous knowledge source they model the flow of information among different reasoning component called context in a declarative way using so called bridge rule where context and bridge rule may be nonmonotonic we considerably generalize mc to managed mc mmcs while the original bridge rule can only add information to context our generalization allows arbitrary operation on context knowledge base to be freely defined e g deletion or revision operator the paper motivates and introduces the generalized framework and present several interesting instance furthermore we consider inconsistency management in mmcs and complexity issue 
multi task learning remains a difficult yet important problem in machine learning in gaussian process the main challenge is the definition of valid kernel covariance function able to capture the relationship between different task this paper present a novel methodology to construct valid multi task covariance function mercer kernel for gaussian process allowing for a combination of kernel with different form the method is based on fourier analysis and is general for arbitrary stationary covariance function analytical solution for cross covariance term between popular form are provided including mat rn squared exponential and sparse covariance function experiment are conducted with both artificial and real datasets demonstrating the benefit of the approach 
with information about the world implicitly embedded in complex high dimensional neural population response the brain must perform some sort of statistical inference on a large scale to form hypothesis about the state of the environment this ability is in part acquired after birth and often with very little feedback to guide learning this is a very difficult learning problem considering the little information about the meaning of neural response available at birth in this paper we address the question of how the brain might solve this problem we present an unsupervised artificial neural network algorithm which take from the self organizing map som algorithm the ability to learn a latent variable model from it input we extend the som algorithm so it learns about the distribution of noise in the input and computes probability density function over the latent variable the algorithm represents these probability density function using population code this is done with very few assumption about the distribution of noise our simulation indicate that our algorithm can learn to perform similar to a maximum likelihood estimator with the added benefit of requiring no a priori knowledge about the input and computing not only best hypothesis but also probability for alternative 
projection in the situation calculus refers to answering query about the future evolution of the modeled domain while progression refers to updating the logical representation of the initial state so that it reflects the change due to an executed action in the general case projection is not decidable and progression may require second order logic in this paper we focus on a recent result about the decidability of projection and use it to drive result for the problem of progression in particular we contribute with the following i a major result showing that for a large class of intuitive action theory with bounded unknown a first order progression always exists and can be computed ii a comprehensive classification of the known class that can be progressed in first order iii a novel account of nondeterministic action in the situation calculus 
we focus on recovering the d euclidean structure in one view from the projection of n parallel conic in this paper this work denotes that the conic dual to the absolute point is the general form of the conic dual to the circular point but it doe not encode the euclidean structure therefore we have to recover the circular point envelope to find out some useful information about the euclidean structure which relies on the fact that the line at infinity and the symmetric axis can be recovered we provide a solution to recover the two line and deduce the constraint for recovering the conic dual to the circular point then apply them on the camera calibration our work relaxes the problem condition and give a more general framework than the past experiment with simulated and real data are carried out to show the validity of the proposed algorithm especially our method is applied in the endoscope operation to calibrate the camera for tracking the surgical tool that is the main interest point we pay attention to 
kalman filtering is a computational tool with widespread application in robotics financial and weather forecasting environmental engineering and defense given observation and state transition model the kalman filter kf recursively estimate the state variable of a dynamic system however the kf requires a cubic time matrix inversion operation at every timestep which prevents it application in domain with large number of state variable we propose relational gaussian model to represent and model dynamic system with large number of variable efficiently furthermore we devise an exact lifted kalman filtering algorithm which take only linear time in the number of random variable at every timestep we prove that our algorithm take linear time in the number of state variable even when individual observation apply to each variable to our knowledge this is the first lifted linear time algorithm for filtering with continuous dynamic relational model 
we propose an online classification approach for co occurrence data which is based on a simple information theoretic principle we further show how to properly estimate the uncertainty associated with each prediction of our scheme and demonstrate how to exploit these uncertainty estimate first in order to abstain highly uncertain prediction and second within an active learning framework in order to preserve classification accuracy while substantially reducing training set size our method is highly efficient in term of run time and memory footprint requirement experimental result in the domain of text classification demonstrate that the classification accuracy of our method is superior or comparable to other state of the art online classification algorithm 
matrix factorization based technique such a non negative matrix factorization nmf and concept factorization cf have attracted great attention in dimension reduction and data clustering both of them are linear learning problem and lead to a sparse representation of the data however the sparsity obtained by these method doe not always satisfy locality condition thus the obtained data representation is not the best this paper introduces a locality constrained concept factorization method which imposes a locality constraint onto the traditional concept factorization by requiring the concept basis vector to be a close to the original data point a possible each data can be represented by a linear combination of only a few basis concept thus our method is able to achieve sparsity and locality at the same time we demonstrate the effectiveness of this novel algorithm through a set of evaluation on real world application 
this paper proposes a simple linear bayesian approach to reinforcement learning we show that with an appropriate basis a bayesian linear gaussian model is sufficient for accurately estimating the system dynamic and in particular when we allow for correlated noise policy are estimated by first sampling a transition model from the current posterior and then performing approximate dynamic programming on the sampled model this form of approximate thompson sampling result in good exploration in unknown environment the approach can also be seen a a bayesian generalisation of least square policy iteration where the empirical transition matrix is replaced with a sample from the posterior 
machine learning is traditionally formalized and researched a the study of learning concept and decision function from labeled example requiring a representation that encodes information about the domain of the decision function to be learned we are interested in providing a way for a human teacher to interact with an automated learner using natural instruction thus allowing the teacher to communicate the relevant domain expertise to the learner without necessarily knowing anything about the internal representation used in the learning process in this paper we suggest to view the process of learning a decision function a a natural language lesson interpretation problem instead of learning from labeled example this interpretation of machine learning is motivated by human learning process in which the learner is given a lesson describing the target concept directly and a few instance exemplifying it we introduce a learning algorithm for the lesson interpretation problem that get feedback from it performance on the final task while learning jointly how to interpret the lesson and how to use this interpretation to do well on the final task this approach alleviates the supervision burden of traditional machine learning by focusing on supplying the learner with only human level task expertise for learning we evaluate our approach by applying it to the rule of the freecell solitaire card game we show that our learning approach can eventually use natural language instruction to learn the target concept and play the game legally furthermore we show that the learned semantic interpreter also generalizes to previously unseen instruction 
parameterized linear system allow for modelling and reasoning over class of polyhedron collection of square rectangle polytopes and so on can readily be defined by mean of linear system with parameter in this paper we investigate the problem of learning a parameterized linear system whose class of polyhedron includes a given set of example polyhedral set and it is minimal 
log linear description logic are a family of probabilistic logic integrating various concept and method from the area of knowledge representation and reasoning and statistical relational ai we define the syntax and semantics of log linear description logic describe a convenient representation a set of first order formula and discus computational and algorithmic aspect of probabilistic query in the language the paper concludes with an experimental evaluation of an implementation of a log linear dl reasoner 
the slow feature analysis sfa unsupervised learning framework extract feature representing the underlying cause of the change within a temporally coherent high dimensional raw sensory input signal we develop the first online version of sfa via a combination of incremental principal component analysis and minor component analysis unlike standard batch based sfa online sfa adapts along with non stationary environment which make it a generally useful unsupervised preprocessor for autonomous learning agent we compare online sfa to batch sfa in several experiment and show that it indeed learns without a teacher to encode the input stream by informative slow feature representing meaningful abstract environmental property we extend online sfa to deep network in hierarchical fashion and use them to successfully extract abstract object position information from high dimensional video 
possibilistic logic is a well known framework for dealing with uncertainty and reasoning under inconsistent knowledge base standard possibilistic logic expression are propositional logic formula associated with positive real degree belonging to however in practice it may be difficult for an expert to provide exact degree associated with formula of a knowledge base this paper proposes a flexible representation of uncertain information where the weight associated with formula are in the form of interval we first study a framework for reasoning with interval based possibilistic knowledge base by extending main concept of possibilistic logic such a the one of necessity and possibility measure we then provide a characterization of an interval based possibilistic logic base by mean of a concept of compatible standard possibilistic logic base we show that interval based possibilistic logic extends possibilistic logic in the case where all interval are singleton lastly we provide computational complexity result of deriving plausible conclusion from interval based possibilistic base and we show that the flexibility in representing uncertain information is handled without extra computational cost 
broadly speaking my research concern combining logic of action and pomdp theory in a coherent theoretically sound language for agent programming we have already developed a logic for specifying partially observable stochastic domain a logic for reasoning with the model specified must still be developed an agent programming language will then be developed and used to design controller for robot 
our work address the problem of generalizing a plan and representing it for efficient execution a key area of automated planning is the study of how to generate a plan for an agent to execute the plan itself may take on many form a sequence of action a partial ordering over a set of action or a procedure like description of what the agent should do once a plan is found the question remains a to how the agent should execute the plan for simple form of representation e g a sequence of action the answer to this question is straightforward however when the plan representation is more expressive e g a golog program or the agent is acting in an uncertain world execution can be considerably more challenging we focus on the problem of how to generalize various plan representation into a form that an agent can use for efficient and robust online execution srivistava et al propose a definition of a generalized plan a an algorithm that map problem instance to a sequence of action that solves the instance our work fit nicely into this formalism and in section we describe how a problem i e a state of the world and goal is mapped to a sequence of action i e what the agent should do 
in some application such a bioinformatics social network analysis and computational criminology it is desirable to find compact cluster formed by a very small portion of object in a large data set since such cluster are comprised of a small number of object they are extraordinary and anomalous with respect to the entire data set this specific type of clustering task cannot be solved well by the conventional clustering method since generally those method try to assign most of the data object into cluster in this paper we model this novel and application inspired task a the problem of mining cohesive anomaly we propose a general framework and a principled approach to tackle the problem the experimental result on both synthetic and real data set verify the effectiveness and efficiency of our approach copyright association for the advancement of artificial intelligence www aaai org all right reserved 
directing robot attention to recognise activity and to anticipate event like goal directed action is a crucial skill for human robot interaction unfortunately issue like intrinsic time constraint the spatially distributed nature of the entailed information source and the existence of a multitude of unobservable state affecting the system like latent intention have long rendered achievement of such skill a rather elusive goal the problem test the limit of current attention control system it requires an integrated solution for tracking exploration and recognition which traditionally have been seen a separate problem in active vision we propose a probabilistic generative framework based on information gain maximisation and a mixture of kalman filter that us prediction in both recognition and attention control this framework can efficiently use the observation of one element in a dynamic environment to provide information on other element and consequently enables guided exploration interestingly the sensor control policy directly derived from first principle represents the intuitive trade off between finding the most discriminative clue and maintaining overall awareness experiment on a simulated humanoid robot observing a human executing goal oriented action demonstrated improvement on recognition time and precision over baseline system 
we present a bimodal method for online planning in partially observable multiagent setting a formalized by a finitely nested interactive partially observable markov decision process i pomdp an agent planning in an environment shared with another update belief both over the physical state and the other agent model in problem where we do not observe other s action explicitly but must infer it from sensing it effect on the state observation are more informative about the other when the belief over the state space ha reduced uncertainty for typical uncertain initial belief we model the agent a if it were acting alone and utilize fast online planning for pomdps subsequently the agent switch to online planning in multiagent setting we maintain tight lower and upper bound at each step and switch over when the difference between them reduces to le than 
nonnegative matrix tri factorization nmtf and it graph regularized extension have been widely used for co clustering task to group data point and feature simultaneously however existing method are sensitive to noise and outlier which is because of the squared loss function is used to measure the quality of data reconstruction and graph regularization in this paper we extend gnmtf by introducing a sparse outlier matrix into the data reconstruction function and applying the l norm to measure graph dual regularization error which lead to a novel robust co clustering rcc method accordingly rcc is expected to obtain a more faithful approximation to the data recovered from sparse outlier and achieve robust regularization by reducing the regularization error of unreliable graph via l norm to solve the optimization problem of rcc an alternating iterative algorithm is provided and it convergence is also proved we also show the connection between the sparse outlier matrix in data reconstruction function and the robust huber m estimator experimental result on real world data set show that our rcc consistently outperforms the other algorithm in term of clustering performance which validates the effectiveness and robustness of the proposed approach 
this paper address the target value search tv problem which is the problem of finding a path between two node in a graph whose cost is a close a possible to a given target value t this problem ha been previously addressed only for directed acyclic graph in this work we develop the theory required to solve this problem optimally for any type of graph we modify traditional heuristic search algorithm for this setting and propose a novel bidirectional search algorithm that is specifically suited for tv the benefit of this bidirectional search algorithm are discussed both theoretically and experimentally on several domain 
fraud detection is a key activity with serious socio economical impact inspection activity associated with this task are usually constrained by limited available resource data analysis method can provide help in the task of deciding where to allocate these limited resource in order to optimise the outcome of the inspection activity this paper present a multi strategy learning method to address the question of which case to inspect first the proposed methodology is based on the utility theory and provides a ranking ordered by decreasing expected outcome of inspecting the candidate case this outcome is a function not only of the probability of the case being fraudulent but also of the inspection cost and expected payoff if the case is confirmed a a fraud the proposed methodology is general and can be useful on fraud detection activity with limited inspection resource we experimentally evaluate our proposal on both an artificial domain and on a real world task 
dynamic epistemic logic del provides a very expressive framework for multi agent planning that can deal with nondeterminism partial observability sensing action and arbitrary nesting of belief about other agent belief however a we show in this paper this expressiveness come at a price the planning framework is undecidable even if we allow only purely epistemic action action that change only belief not ontic fact undecidability hold already in the s setting with at least agent and even with agent in s it show that multi agent planning is robustly undecidable if we assume that agent can reason with an arbitrary nesting of belief about belief we also prove a corollary showing undecidability of the del model checking problem with the star operator on action iteration 
we consider the problem of updating a multi agent system with a set of conditional norm a norm come into effect when it condition becomes true and imposes either an obligation or a prohibition on an agent which remains in force until a state satisfying a deadline condition is reached if the norm is violated a sanction is imposed on the agent we define a notion of a normative update of a multi agent system by a set of conditional norm and study the problem of checking whether the agent s can bring about a state satisfying a property without incurring a specified number of sanction 
update semantics for answer set programming assign model to sequence of answer set program which result from the iterative process of updating program by program each program in the sequence represents an update of the preceding one one of the enduring problem in this context is state condensing or the problem of determining a single logic program that faithfully represents the sequence of program such logic program should be written in the same alphabet have the same stable model and be equivalent to the sequence of program when subject to further update it ha been known for more than a decade that update semantics easily lead to non minimal stable model so an update sequence cannot be represented by a single non disjunctive program on the other hand more expressive class of program were never considered mainly because it wa not clear how they could be updated further in this paper we solve the state condensing problem for two foundational rule update semantics using nested logic program furthermore we also show that disjunctive program with default negation in the head can be used for the same purpose 
many web site collect review of product and service and use them provide ranking of their quality however such ranking are not personalized we investigate how the information in the review written by a particular user can be used to personalize the ranking she is shown we propose a new technique topic profile collaborative filtering where we build user profile from user review text and use these profile to filter other review text with the eye of this user we verify on data from an actual review site that review text and topic profile indeed correlate with rating and show that topic profile collaborative filtering provides both a better mean average error when predicting rating and a better approximation of user preference order 
a new unsupervised feature selection method i e robust unsupervised feature selection ruf is proposed unlike traditional unsupervised feature selection method pseudo cluster label are learned via local learning regularized robust nonnegative matrix factorization during the label learning process feature selection is performed simultaneously by robust joint l norm minimization since ruf utilizes l norm minimization on process of both label learning and feature learning outlier and noise could be effectively handled and redundant or noisy feature could be effectively reduced our method adopts the advantage of robust non negative matrix factorization local learning and robust feature learning in order to make ruf be scalable we design a projected limited memory bfgs based iterative algorithm to efficiently solve the optimization problem of ruf in term of both memory consumption and computation complexity experimental result on different benchmark real world datasets show the promising performance of ruf over the state of the art 
the aspic framework is intermediate in abstraction between dung s argumentation framework and concrete instantiating logic this paper generalises aspic to accommodate classical logic instantiation and adopts a new proposal for evaluating extension attack are used to define the notion of conflict free set while the defeat obtained by applying preference to attack are exclusively used to determine the acceptability of argument key property and rationality postulate are then shown to hold for the new framework 
heuristic search with reachability based heuristic is arguably the most successful paradigm in automated planning to date in it earlier stage of development heuristic search wa proposed a both forward and backward search due to the disadvantage of backward search in the last decade researcher focused mainly on forward search and backward search wa abandoned for the most part a a valid alternative in the last year important advancement regarding both the theoretical understanding and the performance of heuristic search have been achieved applied mainly to forward search planner in this work we revisit regression in planning with reachability based heuristic trying to extrapolate to backward search current line of research that were not a well understood a they are now 
proving that one language is more succinct than another becomes harder when the underlying semantics is stronger we propose to use formula size game a put forward by adler and immerman game that are played on two set of model and that directly link the length of play with the size of the formula using fsgs we prove three succinctness result for m dimensional modal logic in system km a notion of everybody know make the resulting language exponentially more succinct for m in s m the same language becomes more succinct for m and public announcement logic is exponentially more succinct than s m if m the latter settle an open problem raised by lutz 
bracketing induction is the unsupervised learning of hierarchical constituent without labeling their syntactic category such a verb phrase vp from natural raw sentence constituent context model ccm is an effective generative model for the bracketing induction but the ccm computes probability of a constituent in a very straightforward way no matter how long this constituent is such method cause severe data sparse problem because long constituent are more unlikely to appear in test set to overcome the data sparse problem this paper proposes to define a non parametric bayesian prior distribution namely the pitman yor process pyp prior over constituent for constituent smoothing the pyp prior function a a back off smoothing method through using a hierarchical smoothing scheme hs various kind of hs are proposed in this paper we find that two kind of hs are effective attaining or significantly improving the state of the art performance of the bracketing induction evaluated on standard treebanks of various language while another kind of hs which is commonly used for smoothing sequence by n gram markovization is not effective for improving the performance of the ccm 
online feature selection with dynamic feature ha become an active research area in recent year however in some real world application such a image analysis and email spam filtering feature may arrive by group existing online feature selection method evaluate feature individually while existing group feature selection method cannot handle online processing motivated by this we formulate the online group feature selection problem and propose a novel selection approach for this problem our proposed approach consists of two stage online intra group selection and online inter group selection in the intra group selection we use spectral analysis to select discriminative feature in each group when it arrives in the inter group selection we use lasso to select a globally optimal subset of feature this stage procedure continues until there are no more feature to come or some predefined stopping condition are met extensive experiment conducted on benchmark and real world data set demonstrate that our proposed approach outperforms other state of the art online feature selection method 
we present a novel approach for multilabel classification based on an ensemble of bayesian network the class variable are connected by a tree each model of the ensemble us a different class a root of the tree we assume the feature to be conditionally independent given the class thus generalizing the naive bayes assumption to the multi class case this assumption allows u to optimally identify the correlation between class and feature such correlation are moreover shared across all model of the ensemble inference are drawn from the ensemble via logarithmic opinion pooling to minimize hamming loss we compute the marginal probability of the class by running standard inference on each bayesian network in the ensemble and then pooling the inference to instead minimize the subset loss we pool the joint distribution of each model and cast the problem a a map inference in the corresponding graphical model experiment show that the approach is competitive with state of the art method for multilabel classification 
we present a study regarding basic level of concept in conceptual categorization the basic level of concept is an important phenomenon studied in the psychology of concept we propose to utilize this phenomenon in formal concept analysis to select important formal concept such selection is critical because a is well known the number of all concept extracted from data is usually large we review and formalize the main existing psychological approach to basic level which are presented only informally and are not related to any particular formal model of concept in the psychological literature we argue and demonstrate by example that basic level concept may be regarded a interesting informative formal concept from a user viewpoint interestingly our formalization and experiment reveal previously unknown relationship between the existing approach to basic level thus we argue that a formalization of basic level in the framework of formal concept analysis is beneficial for the psychological investigation themselves because it help put them on a solid formal ground 
since wind ha an intrinsically complex and stochastic nature accurate wind power forecast are necessary for the safety and economics of wind energy utilization in this paper we investigate a combination of numeric and probabilistic model one day ahead wind power forecast were made with gaussian process gps applied to the output of a numerical weather prediction nwp model firstly the wind speed data from nwp wa corrected by a gp then a there is always a defined limit on power generated in a wind turbine due the turbine controlling strategy a censored gp wa used to model the relationship between the corrected wind speed and power output to validate the proposed approach two real world datasets were used for model construction and testing the simulation result were compared with the persistence method and artificial neural network anns the proposed model achieves about improvement in forecasting accuracy mean absolute error compared to the ann model on one dataset and nearly improvement on another 
we introduce a new nearest neighbor search algorithm the algorithm build a nearest neighbor graph in an offline phase and when queried with a new point performs hill climbing starting from a randomly sampled node of the graph we provide theoretical guarantee for the accuracy and the computational complexity and empirically show the effectiveness of this algorithm 
the emergence and ubiquity of online social network have enriched web data with evolving interaction and community both at mega scale and in real time this data offer an unprecedented opportunity for studying the interaction between society and disease outbreak the challenge we describe in this data paper is how to extract and leverage epidemic outbreak insight from massive amount of social medium data and how this exercise can benefit medical professional patient and policymakers alike we attempt to prepare the research community for this challenge with four datasets publishing the four datasets will commoditize the data infrastructure to allow a higher and more efficient focal point for the research community 
this paper examines an extended double auction model where market clearing is restricted by temporal constraint it is found that the allocation problem in this model can be effectively transformed into a weighted bipartite matching in graph theory by using the augmentation technique we propose a vickrey clarke grove vcg mechanism in this model and demonstrate the advantage of the payment compared with the classical vcg payment the clarke pivot payment we also show that the algorithm for both allocation and payment calculation run in polynomial time it is expected that the method and result provided in this paper can be applied to the design and analysis of dynamic double auction and future market 
in this paper we present an adaptive graph based personalized recommendation method based on co ranking and query based collaborative diffusion by utilizing the unique network structure of n partite heterogeneous graph we attempt to address the problem of personalized recommendation in a two layer ranking process with the help of reasonable measure of high and low order relationship and analyzing the representation of user s preference in the graph the experiment show that this algorithm can outperform the traditional cf method and achieve competitive performance compared with many model based and graph based recommendation method and have better scalability and flexibility copyright association for the advancement of artificial intelligence www aaai org all right reserved 
designing dialog policy for voice enabled interface is a tailoring job that is most often left to natural language processing expert this job is generally redone for every new dialog task because cross domain transfer is not possible for this reason machine learning method for dialog policy optimization have been investigated during the last year especially reinforcement learning rl is now part of the state of the art in this domain standard rl method require to test more or le random change in the policy on user to ass them a improvement or degradation this is called on policy learning nevertheless it can result in system behavior that are not acceptable by user learning algorithm should ideally infer an optimal strategy by observing interaction generated by a non optimal but acceptable strategy that is learning off policy in this contribution a sample efficient online and off policy reinforcement learning algorithm is proposed to learn an optimal policy from few hundred of dialogue generated with a very simple handcrafted policy 
this paper state the challenge of mechanism design for dynamic environment especially dynamic double auction after a brief review of related work we specify the problem we are tackling and then briefly outline our research plan the result we have achieved to date and the ongoing direction 
in classification problem isotonic regression ha been commonly used to map the prediction score to posterior class probability however isotonic regression may suffer from overfitting and the learned mapping is often discontinuous besides current effort mainly focus on the calibration of a single classifier a different classifier have different strength a combination of them can lead to better performance in this paper we propose a novel probability calibration approach for such an ensemble of classifier we first construct isotonic constraint on the desired probability based on soft voting of the classifier manifold information is also incorporated to combat overfitting and ensure function smoothness computationally the extended isotonic regression model can be learned efficiently by a novel optimization algorithm based on the alternating direction method of multiplier admm experiment on a number of real world data set demonstrate that the proposed approach consistently outperforms independent classifier and other combination of the classifier probability in term of the brier score and auc 
in many real world application of the time series classification problem not only could the negative training instance be missing the number of positive instance available for learning may also be rather limited this ha motivated the development of new classification algorithm that can learn from a small set p of labeled seed positive instance augmented with a set u of unlabeled instance i e pu learning algorithm however existing pu learning algorithm for time series classification have le than satisfactory performance a they are unable to identify the class boundary between positive and negative instance accurately in this paper we propose a novel pu learning algorithm lclc learning from common local cluster for time series classification lclc is designed to effectively identify the ground truth positive and negative boundary resulting in more accurate classifier than those constructed using existing method we have applied lclc to classify time series data from different application domain the experimental result demonstrate that lclc out performs existing method significantly 
this paper introduces the seq bin meta constraint with a polytime algorithm achieving generalized arc consistency seq bin can be used for encoding counting constraint such a change smooth or increasing nvalue for all of them the time and space complexity is linear in the sum of domain size which improves or equal the best known result of the literature 
the road network design problem is to optimize the road network by selecting path to improve or adding path in the existing road network under certain constraint e g the weighted sum of modifying cost since it multi objective nature the road network design problem is often challenging for designer empirically the smaller diameter a road network ha the more connected and efficient the road network is based on this observation we propose a set of constrained convex model for designing road network with small diameter to be specific we theoretically prove that the diameter of the road network which is evaluated w r t the travel time in the network can be bounded by the algebraic connectivity in spectral graph theory since that the upper and lower bound of diameter are inversely proportional to algebraic connectivity then we can focus on increasing the algebraic connectivity instead of reducing the network diameter under the budget constraint the above formulation lead to a semi definite program in which we can get it global solution easily then we present some simulation experiment to show the correctness of our method at last we compare our method with an existing method based on the genetic algorithm 
we investigate the emergence and stability of social convention for efficiently resolving conflict through reinforcement learning facilitation of coordination and conflict resolution is an important issue in multi agent system however exhibiting coordinated and negotiation activity is computationally expensive in this paper we first describe a conflict situation using a markov game which is iterated if the agent fail to resolve their conflict where the repeated failure result in an inefficient society using this game we show that social convention for resolving conflict emerge but their stability and social efficiency depend on the payoff matrix that characterize the agent we also examine how unbalanced population and small heterogeneous agent affect efficiency and stability of the resulting convention our result show that a a type of indecisive agent that is generous for adverse result lead to unstable society and b selfish agent that have an explicit order of benefit make society stable and efficient 
faceted navigation can effectively reduce user effort of reaching targeted resource in database by suggesting dynamic facet value for iterative query refinement a key issue is minimizing the navigation cost in a user query session conventional navigation scheme assumes that at each step user select only one suggested value to figure out resource containing it to make faceted navigation more flexible and effective this paper introduces a multi select scheme where multiple suggested value can be selected at one step and a selected value can be used to either retain or exclude the resource containing it previous algorithm for cost driven value suggestion can hardly work well under our navigation scheme therefore we propose to optimize the navigation cost using the minimum description length principle which can well balance the number of navigation step and the number of suggested value per step under our new scheme an emperical study demonstrates that our approach is more cost saving and efficient than state of the art approach 
it is indispensable for user to evaluate the trustworthiness of other user referred to a advisor to cope with possible misleading opinion provided by them advisor misleading opinion may be induced by their dishonesty subjectivity difference with user or both existing approach do not well distinguish the two different cause in this paper we propose a novel probabilistic graphical trust model to separately consider these two factor involving three type of latent variable benevolence integrity and competence of advisor trust propensity of user and subjectivity difference between user and advisor experimental result on real datasets demonstrate that our method advance state of the art approach to a large extent 
learning concept via instruction and expository text is an important problem for modeling human learning and for making autonomous ai system this paper describes a computational model of the self explanation effect whereby conceptual knowledge is repaired by integrating and explaining new material our model represents conceptual knowledge with compositional model fragment which are used to explain new material via model formulation preference are computed over explanation and conceptual knowledge along several dimension these preference guide knowledge integration and question answering our simulation learns about the human circulatory system using fact from a circulatory system passage used in a previous cognitive psychology experiment we analyze the simulation s performance showing that individual difference in sequence of model learned by student can be explained by different parameter setting in our model 
we propose method for computing semantic relatedness between word or text by using knowledge from hypertext encyclopedia such a wikipedia a network of concept is built by filtering the encyclopedia s article each concept corresponding to an article a random walk model based on the notion of visiting probability vp is employed to compute the distance between node and then between set of node to transfer learning from the network of concept to text analysis task we develop two common representation approach in the first approach the shared representation space is the set of concept in the network and every text is represented in this space in the second approach a latent space is used a the shared representation and a transformation from word to the latent space is trained over vp score we applied our method to four important task in natural language processing word similarity document similarity document clustering and classification and ranking in information retrieval the performance is state of the art or close to it for each task thus demonstrating the generality of the proposed knowledge resource and the associated method 
field classification is an extension of the traditional classification framework by breaking the i i d assumption in field classification pattern occur a group field of homogeneous style by utilizing style consistency classifying group of pattern is often more accurate than classifying single pattern in this paper we extend the bayes decision theory and develop the field bayesian model fbm to deal with field classification specifically we propose to learn a style normalized transformation snt for each field via the snts the data of different field are transformed to a uniform style space i i d space the proposed model is a general and systematic framework under which many probabilistic model can be easily extended for field classification to transfer the model to unseen style we propose a transductive model called transfer bayesian rule tbr based on self training we conducted extensive experiment on face speech and a large scale handwriting dataset and got significant error rate reduction compared to the state of the art method 
in agent oriented programming and planning agent action are typically specified in term of postconditions and the model of execution assumes that the environment carry the action out exactly a specified that is it is assumed that the state of the environment after an action ha been executed will satisfy it postcondition in reality however such environment are rare the actual execution of an action may fail and the envisaged outcome is not met we provide a conceptual framework for reasoning about success and failure of agent behaviour in particular we propose a measure that reflects how good an environment is with respect to agent s capability and a given goal it might pursue we also discus which type of goal are worth pursuing depending on the type of environment the agent is acting in 
in this paper we describe a software assistant agent that can proactively assist human user situated in a time constrained environment to perform normative reasoning reasoning about prohibition and obligation so that the user can focus on her planning objective in order to provide proactive assistance the agent must be able to recognize the user s planned activity reason about potential need of assistance associated with those predicted activity and plan to provide appropriate assistance suitable for newly identified user need to address these specific requirement we develop an agent architecture that integrates user intention recognition normative reasoning over a user s intention and planning execution and replanning for assistive action this paper present the agent architecture and discus practical application of this approach 
in current search engine ranking function are learned from a large number of labeled query url pair in which the label are assigned by human judge describing how well the url match the different query however in commercial search engine collecting high quality label is time consuming and labor intensive to tackle this issue this paper study how to produce the true relevance label for query url pair using clickthrough data by analyzing the correlation between query frequency true relevance label and user behavior we demonstrate that the user who search the query with similar frequency have similar search intent and behavioral characteristic based on such property we propose an efficient discriminative parameter estimation in a multiple instance learning algorithm mil to automatically produce true relevance label for query url pair furthermore we test our approach using a set of real world data extracted from a chinese commercial search engine experimental result not only validate the effectiveness of the proposed approach but also indicate that our approach is more likely to agree with the aggregation of the multiple judgment when strong disagreement exist in the panel of judge in the event that the panel of judge is consensus our approach provides more accurate automatic label result in contrast with other model our approach effectively improves the correlation between automatic label and manual label copyright association for the advancement of artificial intelligence all right reserved 
we extend hybrid possibilistic conditioning to deal with input consisting of a set of triplet composed of propositional formula the level at which the formula should be accepted and the way in which their model should be revised we characterize such conditioning using elementary operation on possibility distribution we then solve a difficult issue that concern the syntactic computation of the revision of possibilistic knowledge base made of weighted formula using hybrid conditioning an important result is that there is no extra computational cost in using hybrid possibilistic conditioning and in particular the size of the revised possibilistic base is polynomial with respect to the size of the initial base and the input 
chinese pinyin input method are very important for chinese language processing in many case user may make typing error for example a user want to type in shenme meaning what in english but may type in shenem instead existing pinyin input method fail in converting such a pinyin sequence with error to the right chinese word to solve this problem we developed an efficient error tolerant pinyin input method called chime that can handle typing error by incorporating state of the art technique and language specific feature the method achieves a better performance than state of the art input method it can efficiently find relevant word in millisecond for an input pinyin sequence 
propagating trust distrust from a set of seed good bad page to the entire web ha been widely used to combat web spam it ha been mentioned that a combined use of good and bad seed can lead to better result however little work ha been known to realize this insight successfully a serious issue of existing algorithm is that trust distrust is propagated in non differential way however it seems to be impossible to implement differential propagation if only trust or distrust is propagated in this paper we view that each web page ha both a trustworthy side and an untrustworthy side and assign two score to each web page t rank scoring the trustworthiness and d rank scoring the untrustworthiness we then propose an integrated framework which propagates both trust and distrust in the framework the propagation of t rank d rank is penalized by the target s current d rank t rank in this way propagating both trust and distrust with target differentiation is implemented the proposed trust distrust rank tdr algorithm not only make full use of both good seed and bad seed but also overcomes the disadvantage of both existing trust propagation and distrust propagation algorithm experimental result show that tdr outperforms other typical anti spam algorithm under various criterion copyright association for the advancement of artificial intelligence all right reserved 
graph clustering ha received growing attention in recent year a an important analytical technique both due to the prevalence of graph data and the usefulness of graph structure for exploiting intrinsic data characteristic however a graph data grows in scale it becomes increasingly more challenging to identify cluster in this paper we propose an efficient clustering algorithm for large scale graph data using spectral method the key idea is to repeatedly generate a small number of supernodes connected to the regular node in order to compress the original graph into a sparse bipartite graph by clustering the bipartite graph using spectral method we are able to greatly improve efficiency without losing considerable clustering power extensive experiment show the effectiveness and efficiency of our approach 
we give a formal definition of generalized planning that is independent of any representation formalism we assume that our generalized plan must work on a set of deterministic environment which are essentially unrelated to each other we prove that generalized planning for a finite set of environment is always decidable and expspace complete our proof is constructive and give u a sound complete and complexity wise optimal technique we also consider infinite set of environment and show that generalized planning for the infinite one dimensional problem known in the literature to be recursively enumerable when restricted to finite state plan is expspace decidable without sequence function and solvable by generalized planning for finite set 
symbolic pdbs and merge and shrink m s are two approach to derive admissible heuristic for optimal planning we present a combination of these technique symbolic merge and shrink sm s which us m s abstraction a a relaxation criterion for a symbolic backward search empirical evaluation show that sm s ha the strength of both technique deriving heuristic at least a good a the best of them for most domain 
community question answering cqa which provides a platform for people with diverse background to share information and knowledge ha become an increasingly popular research topic in this paper we focus on the task of question retrieval the key problem of question retrieval is to measure the similarity between the queried question and the historical question which have been solved by other user the traditional method measure the similarity based on the bag of word bow representation this representation neither capture dependency between related word nor handle synonym or polysemous word in this work we first propose a way to build a concept thesaurus based on the semantic relation extracted from the world knowledge of wikipedia then we develop a unified framework to leverage these semantic relation in order to enhance the question similarity in the concept space experiment conducted on a real cqa data set show that with the help of wikipedia thesaurus the performance of question retrieval is improved a compared to the traditional method 
many information gathering problem require determining the set of point for which an unknown function take value above or below some given threshold level we formalize this task a a classification problem with sequential measurement where the unknown function is modeled a a sample from a gaussian process gp we propose lse an algorithm that guide both sampling and classification based on gp derived confidence bound and provide theoretical guarantee about it sample complexity furthermore we extend lse and it theory to two more natural setting where the threshold level is implicitly defined a a percentage of the unknown maximum of the target function and where sample are selected in batch we evaluate the effectiveness of our proposed method on two problem of practical interest namely autonomous monitoring of algal population in a lake environment and geolocating network latency 
a with admissible heuristic is a very successful approach to optimal planning but how to derive such heuristic automatically merge and shrink abstraction m s is a general approach to heuristic design whose key advantage is it capability to make very fine grained choice in defining abstraction however little is known about how to actually make these choice we address this via the well known notion of bisimulation when aggregating only bisimilar state m s yield a perfect heuristic ala bisimulations are exponentially large even in trivial domain we show how to apply label reduction not distinguishing between certain group of operator without incurring any information loss while potentially reducing bisimulation size exponentially in several benchmark domain the resulting algorithm computes perfect heuristic in polynomial time empirically we show that approximating variant of this algorithm improve the state of the art in m s heuristic in particular a simple hybrid of two such variant is competitive with the leading heuristic lm cut 
object recognition is a key precursory challenge in the field of object manipulation and robotic ai visual reasoning in general recognizing object category particular instance of object and viewpoint pose of object are three critical subproblems robot must solve in order to accurately grasp manipulate object and reason about their environment multi view image of the same object lie on intrinsic low dimensional manifold in descriptor space e g visual depth descriptor space these object manifold share the same topology despite being geometrically different each object manifold can be represented a a deformed version of a unified manifold the object manifold can thus be parametrized by it homeomorphic mapping reconstruction from the unified manifold in this work we construct a manifold descriptor from this mapping between homeomorphic manifold and use it to jointly solve the three challenging recognition sub problem we extensively experiment on a challenging multi modal i e rgbd dataset and other object pose datasets and achieve state ofthe art result copyright association for the advancement of artificial intelligence www aaai org all right reserved 
in this proposal we introduce bayesian abductive logic program balp a probabilistic logic that adapts bayesian logic program blps for abductive reasoning like blps balps also combine first order logic and bayes net however unlike blps which use deduction to construct bayes net balps employ logical abduction a a result balps are more suited for problem like plan activity recognition that require abductive reasoning in order to demonstrate the efficacy of balps we apply it to two abductive reasoning task plan recognition and natural language understanding 
ai planner have to compromise between the speed of the planning process and the quality of the generated plan anytime planner try to balance these objective by finding plan of better quality over time but current anytime planner often do not make effective use of increasing runtime beyond a certain limit we present a new method of continuing plan improvement that work by repeatedly decomposing a given plan into subplans and optimising each subplan locally the decomposition exploit block structured plan deordering to identify coherent subplans which make sense to treat a unit this approach extends the anytime capability of current planner to provide continuing plan quality improvement at any time scale 
real world social network are dynamic in nature company continue to collaborate align strategically acquire and merge over time and receive positive negative impact from other company consequently their performance change with time if one can understand what type of network change affect a company s value he she can predict the future value of the company grasp industry innovation and make business more successful however it often requires continuous record of relational change which are often difficult to track for company and the model of mining longitudinal network are quite complicated in this study we developed algorithm and a system to infer large scale evolutionary company network from public news during then based on how network change over time a well a the financial information of the company we predicted company profit growth this is the first study of longitudinal network mining based company performance analysis in the literature 
we study distributed task allocation problem where cooperative agent need to perform some task simultaneously example are multi agent routing problem where several agent need to visit some target simultaneously for example to move obstacle out of the way cooperatively in this paper we first generalize the concept of reaction function proposed in the literature to characterize the agent cost of performing multiple complex task second we show how agent can construct and approximate reaction function in a distributed way third we show how reaction function can be used by an auctionlike algorithm to allocate task to agent finally we show empirically that the team cost of our algorithm are substantially smaller than those of an existing state of the art allocation algorithm for complex task 
the impossibility result in judgement aggregation show a clash between fair aggregation procedure and rational collective outcome in this paper we are interested in analysing the notion of rational outcome by proposing a proof theoretical understanding of collective rationality in particular we use the analysis of proof and inference provided by linear logic in order to define a fine grained notion of group reasoning that allows for studying collective rationality with respect to a number of logic we analyse the well known paradox in judgement aggregation and we pinpoint the reasoning step that trigger the inconsistency moreover we extend the map of possibility and impossibility result in judgement aggregation by discussing the case of substructural logic in particular we show that there exist fragment of linear logic for which general possibility result can be obtained 
computational social choice literature ha successfully studied the complexity of manipulation in various voting system however the existing model of coalitional manipulation view the manipulating coalition a an exogenous input ignoring the question of the coalition formation process while such analysis is useful a a first approximation a richer framework is required to model voting manipulation in the real world more accurately and in particular to explain how a manipulating coalition arises and chooses it action in this paper we apply tool from cooperative game theory to develop a model that considers the coalition formation process and determines which coalition are likely to form and what action they are likely to take we explore the computational complexity of several standard coalitional game theory solution concept in our setting and study the relationship between our model and the classic coalitional manipulation problem a well a the now standard bribery model 
conventionally the question on a test are assumed to be kept secret from test taker until the test however for test that are taken on a large scale particularly asynchronously this is very hard to achieve for example example toefl ibt and driver s license test question are easily found online this also appears likely to become an issue for massive open online course moocs in this paper we take the loss of confidentiality a a fact even so not all hope is lost a the test taker can memorize only a limited set of question answer and the tester can randomize which question appear on the test we model this a a stackelberg game where the tester commits to a mixed strategy and the follower responds we provide an exponential size linear program formulation prove several np hardness result and give efficient algorithm for special case 
in the literature various approach have been proposed to address the domain adaptation problem in sentiment classification also called cross domain sentiment classification however the adaptation performance normally much suffers when the data distribution in the source and target domain differ significantly in this paper we suggest to perform active learning for cross domain sentiment classification by actively selecting a small amount of labeled data in the target domain accordingly we propose an novel active learning approach for cross domain sentiment classification first we train two individual classifier i e the source and target classifier with the labeled data from the source and target respectively then the two classifier are employed to select informative sample with the selection strategy of query by committee qbc third the two classifier is combined to make the classification decision importantly the two classifier are trained by fully exploiting the unlabeled data in the target domain with the label propagation lp algorithm empirical study demonstrate the effectiveness of our active learning approach for cross domain sentiment classification over some strong baseline 
abduction ha been extensively studied in propositional logic because of it many application in artificial intelligence however it intrinsic complexity ha been a limitation to the implementation of abductive reasoning tool in more expressive logic we have devised such a tool in ground flat equational logic in which literal are equation or disequations between constant our tool is based on the computation of prime implicates it us a relaxed paramodulation calculus designed to generate all prime implicates of a formula together with a carefully defined data structure storing the implicates and able to efficiently detect and remove redundancy in addition to a detailed description of this method we present an analysis of some experimental result 
we address a dynamic decision problem in which decision maker must pay some cost when they change their decision along the way we formalize this problem a dynamic sat dynsat with decision change cost whose goal is to find a sequence of model that minimize the aggregation of the cost for changing variable we provide two solution to solve a specific case of this problem the first us a weighted partial maxsat solver after we encode the entire problem a a weighted partial maxsat problem the second solution which we believe is novel us the lagrangian decomposition technique that divide the entire problem into sub problem each of which can be separately solved by an exact weighted partial maxsat solver and produce both lower and upper bound on the optimal in an anytime manner to compare the performance of these solver we experimented on the random problem and the target tracking problem the experimental result show that a solver based on lagrangian decomposition performs better for the random problem and competitively for the target tracking problem 
matching large ontology is a challenge due to the high time complexity this paper proposes a new matching method for large ontology based on reduction anchor this method ha a distinct advantage over the divide and conquer method because it dose not need to partition large ontology in particular two kind of reduction anchor positive and negative reduction anchor are proposed to reduce the time complexity in matching positive reduction anchor use the concept hierarchy to predict the ignorable similarity calculation negative reduction anchor use the locality of matching to predict the ignorable similarity calculation our experimental result on the real world data set show that the proposed method is efficient for matching large ontology 
homophily and stochastic equivalence are two primary feature of interest in social network recently the multiplicative latent factor model mlfm is proposed to model social network with directed link although mlfm can capture stochastic equivalence it cannot model well homophily in network however many real world network exhibit homophily or both homophily and stochastic equivalence and hence the network structure of these network cannot be modeled well by mlfm in this paper we propose a novel model called generalized latent factor model glfm for social network analysis by enhancing homophily modeling in mlfm we devise a minorization maximization mm algorithm with linear time complexity and convergence guarantee to learn the model parameter extensive experiment on some real world network show that glfm can effectively model homophily to dramatically outperform state of the art method 
airline ticket purchase timing is a strategic problem that requires both historical data and domain knowledge to solve consistently even with some historical information often a feature of modern travel reservation web site it is difficult for consumer to make true cost minimizing decision to address this problem we introduce an automated agent which is able to optimize purchase timing on behalf of customer and provide performance estimate of it computed action policy based on past performance we apply machine learning to recent ticket price quote from many competing airline for the target flight route our novelty lie in extending this using a systematic feature extraction technique incorporating elementary user provided domain knowledge that greatly enhances the performance of machine learning algorithm using this technique our agent achieves much closer to the optimal purchase policy than other proposed decision theoretic approach for this domain 
this paper show how belief revision technique can be used in defeasible logic to change rule based theory characterizing the deliberation process of cognitive agent we discus intention reconsideration a a strategy to make agent compliant with the norm regulating their behavior 
we concern the problem of learning a mahalanobis distance metric for improving nearest neighbor classification our work is built upon the large margin nearest neighbor lmnn classification framework due to the semidefiniteness constraint in the optimization problem of lmnn it is not scalable in term of the dimensionality of the input data the original lmnn solver partially alleviates this problem by adopting alternating projection method instead of standard interior point method still at each iteration the computation complexity is at least o d d is the dimension of input data in this work we propose a column generation based algorithm to solve the lmnn optimization problem much more efficiently our algorithm is much more scalable in that at each iteration it doe not need full eigen decomposition instead we only need to find the leading eigenvalue and it corresponding eigenvector which is of o d complexity experiment show the efficiency and efficacy of our algorithm copyright association for the advancement of artificial intelligence www aaai org all right reserved 
understanding the molecular mechanism of life requires decoding the function of the protein in an organism various high throughput experimental technique have been developed to characterize biological system at the genome scale a fundamental challenge of the post genomic era is to assign biological function to all the protein encoded by the genome using high throughput biological data to address this challenge we propose a novel laplacian network partitioning incorporating function category correlation lnpc method to predict protein function on protein protein interaction ppi network by optimizing a laplacian based quotient objective function that seek the optimal network configuration to maximize consistent function assignment over edge on the whole graph unlike the existing approach that have no unique optimization solution our optimization problem ha unique global solution by eigen decomposition method the correlation among protein function category are quantified and incorporated into a correlated protein affinity graph which is integrated into the ppi graph to significantly improve the protein function prediction accuracy we apply our new method to the biogrid dataset for the saccharomyces cerevisiae specie using the mips annotation scheme our new method outperforms other related state of the art approach more than by the average precision of function prediction and by the average f score 
dimensionality reduction is a very important topic in machine learning it can be generally classified into two category feature selection and subspace learning in the past decade many method have been proposed for dimensionality reduction however most of these work study feature selection and subspace learning independently in this paper we present a framework for joint feature selection and subspace learning we reformulate the subspace learning problem and use l norm on the projection matrix to achieve row sparsity which lead to selecting relevant feature and learning transformation simultaneously we discus two situation of the proposed framework and present their optimization algorithm experiment on benchmark face recognition data set illustrate that the proposed framework outperforms the state of the art method overwhelmingly 
largely motivated by semantic web application many highly scalable but incomplete query answering system have been recently developed evaluating the scalability completeness trade off exhibited by such system is an important requirement for many application in this paper we address the problem of formally comparing complete and incomplete system given an ontology schema or tbox t we formulate precise condition on tboxes t expressed in the el ql or rl profile of owl under which an incomplete system is indistinguishable from a complete one w r t t regardless of the input query and data our result also allow u to quantify the degree of incompleteness of a given system w r t t a well a to automatically identify concrete query and data pattern for which the incomplete system will miss answer 
this paper focus on analyzing and predicting not answered question in community based question answering cqa service such a yahoo answer in cqa user express their information need by submitting question and await answer from other user one of the key problem of this pattern is that sometimes no one help to give answer in this paper we analyze the not answered question and give a first try of predicting whether question will receive answer more specifically we first analyze the question of yahoo answer based on the feature selected from different perspective then we formalize the prediction problem a supervised learning task and leverage the proposed feature to make prediction extensive experiment are made on question collected from yahoo answer copyright association for the advancement of artificial intelligence all right reserved 
many application of causal inference such a finding the relationship between stock price and news report involve both discrete and continuous variable observed over time inference with these complex set of temporal data though ha remained difficult and required a number of simplification we show that recent approach for inferring temporal relationship represented a logical formula can be adapted for inference with continuous valued effect building on advance in logic pctlc an extension of pctl with numerical constraint is introduced here to allow representation and inference of relationship with a mixture of discrete and continuous component then finding significant relationship in the continuous case can be done using the conditional expectation of an effect rather than it conditional probability we evaluate this approach on both synthetically generated and actual financial market data demonstrating that it can allow u to answer different question than the discrete approach can 
an extension of the csp optimization framework tailored to identify fair solution to instance involving multiple optimization function is studied two setting are considered based on the maximization of the minimum value over all the given function max min approach and on it lexicographical refinement where over all solution maximizing the minimum value those maximizing the second minimum value are preferred and so on until all function are considered lexmax min approach for both setting the complexity of computing an optimal solution is analyzed and the tractability frontier is charted for acyclic instance w r t the number and the domain of the function to be optimized larger island of tractability are then identified via a novel structural approach based on a notion of guard that is designed to deal with the interaction among constraint scope and optimization function 
accurate estimate of daily crop evapotranspiration et are needed for efficient irrigation management in region where crop water demand exceeds rainfall daily grass or alfalfa reference et value and crop coefficient are widely used to estimate crop water demand inaccurate reference et estimate can hence have a tremendous impact on irrigation cost and the demand on freshwater resource et network calculate reference et using precise measurement of meteorological data these network are typically characterized by gap in spatial coverage and lack of sufficient funding creating an immediate need for alternative source that can fill data gap without high cost although non agricultural weather station provide publicly accessible meteorological data there are concern that the data may be unsuitable for estimating reference et due to factor such a weather station siting data format and quality control issue the objective of our research is to enable the use of alternative data source adapting sophisticated machine learning algorithm such a gaussian process model and neural network to discover and model the nonlinear relationship between non et weather station data and the reference et computed by et network using data from the texas high plain region in the u s we demonstrate significant improvement in estimation accuracy in comparison with baseline regression model typically used for irrigation management application 
compared with supervised learning for feature selection it is much more difficult to select the discriminative feature in unsupervised learning due to the lack of label information traditional unsupervised feature selection algorithm usually select the feature which best preserve the data distribution e g manifold structure of the whole feature set under the assumption that the class label of input data can be predicted by a linear classifier we incorporate discriminative analysis and l norm minimization into a joint framework for unsupervised feature selection different from existing unsupervised feature selection algorithm our algorithm selects the most discriminative feature subset from the whole feature set in batch mode extensive experiment on different data type demonstrates the effectiveness of our algorithm 
discriminative structured prediction model have been widely used in many natural language processing task but it is challenging to apply the method to semantic parsing in this paper by introducing hybrid tree a a latent structure variable to close the gap between the input sentence and output representation we formulate semantic parsing a a structured prediction problem based on the latent variable perceptron model incorporated with a tree edit distance loss a optimization criterion the proposed approach maintains the advantage of a discriminative model in accommodating flexible combination of feature and naturally incorporates an efficient decoding algorithm in learning and inference furthermore in order to enhance the efficiency and accuracy of inference we design an effective approach based on vector space model to extract a smaller subset of relevant mr production for test example experimental result on publicly available corpus show that our approach significantly outperforms previous system 
the problem of influence maximization i e mining top k influential node from a social network such that the spread of influence in the network is maximized is np hard most of the existing algorithm for the problem are based on greedy algorithm although greedy algorithm can achieve a good approximation it is computational expensive in this paper we propose a totally different approach based on simulated annealing sa for the influence maximization problem this is the first sa based algorithm for the problem additionally we propose two heuristic method to accelerate the convergence process of sa and a new method of computing influence to speed up the proposed algorithm experimental result on four real network show that the proposed algorithm run faster than the state of the art greedy algorithm by order of magnitude while being able to improve the accuracy of greedy algorithm copyright association for the advancement of artificial intelligence all right reserved 
fashion magazine contain a number of photograph of fashion model and their clothing coordinate serve a useful reference in this paper we propose a recommender system for clothing coordinate using full body photograph from fashion magazine the task is that given a photograph of a fashion item e g top a a query to recommend a photograph of other fashion item e g bottom that is appropriate to the query with the proposed method we use a probabilistic topic model for learning information about coordinate from visual feature in each fashion item region we demonstrate the effectiveness of the proposed method using real photograph from a fashion magazine and two fashion style sharing service with the task of making top bottom recommendation given bottom top photograph 
in order to deal with ambiguity in natural language it is common to organise word according to their sens in synset which are group of synonymous word that can be seen a concept the manual creation of a broad coverage synset base is a time consuming task so we take advantage of dictionary definition for extracting synonymy pair and clustering for identifying synset since word sens are not discrete we create fuzzy synset where each word ha a membership degree we report on the result of the creation of a fuzzy synset base for portuguese from three electronic dictionary the resulting resource is larger than existing hancrafted portuguese thesaurus 
the amount of video available on the web is growing explosively while some video are very interesting and receive high rating from viewer many of them are le interesting or even boring this paper conduct a pilot study on the understanding of human perception of video interestingness and demonstrates a simple computational method to identify more interesting video to this end we first construct two datasets of flickr and youtube video respectively human judgement of interestingness are collected and used a the groundtruth for training computational model we evaluate several off the shelf visual and audio feature that are potentially useful for predicting interestingness on both datasets result indicate that audio and visual feature are equally important and the combination of both modality show very promising result copyright association for the advancement of artificial intelligence www aaai org all right reserved 
heuristic search is a central component of many important application in ai including automated planning while we can find optimal solution to heuristic search problem doing so may take hour or day for practical application this is unacceptably slow and we must rely on algorithm which find solution of high but not optimal quality or one which bound the time used directly in my dissertation i present and analyze algorithm for the following setting quality bounded heuristic search and time bounded heuristic search the central theme of my doctoral work will be that taking advantage of additional information can improve the performance of heuristic search algorithm 
the use of ontology for accessing data is one of the most exciting new application of description logic in database and other information system a realistic way of realising sufficiently scalable ontology based data access in practice is by reduction to querying relational database in this paper we describe the combined approach which incorporates the information given by the ontology into the data and employ query rewriting to eliminate spurious answer we illustrate this approach for ontology given in the dl lite family of description logic and briefly discus the result obtained for the el family 
learning to rank ha received great attention in recent year a it play a crucial role in information retrieval the existing concept of learning to rank assumes that each training sample is associated with an instance and a reliable label however in practice this assumption doe not necessarily hold true this study focus on the learning to rank when each training instance is labeled by multiple annotator that may be unreliable in such a scenario no accurate label can be obtained this study proposes two learning approach one is to simply estimate the ground truth first and then to learn a ranking model with it the second approach is a maximum likelihood learning approach which estimate the ground truth and learns the ranking model iteratively the two approach have been tested on both synthetic and real world data the result reveal that the maximum likelihood approach outperforms the first approach significantly and is comparable of achieving result with the learning model considering reliable label further more both the approach have been applied for ranking the web visual clutter 
determining the complexity of perfect information trick taking card game is a long standing open problem this question is worth addressing not only because of the popularity of these game among human player e g double dummy bridge but also because of it practical importance a a building block in state of the art playing engine for contract bridge skat heart and spade we define a general class of perfect information two player trick taking card game dealing with arbitrary number of hand suit and suit length we investigate the complexity of determining the winner in various fragment of this game class our main result is a proof of pspace completeness for a fragment with bounded number of hand through a reduction from generalized geography combining our result with w stlund s tractability result give further insight in the complexity landscape of trick taking card game 
brain computer interface bcis allow a user to directly control device such a cursor and robot using brain signal non invasive bcis e g those based on electroencephalographic eeg signal recorded from the scalp suffer from low signal to noise ratio which limit the bandwidth of control invasive bcis allow fine grained control but can leave user exhausted since control is typically exerted on a moment by moment basis in this paper we address these problem by proposing a new adaptive hierarchical architecture for brain computer interfacing the approach allows a user to teach the bci new skill on the fly these learned skill are later invoked directly a high level command relieving the user of tedious low level control we report result from four subject who used a hierarchical eeg based bci to successfully train and control a humanoid robot in a virtual home environment gaussian process were used for learning high level command allowing a bci to switch between autonomous and user guided mode based on the current estimate of uncertainty we also report the first instance of multi tasking in a bci involving simultaneous control of two different device by a single user our result suggest that hierarchical bcis can provide a flexible and robust way of controlling complex robotic device in real world environment 
we analyze the complexity of reasoning in el with defeasible inclusion and extension thereof the result by bonatti et al a are extended by proving tight lower complexity bound and by relaxing the syntactic restriction adopted there we further extend the old framework by supporting arbitrary priority relation 
this paper provides algorithm for predicting the size of the expanded search tree est of depth first branch and bound algorithm dfbnb for optimization task the prediction algorithm is implemented and evaluated in the context of solving combinatorial optimization problem over graphical model such a bayesian and markov network our method extend to dfbnb the approach provided by knuth chen scheme that were designed and applied for predicting the est size of backtracking search algorithm our empirical result demonstrate good prediction which are superior to competing scheme 
extracting a subset of a given ontology that capture all the ontology s knowledge about a specified set of term is a well understood task this task can be based for instance on locality based module however a single module doe not allow u to understand neither topicality connectedness structure or superfluous part of an ontology nor agreement between actual and intended modeling the strong logical property of locality based module suggest that the family of all such module of an ontology can support comprehension of the ontology a a whole however extracting that family is not feasible since the number of locality based module of an ontology can be exponential w r t it size in this paper we report on a new approach that enables u to efficiently extract a polynomial representation of the family of all locality based module of an ontology we also describe the fundamental algorithm to pursue this task and report on experiment carried out and result obtained 
this paper present the current state of this research work aimed to develop a methodology for designing adaptive virtual organization this paper includes both completed and remaining work on this topic 
cooperative path finding can be abstracted a computing non colliding path for multiple agent between their start and goal location on a graph this paper proposes a fast algorithm that can provide completeness guarantee for a general class of problem without any assumption about the graph s topology specifically the approach can address any solvable instance where there are at most n agent in a graph of size n the algorithm employ two primitive a push operation where agent move towards their goal up to the point that no progress can be made and a swap operation that allows two agent to swap position without altering the configuration of other agent simulated experiment are provided on hard instance of cooperative path finding including comparison against alternative method the result are favorable for the proposed algorithm and show that the technique scale to problem that require high level of coordination involving hundred of agent 
online forum contain interactive and semantically related discussion on various question extracted question answer archive is invaluable knowledge which can be used to improve question answering service in this paper we address the problem of question suggestion which target at suggesting question that are semantically related to a queried question existing bag of word approach suffer from the shortcoming that they could not bridge the lexical chasm between semantically related question therefore we present a new framework to suggest question and propose the topic enhanced translation based language model topictrlm which fuse both the lexical and latent semantic knowledge extensive experiment have been conducted with a large real world data set experimental result indicate our approach is very effective and outperforms other popular method in several metric copyright association for the advancement of artificial intelligence all right reserved 
the hypothesis is presented that cognitive synergy proactive and mutually assistive feedback between different cognitive process associated with different type of memory may serve a a foundation for advanced artificial general intelligence a specific ai architecture founded on this idea opencogprime is described in the context of it application to control virtual agent and robot the manifestation of cognitive synergy in opencogprime s procedural and declarative learning algorithm are discussed in some detail copyright association for the advancement of artificial intelligence www aaai org all right reserved 
manifold alignment is to extract the shared latent semantic structure from multiple manifold the joint adjacency matrix play a key role in manifold alignment to construct the matrix it is crucial to get more corresponding pair this paper proposes an approach to obtain more and reliable corresponding pair in term of local structure correspondence the sparse reconstruction weight matrix of each manifold is established to preserve the local geometry of the original data set the sparse correspondence matrix are constructed using the sparse local structure of corresponding pair across manifold further more a new energy function for manifold alignment is proposed to simultaneously match the corresponding instance and preserve the local geometry of each manifold the shared low dimensional embedding which provides better description for the intrinsic geometry and relation between different manifold can be obtained by solving the optimization problem with closed form solution experiment demonstrate the effectiveness of the proposed algorithm 
a better similarity mapping function across heterogeneous high dimensional feature is very desirable for many application involving multi modal data in this paper we introduce coupled dictionary learning dl into supervised sparse coding for multi modal crossmedia retrieval we call this supervised coupleddictionary learning with group structure for multi modal retrieval slim slim formulates the multimodal mapping a a constrained dictionary learning problem by utilizing the intrinsic power of dl to deal with the heterogeneous feature slim extends unimodal dl to multi modal dl moreover the label information is employed in slim to discover the shared structure inside intra modality within the same class by a mixed norm i e norm a a result the multimodal retrieval is conducted via a set of jointly learned mapping function across multi modal data the experimental result show the effectiveness of our proposed model when applied to cross medium retrieval copyright association for the advancement of artificial intelligence www aaai org all right reserved 
automatic mathematical solution assessment check the equivalence of mathematical expression in the user answer and standard solution it is a challenging problem a the semantics of mathematical expression are highly symbolic and equivalent mathematical expression can be expressed in different form in this paper we propose an effective probabilistic equivalence verification pev approach for automatic mathematical solution assessment the proposed pev approach is a randomized method based on the probabilistic numerical equivalence testing of two mathematical expression it can avoid false negative error completely while guaranteeing a small probability of false positive error to occur the performance result have shown that the proposed pev approach ha outperformed other popular technique in computer algebra system such a maple and mathematica 
in this paper we present bench biclustering driven ensemble of classifier an algorithm to construct an ensemble of classifier through concurrent feature and data point selection guided by unsupervised knowledge obtained from biclustering bench is designed for underdetermined problem in our experiment we use bayesian belief network bbn classifier a base classifier in the ensemble however bench can be applied to other classification model a well we show that bench is able to increase prediction accuracy of a single classifier and traditional ensemble of classifier by up to on three microarray datasets using various weighting scheme for combining individual prediction in the ensemble 
we propose a new additive decomposition of probability table that preserve equivalence of the joint distribution while reducing the size of potential without extra variable we formulate the most probable explanation mpe problem in belief network a a weighted constraint satisfaction problem wcsp our pairwise decomposition allows to replace a cost function with smaller arity function the resulting pairwise decomposed wcsp is then easier to solve using state of the art wcsp technique although testing pairwise decomposition is equivalent to testing pairwise independence in the original belief network we show how to efficiently test and enforce it even in the presence of hard constraint furthermore we infer additional information from the resulting nonbinary cost function by projecting subtracting them on binary function we observed huge improvement by preprocessing with pairwise decomposition and project subtract compared to the current state of the art solver on two difficult set of benchmark 
this paper present an approach to belief revision in which revision is a function from a belief state and a finite set of formula to a new belief state in the interesting case the set for revision s may be inconsistent but individual member of s are consistent we argue that s will still contain interesting information regarding revision in particular maximum consistent subset of s will determine candidate formula for the revision process and the agent s associated faithful ranking will determine the plausibility of such candidate formula postulate and semantic condition characterizing this approach are given and representation result are provided a a consequence of this approach we argue that revision by a sequence of formula usually considered a a problem of iterated revision is more appropriately regarded a revision by the possibly inconsistent set of these formula hence we suggest that revision by a sequence of formula is foremost a problem of uniterated set revision 
subjective assessment sa are assigned by user against item such a elegant and gorgeous and are common in review tag in many online site however previous study fail to effectively use sa for improving recommendation because few user rate the same item with the same sa which trigger the sparsity problem in collaborative filtering we propose a novel algorithm that link a taxonomy of item to a taxonomy of sa to ass user interest in detail that is it merges the sa assigned by user against an item into subjective class sc and reflects the sa sc assigned to an item to it class thus it can measure the similarity of user from not only sa sc assigned to item but also their class which overcomes the sparsity problem our evaluation which us data from a popular restaurant review site show that our method generates more accurate recommendation than previous method furthermore we find that sa frequently assigned on a few item class are more useful than those widely assigned against many item class in term of recommendation accuracy 
in statistical word alignment for machine translation function word usually cause poor aligning performance because they do not have clear correspondence between different language this paper proposes a novel approach to improve word alignment by pruning alignment of function word from an existing alignment model with high precision and recall based on monolingual and bilingual frequency characteristic a language independent function word recognition algorithm is first proposed then a group of carefully defined syntactic structure combined with content word alignment are used for further function word alignment pruning the experimental result show that the proposed approach improves both the quality of word alignment and the performance of statistical machine translation on chinese to english german to english and french to english language pair 
recently large amount of data are being published using semantic web standard simultaneously there ha been a steady rise in link between object from multiple source however the ontology behind these source have remained largely disconnected thereby challenging the interoperability goal of the semantic web we address this problem by automatically finding alignment between concept from multiple linked data source instead of only considering the existing concept in each ontology we hypothesize new composite concept defined using conjunction and disjunction of rdf type and value restriction and generate alignment between them in addition our technique provide a novel method for curating the linked data web by pointing to likely incorrect or missing assertion our approach provides a deeper understanding of the relationship between linked data source and increase the interoperability among previously disconnected ontology 
robot depend on captured image for perceiving the environment a robot can replace a human in capturing quality photograph for publishing in this paper we employ an iterative photo capture by robot by repositioning itself to capture good quality photograph our image quality assessment approach is based on few high level feature of the image combined with some of the aesthetic guideline of professional photography our system can also be used in web image search application to rank image we test our quality assessment approach on a large and diversified dataset and our system is able to achieve a classification accuracy of we ass the aesthetic error in the captured image and estimate the change required in orientation of the robot to retake an aesthetically better photograph our experiment are conducted on nao robot with no stereo vision the result demonstrate that our system can be used to capture professional photograph which are in accord with the human professional photography 
this paper considers inference from multinomial data and address the problem of choosing the strength of the dirichlet prior under a mean squared error criterion we compare the maximum likelihood estimator mle and the most commonly used bayesian estimator obtained by assuming a prior dirichlet distribution with noninformative prior parameter that is the parameter of the dirichlet are equal and altogether sum up to the so called strength of the prior under this criterion mle becomes more preferable than the bayesian estimator at the increase of the number of category k of the multinomial because non informative bayesian estimator induce a region where they are dominant that quickly shrink with the increase of k this can be avoided if the strength of the prior is not kept constant but decreased with the number of category we argue that the strength should decrease at least k time faster than usual estimator do 
multi objective optimization problem arise frequently in application but can often only be solved approximately by heuristic approach evolutionary algorithm have been widely used to tackle multi objective problem these algorithm use different measure to ensure diversity in the objective space but are not guided by a formal notion of approximation we present a new framework of an evolutionary algorithm for multi objective optimization that allows to work with a formal notion of approximation our experimental result show that our approach outperforms state of the art evolutionary algorithm in term of the quality of the approximation that is obtained in particular for problem with many objective 
decentralized partially observable markov decision process dec pomdps are used to plan policy for multiple agent that must maximize a joint reward function but do not communicate with each other the agent act under uncertainty about each other and the environment this planning task arises in optimization of wireless network and other scenario where communication between agent is restricted by cost or physical limit dec pomdps are a promising solution but optimizing policy quickly becomes computationally intractable when problem size grows factored dec pomdps allow large problem to be described in compact form but have the same worst case complexity a non factored dec pomdps we propose an efficient optimization algorithm for large factored infinite horizon dec pomdps we formulate expectation maximization based optimization into a new form where complexity can be kept tractable by factored approximation our method performs well and it can solve problem with more agent and larger state space than state of the art dec pomdp method we give result for factored infinite horizon dec pomdp problem with up to agent 
multi label classification is a critical problem in many area of data analysis such a image labeling and text categorization in this paper we propose a probabilistic multi label classification model based on novel sparse feature learning by employing an individual sparsity inducing l norm and a group sparsity inducing l norm the proposed model ha the capacity of capturing both label interdependency and common predictive model structure we formulate this sparse norm regularized learning problem a a non smooth convex optimization problem and develop a fast proximal gradient algorithm to solve it for an optimal solution our empirical study demonstrates the efficacy of the proposed method on a set of multi label task given a limited number of labeled training instance 
recent year have witnessed an increasing number of application involving data with structural dependency and graph representation for these application it is very common that their class distribution is imbalanced with minority sample being only a small portion of the population such imbalanced class distribution impose significant challenge to the learning algorithm this problem is further complicated with the presence of noise or outlier in the graph data in this paper we propose an imbalanced graph boosting algorithm igboost that progressively selects informative subgraph pattern from imbalanced graph data for learning to handle class imbalance we take class distribution into consideration to assign different weight value to graph the distance of each graph to it class center is also considered to adjust the weight to reduce the impact of noisy graph data the weight value are integrated into the iterative subgraph feature selection and margin learning process to achieve maximum benefit experiment on real world graph data with different degree of class imbalance and noise demonstrate the algorithm performance 
a human s ability to diagnose error gather data and generate feature in order to build better model is largely untapped we hypothesize that analyzing result from multiple model can help people diagnose error by understanding relationship among data feature and algorithm these relationship might otherwise be masked by the bias inherent to any individual model we demonstrate this approach in our prospect system show how multiple model can be used to detect label noise and aid in generating new feature and validate our method in a pair of experiment 
post editing feedback provided by user of on line translation service offer an excellent opportunity for automatic improvement of statistical machine translation smt system however feedback provided by casual user is very noisy and must be automatically filtered in order to identify the potentially useful case we present a study on automatic feedback filtering in a real weblog collected from reverso net we extend and re annotate a training corpus define an extended set of simple feature and approach the problem a a binary classification task experimenting with linear and kernel based classifier and feature selection result on the feedback filtering task show a significant improvement over the majority class but also a precision ceiling around this reflects the inherent difficulty of the problem and indicates that shallow feature cannot fully capture the semantic nature of the problem despite the modest result on the filtering task the classifier are proven effective in an application based evaluation the incorporation of a filtered set of feedback instance selected from a larger corpus significantly improves the performance of a phrase based smt system according to a set of standard evaluation metric 
nowadays nearest neighbor search becomes more and more important when facing the challenge of big data traditionally to solve this problem researcher mainly focus on building effective data structure such a hierarchical k mean tree or using hashing method to accelerate the query process in this paper we propose a novel unified approximate nearest neighbor search scheme to combine the advantage of both the effective data structure and the fast hamming distance computation in hashing method in this way the searching procedure can be further accelerated computational complexity analysis and extensive experiment have demonstrated the effectiveness of our proposed scheme 
diffusion process in network are increasingly used to model dynamic phenomenon such a the spread of information wildlife or social influence our work address the problem of learning the underlying parameter that govern such a diffusion process by observing the time at which node become active a key advantage of our approach is that unlike previous work it can tolerate missing observation for some node in the diffusion process having incomplete observation is characteristic of offline network used to model the spread of wildlife we develop an em algorithm to address parameter learning in such setting since both the e and m step are computationally challenging we employ a number of optimization method such a nonlinear and difference of convex programming to address these challenge evaluation of the approach on the red cockaded woodpecker conservation problem show that it is highly robust and accurately learns parameter in various setting even with more than missing data 
balanced multi way number partitioning bmnp seek to split a collection of number into subset with roughly the same cardinality and subset sum the problem is np hard and there are several exact and approximate algorithm for it however existing exact algorithm solve only the simpler balanced two way number partitioning variant whereas the most effective approximate algorithm bldm may produce widely varying subset sum in this paper we introduce the lrm algorithm that lower the expected spread in subset sum to one third that of bldm for uniformly distributed number and odd subset cardinality we also propose meld a novel strategy for skewed number distribution a combination of lrm and meld lead to a heuristic technique that consistently achieves a narrower spread of subset sum than bldm 
in this paper we propose a mechanism that encourages agent participating in an open ma to follow a desirable behaviour by introducing modification in the environment this mechanism is deployed by using an infrastructure based on institutional agent called incentivators each external agent is assigned to an incentivator that is able to discover it preference and to learn the suitable modification in the environment in order to improve the global utility of a system in response to inadequate design or change in the population of participating agent the mechanism is evaluated in a p p scenario 
belief change is an important research topic in ai it becomes more perplexing in multi agent setting since the action of an agent may be partially observable to other agent in this paper we present a general approach to reasoning about action and belief change in multi agent setting our approach is based on a multiagent extension to the situation calculus augmented by a plausibility relation over situation and another one over action which is used to represent agent different perspective on action when an action is performed we update the agent plausibility order on situation by giving priority to the plausibility order on action in line with the agm approach of giving priority to new information we show that our notion of belief satisfies kd property a to the special case of belief change of a single agent we show that our framework satisfies most of the classical agm km and dp postulate we also present property concerning the change of common knowledge and belief of a group of agent copyright association for the advancement of artificial intelligence www aaai org all right reserved 
decision rule which can provide good interpretability and flexibility for data mining task have received very little attention in the stream mining community so far in this work we introduce a new algorithm to learn rule set designed for open ended data stream the proposed algorithm is able to continuously learn compact ordered and unordered rule set the experimental evaluation show competitive result in comparison with vfdt and c rule 
user centered programming by demonstration is an approach that place the need of people above algorithmic constraint and requirement in this paper we present a user centered programming by demonstration project for authoring interactive robotic locomotion style the style in which a robot move about a space expressed through it motion can be used for communication for example a robot could move aggressively in reaction to a person s action or alternatively react using careful submissive movement we present a new demonstration interface algorithm and evaluation result 
envy freeness is a well known fairness concept for analyzing mechanism it traditional definition requires that no individual envy another individual however an individual or a group of agent may envy another group even if she or they doe not envy another individual in mechanism with monetary transfer such a combinatorial auction considering such fairness requirement which are refinement of traditional envy freeness is meaningful and brings up a new interesting research direction in mechanism design in this paper we introduce two new concept of fairness called envy freeness of an individual toward a group and envy freeness of a group toward a group they are natural extension of traditional envy freeness we discus combinatorial auction mechanism that satisfy these concept first we characterize such mechanism by focusing on their allocation rule then we clarify the connection between these concept and three other property the core strategy proofness and false name proofness 
there ha been an increasing interest in applying biological principle to the design and control of robot unlike industrial robot that are programmed to execute a rather limited number of task the new generation of bio inspired robot is expected to display a wide range of behaviour in unpredictable environment a well a to interact safely and smoothly with human co worker in this article we put forward some of the property that will characterize these new robot soft material flexible and stretchable sensor modular and efficient actuator self organization and distributed control we introduce a number of design principle in particular we try to comprehend the novel design space that now includes soft material and requires a completely different way of thinking about control we also introduce a recent case study of developing a complex humanoid robot discus the lesson learned and speculate about future challenge and perspective 
bayesian approach to preference learning using gaussian process gps are attractive due to their ability to explicitly model uncertainty in user latent utility function unfortunately existing technique have cubic time complexity in the number of user which render this approach intractable for collaborative preference learning over a large user base exploiting the observation that user population often decompose into community of shared preference we model user preference a an infinite dirichlet process dp mixture of community and learn a the expected number of preference community represented in the data b a gp based preference model over item tailored to each community and c the mixture weight representing each user s fraction of community membership this result in a learning and inference process that scale linearly in the number of user rather than cubicly and additionally provides the ability to analyze individual community preference and their associated member we evaluate our approach on a variety of preference data source including amazon mechanical turk showing that our method is more scalable and a accurate a previous gp based preference learning work 
many task in probabilistic reasoning can be cast a max sum product problem a hard class of combinatorial problem we describe our result in obtaining a new approximation scheme for the problem that can be turned into an anytime procedure for many task this scheme can be shown to be asymptotically the best possible heuristic 
we put forward a cutoff technique for determining the number of agent that is sufficient to consider when checking temporal epistemic specification on a system of any size we identify a special class of interleaved interpreted system for which we give a parameterised semantics and an abstraction methodology this enables u to overcome the significant limitation in expressivity present in the state of the art we present an implementation and discus experimental result 
in beijing most taxi driver intentionally avoid working during peak hour despite of the huge customer demand within these peak period this dilemma is mainly due to the fact that taxi driver congestion cost are not reflected in the current taxi fare structure to resolve this problem we propose a new pricing scheme to provide taxi driver with extra incentive to work during peak hour this differs from previous study of taxi market by considering market variance over multiple period taxi driver profit driven decision and their scheduling constraint regarding the interdependence among different period the major challenge of this research is the computational intensiveness to identify optimal strategy due to the exponentially large size of a taxi driver s strategy space and the scheduling constraint we develop an atom schedule method to overcome these issue it reduces the magnitude of the problem while satisfying the constraint to filter out infeasible pure strategy simulation result based on real data show the effectiveness of the proposed method which open up a new door to improving the efficiency of taxi market in megacities e g beijing 
recent year have witnessed increased interest in measuring authority and modelling influence in social network for a long time pagerank ha been widely used for authority computation and ha also been adopted a a solid baseline for evaluating social influence related application however the connection between authority measurement and influence modelling is not clearly established to this end in this paper we provide a focused study on understanding of pagerank a well a the relationship between pagerank and social influence analysis along this line we first propose a linear social influence model and reveal that this model is essentially pagerank with prior also we show that the authority computation by pagerank can be enhanced with more generalized prior moreover to deal with the computational challenge of pagerank with general prior we provide an upper bound for top authoritative node identification finally the experimental result on the scientific collaboration network validate the effectiveness of the proposed social influence model 
to improve the current real world deployment of stackelberg security game ssgs it is critical now to efficiently incorporate model of adversary bounded rationality in large scale ssgs unfortunately previously proposed branch and price approach fail to scale up given the non convexity of such model a we show with a realization called cocomo therefore we next present a novel cutting plane algorithm called blade to scale up ssgs with complex adversary model with three key novelty i an efficient scalable separation oracle to generate deep cut ii a heuristic that us gradient to further improve the cut iii technique for quality efficiency tradeoff 
we explore in this paper efficient algorithmic solution to robust subspace segmentation we propose the ssqp namely subspace segmentation via quadratic programming to partition data drawn from multiple subspace into multiple cluster the basic idea of ssqp is to express each datum a the linear combination of other data regularized by an overall term targeting zero reconstruction coefficient over vector from different subspace the derived coefficient matrix by solving a quadratic programming problem is taken a an affinity matrix upon which spectral clustering is applied to obtain the ultimate segmentation result similar to sparse subspace clustering scc and low rank representation lrr ssqp is robust to data noise a validated by experiment on toy data experiment on hopkins database show that ssqp can achieve competitive accuracy a scc and lrr in segmenting affine subspace while experimental result on the extended yale face database b demonstrate ssqp s superiority over scc and lrr beyond segmentation accuracy all experiment show that ssqp is much faster than both ssc and lrr in the practice of subspace segmentation copyright association for the advancement of artificial intelligence all right reserved 
we study a logical system fqht that is appropriate for reasoning about nonmonotonic theory with intensional function a treated in the approach of bartholomew and lee we provide a logical semantics a gentzen style proof theory and establish completeness result the adequacy of the approach is demonstrated by showing that it capture the bartholemew lee semantics and satisfies a strong equivalence property 
we study robust high dimensional estimation of generalized linear model glms where a small number k of the n observation can be arbitrarily corrupted and where the true parameter is high dimensional in the p n regime but only ha a small number s of non zero entry there ha been some recent work connecting robustness and sparsity in the context of linear regression with corrupted observation by using an explicitly modeled outlier response vector that is assumed to be sparse interestingly we show in the glm setting such explicit outlier response modeling can be performed in two distinct way for each of these two approach we give l error bound for parameter estimation for general value of the tuple n p s k 
human behavior analysis in uncontrolled environment can be categorized in two main challenge feature extraction and behavior analysis from a set of corporal language vocabulary in this work we present our achievement characterizing some simple behavior from visual data on different real application and discus our plan for future work low level vocabulary definition from bag of gesture unit and high level modelling and inference of human behavior 
we investigate the possibility of using structured data to improve search over unstructured document in particular we use relevance feedback to create a virtuous cycle between structured data from the semantic web and web page from the hypertext web previous approach have generally considered searching over the semantic web and hypertext web to be entirely disparate indexing and searching over different domain our novel approach is to use relevance feedback from hypertext web result to improve semantic web search and result from the semantic web to improve the retrieval of hypertext web data in both case our evaluation is based on certain kind of informational query abstract concept people and place selected from a real life query log and checked by human judge we show our relevance model based system is better than the performance of real world search engine for both hypertext and semantic web search and we also investigate semantic web inference and pseudo relevance feedback 
knowledge acquisition is the essential process of extracting and encoding knowledge both domain specific and commonsense to be used in intelligent system while many large knowledge base have been constructed none is close to complete this paper present an approach to improving a knowledge base efficiently under resource constraint using a guiding knowledge base question are generated from a weak form of similarity based inference given the glossary mapping between two knowledge base the candidate question are prioritized in term of the concept coverage of the target knowledge experiment were conducted to find question to grow the chinese conceptnet using the english conceptnet a a guide the result were evaluated by online user to verify that of the question and of the answer are good in addition the answer collected in a six week period showed consistent improvement to a increase in concept coverage of the chinese commonsense knowledge base against the english conceptnet 
in model based diagnosis a diagnostic algorithm is typically used to compute diagnosis using a model of a real world system and some observation contrary to classical hypothesis in real world application it is sometimes the case that either the model the observation or the diagnostic algorithm are abnormal with respect to some required property with possibly huge economical consequence determining which abnormality exist constitutes a meta diagnostic problem we contribute first with a general theory of meta diagnosis with clear semantics to handle this problem second we propose a series of typically required property and relate them between themselves finally using our meta diagnostic framework and the studied property and relation we model and solve some common meta diagnostic problem 
optimal decentralized decision making in a team of cooperative agent a formalized by decentralized pomdps is a notoriously hard problem a major obstacle is that the agent do not have access to a sufficient statistic during execution which mean that they need to base their action on their history of observation a consequence is that even during off line planning the choice of decision rule for different stage is tightly interwoven decision of earlier stage affect how to act optimally at later stage and the optimal value function for a stage is known to have a dependence on the decision made up to that point this paper make a contribution to the theory of decentralized pomdps by showing how this dependence on the past joint policy can be replaced by a sufficient statistic these result are extended to the case of k step delayed communication the paper investigates the practical implication a well a the effectiveness of a new pruning technique for maa method in a number of benchmark problem and discus future avenue of research opened by these contribution 
probabilistic logic combine the expressive power of logic with the ability to reason with uncertainty several probabilistic logic language have been proposed in the past each of them with their own feature in this paper we propose a new probabilistic constraint logic programming language which combine constraint logic programming with probabilistic reasoning the language support modeling of discrete a well a continuous probability distribution by expressing constraint on random variable we introduce the declarative semantics of this language present an exact inference algorithm to derive bound on the joint probability distribution consistent with the specified constraint and give experimental result the result obtained are encouraging indicating that inference in our language is feasible for solving challenging problem 
today mobile robot are increasingly expected to operate in ever more complex and dynamic environment in order to carry out many of the higher level task envisioned a semantic understanding of a workspace is pivotal here our field ha benefited significantly from success in machine learning and vision application in robotics of off the shelf object detector are plentiful this paper outline an online any time planning framework enabling the active exploration of such detection our approach exploit the ability to move to different vantage point and implicitly weighs the benefit of gaining more certainty about the existence of an object against the physical cost of the exploration required the result is a robot which plan trajectory specifically to decrease the entropy of putative detection our system is demonstrated to significantly improve detection performance and trajectory length in simulated and real robot experiment 
during the design of a microprocessor design space exploration dse is a critical step which determines the appropriate design configuration of the microprocessor in the computer architecture community supervised learning technique have been applied to dse to build model for predicting the quality of design configuration for supervised learning however considerable simulation cost are required for attaining the labeled design configuration given limited resource it is difficult to achieve high accuracy in this paper inspired by recent advance in semi supervised learning we propose the comt approach which can exploit unlabeled design configuration to improve the model in addition to an improved predictive accuracy comt is able to guide the design of microprocessor owing to the use of comprehensible model tree empirical study demonstrates that comt significantly outperforms state of the art dse technique through reducing mean squared error by to and thus promising architecture can be attained more efficiently 
the goal of this paper is a systematic parameterized complexity analysis of different variant of propositional strip planning we identify several natural problem parameter and study all possible combination of parameter in different setting these setting arise for instance from the distinction if negative effect of action are allowed or not we provide a complete picture by establishing for each case either paranp hardness i e the parameter combination doe not help or w t completeness with t i e fixed parameter intractability or fpt i e fixed parameter tractability 
several logistics service provider serve a certain number of customer geographically spread over an area of operation they would like to coordinate their operation so a to minimize overall cost at the same time they would like to keep information about their cost constraint and preference private thus precluding conventional negotiation we show how ai technique in particular distributed constraint optimization dcop can be integrated with cryptographic technique to allow such coordination without revealing agent private information the problem of assigning customer to company is formulated a a dcop for which we propose two novel privacy preserving algorithm we compare their performance and privacy property on a set of vehicle routing problem benchmark 
the goal of rating based recommender system is to make personalized prediction and recommendation for individual user by leveraging the preference of a community of user with respect to a collection of item like song or movie recommender system are often based on intricate statistical model that are estimated from data set containing a very high proportion of missing rating this work describes evidence of a basic incompatibility between the property of recommender system data set and the assumption required for valid estimation and evaluation of statistical model in the presence of missing data we discus the implication of this problem and describe extended modelling and evaluation framework that attempt to circumvent it we present prediction and ranking result showing that model developed and tested under these extended framework can significantly outperform standard model 
it ha been known for a long time that visual task such a reading counting and searching greatly influence eye movement pattern perhaps the best known demonstration of this is the celebrated study of yarbus showing that different eye movement trajectory emerge depending on the visual task that the viewer are given the objective of this paper is to develop an inverse yarbus process whereby we can infer the visual task by observing the measurement of a viewer s eye movement while executing the visual task the method we are proposing is to use hidden markov model hmms to create a probabilistic framework to infer the viewer s task from eye movement 
domain independent optimal planning ha seen important breakthrough in recent year with the development of tractable and informative admissible heuristic suitable for planner based on forward state space search these heuristic allow planner to optimally solve an important number of benchmark problem including problem that are quite involved and difficult for the layman in this paper we present a new admissible heuristic that is obtained from the state equation associated to the petri net representation of the planning problem the new heuristic that doe not fall into one of the four standard class can be computed in polynomial time and is competitive with the current state of the art for optimal planning a empirically demonstrated over a large number of problem mainly because it often show an improved quality to cost ratio the new heuristic applies to sa planning task with arbitrary non negative action cost 
we consider semi autonomous agent that have uncertain knowledge about their environment but can ask what action the operator would prefer taking in the current or in a potential future state asking query can help improve behavior but if query come at a cost e g due to limited operator attention the number of query need to be minimized we develop a new algorithm for selecting action query by adapting the recently proposed expected myopic gain emg from it prior use in setting with reward or transition probability query to our setting of action query and empirically compare it to the current state of the art 
the study of social norm is one of the most active topic studied by researcher from several area such a multiagent system economics or social science self organizing system have taken social norm a a mechanism that distribute the self governance task to all their member this thesis tackle the problem of analyzing the specific condition that catalyze the imposition on a certain pre existing norm we have taken a cross methodological approach where we combine theory from complex system agent based social simulation empirical laboratory experiment with human and behavioural modelling for obtaining a more comprehensive understanding of social norm in virtual society 
water pipe failure can not only have a great impact on people s daily life but also cause significant waste of water which is an essential and precious resource to human being a a result preventative maintenance for water pipe particularly in urban scale network is of great importance for a sustainable society to achieve effective replacement and rehabilitation failure prediction aim to proactively find those most likely to fail pipe becomes vital and ha been attracting more attention from both academia and industry especially from the civil engineering field this paper present an already deployed industrial computational system for pipe failure prediction a an alternative to risk matrix method often depending on ad hoc domain heuristic learning based method are adopted using the attribute with respect to physical environmental operational condition and etc further challenge arises in practice when lacking of profile attribute a dive into the failure record show that the failure event sequence typically exhibit temporal clustering pattern which motivates u to use the stochastic process to tackle the failure prediction task specifically the failure sequence is formulated a a self exciting stochastic process which is to our best knowledge a novel formulation for pipe failure prediction and we show that it outperforms a baseline assuming the failure risk grows linearly with aging broad new problem and research point for the machine learning community are also introduced for future work 
this paper study the problem of image annotation in a multi modal setting where both visual and textual information are available we propose multimodal multi instance multi label latent dirichlet allocation m lda where the model consists of a visual label part a textual label part and a label topic part the basic idea is that the topic decided by the visual information and the topic decided by the textual information should be consistent leading to the correct label assignment particularly m lda is able to annotate image region thus provides a promising way to understand the relation between input pattern and output semantics experiment on corel k and imageclef validate the effectiveness of the proposed method 
we formalize reasoning about fuzzy belief and fuzzy common belief especially incomparable belief in multi agent system by using a logical system based on fitting s many valued modal logic where incomparable belief mean belief whose degree are not totally ordered completeness and decidability result for the logic of fuzzy belief and common belief are established while implicitly exploiting the duality theoretic perspective on fitting s logic that build upon the author s previous work a conceptually novel feature is that incomparable belief and qualitative fuzziness can be formalized in the developed system whereas they cannot be formalized in previously proposed system for reasoning about fuzzy belief we believe that belief degree can ultimately be reduced to truth degree and we call this the reduction thesis about belief degree which is assumed in the present paper and motivates an axiom of our system we finally argue that fuzzy reasoning shed new light on old epistemic issue such a coordinated attack problem 
in question answering answer extraction aim to pin point the exact answer from passage however most previous method perform such extraction on each passage separately without considering clue provided in other passage this paper present a novel approach to extract answer by fully leveraging connection among different passage specially extraction is performed on a passage graph which is built by adding link upon multiple passage different passage are connected by linking word with the same stem we use the factor graph a our model for answer extraction experimental result on multiple qa data set demonstrate that our method significantly improves the performance of answer extraction 
wastl introduced for first time a tableau like method based on an inference system for deriving all minimal key from a relational schema he introduced two inference rule and built an automated method over them in this work we tackle the key finding problem with a tableau method but we will use two inference rule inspired by the simplification logic for functional dependency wastl s method requires the input to be a set of functional dependency fds with atomic right hand side therefore it is necessary to apply fragmentation rule with the consequent increasing of the input the main novelty of our rule is that they deal with generalized formula avoiding the fragmentation needed in the former tableau finally we illustrate the advantage of our new tableau method with an experiment 
the chance constrained stochastic programming ccsp is one of the model for decision making under uncertainty in this paper we consider the special case of the ccsp in which only the right hand side vector is random with a discrete distribution having a finite support the unit commitment problem is one of the application of the special case of the ccsp existing method for exactly solving the ccsp problem require an enumeration of scenario when they model a ccsp problem using a mixed integer programming mip we show how to reduce the number of scenario enumerated in the mip model in addition we give another compact mip formulation to approximately solve the ccsp problem 
to help user quickly understand the major opinion from massive online review it is important to automatically reveal the latent structure of the aspect sentiment polarity and the association between them however there is little work available to do this effectively in this paper we propose a hierarchical aspect sentiment model hasm to discover a hierarchical structure of aspect based sentiment from unlabeled online review in hasm the whole structure is a tree each node itself is a two level tree whose root represents an aspect and the child represent the sentiment polarity associated with it each aspect or sentiment polarity is modeled a a distribution of word to automatically extract both the structure and parameter of the tree we use a bayesian nonparametric model recursive chinese restaurant process rcrp a the prior and jointly infer the aspect sentiment tree from the review text experiment on two real datasets show that our model is comparable to two other hierarchical topic model in term of quantitative measure of topic tree it is also shown that our model achieves better sentence level classification accuracy than previously proposed aspect sentiment joint model copyright association for the advancement of artificial intelligence www aaai org all right reserved 
in this paper we present a new algorithm for generatively learning the structure of markov logic network this algorithm relies on a graph of predicate which summarizes the link existing between predicate and on relational information between ground atom in the training database candidate clause are produced by mean of a heuristical variabilization technique according to our first experiment this approach appears to be promising 
all pay auction a common mechanism for various human and agent interaction suffers like many other mechanism from the possibility of player failure to participate in the auction we model such failure and show how they affect the equilibrium state revealing various property such a the lack of influence of the most likely to participate player on the behavior of the other player we perform this analysis with two scenario the sum profit model where the auctioneer obtains the sum of all submitted bid and the max profit model of crowd sourcing contest where the auctioneer can only use the best submission and thus obtains only the winning bid furthermore we examine various method of influencing the probability of participation such a the effect of misreporting one s own probability of participating and how influencing another player s participation chance e g sabotage change the player s strategy 
null 
the pharmaceutical industry consumer protection group user of medication and government oversight agency are all strongly interested in identifying adverse reaction to drug while a clinical trial of a drug may use only a thousand patient once a drug is released on the market it may be taken by million of patient a a result in many case adverse drug event ade are observed in the broader population that were not identified during clinical trial therefore there is a need for continued post marketing surveillance of drug to identify previously unanticipated ade this paper cast this problem a a reverse machine learning task related to relational subgroup discovery and provides an initial evaluation of this approach based on experiment with an actual emr ehr and known adverse drug event 
we address a difficult yet under investigated class of planning problem fully observable nondeterministic fond planning problem with strong cyclic solution the difficulty of these strong cyclic fond planning problem stem from the large size of the state space hence to achieve efficient planning a planner ha to cope with the explosion in the size of the state space by planning along the direction that allow the goal to be reached quickly a major challenge is how would one know which state and search direction are relevant before the search for a solution ha even begun we first describe an ndp motivated strong cyclic algorithm that without addressing the above challenge can already outperform state of the art fond planner and then extend this ndp motivated planner with a novel heuristic that address the challenge 
semi supervised learning algorithm commonly incorporate the available background knowledge such that an expression of the derived model s quality is improved depending on the specific context quality can take several form and can be related to the generalization performance or to a simple clustering coherence measure recently a novel perspective of semi supervised learning ha been put forward that associate semi supervised clustering with the efficiency of spectral method more precisely it ha been demonstrated that the appropriate use of partial supervision can bias the data laplacian matrix such that the necessary eigenvector computation are provably accelerated this result allows data mining practitioner to use background knowledge not only for improving the quality of clustering result but also for accelerating the required computation in this paper we initially provide a high level overview of the relevant efficiency maximizing semi supervised method such that their theoretical intuition are comprehensively outlined consecutively we demonstrate how these method can be extended to handle multiple cluster and also discus possible issue that may arise in the continuous semi supervised solution finally we illustrate the proposed extension empirically in the context of text clustering 
energy system researcher are proposing a broad range of future smart energy infrastructure to promote more efficient management of energy resource this paper considers how consumer might relate to these future smart grid within the uk to address this challenge we exploited a combination of demonstration and animated sketch to convey the nature of a future smart energy infrastructure based on software agent user reaction suggested that although they felt an obligation to engage with energy issue they were principally disinterested user showed a considerable lack of trust in energy company raising a dilemma of design while user might welcome agent to help in engaging with complex energy infrastructure they had little faith in those that might provide them this suggests the need to consider how to design software agent to enhance trust in these socio economic setting 
supervised feature selection determines feature relevance by evaluating feature s correlation with the class joint minimization of a classifier s loss function and an l norm regularization ha been shown to be effective for feature selection however the appropriate feature subset learned from different classifier loss function may be different le effort ha been made on improving the performance of feature selection by the ensemble of different classifier criterion and take advantage of them furthermore for the case when only a few labeled data per class are available over fitting would be a potential problem and the performance of each classifier is restrained in this paper we add a joint l norm on multiple feature selection matrix to ensemble different classifier loss function into a joint optimization framework this added co regularization term ha twofold role in enhancing the effect of regularization for each criterion and uncovering common irrelevant feature the problem of over fitting can be alleviated and thus the performance of feature selection is improved extensive experiment on different data type demonstrates the effectiveness of our algorithm 
single peakedness is one of the most commonly used domain restriction in social choice however the extent to which agent preference are single peaked in practice and the extent to which recent proposal for approximate single peakedness can further help explain voter preference is unclear in this article we ass the ability of both single dimensional and multi dimensional approximation to explain preference profile drawn from several real world election we develop a simple branch and bound algorithm that find multi dimensional single peaked ax that best fit a given profile and which work with several form of approximation empirical result on two election data set show that preference in these election are far from single peaked in any one dimensional space but are nearly single peaked in two dimension our algorithm are reasonably efficient in practice and also show excellent anytime performance 
rcc is an important and well known calculus for representing and reasoning about mereological relation among many other application it is pivotal in the formalization of commonsense reasoning about natural category in particular it allows for a qualitative representation of conceptual space in the sense of g rdenfors to further the role of rcc a a vehicle for conceptual reasoning in this paper we combine rcc relation with information about betweenness of region the resulting calculus allows u to express for instance that some part but not all of region b is between region a and c we show how consistency can be decided in polynomial time for atomic network even when region are required to be convex from an application perspective the ability to express betweenness information allows u to use rcc a a basis for interpolative reasoning while the restriction to convex region ensures that all consistent network can be faithfully represented a a conceptual space 
people s facial expression whether made consciously or subconsciously continuously reveal their state of mind this work proposes a method for predicting people s strategic decision based on their facial expression we designed a new version of the centipede game that intorduces an incentive for the human participant to hide her facial expression we recorded on video participant who played several game of our centipede version and concurrently logged their decision throughout the game the video snippet of the participant face prior to their decision is represented a a fixed size vector by estimating the covariance matrix of key facial point which change over time this vector serf a input to a classifier that is trained to predict the participant s decision we compare several training technique all of which are designed to work with the imbalanced decision typically made by the player of the game furthermore we investigate adaptation of the trained model to each player individually while taking into account the player s facial expression in the previous game the result show that our method outperforms standard svm a well a human in predicting subject strategic decision to the best of our knowledge this is the first study to present a methodology for predicting people s strategic decision when there is an incentive to hide facial expression 
we develop a novel circuit level stochastic local search sl method d crsat for boolean satisfiability by integrating a structure based heuristic into the recent crsat algorithm d crsat significantly improves on crsat on real world application benchmark on which other current cnf and circuit level sl method tend to perform weakly we also give an intricate proof of probabilistically approximate completeness for d crsat highlighting key feature of the method 
statutory record of underground utility apparatus such a pipe and cable are notoriously inaccurate so street survey are usually undertaken before road excavation take place to minimize the extent and duration of excavation and for health and safety reason this involves the use of sensor such a ground penetrating radar gpr the gpr scan are then manually interpreted and combined with the expectation from the utility record and other data such a surveyed manhole the task is complex owing to the difficulty in interpreting the sensor data and the spatial complexity and extent of under street asset we explore the application of ai technique in particular bayesian data fusion bdf to automatically generate map of buried apparatus hypothesis about the spatial location and direction of buried asset are extracted by identifying hyperbolae in the gpr scan the spatial location of surveyed manhole provides further input to the algorithm a well a the prior expectation from the statutory record these three data source are used to produce the most probable map of the buried asset experimental result on real and simulated data set are presented 
we study the relationship between optimal planning algorithm in the form of iterative deepening a with forward state space search and the reduction of the problem to sat our result establish a strict dominance relation between the two approach any iterative deepening a search can be efficiently simulated in the sat framework assuming that the heuristic ha been encoded in the sat problem but the opposite is not possible a a and ida search sometimes take exponentially longer 
the community based generation of content ha been tremendously successful in the world wide web people help each other by providing information that could be useful to others we are trying to transfer this approach to robotics in order to help robot acquire the vast amount of knowledge needed to competently perform everyday task roboearth is intended to be a web community by robot for robot to autonomously share description of task they have learned object model they have created and environment they have explored in this paper we report on the formal language we developed for encoding this information and present our approach to solve the inference problem related to finding information to determining if information is usable by a robot and to grounding it on the robot platform 
in this paper we describe how we integrated an artificial intelligence ai system into the pubmed search website using augmented browsing technology our system dynamically enriches the pubmed search result displayed in a user s browser with semantic annotation provided by several natural language processing nlp subsystem including a sentence splitter a part of speech tagger a named entity recognizer a section categorizer and a gene normalizer gn after our system is installed the pubmed search result page is modified on the fly to categorize section and provide additional information on gene and gene product indentified by our nlp subsystem in addition gn involves three main step candidate id matching false positive filtering and disambiguation which are highly dependent on each other we propose a joint model using a markov logic network mln to model the dependency found in gn the experimental result show that our joint model outperforms a baseline system that executes the three step separately the developed system is available at http site google com site pubmedannotationtool ijcai home 
most of the sequential pattern extraction method proposed so far deal with pattern composed of event linked by temporal relationship based on simple precedence between instant in many real situation some quantitative information about event duration or inter event delay is necessary to discriminate phenomenon we propose the algorithm qtiprefixspan for extracting temporal pattern composed of event to which temporal interval describing their position in time and their duration are associated it extends algorithm prefixspan with a multi dimensional interval clustering step for extracting the representative temporal interval associated to event in pattern experiment on simulated data show that our algorithm is efficient for extracting precise pattern even in noisy context and that it improves the performance of a former algorithm which used a clustering method based on the em algorithm 
this paper detail the development and evaluation of astontac an energy broker that successfully participated in the power trading agent competition power tac astontac buy electrical energy from the wholesale market and sell it in the retail market the main focus of the paper is on the broker s bidding strategy in the wholesale market in particular it employ markov decision process mdp to purchase energy at low price in a day ahead power wholesale market and keep energy supply and demand balanced moreover we explain how the agent us non homogeneous hidden markov model nhhmm to forecast energy demand and price an evaluation and analysis of the power tac final show that astontac is the only agent that can buy energy at low price in the wholesale market and keep energy imbalance low 
the computational complexity of relevant core related question for coalitional game is addressed from the coalition structure viewpoint i e without assuming that the grand coalition necessarily form in the analysis game are assumed to be in compact form i e their worth function are implicitly given a polynomial time computable function over succinct game encoding provided a input within this setting a complete picture of the complexity issue arising with the core a well a with the related stability concept of least core and cost of stability is depicted in particular the special case of superadditive game and of game whose set of feasible coalition are restricted over tree like interaction graph are also studied 
collaborative filtering a widely used user centric recommendation technique predicts an item s rating by aggregating it rating from similar user user similarity is usually calculated by cosine similarity or pearson correlation coefficient however both of them consider only the direction of rating vector and suffer from a range of drawback to solve these issue we propose a novel bayesian similarity measure based on the dirichlet distribution taking into consideration both the direction and length of rating vector further our principled method reduces correlation due to chance experimental result on six real world data set show that our method achieves superior accuracy 
planning under uncertainty for multiagent system can be formalized a a decentralized partially observable markov decision process we advance the state of the art for optimal solution of this model building on the multiagent a heuristic search method a key insight is that we can avoid the full expansion of a search node that generates a number of child that is doubly exponential in the node s depth instead we incrementally expand the child only when a next child might have the highest heuristic value we target a subsequent bottleneck by introducing a more memory efficient representation for our heuristic function proof is given that the resulting algorithm is correct and experiment demonstrate a significant speedup over the state of the art allowing for optimal solution over longer horizon for many benchmark problem 
we propose a logical framework to represent and reason about agent interaction in normative system our starting point is a dynamic logic of propositional assignment whose satisfiability problem is pspace complete we show that it embeds coalition logic of propositional control cl pc and that various notion of ability and capability can be captured in it we illustrate it on a water resource management case study finally we show how the logic can be easily extended in order to represent constitutive rule which are also an essential component of the modelling of social reality 
learning for sentence re writing is a fundamental task in natural language processing and information retrieval in this paper we propose a new class of kernel function referred to a string rewriting kernel to address the problem a string re writing kernel measure the similarity between two pair of string it can capture the lexical and structural similarity between sentence pair without the need of constructing syntactic tree we further propose an instance of string re writing kernel which can be computed efficiently experimental result on benchmark datasets show that our method can achieve comparable result with state of the art method on two sentence rewriting learning task paraphrase identification and recognizing textual entailment 
special purpose constraint propagation algorithm such a those for the element constraint frequently make implicit use of short support by examining a subset of the variable they can infer support for all other variable and value and save substantial work however to date general purpose propagation algorithm such a gac schema rely upon support involving all variable we demonstrate how to employ short support in a new general purpose propagation algorithm called shortgac this work when provided with either an explicit list of allowed short tuples or a function to calculate the next supporting short tuple empirical analysis demonstrate the efficiency of shortgac compared to other general purpose propagation algorithm in some case shortgac even exhibit similar performance to special purpose propagator 
graphical model are one of the most prominent framework to model complex system and efficiently query them their underlying algebraic property are captured by a valuation structure that most usually is a semiring depending on the semiring of choice we can capture probabilistic model constraint network cost network etc in this paper we address the partitioning problem which occurs in many approximation technique such a mini bucket elimination and joingraph propagation algorithm roghly speaking subject to complexity bound the algorithm need to find a partition of a set of factor such that best approximates the whole set while this problem ha been addressed in the past in a particular case we present here a general description furthermore we also propose a general partitioning scheme our proposal is general in the sense that it is presented in term of a generic semiring with the only additional requirement of a division operation and a refinement of it order the proposed algorithm instantiates to the particular task of computing the probability of evidence but also applies directly to other important reasoning task we demonstrate it good empirical behaviour on the problem of computing the most probable explanation 
replanning via determinization is a recent popular approach for online planning in mdps in this paper we adapt this idea to classical nonstochastic domain with partial information and sensing action at each step we generate a candidate plan which solves a classical planning problem induced by the original problem we execute this plan a long a it is safe to do so when this is no longer the case we replan the classical planning problem we generate is based on the t translation in which the classical state capture the knowledge state of the agent we overcome the non determinism in sensing action and the large domain size introduced by t by using state sampling our planner also employ a novel lazy regression based method for querying the belief state 
monte carlo tree search mcts ha proved a remarkably effective decision mechanism in many different game domain including computer go and general game playing ggp however in ggp where many disparate game are played certain type of game have proved to be particularly problematic for mcts one of the problem are game tree with so called optimistic move that is bad move that superficially look good but potentially require much simulation effort to prove otherwise such scenario can be difficult to identify in real time and can lead to suboptimal or even harmful decision in this paper we investigate a selection strategy for mcts to alleviate this problem the strategy called sufficiency threshold concentrate simulation effort better for resolving potential optimistic move scenario the improved strategy is evaluated empirically in an n arm bandit test domain for highlighting it property a well a in a state of the art ggp agent to demonstrate it effectiveness in practice the new strategy show significant improvement in both domain 
we report the result obtained during the verification of autosub an autonomous underwater vehicle used for deep oceanic exploration our starting point is the simulink matlab engineering model of the submarine which is discretised by a compiler into a representation suitable for model checking we ass the ability of the vehicle to function under degraded condition by injecting fault automatically into the discretised model the resulting system is analysed by mean of the model checker mcmas and conclusion are drawn on the system s ability to withstand fault and to perform self diagnosis and recovery we present lesson learnt from this and suggest a general method for verifying autonomous vehicle 
a new algorithm is developed in this paper to support automatic name face alignment for achieving more accurate cross medium news retrieval we focus on extracting valuable information from large amount of news image and their caption where multi level image caption pair are constructed for characterizing both significant name with higher salience and their cohesion with human face extracted from news image to remedy the issue of lacking enough related information for rare name web mining is introduced to acquire the extra multimodal information we also emphasize on an optimization mechanism by our improved self adaptive simulated annealing genetic algorithm to verify the feasibility of alignment combination our experiment have obtained very positive result 
this paper concern the creation of an efficient continuous non parametric representation of surface implicit in d laser data a typically recorded by mobile robot our approach explicitly leverage the probabilistic nature of gaussian process regression to provide for a principled adaptive subsampling which automatically prune redundant data the algorithm place no restriction on the complexity of the underlying surface and enables prediction at arbitrary location and density we present result using real and synthetic data and show that our approach attains decimation factor in excess of two order of magnitude without significant degradation in fidelity of the workspace reconstruction 
this paper considers matrix model a class of csps which generally exhibit significant symmetry it proposed the idea of lexleader feasibility checker that verify during search whether the current partial assignment can be extended into a canonical solution the feasibility checker are based on a novel result by katsirelos et al on how to check efficiently whether a solution is canonical the paper generalizes this result to partial assignment various variable ordering and value symmetry empirical result on standard benchmark show that feasibility checker may bring significant performance gain when jointly used with doublelex or snakelex 
fault diagnosis of web service composition at run time is appealing in creating a consolidated distributed application for this purpose we propose a hybrid model based diagnosis method which exploit service process description or historical execution information to enhance service composition model and localize fault by comparing the exceptional execution and the correct execution with the maximum likelihood experiment are conducted to evaluate the effectiveness of our method in web service composition fault diagnosis copyright association for the advancement of artificial intelligence www aaai org all right reserved 
in this paper the fixed point semantics developed in lobo et al is generalized to disjunctive logic program with default negation and over arbitrary structure and proved to coincide with the stable model semantics by using the tool of ultra product a preservation theorem which asserts that a disjunctive logic program without default negation is bounded with respect to the proposed semantics if and only if it ha a first order equivalent is then obtained for the disjunctive logic program with default negation a sufficient condition assuring the first order expressibility is also proposed 
principal component analysis pca is one of the most important method to handle high dimensional data however the high computational complexity make it hard to apply to the large scale data with high dimensionality and the used l norm make it sensitive to outlier a recent work proposed principal component analysis based on l normmaximization which is efficient and robust to outlier in that work a greedy strategy wa applied due to the difficulty of directly solving the l norm maximization problem which is easy to get stuck in local solution in this paper we first propose an efficient optimization algorithmto solve a general l norm maximization problem and then propose a robust principal component analysis with non greedy l norm maximization experimental result on real world datasets show that the nongreedy method always obtains much better solution than that of the greedy method 
recently ontology stream reasoning ha been introduced a a multidisciplinary approach merging synergy from artificial intelligence database world wide web to reason on semantic augmented data stream although knowledge evolution and real time reasoning have been largely addressed in ontology stream the challenge of predicting it future or missing knowledge remains open and yet unexplored we tackle predictive reasoning a a correlation and interpretation of past semantics augmented data over exogenous ontology stream consistent prediction are constructed a description logic entailment by selecting and applying relevant cross stream association rule the experiment have shown accurate prediction with real and live stream data from dublin city in ireland 
planning with partial observability can be formulated a a non deterministic search problem in belief space the problem is harder than classical planning a keeping track of belief is harder than keeping track of state and searching for action policy is harder than searching for action sequence in this work we develop a framework for partial observability that avoids these limitation and lead to a planner that scale up to larger problem for this the class of problem is restricted to those in which the non unary clause representing the uncertainty about the initial situation are invariant and variable that are hidden in the initial situation do not appear in the body of conditional effect which are all assumed to be deterministic we show that such problem can be translated in linear time into equivalent fully observable non deterministic planning problem and that an slight extension of this translation render the problem solvable by mean of classical planner the whole approach is sound and complete provided that in addition the state space is connected experiment are also reported 
multiple task learning mtl is becoming popular due to it theoretical advance and empirical success the key idea of mtl is to explore the hidden relationship among multiple task to enhance learning performance recently many mtl algorithm have been developed and applied to various problem such a feature selection and kernel learning however most existing method highly relied on certain assumption of the task relationship for instance several work assumed that there is a major task group and several outlier task and used a decomposition approach to identify the group structure and outlier task simultaneously in this paper we adopt a more general formulation for mtl without making specific structure assumption instead of performing model decomposition we directly impose an elastic net regularization with a mixture of the structure and outlier penalty and formulate the objective a an unconstrained convex problem to derive the optimal solution efficiently we propose to use an iteratively reweighted least square irls method with a preconditioned conjugate gradient which is computationally affordable for high dimensional data extensive experiment are conducted over both synthetic and real data and comparison with several state of the art algorithm clearly show the superior performance of the proposed method 
we present algorithm for generating alternative solution for explicit acyclic and or structure in non decreasing order of cost our algorithm use a best first search technique and report the solution using an implicit representation ordered by cost experiment on randomly constructed and or dag and problem domain including matrix chain multiplication finding the secondary structure of rna etc show that the proposed algorithm perform favorably to the existing approach in term of time and space 
based on psychological attribution theory this paper present a domain independent computational model to automate social causality and responsibility judgment according to an agent s causal knowledge and observation of interaction the proposed model is also empirically validated via experimental study 
classical constraint satisfaction problem csps are commonly defined on finite domain in real life constrained variable can evolve over time a variable can actually take an infinite sequence of value over discrete time point in this paper we propose constraint programming on infinite data stream which provides a natural way to model constrained time varying problem in our framework variable domain are specified by regular language we introduce special stream operator a basis to form stream expression and constraint stream csps have infinite search space we propose a search procedure that can recognize and avoid infinite search over duplicate search space the solution set of a stream csp can be represented by a b chi automaton allowing stream value to be non periodic consistency notion are defined to reduce the search space early we illustrate the feasibility of the framework by example and experiment 
in this paper we present a novel user interface that integrates two popular approach to language translation for traveler allowing multimodal communication between the party involved the picture book in which the user simply point to multiple picture icon representing what they want to say and the statistical machine translation smt system that can translate arbitrary word sequence our prototype system tightly couple both process within a translation framework that inherits many of the the positive feature of both approach while at the same time mitigating their main weakness our system differs from traditional approach in that it mode of input is a sequence of picture rather than text or speech text in the source language is generated automatically and is used a a detailed representation of the intended meaning the picture sequence which not only provides a rapid method to communicate basic concept but also give a second opinion on the machine transition output that catch machine translation error and allows the user to retry the translation avoiding misunderstanding 
predicting trust among the agent is of great importance to various open distributed setting e g emarket peer to peer network etc in that dishonest agent can easily join the system and achieve their goal by circumventing agreed rule or gaining unfair advantage etc most existing trust mechanism derive trust by statistically investigating the target agent s historical information however even if rich historical information is available it is challenging to model an agent s behavior since an intelligent agent may strategically change it behavior to maximize it profit we therefore propose a trust prediction approach to capture dynamic behavior of the target agent specifically we first identify feature which are capable of describing representing context of a transaction then we use these feature to measure similarity between context of the potential transaction and that of previous transaction to estimate trustworthiness of the potential transaction based on previous similar transaction outcome evaluation using real auction data and synthetic data demonstrates efficacy of our approach in comparison with an existing representative trust mechanism 
in past decade more and more data are collected from multiple source or represented by multiple view where different view describe distinct perspective of the data although each view could be individually used for finding pattern by clustering the clustering performance could be more accurate by exploring the rich information among multiple view several multi view clustering method have been proposed to unsupervised integrate different view of data however they are graph based approach e g based on spectral clustering such that they cannot handle the large scale data how to combine these heterogeneous feature for unsupervised large scale data clustering ha become a challenging problem in this paper we propose a new robust large scale multi view clustering method to integrate heterogeneous representation of largescale data we evaluate the proposed new method by six benchmark data set and compared the performance with several commonly used clustering approach a well a the baseline multi view clustering method in all experimental result our proposed method consistently achieve superior clustering performance 
a a doxastic counterpart to epistemic logic based on s we study the modal logic ksd that can be viewed a an approach to modelling a kind of objective and fair belief we apply ksd to the problem of minimal belief and develop an alternative approach to nonmonotonic modal logic using a weaker concept of expansion this corresponds to a certain minimal kind of ksd model and yield a new type of nonmonotonic doxastic reasoning 
network alarm triage refers to grouping and prioritizing a stream of low level device health information to help operator find and fix problem today this process tends to be largely manual because existing rule based tool cannot easily evolve with the network we present cuet a system that us interactive machine learning to constantly learn from the triaging decision of operator it then us that learning in novel visualization to help them quickly and accurately triage alarm unlike prior interactive machine learning system cuet handle a highly dynamic environment where the group of interest are not known a priori and evolve constantly our evaluation with real operator and data from a large network show that cuet significantly improves the speed and accuracy of alarm triage 
some of the most successful machine learning algorithm such a support vector machine are based on learning linear and kernel predictor with respect to a convex loss function such a the hinge loss for classification purpose a more natural loss function is the loss however using it lead to a non convex problem for which there is no known efficient algorithm in this paper we describe and analyze a new algorithm for learning linear or kernel predictor with respect to the loss function the algorithm is parameterized by l which quantifies the effective width around the decision boundary in which the predictor may be uncertain we show that without any distributional assumption and for any fixed l the algorithm run in polynomial time and learns a classifier which is worse than the optimal such classifier by at most we also prove a hardness result showing that under a certain cryptographic assumption no algorithm can learn such classifier in time polynomial in l 
this paper concern building probabilistic model with an underlying ontology that defines the class and property used in the model in particular it considers the problem of reasoning with property that may not always be defined furthermore we may even be uncertain about whether a property is defined for a given individual one approach is to explicitly add a value undefined to the range of random variable forming extended belief network however adding an extra value to a random variable s range ha a large computational overhead in this paper we propose an alternative ontologically based belief network where all property are only used when they are defined and we show how probabilistic reasoning can be carried out without explicitly using the value undefined during inference we prove this is equivalent to reasoning with the corresponding extended belief network and empirically demonstrate that inference becomes more efficient 
belief tracking is a basic problem in planning with sensing while the problem is intractable it ha been recently shown that for both deterministic and non deterministic system expressed in compact form it can be done in time and space that are exponential in the problem width the width measure the maximum number of state variable that are all relevant to a given precondition or goal in this work we extend this result both theoretically and practically first we introduce an alternative decomposition scheme and algorithm with the same time complexity but different completeness guarantee whose space complexity is much smaller exponential in the causal width of the problem that measure the number of state variable that are causally relevant to a given precondition goal or observable second we introduce a fast meaningful and powerful approximation that trade completeness by speed and is both time and space exponential in the problem causal width it is then shown empirically that the algorithm combined with simple heuristic yield state of the art real time performance in domain with high width but low causal width such a minesweeper battleship and wumpus 
cost based abduction cba is an ai model for reasoning under uncertainty in cba evidence to be explained is treated a a goal which is true and must be proven each proof of the goal is viewed a a feasible explanation and ha a cost equal to the sum of the cost of all hypothesis that are assumed to complete the proof the aim is to find the least cost proof this paper us cba to develop a novel method for modeling genetic regulatory network grn and explaining genetic knock out effect constructing grn using multiple data source is a fundamental problem in computational biology we show that cba is a powerful formalism for modeling grn that can easily and effectively integrate multiple source of biological data in this paper we use three different biological data source protein dna protein protein and gene knock out data using this data we first create an un annotated graph cba then annotates the graph by assigning a sign and a direction to each edge our biological result are promising however this manuscript focus on the mathematical modeling of the application the advantage of cba and it relation to bayesian inference are also presented 
we consider stochastic multiarmed bandit problem where each arm generates i i d reward according to an unknown distribution whereas classical bandit solution only maximize the expected reward we consider the problem of minimizing risk using notion such a the value at risk the average value at risk and the mean variance risk we present algorithm to minimize the risk over a single and multiple time period along with pac accuracy guarantee given a finite number of reward sample in the single period case we show that finding the arm with least risk requires not many more sample than the arm with highest expected reward although minimizing the multiperiod value at risk is known to be hard we present an algorithm with comparable sample complexity under additional assumption 
the paper investigates a novel approach based on constraint logic programming clp to predict potential d conformation of a protein via fragment assembly the fragment are extracted and clustered by a preprocessor from a database of known protein structure assembling fragment into a complete conformation is modeled a a constraint satisfaction problem solved using clp the approach make use of a simplified c side chain centroid protein model that offer efficiency and a good approximation for space filling the approach adapts existing energy model for protein representation and applies a large neighboring search lns strategy the result show the feasibility and efficiency of the method and the declarative nature of the approach simplifies the introduction of additional knowledge and variation of the model 
grammatical agreement is present in many of the world s language today and ha become an essential feature that guide linguistic processing when two word in a sentence are said to agree this mean that they share certain feature such a gender number person or others the primary hypothesis of this paper is that marking agreement within one linguistic phrase reduces processing effort a phrasal constituent can more easily be recognized the drive to reduce processing effort introduces the rise of agreement marking in a population of multiple agent by mean of an incrementally aligned mapping between the most discriminatory feature of a particular linguistic unit and their associative marker a series of experiment compare feature selection method for one to one agreement mapping and show how an agreement system can be bootstrapped 
we address the problem of forecasting the usage of multiple electrical appliance by domestic user with the aim of providing suggestion about the best time to run appliance in order to reduce carbon emission and save money assuming time of use pricing while minimising the impact on the user daily habit an important challenge related to this problem is the modelling the everyday routine of the consumer and of the interdependency between the use of different appliance given this we develop an important building block of future home energy management system a prediction algorithm based on a graphical model that capture the everyday habit and the inter dependency between appliance by exploiting their periodic feature we demonstrate through extensive empirical evaluation on real world data from a prominent database that our approach outperforms existing method by up to 
recent advance in solution to hybrid mdps with discrete and continuous state and action space have significantly extended the class of mdps for which exact solution can be derived albeit at the expense of a restricted transition noise model in this paper we work around limitation of previous solution by adopting a robust optimization approach in which nature is allowed to adversarially determine transition noise within pre specified confidence interval this allows one to derive an optimal policy with an arbitrary user specified level of success probability and significantly extends the class of transition noise model for which hybrid mdps can be solved this work also significantly extends result for the related chance constrained approach in stochastic hybrid control to accommodate state dependent noise we demonstrate our approach working on a variety of hybrid mdps taken from ai planning operation research and control theory noting that this is the first time robust solution with strong guarantee over all state have been automatically derived for such problem 
this paper present a unified framework for intra view and inter view constraint propagation on multi view data pairwise constraint propagation ha been studied extensively where each pair wise constraint is defined over a pair of data point from a single view in contrast very little attention ha been paid to inter view constraint propagation which is more challenging since each pair wise constraint is now defined over a pair of data point from different view although both interview and inter view constraint propagation are crucial for multi view task most previous method can not handle them simultaneously to address this challenging issue we propose to decompose these two type of constraint propagation into semi supervised learning sub problem so that they can be uniformly solved based on the traditional label propagation technique to further integrate them into a unified framework we utilize the result of intra view constraint propagation to adjust the similarity matrix of each view and then perform inter view constraint propagation with the adjusted similarity matrix the experimental result in cross view retrieval have shown the superior performance of our unified constraint propagation association for the advancement of artificial intelligence www aaai org all right reserved 
in this work we consider function free existential rule extended with nonmonotonic negation under a stable model semantics we present new acyclicity and stratification condition that identify a large class of rule set having finite unique stable model and we show how the addition of constraint on the input fact can further extend this class checking these condition is computationally feasible and we provide tight complexity bound finally we demonstrate how these new method allowed u to solve relevant reasoning problem over a real world knowledge base from biochemistry using an off the shelf answer set programming engine 
with the progress in machine translation it becomes more subtle to develop the evaluation metric capturing the system difference in comparison to the human translation in contrast to the current effort in leveraging more linguistic information to depict translation quality this paper take the thread of combining language independent feature for a robust solution to mt evaluation metric to compete with finer granularity of modeling brought by linguistic feature the proposed method augments the word level metric by a letter based calculation an empirical study is then conducted over wmt data to train the metric by ranking svm the result reveal that the integration of current language independent metric can generate well enough performance for a variety of language time split data validation is promising a a better training setting though the greedy strategy also work well 
relational topic model have shown promise on analyzing document network structure and discovering latent topic representation this paper present three extension unlike the common link likelihood with a diagonal weight matrix that allows the same topic interaction only we generalize it to use a full weight matrix that capture all pairwise topic interaction and is applicable to asymmetric network instead of doing standard bayesian inference we perform regularized bayesian inference with a regularization parameter to deal with the imbalanced link structure issue in common real network and instead of doing variational approximation with strict mean field assumption we present a collapsed gibbs sampling algorithm for the generalized relational topic model without making restricting assumption experimental result demonstrate the significance of these extension on improving the prediction performance and the time efficiency can be dramatically improved with a simple fast approximation method 
we construct an homogeneous and categorical representation of the relation algebra rcc which is one of the fundamental formalism for spatial reasoning a a consequence we obtain that the network consistency problem for rcc can be solved in polynomial time for network of bounded treewidth 
willow is a free text adaptive computer assisted assessment system which support natural language processing and user modeling in this paper we discus the benefit coming from extending willow with recommendation the approach combine human computer interaction method to elicit the recommendation with data mining technique to adjust their definition following a scenario based approach recommendation were designed and delivered in a large scale evaluation with learner a statistically significant positive impact wa found on indicator dealing with the engagement in the course the learning effectiveness and efficiency a well a the knowledge acquisition we present the overall system functionality the interaction among the different subsystem involved and some evaluation finding 
artifact centric system are a novel paradigm in service oriented computing in the present contribution we show that model checking bounded non uniform artifact centric system is undecidable we provide a partial model checking procedure for artifact centric system against the universal fragment of a first order version of the logic ctl we obtain this result by introducing a counterpart semantics and developing an abstraction methodology operating on these structure this enables u to generate finite abstraction of infinite artifact centric system hence perform verification on abstract model 
the environment is an essential component of multi agent system and is often used to coordinate the behaviour of individual agent recently many language have been proposed to specify and implement multi agent environment in term of social and normative concept in this paper we first introduce a formal setting of multi agent environment which abstract from concrete specification language we extend this formal setting with norm and sanction and show how concept from mechanism design can be used to formally analyse and verify whether specific normative behaviour can be enforced or implemented if agent follow their subjective preference we also consider complexity issue of associated problem 
we develop a real time algorithm based on a monte carlo game tree search for solving a quantified constraint satisfaction problem qcsp which is a csp where some variable are universally quantified a universally quantified variable represents a choice of nature or an adversary the goal of a qcsp is to make a robust plan against an adversary however obtaining a complete plan off line is intractable when the size of the problem becomes large thus we need to develop a realtime algorithmthat sequentially selects a promising value at each deadline such a problem ha been considered in the field of game tree search in a standard game tree search algorithm developing a good static evaluation function is crucial however developing a good static evaluation function for a qcsp is very difficult since it must estimate the possibility that a partially assigned qcsp is solvable thus we apply a monte carlo game tree search technique called uct however the simple application of the uct algorithm doe not work since the player and the adversary are asymmetric i e finding a game sequence where the player win is very rare we overcome this difficulty by introducing constraint propagation technique we experimentally compare the winning probability of our uct based algorithm and the state of the art alpha beta search algorithm our result show that our algorithm outperforms the state of the art algorithm in large scale problem 
we present partial tree linearization a generalized word ordering i e ordering a set of input word into a grammatical and fluent sentence task for text to text application recent study of word ordering can be categorized into either abstract word ordering no input syntax except for po or tree linearization input word are associated with a full unordered syntax tree partial tree linearization cover the whole spectrum of input between these two extreme by allowing po and dependency relation to be associated with any subset of input word partial tree linearization is more practical for a dependency based nlg pipeline such a transfer based mt and abstractive text summarization in addition a partial tree linearizer can also perform abstract word ordering and full tree linearization our system achieves the best published result on standard ptb evaluation of these task 
researcher have made significant stride in developing recognition technique for surface sketch with realized and potential application to motivate extending these technique towards analogous surfaceless sketch yet surface sketch recognition technique remain largely untested in surfaceless environment and are still highly constrained for related surfaceless gesture recognition technique the focus of the research is to investigate the performance of surface sketch recognition technique in more challenging surfaceless environment with the aim of addressing existing limitation through improved surfaceless sketch recognition technique 
multiobjective dynamic programming modp is a general problem solving method used to determine the set of pareto optimal solution in optimization problem involving discrete decision variable and multiple objective it applies to combinatorial problem in which pareto optimality of a solution extends to all it sub solution bellman principle in this paper we focus on the determination of the preferred tradeoff in the pareto set where preference is measured by a choquet integral this model provides high descriptive possibility but the associated preference generally do not meet the bellman principle thus preventing any straightforward adaptation of modp to overcome this difficulty we introduce here a general family of dominance rule enabling an early pruning of some pareto optimal sub solution that cannot lead to a choquet optimum within this family we identify the most efficient dominance rule and show how they can be incorporated into a modp algorithm then we report numerical test showing the actual efficiency of this approach to find choquet optimal tradeoff in multiobjective knapsack problem 
automatically discovering cross lingual link cl between wikis can largely enrich the cross lingual knowledge and facilitate knowledge sharing across different language in most existing approach for cross lingual knowledge linking the seed cl and the inner link structure are two important factor for finding new cl when there are insufficient seed cl and inner link discovering new cl becomes a challenging problem in this paper we propose an approach that boost cross lingual knowledge linking by concept annotation given a small number of seed cl and inner link our approach first enriches the inner link in wikis by using concept annotation method and then predicts new cl with a regression based learning model these two step mutually reinforce each other and are executed iteratively to find a many cl a possible experimental result on the english and chinese wikipedia data show that the concept annotation can effectively improve the quantity and quality of predicted cl with seed cl and of the original inner link in wikipedia our approach discovered more cl in four run when using concept annotation 
there are many significant research project focused on providing semantic web repository that are scalable and efficient however the true value of the semantic web architecture is it ability to represent meaningful knowledge and not just data therefore a semantic web knowledge base should do more than retrieve collection of triple we propose rdfkb resource description knowledge base a complete semantic web knowledge case rdfkb is a solution for managing persisting and querying semantic web knowledge our experiment with real world and synthetic datasets demonstrate that rdfkb achieves superior query performance to other state of the art solution the key feature of rdfkb that differentiate it from other solution are a simple and efficient process for data addition deletion and update that doe not involve reprocessing the dataset materialization of inferred triple at addition time without performance degradation materialization of uncertain information and support for query involving probability distributed inference across datasets ability to apply alignment to the dataset and perform query against multiple source using alignment rdfkb allows more knowledge to be stored and retrieved it is a repository not just for rdf datasets but also for inferred triple probability information and lineage information rdfkb provides a complete and efficient rdf data repository and knowledge base 
distributed constraint optimization problem dcops are a model for representing multi agent system in which agent cooperate to optimize a global objective the dcop model ha two main advantage it can represent a wide range of problem domain and it support the development of generic algorithm to solve them firstly this paper present some advance in both complete and approximate dcop algorithm secondly it explains that the dcop model make a number of unrealistic assumption that severely limit it range of application finally it point out hint on how to tackle such limitation 
the borda voting rule is a positional scoring rule where for m candidate for every vote the first candidate receives m point the second m point and so on a borda winner is a candidate with highest total score it ha been a prominent open problem to determine the computational complexity of unweighted coalitional manipulation under borda can one add a certain number of additional vote called manipulator to an election such that a distinguished candidate becomes a winner we settle this open problem by showing np hardness even for two manipulator and three input vote moreover we discus extension and limitation of this hardness result 
recent year have witnessed the growing popularity of hashing for efficient large scale similarity search it ha been shown that the hashing quality could be boosted by hash function learning hfl in this paper we study hfl in the context of multimodal data for cross view similarity search we present a novel multimodal hfl method called parametric local multimodal hashing plmh which learns a set of hash function to locally adapt to the data structure of each modality to balance locality and computational efficiency the hashing projection matrix of each instance is parameterized with guaranteed approximation error bound a a linear combination of basis hashing projection of a small set of anchor point a local optimal conjugate gradient algorithm is designed to learn the hash function for each bit and the overall hash code are learned in a sequential manner to progressively minimize the bias experimental evaluation on cross medium retrieval task demonstrate that plmh performs competitively against the state of the art method 
it is a classic result in database theory that conjunctive query cq answering which is np complete in general is feasible in polynomial time when restricted to acyclic query subsequent result identified more general structural property of cqs like bounded treewidth which ensure tractable query evaluation in this paper we lift these tractability result to knowledge base formulated in the lightweight description logic dl lite and elh the proof exploit known property of query match in these logic and involves a query dependent modification of the data to obtain a more practical approach we propose a concrete polynomial time algorithm for answering acyclic cqs based on rewriting query into datalog program a preliminary evaluation suggests the interest of our approach for handling large acyclic cqs 
most time series data mining algorithm use similarity search a a core subroutine and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithm the difficulty of scaling search to large datasets largely explains why most academic work on time series data mining ha plateaued at considering a few million of time series object while much of industry and science sits on billion of time series object waiting to be explored in this work we show that by using a combination of four novel idea we can search and mine truly massive time series for the first time we demonstrate the following extremely unintuitive fact in large datasets we can exactly search under dtw much more quickly than the current state of the art euclidean distance search algorithm we demonstrate our work on the largest set of time series experiment ever attempted we show that our idea allow u to solve higher level time series data mining problem at scale that would otherwise be untenable 
empty element ee play a critical role in chinese syntactic semantic and discourse analysis previous study employ a language independent sentence level approach to ee recovery by casting it a a linear tagging or structured parsing problem in comparison this paper proposes a clause level hybrid approach to address specific problem in chinese ee recovery which recovers ee in chinese language from the clause perspective and integrates the advantage of both linear tagging and structured parsing in particular a comma disambiguation method is employed to improve syntactic parsing and help determine clause in chinese in this way the noise introduced by sentence level syntactic parsing and multiple ee in the same position of a linear sentence can be well addressed evaluation on chinese treebank show the significant performance improvement of our clause level hybrid approach over the state of the art sentence level baseline and it great impact on a state of the art chinese syntactic parser 
large scale observational datasets are prevalent in many area of research including biomedical informatics computational social science and finance however our ability to use these data for decision making lag behind our ability to collect and mine them one reason for this is the lack of method for inferring the causal impact of rare event in case such a the monitoring of continuous data stream from intensive care patient social medium or finance though rare event may in fact be the most important one signaling critical change in a patient s status or trading volume while prior data mining approach can identify or predict rare event they cannot determine their impact and probabilistic causal inference method fail to handle inference with infrequent event instead we develop a new approach to finding the causal impact of rare event that leverage the large amount of data available to infer a model of a system s functioning and evaluates how rare event explain deviation from usual behavior using simulated data we evaluate the approach and compare it against others demonstrating that it can accurately infer the effect of rare event 
this paper present a reranking approach to combining constituent and dependency parsing aimed at improving parsing performance on both side most previous combination method rely on complicated joint decoding to integrate graphand transition based dependency model instead our approach make use of a high performance probabilistic context free grammar pcfg model to output k best candidate constituent tree and then a dependency parsing model to rerank the tree by their score from both model so a to get the most probable parse experimental result show that this reranking approach achieves the highest accuracy of constituent and dependency parsing on chinese treebank ctb and a comparable performance to the state of the art on english treebank wsj 
determining protein function constitutes an exercise in integrating information derived from several heterogeneous high throughput experiment to utilize the information spread across multiple source in a combined fashion these data source are transformed into kernel several protein function prediction method follow a two phased approach they first optimize the weight on individual kernel to produce a composite kernel and then train a classifier on the composite kernel a such these method result in an optimal composite kernel but not necessarily in an optimal classifier on the other hand some method optimize the loss of binary classifier and learn weight for the different kernel iteratively a protein ha multiple function and each function can be viewed a a label these method solve the problem of optimizing weight on the input kernel for each of the label this is computationally expensive and ignores inter label correlation in this paper we propose a method called protein function prediction by integrating multiple kernel promk promk iteratively optimizes the phase of learning optimal weight and reducing the empirical loss of a multi label classifier for each of the label simultaneously using a combined objective function promk can assign larger weight to smooth kernel and downgrade the weight on noisy kernel we evaluate the ability of promk to predict the function of protein using several standard benchmark we show that our approach performs better than previously proposed protein function prediction approach that integrate data from multiple network and multi label multiple kernel learning method 
coreference resolution is the problem of clustering mention into entity and is very critical for natural language understanding this paper study the problem of coreference resolution in the context of the important domain of clinical text clinical text is unique because it requires significant use of domain knowledge to support coreference resolution it also ha specific discourse characteristic which impose several constraint on coreference decision we present a principled framework to incorporate knowledge based constraint in the coreference model we also show that different pronoun behave quite differently necessitating the development of distinct way for resolving different pronoun our method result in significant performance improvement and we report the best result on a clinical corpus that ha been used in coreference shared task moreover for the first time we report the result for end to end coreference resolution on this corpus 
it is well known that satisfiability and hence validity in the minimal classical modal logic is a pspace complete problem in this paper we consider the satisfiability and validity problem here they are not dual although mutually reducible for the minimal modal logic over a finite lukasiewicz chain and show that they also are pspace complete this result is also true when adding either the delta operator or truth constant in the language i e in all these case it is pspace complete 
distributed constraint optimization problem dcops can be optimally solved by distributed search algorithm such a adopt and bnb adopt in centralized solving maintaining soft arc consistency during search ha proved to be beneficial for performance in this thesis we aim to explore the maintenance of different level of soft arc consistency in distributed search when solving dcops 
consider a combinatorial state space s such a the set of all truth assignment to n boolean variable given a partition of s we consider the problem of estimating the size of all the subset in which s is divided this problem also known a computing the density of state is quite general and ha many application for instance if we consider a boolean formula in cnf and we partition according to the number of violated constraint computing the density of state is a generalization of both sat maxsat and model counting we propose a novel markov chain monte carlo algorithm to compute the density of state of boolean formula that is based on a flat histogram approach our method represents a new approach to a variety of inference learning and counting problem we demonstrate it practical effectiveness by showing that the method converges quickly to an accurate solution on a range of synthetic and real world instance 
in this paper we study user modeling on twitter we investigate different strategy for mining user interest profile from microblogging activity ranging from strategy that analyze the semantic meaning of twitter message to strategy that adapt to temporal pattern that can be observed in the microblogging behavior we evaluate the quality of the user modeling method in the context of a personalized news recommendation system our result reveals that an understanding of the semantic meaning of microposts is key for generating high quality user profile 
a fundamental step in sentence comprehension involves assigning semantic role to sentence constituent to accomplish this the listener must parse the sentence find constituent that are candidate argument and assign semantic role to those constituent where do child learning their first language begin in solving this problem even assuming child can derive a rough meaning for the sentence from the situation how do they begin to map this meaning to the structure and the structure to the form of the sentence in this paper we use feedback from a semantic role labeling srl task to improve the intermediate syntactic representation that feed the srl we accomplish this by training an intermediate classifier using signal derived from latent structure optimization technique by using a separate classifier to predict internal structure we see benefit due to knowledge embedded in the classifier s feature representation this extra structure allows the system to begin to learn using weaker more plausible semantic feedback 
we provide reformulations and generalization of both the semantics of logic program by faber leone and pfeifer and it extension to arbitrary propositional formula by truszczynski unlike the previous definition our generalization refer neither to grounding nor to fixpoints and apply to first order formula containing aggregate expression in the same spirit a the first order stable model semantics proposed by ferraris lee and lifschitz the semantics proposed here are based on syntactic transformation that are similar to circumscription the reformulations provide useful insight into the flp semantics and it relationship to circumscription and the first order stable model semantics 
many important problem can be compactly represented a quantified boolean formula qbf and solved by general qbf solver to date qbf solver have mainly focused on determining whether or not the input qbf is true or false however additional important information about an application can be gathered from it qbf formulation in this paper we demonstrate that a circuitbased qbf solver can be exploited to obtain a qresolution proof of the truth or the falsity of a qbf qbfs have a natural interpretation a a two person game and our main result is to show how via a simple computation the move for the winning player can be computed directly from these proof this result show that the proof is a representation of the winning strategy in previous approach the winning strategy ha often been represented in a way that make it hard to verify in our approach the correctness of the strategy follows directly from the correctness of the proof which is relatively easy to verify 
we propose a novel online planning algorithm for ad hoc team setting challenging situation in which an agent must collaborate with unknown teammate without prior coordination our approach is based on constructing and solving a series of stage game and then using biased adaptive play to choose action the utility function in each stage game is estimated via monte carlo tree search using the uct algorithm we establish analytically the convergence of the algorithm and show that it performs well in a variety of ad hoc team domain 
nonnegative matrix factorization nmf based coclustering method have attracted increasing attention in recent year because of their mathematical elegance and encouraging empirical result however the algorithm to solve nmf problem usually involve intensive matrix multiplication which make them computationally inefficient in this paper instead of constraining the factor matrix of nmf to be nonnegative a existing method we propose a novel fast nonnegative matrix trifactorization fnmtf approach to constrain them to be cluster indicator matrix a special type of nonnegative matrix a a result the optimization problem of our approach can be decoupled which result in much smaller size subproblems requiring much le matrix multiplication such that our approach work well for large scale input data moreover the resulted factor matrix can directly assign cluster label to data point and feature due to the nature of indicator matrix in addition through exploiting the manifold structure in both data and feature space we further introduce the locality preserved fnmtf lp fnmtf approach by which the clustering performance is improved the promising result in extensive experimental evaluation validate the effectiveness of the proposed method 
bag of word approach ha played an important role in recent work for image classification in consideration of efficiency most method use kmeans clustering to generate the codebook the obtained codebooks often lose the cluster size and shape information with distortion error and low discriminative power though some effort have been made to optimize codebook in sparse coding they usually incur higher computational cost moreover they ignore the correlation between code in the following coding stage that lead to low discriminative power of the final representation in this paper we propose a bilevel visual word coding approach in consideration of representation ability discriminative power and efficiency in the bilevel codebook generation stage k mean and an efficient spectral clustering are respectively run in each level by taking both class information and the shape of each visual word cluster into account to obtain discriminative representation in the coding stage we design a certain localized coding rule with bilevel codebook to select local base to further achieve an efficient coding referring to this rule an online method is proposed to efficiently learn a projection of local descriptor to the visual word in the codebook after projection coding can be efficiently completed by a low dimensional localized soft assignment experimental result show that our proposed bilevel visual word coding approach outperforms the state of the art approach for image classification 
we consider the following sequential allocation process a benevolent central authority ha to allocate a set of indivisible good to a set of agent whose preference it is totally ignorant of we consider the process of allocating object one after the other by designating an agent and asking her to pick one of the object among those that remain the problem consists in choosing the best sequence of agent according to some optimality criterion we assume that agent have additive preference over object the choice of an optimality criterion depends on three parameter how utility of object are related to their ranking in an agent s preference relation how the preference of different agent are correlated and how social welfare is defined from the agent utility we address the computation of a sequence maximizing expected social welfare under several assumption we also address strategical issue 
this paper is about transforming constraint network to accommodate additional constraint in specific way the focus is on two intertwined issue first we investigate how partial solution to an initial network can be preserved from the potential impact of additional constraint second we study how more permissive constraint which are intended to enlarge the set of solution can be accommodated in a constraint network these two problem are studied in the general case and the light is shed on their relationship a case study is then investigated where a more permissive additional constraint is taken into account through a form of network relaxation while some previous partial solution are preserved at the same time 
we study a recently developed centrality metric to identify key player in terrorist organisation due to lindelauf et al this metric which involves computation of the shapley value for connectivity game on graph proposed by amer and gimenez wa shown to produce substantially better result than previously used standard centrality in this paper we present the first computational analysis of this class of coalitional game and propose two algorithm for computing lindelauf et al s centrality metric our first algorithm is exact and run in time linear by number of connected subgraphs in the network a shown in the numerical simulation our algorithm identifies key player in the wtc terrorist network constructed of member and link in le than minute in contrast a general purpose shapley value algorithm would require week to solve this problem our second algorithm is approximate and can be used to study much larger network 
in this paper we study algorithm for probabilistic satisfiability psat an np complete problem and their empiric complexity distribution we define a psat normal form based on which we propose two logic based algorithm a reduction of normal form psat instance to sat and a linearalgebraic algorithmwith a logic based column generation strategy we conclude that both algorithm present a phase transition behaviour and that the latter ha a much better performance 
cross lingual entity linking mean linking an entity mention in a background source document in one language with the corresponding real world entity in a knowledge base written in the other language the key problem is to measure the similarity score between the context of the entity mention and the document of the candidate entity this paper present a general framework for doing cross lingual entity linking by leveraging a large scale and bilingual knowledge base wikipedia we introduce a bilingual topic model that mining bilingual topic from this knowledge base with the assumption that the same wikipedia concept document of two different language share the same semantic topic distribution the extracted topic have two type of representation with each type corresponding to one language thus both the context of the entity mention and the document of the candidate entity can be represented in a space using the same semantic topic we use these topic to do cross lingual entity linking experimental result show that the proposed approach can obtain the competitive result compared with the state of art approach 
collaborative filtering cf technique recommend item to user based on their historical rating in real world scenario user interest may drift over time since they are affected by mood context and pop culture trend this lead to the fact that a user s historical rating comprise many aspect of user interest spanning a long time period however at a certain time slice one user s interest may only focus on one or a couple of aspect thus cf technique based on the entire historical rating may recommend inappropriate item in this paper we consider modeling user interest drift over time based on the assumption that each user ha multiple counterpart over temporal domain and successive counterpart are closely related we adopt the cross domain cf framework to share the static group level rating matrix across temporal domain and let user interest distribution over item group drift slightly between successive temporal domain the derived method is based on a bayesian latent factor model which can be inferred using gibbs sampling our experimental result show that our method can achieve state of the art recommendation performance a well a explicitly track and visualize user interest drift over time 
after a disaster human rescuer may have to wait for better condition before beginning to search for survivor a team of robot could enter long before the human and scope out the environment to gather information that could help to prioritize task for the rescue operation we have developed an algorithm to allow a small group of robot to progressively explore an unknown environment moving a a group until full exploration is achieved the novel concept behind this algorithm come from the way in which the team stay together a a group maintaining communication in order to ensure full exploration a well a a path to the exit we demonstrate in simulation that the algorithm work in multiple environment under varying condition 
this paper investigates the effectiveness of two state representation cnf and dnf in contingent planning to this end we developed a new contingent planner called cnfct using the and or forward search algorithm prao to et al and an extension of the cnf representation of to et al for conformant planning to handle nondeterministic and sensing action for contingent planning the study us cnfct and dnfct to et al and proposes a new heuristic function for both planner the experiment demonstrate that both cnfct and dnfct offer very competitive performance in a large range of benchmark but neither of the two representation is a clear winner over the other the paper identifies property of the representation scheme that can affect their performance on different problem 
we present a new approach to characterizing the semantics for the integration of rule and first order logic in general and description logic in particular based on a circumscription characterization of answer set programming introduced earlier by lin and zhou we show that both rosati s semantics based on nm model and lukasiewicz s answer set semantics can be characterized by circumscription and the difference between the two can be seen a a matter of circumscription policy this approach lead to a number of new insight first we rebut a criticism on lukasiewicz s semantics for it inability to reason for negative consequence second our approach lead to a spectrum of possible semantics based on different circumscription policy and show a clear picture of how they are related finally we show that the idea of this paper can be applied to first order general stable model copyright association for the advancement of artificial intelligence all right reserved 
multi class problem are everywhere given an input the goal is to predict one of a few possible class most previous work reduced learning to minimizing the empirical loss over some training set and an additional regularization term prompting simple model or some other prior knowledge many learning regularization promote sparsity that is small model or small number of feature a performed in group lasso yet such model do not always represent the class well in some problem for each class there is a small set of feature that represents it well yet the union of these set is not small we propose to use other regularization that promote this type of sparsity analyze the generalization property of such formulation and show empirically that indeed these regularization not only perform well but also promote such sparsity structure 
conjunctive regular path query are an expressive extension of the well known class of conjunctive query and have been extensively studied in the database community somewhat surprisingly there ha been little work aimed at using such query in the context of description logic dl knowledge base and all existing result target expressive dl even though lightweight dl are considered better suited for data intensive application this paper aim to bridge this gap by providing algorithm and tight complexity bound for answering two way conjunctive regular path query over dl knowledge base formulated in lightweight dl of the dl lite and el family 
we study the complexity of approximate winner determination under monroe s and chamberlin courant s multiwinner voting rule where we focus on the total dis satisfaction of the voter the utilitarian case or the dis satisfaction of the worst off voter the egalitarian case we show good approximation algorithm for the satisfaction based utilitarian case and inapproximability result for the remaining setting 
in this paper we tackle the problem of recommendation in the scenario with binary relevance data when only a few k item are recommended to individual user past work on collaborative filtering cf ha either not addressed the ranking problem for binary relevance datasets or not specifically focused on improving top k recommendation to solve the problem we propose a new cf approach collaborative le is more filtering climf in climf the model parameter are learned by directly maximizing the mean reciprocal rank mrr which is a well known information retrieval metric for capturing the performance of top k recommendation we achieve linear computational complexity by introducing a lower bound of the smoothed reciprocal rank metric experiment on two social network datasets show that climf significantly outperforms a naive baseline and two state of the art cf method 
in this paper we address the problem of matrix factorization on compressively sampled measurement which are obtained by random projection while this approach improves the scalability of matrix factorization it performance is not satisfactory we present a matrix co factorization method where compressed measurement and a small number of uncompressed measurement are jointly decomposed sharing a factor matrix we evaluate the performance of three matrix factorization method in term of cram r rao bound including matrix factorization on uncompressed data mf matrix factorization on compressed data c mf matrix co factorization on compressed and uncompressed data c mcf numerical experiment demonstrate that c mcf improves the performance of c mf emphasizing the useful behavior of exploiting side information a small number of uncompressed measurement 
we consider a generalization of instance retrieval over knowledge base that provides user with assertion in which description of qualifying object are given in addition to their identifier notably this involves a transfer of basic database paradigm involving caching and query rewriting in the context of an assertion retrieval algebra we present an optimization framework for this algebra with a focus on finding plan that avoid any need for general knowledge base reasoning at query execution time when sufficient cached result of earlier request exist 
this paper is devoted to complexity result regarding specific measure of proximity to single peakedness and single crossingness called single peaked width cornaz et al and single crossing width thanks to the use of the pq tree data structure booth and lueker we show that both problem are polynomial time solvable in the general case while it wa only known for single peaked width and in the case of narcissistic preference furthermore we establish one of the first result to our knowledge concerning the effect of nearly single peaked electorate on the complexity of an np hard voting system namely we show the fixed parameter tractability of kemeny election with respect to the parameter single peaked width and single crossing width 
we propose a manifold alignment based approach for heterogeneous domain adaptation a key aspect of this approach is to construct mapping to link different feature space in order to transfer knowledge across domain the new approach can reuse labeled data from multiple source domain in a target domain even in the case when the input domain do not share any common feature or instance a a pre processing step our approach can also be combined with existing domain adaptation approach to learn a common feature space for all input domain this paper extends existing manifold alignment approach by making use of label rather than correspondence to align the manifold this extension significantly broadens the application scope of manifold alignment since the correspondence relationship required by existing alignment approach is hard to obtain in many application 
partial order plan pop have the capacity to compactly represent numerous distinct plan linearizations and a a consequence are inherently robust we exploit this robustness to do effective execution monitoring we characterize the condition under which a pop remains viable a the regression of the goal through the structure of a pop we then develop a method for pop execution monitoring via a structured policy expressed a an ordered algebraic decision diagram the policy encompasses both state evaluation and action selection enabling an agent to seamlessly switch between pop linearizations to accommodate unexpected change during execution we demonstrate the effectiveness of our approach by comparing it empirically and analytically to a standard technique for execution monitoring of sequential plan on standard benchmark planning domain our approach is to time faster and up to time more robust than comparable monitoring of a sequential plan on pop that have few ordering constraint among action our approach is significantly more robust with the ability to continue executing in up to an exponential number of additional state 
consequence based ontology reasoning procedure have so far been known only for horn ontology language a difficulty in extending such procedure is that non horn axiom seem to require reasoning by case which cause non determinism in tableau based procedure in this paper we present a consequence based procedure for alch that overcomes this difficulty by using rule similar to ordered resolution to deal with disjunctive axiom in a deterministic way it retains all the favourable attribute of existing consequence based procedure such a goal directed one pas classification optimal worst case complexity and pay a you go behaviour our preliminary empirical evaluation suggests that the procedure scale well to non horn ontology 
given a set of data recorded by observing the decision of an expert player we present a case based framework that allows the successful generalisation of those decision in the game of no limit texas hold em we address the problem of determining a suitable action abstraction and the resulting state translation that is required to map real value bet amount into a discrete set of abstract action we also detail the similarity metric used in order to identify similar scenario without which no generalisation of playing decision would be possible we show that we were able to successfully generalise no limit betting decision from recorded data via our agent sartrenl which achieved a th place finish out of opponent at the annual computer poker competition 
we will show how human computation insight can be key to identifying so called backdoor variable in combinatorial optimization problem backdoor variable can be used to obtain dramatic speedup in combinatorial search our approach leverage the complementary strength of human input based on a visual identification of problem structure crowdsourcing and the power of combinatorial solver to exploit complex constraint we describe our work in the context of the domain of material discovery the motivation for considering the material discovery domain come from the fact that new material can provide solution for key challenge in sustainability e g in energy new catalyst for more efficient fuel cell technology 
in low rank sparse matrix decomposition the entry of the sparse part are often assumed to be i i d sampled from a random distribution but the structure of sparse part a the central interest of many problem ha been rarely studied one motivating problem is tracking multiple sparse object flow motion in video we introduce shifted subspace tracking sst to segment the motion and recover their trajectory by exploring the low rank property of background and the shifted subspace property of each motion sst is composed of two step background modeling and flow tracking in step we propose semi soft godec to separate all the motion from the low rank background l a a sparse outlier s it soft thresholding in updating s significantly speed up godec and facilitates the parameter tuning in step we update x a s obtained in step and develop sst algorithm further decomposing x a x i k l i o i s g wherein l i is a low rank matrix storing the ith flow after transformation i sst algorithm solves k sub problem in sequel by alternating minimization each of which recovers one l i and it i by randomized method sparsity of l i and between frame affinity are leveraged to save computation we justify the effectiveness of sst on surveillance video sequence 
classical planning ha been notably successful in synthesizing finite plan to achieve state where propositional goal hold in the last few year classical planning ha also been extended to incorporate temporally extended goal expressed in temporal logic such a ltl to impose restriction on the state sequence generated by finite plan in this work we take the next step and consider the computation of infinite plan for achieving arbitrary ltl goal we show that infinite plan can also be obtained efficiently by calling a classical planner once over a classical planning encoding that represents and extends the composition of the planning domain and the b chi automaton representing the goal this compilation scheme ha been implemented and a number of experiment are reported 
following the recent trend of studying the theory of belief revision under the horn fragment of propositional logic this paper develops a fully characterised horn contraction which is analogous to the traditional transitively relational partial meet contraction alchourr n et al this horn contraction extends the partial meet horn contraction studied in delgrande and wassermann so that it is guided by a transitive relation that model the ordering of plausibility over set of belief 
the most critical challenge for the recommendation system is to achieve the high prediction quality on the large scale sparse data contributed by the user in this paper we present a novel approach to the social recommendation problem which take the advantage of the graph laplacian regularization to capture the underlying social relationship among the user differently from the previous approach that are based on the conventional gradient descent optimization we formulate the presented graph laplacian regularized social recommendation problem into a low rank semidefinite program which is able to be efficiently solved by the quasi newton algorithm we have conducted the empirical evaluation on a large scale dataset of high sparsity the promising experimental result show that our method is very effective and efficient for the social recommendation task copyright association for the advancement of artificial intelligence all right reserved 
matrix factorization mf is a popular collaborative filtering approach for recommender system due to it simplicity and effectiveness existing mf method either assume that all latent feature are uncorrelated or assume that all are correlated to address the important issue of what structure should be imposed on the feature we investigate the covariance matrix of the latent feature learned from real data based on the finding we propose an mf model with a sparse covariance prior which favor a sparse yet non diagonal covariance matrix not only can this reflect the semantics more faithfully but imposing sparsity can also have a side effect of preventing overfitting starting from a probabilistic generative model with a sparse covariance prior we formulate the model inference problem a a maximum a posteriori map estimation problem the optimization procedure make use of stochastic gradient descent and majorization minimization for empirical validation we conduct experiment using the movielens and netflix datasets to compare the proposed method with two strong baseline which use different prior experimental result show that our sparse covariance prior can lead to performance improvement 
the mixture of multivariate bernoulli distribution mmb is a statistical model for high dimensional binary data in widespread use recently the mmb ha been used to model the sequence of packet reception and loss of wireless link in sensor network given an mmb trained on long data trace recorded from link of a deployed network one can then use sample from the mmb to test different routing algorithm for a long a desired however learning an accurate model for a new link requires collecting from it long trace over period of hour a costly process in practice e g limited battery life we propose an algorithm that can adapt a preexisting mmb trained with extensive data to a new link from which very limited data is available our approach constrains the new mmb s parameter through a nonlinear transformation of the existing mmb s parameter the transformation ha a small number of parameter that are estimated using a generalized em algorithm with an inner loop of bfgs iteration we demonstrate the efficacy of the approach using the mnist dataset of handwritten digit and wireless link data from a sensor network we show we can learn accurate model from data trace of about minute about time shorter than needed if training an mmb from scratch 
sequential anomaly detection is a challenging problem due to the one class nature of the data i e data is collected from only one class and the temporal dependence in sequential data we present one class conditional random field occrf for sequential anomaly detection that learn from a one class dataset and capture the temporal dependence structure in an unsupervised fashion we propose a hinge loss in a regularized risk minimization framework that maximizes the margin between each sequence being classified a normal and abnormal this allows our model to accept most but not all of the training data a normal yet keep the solution space tight experimental result on a number of real world datasets show our model outperforming several baseline we also report an exploratory study on detecting abnormal organizational behavior in enterprise social network 
traffic sensing is a key baseline input for sustainable city to plan and administer demand supply management through better road network public transportation urban policy etc human sense the environment frugally using a combination of complementary information signal from different sensor for example by viewing and or hearing traffic one could identify the state of traffic on the road in this paper we demonstrate a fusion based learning approach to classify the traffic state using low cost audio and image data analysis using real world dataset road side collected traffic acoustic signal and traffic image snapshot obtained from fixed camera are used to classify the traffic condition into three broad class viz jam medium and free the classification is done on sec audio image snapshot in that sec data tuple we extract traffic relevant feature from audio and image data to form a composite feature vector in particular we extract the audio feature comprising mfcc mel frequency cepstral coefficient classifier based feature honk event and energy peak a simple heuristic based image classifier is used where vehicular density and number of corner point within the road segment are estimated and are used a feature for traffic sensing finally the composite vector is tested for it ability to discriminate the traffic class using decision tree classifier svm classifier discriminant classifier and logistic regression based classifier information fusion at multiple level audio image overall show consistently better performance than individual level decision making low cost sensor fusion based on complementary weak classifier and noisy feature still generates high quality result with an overall accuracy of 
the vcg mechanism is the gold standard for combinatorial auction ca and it maximizes social welfare in contrast the revenue maximizing aka optimal ca is unknown and designing one is np hard therefore research on optimal ca ha progressed into special setting notably levin derived the optimal ca for complement when each agent s private type is one dimensional this doe not fall inside the well studied single parameter environment we introduce a new research avenue for increasing revenue where we poke hole in the allocation space based on the bid and then use a welfare maximizing allocation rule within the remaining allocation set in this paper the first step down this avenue we introduce a new form of reserve pricing into ca we show that levin s optimal revenue can be approximated by using monopoly reserve price to curtail the allocation set followed by welfare maximizing allocation and levin s payment rule a key lemma of potential independent interest is that the expected revenue from any truthful allocation monotonic mechanism equal the expected virtual valuation this generalizes myerson s lemma from the single parameter environment our mechanism is close to the gold standard and thus easier to adopt than levin s it also requires le information about the prior over the bidder type and is always more efficient finally we show that the optimal revenue can be approximated even if the reserve pricing is required to be symmetric across bidder 
real time agent centric algorithm have been used for learning and solving problem since the introduction of the lrta algorithm in in this time period numerous variant have been produced however they have generally followed the same approach in varying parameter to learn a heuristic which estimate the remaining cost to arrive at a goal state recently a different approach rib wa suggested which instead of learning cost to the goal learns cost from the start state rib can solve some problem faster but in other problem ha poor performance we present a new algorithm f cost learning real time a f lrta which combine both approach simultaneously learning distance from the start and heuristic to the goal an empirical evaluation demonstrates that f lrta outperforms both rib and lrta style approach in a range of scenario 
in social choice setting with strict preference random dictatorship rule were characterized by gibbard a the only randomized social choice function that satisfy strategyproofness and ex post efficiency in the more general domain with indifference rsd random serial dictatorship rule are the well known and perhaps only known generalization of random dictatorship we present a new generalization of random dictatorship for indifference called maximal recursive mr rule a an alternative to rsd we show that mr is polynomial time computable weakly strategyproof with respect to stochastic dominance and in some respect outperforms rsd on efficiency 
social influence ha become the essential factor which drive the dynamic evolution process of social network structure and user behavior previous research often focus on social influence analysis in network level or topic level in this paper we concentrate on predicting item level social influence to reveal the user influence in a more fine grained level we formulate the social influence prediction problem a the estimation of a user post matrix where each entry in the matrix represents the social influence strength the corresponding user ha given the corresponding web post to deal with the sparsity and complex factor challenge in the research we model the problem by extending the probabilistic matrix factorization method to incorporate rich prior knowledge on both user dimension and web post dimension and propose the probabilistic hybrid factor matrix factorization phf mf approach intensive experiment are conducted on a real world online social network to demonstrate the advantage and characteristic of the proposed method copyright association for the advancement of artificial intelligence all right reserved 
our aim is to investigate ontology based data access over temporal data with validity time and ontology capable of temporal conceptual modelling to this end we design a temporal description logic tql that extends the standard ontology language owl ql provides basic mean for temporal conceptual modelling and ensures first order rewritability of conjunctive query for suitably defined data instance with validity time 
hash distributed a hda is a parallel a algorithm that is proven to be effective in optimal sequential planning with unit edge cost hda leverage the zobrist function to almost uniformly distribute and schedule work among processor this paper evaluates the performance of hda in optimal sequence alignment we observe that with a large number of cpu core hda suffers from an increase of search overhead caused by reexpansions of state in the closed list due to nonuniform edge cost in this domain we therefore present a new work distribution strategy limiting processor to distribute work thus increasing the possibility of detecting such duplicate search effort we evaluate the performance of this approach on a cluster of multi core machine and show that the approach scale well up to cpu core 
this paper formulates learning optimal bayesian network a a shortest path finding problem an a search algorithm is introduced to solve the problem with the guidance of a consistent heuristic the algorithm learns an optimal bayesian network by only searching the most promising part of the solution space empirical result show that the a search algorithm significantly improves the time and space efficiency of existing method on a set of benchmark datasets 
learning in automated negotiation while useful is hard because of the indirect way the target function can be observed and the limited amount of experience available to learn from this paper proposes two novel opponent modeling technique based on deep learning method moreover to improve the learning efficacy of negotiating agent the second approach is also capable of transferring knowledge efficiently between negotiation task transfer is conducted by automatically mapping the source knowledge to the target in a rich feature space experiment show that using these technique the proposed strategy outperform existing state of the art agent in highly competitive and complex negotiation domain furthermore the empirical game theoretic analysis reveals the robustness of the proposed strategy 
we report on result from experiment where human trader interact with software agent trader in a real time asynchronous continuous double auction cda experimental economics system our experiment are inspired by the seminal work reported by ibm at ijcai da et al where it wa demonstrated that software agent trader could consistently outperform human trader in real time cda market ibm tested two trading agent strategy zip and a modified version of gd and in a subsequent paper they reported on a new strategy called gdx that wa demonstrated to outperform gd and zip in agent v agent cda competition on which basis it wa claimed that gdx may offer the best performance of any published cda bidding strategy tesauro and bredin in this paper we employ experiment method similar to those pioneered by ibm to test the performance of adaptive aggressive aa algorithmic trader vytelingum the result presented here confirm vytelingum s claim that aa outperforms zip gd and gdx in agent v agent experiment we then present the first result from testing aa against human trader in human v agent cda experiment and demonstrate that aa s performance against human trader is superior to that of zip gd and gdx we therefore claim that on the basis of the available evidence aa may offer the best performance of any published bidding strategy 
a adversarial environment become more complex it is increasingly crucial for agent to exploit the mistake of weaker opponent particularly in the context of winning tournament and competition in this work we present a simple post processing technique which we call perfect information post mortem analysis pipma that can quickly ass the playing strength of an opponent in certain class of game environment we apply this technique to skat a popular german card game and show that we can achieve substantial performance gain against not only player weaker than our program but against stronger player a well most importantly pipma can model the opponent after only a handful of game to our knowledge this make our work the first successful example of an opponent modelling technique that can adapt it play to a particular opponent in real time in a complex game setting 
we provide both a semantic interpretation and logical inferential characterization of the markov principle that underlies the main action theory in ai this principle will be shown to constitute a nonmonotonic assumption that justifies the actual restriction on action description in these theory a well a constraint on allowable query it will be shown also that the well known regression principle is a consequence of the markov assumption and it is valid also for non deterministic domain 
on line portfolio selection ha been attracting increasing interest from artificial intelligence community in recent decade mean reversion a one most frequent pattern in financial market play an important role in some state of the art strategy though successful in certain datasets existing mean reversion strategy do not fully consider noise and outlier in the data leading to estimation error and thus non optimal portfolio which result in poor performance in practice to overcome the limitation we propose to exploit the reversion phenomenon by robust l median estimator and design a novel on line portfolio selection strategy named robust median reversion rmr which make optimal portfolio based on the improved reversion estimation empirical result on various real market show that rmr can overcome the drawback of existing mean reversion algorithm and achieve significantly better result finally rmr run in linear time and thus is suitable for large scale trading application 
traditional sentiment analysis mainly considers binary classification of review but in many real world sentiment classification problem non binary review rating are more useful this is especially true when consumer wish to compare two product both of which are not negative previous work ha addressed this problem by extracting various feature from the review text for learning a predictor since the same word may have different sentiment effect when used by different reviewer on different product we argue that it is necessary to model such reviewer and product dependent effect in order to predict review rating more accurately in this paper we propose a novel learning framework to incorporate reviewer and product information into the text based learner for rating prediction the reviewer product and text feature are modeled a a three dimension tensor tensor factorization technique can then be employed to reduce the data sparsity problem we perform extensive experiment to demonstrate the effectiveness of our model which ha a significant improvement compared to state of the art method especially for review with unpopular product and inactive reviewer 
when making a mistake individual can apologize to secure further cooperation even if the apology is costly similarly individual arrange commitment to guarantee that an action such a a cooperative one is in the others best interest and thus will be carried out to avoid eventual penalty for commitment failure hence both apology and commitment should go side by side in behavioral evolution here we provide a computational model showing that apologizing act are rare in non committed interaction especially whenever cooperation is very costly and that arranging prior commitment can considerably increase the frequency of such behavior in addition we show that in both case with or without commitment apology work only if it is sincere i e costly enough most interestingly our model predicts that individual tend to use much costlier apology in committed relationship than otherwise because it help better identify free rider such a fake committers commitment bring about sincerity furthermore we show that this strategy of apology supported by commitment outperforms the famous existent strategy of the iterated prisoner s dilemma 
convolution tree kernel have been successfully applied to many language processing task for achieving state of the art accuracy unfortunately higher computational complexity of learning with kernel w r t using explicit feature vector make them le attractive for large scale data in this paper we study the latest approach to solve such problem ranging from feature hashing to reverse kernel engineering and approximate cutting plane training with model compression we derive a novel method that relies on reverse kernel engineering together with an efficient kernel learning method the approach give the advantage of using tree kernel to automatically generate rich structured feature space and working in the linear space where learning and testing is fast we experimented with training set up to million example from semantic role labeling the result show that i the choice of correct structural feature is essential and ii we can speed up training from week to le than minute 
human environment are challenging for robot which need to be trainable by lay people and learn new behaviour rapidly without disrupting much the ongoing activity a system that integrates ai technique for planning and learning is here proposed to satisfy these strong demand the approach rapidly learns planning operator from few action experience using a competitive strategy where many alternative of cause effect explanation are evaluated in parallel and the most successful one are used to generate the operator the success of a cause effect explanation is evaluated by a probabilistic estimate that compensates the lack of experience producing more confident estimation and speeding up the learning in relation to other known estimate the system operates without task interruption by integrating in the planning learning loop a human teacher that support the planner in making decision all the mechanism are integrated and synchronized in the robot using a general decision making framework the feasibility and scalability of the architecture are evaluated in two different robot platform a st ubli arm and the humanoid armar iii 
in real world recommender system some user are easily influenced by new product and whereas others are unwilling to change their mind so the preference varying speed for user are different based on this observation we propose a dynamic nonlinear matrix factorization model for collaborative filtering aimed to improve the rating prediction performance a well a track the preference varying speed for different user we assume that user preference change smoothly over time and the preference varying speed for user are different these two assumption are incorporated into the proposed model a prior knowledge on user feature vector which can be learned efficiently by map estimation the experimental result show that our method not only achieves state of the art performance in the rating prediction task but also provides an effective way to track user preference varying speed copyright association for the advancement of artificial intelligence all right reserved 
in this paper we propose a novel method to select the most informative subset of feature which ha little redundancy and very strong discriminating power our proposed approach automatically determines the optimal number of feature and selects the best subset accordingly by maximizing the average pairwise informativeness thus ha obvious advantage over traditional filter method by relaxing the essential combinatorial optimization problem into the standard quadratic programming problem the most informative feature subset can be obtained efficiently and a strategy to dynamically compute the redundancy between feature pair further greatly accelerates our method through avoiding unnecessary computation of mutual information a shown by the extensive experiment the proposed method can successfully select the most informative subset of feature and the obtained classification result significantly outperform the state of the art result on most test datasets copyright association for the advancement of artificial intelligence all right reserved 
we have developed a method for using confidence score to integrate label provided by crowdsourcing worker although confidence score can be useful information for estimating the quality of the provided label a way to effectively incorporate them into the integration process ha not been established moreover some worker are overconfident about the quality of their label while others are underconfident and some worker are quite accurate in judging the quality of their label this differing reliability of the confidence score among worker mean that the probability distribution for the reported confidence score differ among worker to address this problem we extended the dawid skene model and created two probabilistic model in which the value of unobserved true label are inferred from the observed provided label and reported confidence score by using the expectation maximization algorithm result of experiment using actual crowdsourced data for image labeling and binary question answering task showed that incorporating worker confidence score can improve the accuracy of integrated crowdsourced label 
topic model have a wide range of application including modeling of text document image user preference product ranking and many others however learning optimal model may be difficult especially for large problem the reason is that inference technique such a gibbs sampling often converge to suboptimal model due to the abundance of local minimum in large datasets in this paper we propose a general method of improving the performance of topic model the method called grouping transform work by introducing auxiliary variable which represent assignment of the original model token to group using these auxiliary variable it becomes possible to resample an entire group of token at a time this allows the sampler to make larger state space move a a result better model are learned and performance is improved the proposed idea are illustrated on several topic model and several text and image datasets we show that the grouping transform significantly improves performance over standard model 
this paper is devoted to sequential decision making under uncertainty in the multi prior framework of gilboa and schmeidler in this setting a set of probability measure prior is defined instead of a single one and the decision maker selects a strategy that maximizes the minimum possible value of expected utility over this set of prior we are interested here in the resolute choice approach where one initially commits to a complete strategy and never deviate from it later given a decision tree representation with multiple prior we study the problem of determining an optimal strategy from the root according to min expected utility we prove the intractability of evaluating a strategy in the general case we then identify different property of a decision tree that enable to design dedicated resolution procedure finally experimental result are presented that evaluate these procedure 
we target the problem of accuracy and robustness in causal inference from finite data set our aim is to combine the inherent robustness of the bayesian approach with the theoretical strength and clarity of constraint based method we use a bayesian score to obtain probability estimate on the input statement used in a constraint based procedure these are subsequently processed in decreasing order of reliability letting more reliable decision take precedence in case of conflict until a single output model is obtained test show that a basic implementation of the resulting bayesian constraint based causal discovery bccd algorithm already outperforms established procedure such a fci and conservative pc it indicates which causal decision in the output have high reliability and which do not the approach is easily adapted to other application area such a complex independence test 
team formation is a critical step in deploying a multi agent team in some scenario agent coordinate by voting continuously when forming such team should we focus on the diversity of the team or on the strength of each member can a team of diverse and weak agent outperform a uniform team of strong agent we propose a new model to address these question our key contribution include i we show that a diverse team can overcome a uniform team and we give the necessary condition for it to happen ii we present optimal voting rule for a diverse team iii we perform synthetic experiment that demonstrate that both diversity and strength contribute to the performance of a team iv we show experiment that demonstrate the usefulness of our model in one of the most difficult challenge for artificial intelligence computer go 
top k voting is an especially natural form of partial vote elicitation in which only length k prefix of ranking are elicited we analyze the ability of top k vote elicitation to correctly determine true winner with high probability given probabilistic model of voter preference and candidate availability we provide bound on the minimal value of k required to determine the correct winner under the plurality and borda voting rule considering both worst case preference profile and profile drawn from the impartial culture and mallow probabilistic model we also derive condition under which the special case of zero elicitation i e k produce the correct winner we provide empirical result that confirm the value of top k voting 
entity in two dimensional space are often approximated using rectangle that are parallel to the two ax that define the space so called minimum bounding rectangle mbrs mbrs are popular in computer vision and other area a they are easy to obtain and easy to represent in the area of qualitative spatial reasoning many different spatial representation are based on mbrs surprisingly there ha been no such representation proposed for general rectangle i e rectangle that can have any angle nor for general solid rectangle gsr that cannot penetrate each other gsr are often used in computer graphic and computer game such a angry bird where they form the building block of more complicated structure in order to represent and reason about these structure we need a spatial representation that allows u to use gsr a the basic spatial entity in this paper we develop and analyze a qualitative spatial representation for gsr we apply our representation and the corresponding reasoning method to solve a very interesting practical problem assuming we want to detect gsr in computer game but computer vision can only detect mbrs how can we infer the gsr from the given mbrs we evaluate our solution and test it usefulness in a real gaming scenario 
lexical cohesion arises from a chain of lexical item that establish link between sentence in a text in this paper we propose three different model to capture lexical cohesion for document level machine translation a a direct reward model where translation hypothesis are rewarded whenever lexical cohesion device occur in them b a conditional probability model where the appropriateness of using lexical cohesion device is measured and c a mutual information trigger model where a lexical cohesion relation is considered a a trigger pair and the strength of the association between the trigger and the triggered item is estimated by mutual information we integrate the three model into hierarchical phrase based machine translation and evaluate their effectiveness on the nist chinese english translation task with large scale training data experiment result show that all three model can achieve substantial improvement over the baseline and that the mutual information trigger model performs better than the others 
we consider the problem of equitably allocating a set of indivisible good to n agent so a to maximize the utility of the least happy agent demko and hill showed the existence of an allocation where every agent value his share at least vn which is a family of nonincreasing function in a parameter defined a the maximum value assigned by an agent to a single good a deterministic algorithm returning such an allocation in polynomial time wa proposed markakis and psomas interestingly vn is tight for some value of i e it is the best lower bound on the valuation of the least happy agent however it is not true for all value of we propose a family of function wn such that wn x vn x for all x and wn x vn x for value of x where vn x is not tight the new function wn apply on a problem which generalizes the allocation of indivisible good it is to find a solution base in a matroid which is common to n agent our result are constructive they are achieved by analyzing an extension of the algorithm of markakis and psomas 
weighted voting game wvgs model decision making body such a parliament and council in such setting it is often important to provide a measure of the influence a player ha on the vote two highly popular such measure are the shapley shubik power index and the banzhaf power index given a power measure proportional representation is the property of having player voting power proportional to the number of parliament seat they receive approximate proportional representation w r t the banzhaf power index can be ensured by changing the number of parliament seat each party receives this is known a penrose s square root method however a discrepancy between player weight and parliament seat is often undesirable or unfeasible a simpler way of achieving approximate proportional representation is by changing the quota i e the number of vote required in order to pas a bill it is known that a player s shapley shubik power index is proportional to his weight when one chooses a quota at random that is when taking a random quota proportional representation hold in expectation in our work we show that not only doe proportional representation hold in expectation it also hold for many quota we do so by providing bound on the variance of the shapley value when the quota is chosen at random assuming certain weight distribution we further explore the case where weight are sampled from i i d binomial distribution for this case we show good bound on an important parameter governing the behavior of the variance a well a substantiating our claim with empirical analysis 
the development of service robot ha gained more and more attention over the last year advanced robot have to cope with many different situation and contingency while executing concurrent and interruptable complex task to manage the sheer variety of different execution variant the robot ha to decide at run time for the most appropriate behavior to execute that requires task coordination mechanism that provide the flexibility to adapt at run time and allow to balance between alternative 
this research proposes the use of imitation based learning to build collaborative strategy for a team of agent imitation based learning involves learning from an expert by observing her demonstrating a task and then replicating it this mechanism make it extremely easy for a knowledge engineer to transfer knowledge to a software agent via human demonstration this research aim to apply imitation to learn not only the strategy of an individual agent but also the collaborative strategy of a team of agent to achieve a common goal the effectiveness of the proposed methodology is being assessed in the domain of robocup soccer simulation d which is a promising platform to address many of the complex real world problem and offer a truly dynamic stochastic and partially observable environment 
when labeling object via internet based outsourcing system the labelers may have bias because they lack expertise dedication and personal preference these reason cause imbalanced multiple noisy labeling to deal with the imbalance labeling issue we propose an agnostic algorithm plat positive label frequency threshold which doe not need any information about quality of labelers and underlying class distribution simulation on eight realworld datasets with different underlying class distribution demonstrate that plat not only effectively deal with the imbalanced multiple noisy labeling problem that off theshelf agnostic method cannot cope with but also performs nearly the same a majority voting under the circumstance that labelers have no bias copyright association for the advancement of artificial intelligence www aaai org all right reserved 
since the seminal work of sampath et al in despite the subsequent flourishing of technique on diagnosis of discrete event system des the basic notion of fault and diagnosis have been remaining conceptually unchanged fault are defined at component level and diagnosis incorporate the occurrence of component fault within system evolution diagnosis is context free a this approach may be unsatisfactory for a complex de whose topology is organized in a hierarchy of abstraction we propose to define different diagnosis rule for different subsystem in the hierarchy relevant fault pattern are specified a regular expression on pattern of lower level subsystem separation of concern is achieved and the expressive power of diagnosis is enhanced each subsystem ha it proper set of diagnosis rule which may or may not depend on the rule of other subsystem diagnosis is no longer anchored to component it becomes context sensitive the approach yield seemingly contradictory but nonetheless possible scenario a subsystem can be normal despite the faulty behavior of a number of it component positive paradox also it can be faulty despite the normal behavior of all it component negative paradox 
evaluating the quality of ranking function is a core task in web search and other information retrieval domain because query distribution and item relevance change over time ranking model often cannot be evaluated accurately on held out training data instead considerable effort is spent on manually labeling the relevance of query result for test query in order to track ranking performance we address the problem of estimating ranking performance a accurately a possible on a fixed labeling budget estimate are based on a set of most informative test query selected by an active sampling distribution query labeling cost depend on the number of result item and item specific attribute such a document length we derive cost optimal sampling distribution for commonly used ranking performance measure experiment on web search engine data illustrate significant reduction in labeling cost 
the description logic el is used to formulate several large biomedical ontology fuzzy extension of el can express the vagueness inherent in many biomedical concept we study the reasoning problem of deciding positive subsumption in fuzzy el with semantics based on general t norm we show that the complexity of this problem depends on the specific t norm chosen more precisely if the t norm ha zero divisor then the problem is co np hard otherwise it can be decided in polynomial time we also show that the best subsumption degree cannot be computed in polynomial time if the t norm contains the lukasiewicz t norm 
we study the problem of dealing with inconsistency in description logic dl ontology we consider inconsistency tolerant semantics recently proposed in the literature called ar semantics and car semantics which are based on repairing i e modifying in a minimal way the extensional knowledge abox while keeping the intensional knowledge tbox untouched we study instance checking and conjunctive query entailment under the above inconsistency tolerant semantics for a wide spectrum of dl ranging from tractable one el to very expressive one shiq showing that reasoning under the above semantics is inherently intractable even for very simple dl to the aim of overcoming such a high computational complexity of reasoning we study sound approximation of the above semantics surprisingly our computational analysis show that reasoning under the approximated semantics is intractable even for tractable dl finally we identify suitable language restriction of such dl allowing for tractable reasoning under inconsistency tolerant semantics 
we consider an extension of the propositional modal logic s which allows to act not only on isolated formula but also on set of formula the interpretation of is then given by the tangled closure of the valuation of formula in which over finite transitive reflexive model indicates the existence of a cluster satisfying this extension ha been shown to be more expressive than the basic modal language for example it is equivalent to the bisimulation invariant fragment of fol over finite s model whereas the basic modal language is weaker however previous analysis of this logic have been entirely semantic and no proof system wa available in this paper we present a sound proof system for the polyadic s and prove that it is complete the axiomatization is fairly standard adding only the fixpoint axiom of the tangled closure to the usual s axiom the proof proceeds by explicitly constructing a finite model from a consistent set of formula 
variable selection problem are typically addressed under a penalized optimization framework nonconvex penalty such a the minimax concave plus mcp and smoothly clipped absolute deviation scad have been demonstrated to have the property of sparsity practically and theoretically in this paper we propose a new nonconvex penalty that we call exponential type penalty the exponential type penalty is characterized by a positive parameter which establishes a connection with the and penalty we apply this new penalty to sparse supervised learning problem to solve to resulting optimization problem we resort to a reweighted minimization method moreover we devise an efficient method for the adaptive update of the tuning parameter our experimental result are encouraging they show that the exponential type penalty is competitive with mcp and scad copyright association for the advancement of artificial intelligence all right reserved 
abox abduction is an important aspect for abductive reasoning in description logic dl it find all minimal set of abox axiom that should be added to a background ontology to enforce entailment of a specified set of abox axiom a far a we know by now there is only one abox abduction method in expressive dl computing abductive solution with certain minimality however the method target an abox abduction problem that may have infinitely many abductive solution and may not output an abductive solution in finite time hence in this paper we propose a new abox abduction problem which ha only finitely many abductive solution and also propose a novel method to solve it the method reduces the original problem to an abduction problem in logic programming and solves it with prolog engine experimental result show that the method is able to compute abductive solution in benchmark owl dl ontology with large aboxes copyright association for the advancement of artificial intelligence all right reserved 
cross domain learning target at leveraging the knowledge from source domain to train accurate model for the test data from target domain with different but related data distribution to tackle the challenge of data distribution difference in term of raw feature previous work proposed to mine high level concept e g word cluster across data domain which show to be more appropriate for classification however all these work assume that the same set of concept are shared in the source and target domain in spite that some distinct concept may exist only in one of the data domain thus we need a general framework which can incorporate both shared and distinct concept for cross domain classification to this end we develop a probabilistic model by which both the shared and distinct concept can be learned by the em process which optimizes the data likelihood to validate the effectiveness of this model we intentionally construct the classification task where the distinct concept exist in the data domain the systematic experiment demonstrate the superiority of our model over all compared baseline especially on those much more challenging task 
medical treatment decision making is a good application of knowledge representation and reasoning we are particularly interested in using them to resolve treatment conflict a complicated condition when two treatment cannot be given simultaneously to a patient suffering from multiple symptom the logic system is required to reason on case with and without treatment conflict thanks to the nonmonotonicity of answer set programming asp we give an elegant solution for resolving a medical treatment conflict on an example problem and show the importance of nonmonotonicity in medical reasoning copyright association for the advancement of artificial intelligence all right reserved 
recommender system are becoming tool of choice to select the online information relevant to a given user collaborative filtering is the most popular approach to building recommender system and ha been successfully employed in many application with the advent of online social network the social network based approach to recommendation ha emerged this approach assumes a social network among user and make recommendation for a user based on the rating of the user who have direct or indirect social relation with the given user a one of their major benefit social network based approach have been shown to reduce the problem with cold start user in this paper we explore a model based approach for recommendation in social network employing matrix factorization technique advancing previous work we incorporate the mechanism of trust propagation into the model in a principled way trust propagation ha been shown to be a crucial phenomenon in the social science in social network analysis and in trust based recommendation we have conducted experiment on two real life data set our experiment demonstrate that modeling trust propagation lead to a substantial increase in recommendation accuracy in particular for cold start user 
we propose a collaborative filtering cf recommendation framework which is based on viewing user feedback on product a ordinal rather than the more common numerical view such an ordinal view frequently provides a more natural reflection of the user intention when providing qualitative rating allowing user to have different internal scoring scale moreover we can address scenario where assigning numerical score to different type of user feedback would not be easy the framework can wrap most collaborative filtering algorithm enabling algorithm previously designed for numerical value to handle ordinal value we demonstrate our framework by wrapping a leading matrix factorization cf method a cornerstone of our method is it ability to predict a full probability distribution of the expected item rating rather than only a single score for an item one of the advantage this brings is a novel approach to estimating the confidence level in each individual prediction compared to previous approach to confidence estimation ours is more principled and empirically superior in it accuracy we demonstrate the efficacy of the approach on two of the largest publicly available datasets the netflix data and the yahoo music data 
we present a formal investigation of artifact based system a relatively novel framework in service oriented computing aimed at laying the foundation for verifying these system through model checking we present an infinite state computationally grounded semantics for these system that allows u to reason about temporal epistemic specification we present abstraction technique for the semantics that guarantee transfer of satisfaction from the abstract system to the concrete one 
in this paper we state the challenge of high level program execution in multi agent setting we first introduce high level program execution and the related work then we describe the completed work the future work and it approach we conclude with the expected contribution of our research 
algorithm for stable marriage and related matching problem typically assume that full preference information is available while the gale shapley algorithm can be viewed a a mean of eliciting preference incrementally it doe not prescribe a general mean for matching with incomplete information nor is it designed to minimize elicitation we propose the use of maximum regret to measure the inverse degree of stability of a matching with partial preference minimax regret to find matchings that are maximally stable in the presence of partial preference and heuristic elicitation scheme that use max regret to determine relevant preference query we show that several of our scheme find stable matchings while eliciting considerably le preference information than gale shapley and are much more appropriate in setting where approximate stability is viable 
in recent year predicate invention ha been under explored within inductive logic programming due to difficulty in formulating efficient search mechanism however a recent paper demonstrated that both predicate invention and the learning of recursion can be efficiently implemented for regular and context free grammar by way of abduction with respect to a meta interpreter new predicate symbol are introduced a constant representing existentially quantified higher order variable in this paper we generalise the approach of meta interpretive learning mil to that of learning higher order dyadic datalog program we show that with an infinite signature the higher order dyadic datalog class h ha universal turing expressivity though h is decidable given a finite signature additionally we show that knuth bendix ordering of the hypothesis space together with logarithmic clause bounding allows our dyadic mil implementation metagold to pac learn minimal cardinailty h definition this result is consistent with our experiment which indicate that metagold efficiently learns compact h definition involving predicate invention for robotic strategy and higher order concept in the nell language learning domain 
recent year have seen a great interest in using deep architecture for feature learning from data one drawback of the commonly used unsupervised deep feature learning method is that for supervised or semi supervised learning task the information in the target variable are not used until the final stage when the classifier or regressor is trained on the learned feature this could lead to over generalized feature that are not competitive on the specific supervised or semi supervised learning task in this work we describe a new learning method that combine deep feature learning on mixed labeled and unlabeled data set specifically we describe a weakly supervised learning method of a prior supervised convolutional stacked auto encoders pcsa of which information in the target variable is represented probabilistically using a gaussian bernoulli restricted boltzmann machine rbm we apply this method to the decoding problem of an ecog based brain computer interface bci system our experimental result show that pcsa achieves significant improvement in decoding performance on benchmark data set compared to the unsupervised feature learning a well a to the current state of the art algorithm that are based on manually crafted feature 
consider a video surveillance application that monitor some location the application know a set of activity model that are either normal or abnormal or both but in addition the application want to find video segment that are unexplained by any of the known activity model these unexplained video segment may correspond to activity for which no previous activity model existed in this paper we formally define what it mean for a given video segment to be unexplained totally or partially w r t a given set of activity model and a probability threshold we develop two algorithm findtua and findpua to identify totally and partially unexplained activity respectively and show that both algorithm use important pruning method we report on experiment with a prototype implementation showing that the algorithm both run efficiently and are accurate 
coordination in cooperative multiagent system is an important problem in multiagent learning literature in practical complex environment the interaction between agent can be sparse and each agent s interacting partner may change frequently and randomly to this end we investigate the multiagent coordination problem in cooperative environment under the social learning framework we consider a large population of agent where each agent interacts with another agent randomly chosen from the population in each round each agent learns it policy through repeated interaction with the rest of agent via social learning it is not clear a priori if all agent can learn a consistent optimal coordination policy in such a situation we distinguish two type of learner individual action learner and joint action learner the learning performance of both learner are evaluated under a number of challenging cooperative game and the influence of the information sharing degree on the learning performance is investigated a well 
this paper proposes a new algorithm to compute the resilience of a social system or an ecosystem when it is defined in the framework of the mathematical viability theory it is applied to the problem of language coexistence although bilingual society do exist many language have disappeared and some seem endangered presently mathematical model of language competition generally conclude that one language will disappear except when the relative prestige of the language can be modified the viability theory provides concept and tool that are suitable to study the resilience but with severe computational limit since it us extensive search on regular grid the method we propose considers the computation of the viability output set a an active learning problem with the objective of restraining the number of call to the model and information storage we adapt a kd tree algorithm to approximate the level set of the resilience value we prove that this algorithm converges to the output set defined by the viability theory viability kernel and capture basin the resilience value we compute can then be used to propose a policy of action in risky situation such a migration flow 
query recommendation can not only effectively facilitate user to obtain their desired information but also increase ad click through rate this paper present a general and highly efficient method for query recommendation given query session we automatically generate many similar and dissimilar query pair a the prior knowledge then we learn a transformation from the prior knowledge to move similar query closer such that similar query tend to have similar hash value this is formulated a minimizing the empirical error on the prior knowledge while maximizing the gap between the data and some partition hyperplanes randomly generated in advance in the recommendation stage we search query that have similar hash value to the given query rank the found query and return the top k query a the recommendation result all the experimental result demonstrate that our method achieves encouraging result in term of efficiency and recommendation performance copyright association for the advancement of artificial intelligence all right reserved 
a fault represents some erroneous operation of a system that could result from an action selection error or some abnormal condition we formally define error model that characterize the likelihood of various fault and consider the problem of fault tolerant planning which optimizes performance given an error model we show that factoring the possibility of error significantly degrades the performance of stochastic planning algorithm such a lao because the number of reachable state grows dramatically we introduce an approach to plan for a bounded number of fault and analyze it theoretical property when combined with a continual planning paradigm the k fault tolerant planning method can produce near optimal performance even when the number of fault exceeds the bound empirical result in two challenging domain confirm the effectiveness of the approach in handling different type of runtime error 
in multi issue automated negotiation against unknown opponent a key part of effective negotiation is the choice of concession strategy in this paper we develop a principled concession strategy based on gaussian process predicting the opponent s future behaviour we then use this to set the agent s concession rate dynamically during a single negotiation session we analyse the performance of our strategy and show that it outperforms the state of the art negotiating agent from the automated negotiating agent competition in both a tournament setting and in self play across a variety of negotiation domain 
despite the importance of propositional logic in artificial intelligence the notion of language independence in the propositional setting not to be confound with syntax independence ha not received much attention so far in this paper we define language independence for a propositional operator a robustness w r t symbol translation we provide a number of characterization result for such translation we motivate the need to focus on symbol translation of restricted type and identify several family of interest we identify the computational complexity of recognizing symbol translation from those family finally a a case study we investigate the robustness of belief revision merging operator w r t translation of different type it turn out that rational belief revision merging operator are not guaranteed to offer the most basic yet non trivial form of language independence operator based on the hamming distance do not suffer from this drawback but are le robust than operator based on the drastic distance 
a new semantic forgetting for answer set program asp called sm forgetting is proposed in the paper it distinguishes itself from the others in that it preserve not only skeptical and credulous consequence on unforgotten variable but also strong equivalence forgetting same variable in strongly equivalent logic program ha strongly equivalent result the forgetting present a positive answer to gabbay pearce and valverde s open question if asp ha uniform interpolation property we also investigate some property algorithm and computational complexity for the forgetting it show that computing the forgetting result is generally intractable even for horn logic program 
cross domain collaborative filtering cdcf which aim to leverage data from multiple domain to relieve the data sparsity issue is becoming an emerging research topic in recent year however current cdcf method that mainly consider user and item factor but largely neglect the heterogeneity of domain may lead to improper knowledge transfer issue to address this problem we propose a novel cdcf model the bilinear multilevel analysis blma which seamlessly introduces multilevel analysis theory to the most successful collaborative filtering method matrix factorization mf specifically we employ blma to more efficiently address the determinant of rating from a hierarchical view by jointly considering domain community and user effect so a to overcome the issue caused by traditional mf approach moreover a parallel gibbs sampler is provided to learn these effect finally experiment conducted on a realworld dataset demonstrate the superiority of the blma over other state of the art method 
the best practice method for managing ecological system under uncertainty is adaptive management am an iterative process of reducing uncertainty while simultaneously optimizing a management objective existing solution method used for am problem assume that the system dynamic are stationary i e described by one of a set of pre defined model in reality ecological system are rarely stationary and evolve over time importantly the effect of climate change on population are unlikely to be captured by stationary model practitioner need efficient algorithm to implement am on real world problem am can be formulated a a hidden model markov decision process hmmdp which allows the state space to be factored and show promise for the rapid resolution of large problem we provide an ecological dataset and performance metric for the am of a network of shorebird specie utilizing the east asian australasian flyway given uncertainty about the rate of sea level rise the non stationary system is modelled a a stationary pomdp containing hidden alternative model with known probability of transition between them we challenge the pomdp community to exploit the simplification allowed by structuring the am problem a an hmmdp and improve our benchmark solution 
qualitative representation of spatial knowledge have been widely studied and a variety of framework are used to express relationship between static region dynamic region present a much greater challenge but are important in practical application such a describing crowd of people moving over time previous work ha analysed change a region merge and split and a new region are created and existing one disappear we present a novel framework for the qualitative description of spatial region based on two level of granularity introducing granularity yield significantly more informative qualitative description than are available from a single level of detail the formal model represents a region which may have multiple component a a bipartite graph where the node are the component of the region at a fine level of detail and at a coarse level the edge of the graph model the way that a component in the coarse view can be made up of part of component at the more detailed level we show that all graph of this form except for some degenerate case can be realized a region in a discrete space of pixel and we develop a theory of relation between these graph to model the dynamic behaviour of region 
this paper present a specialised bayesian model for analysing the covariance of data that are observed in the form of matrix which is particularly suitable for image compared to existing general purpose covariance learning technique we exploit the fact that the variable are organised a an array with two set of ordered index which induces innate relationship between the variable specifically we adopt a factorised structure for the covariance matrix the covariance of two variable is represented by the product of the covariance of the two corresponding row and that of the two column the factor i e the row wise and column wise covariance matrix are estimated by bayesian inference with sparse prior empirical study ha been conducted on image analysis the model first learns correlation between the row and column in an image plane then the correlation between individual pixel can be inferred by their location this scheme utilises the structural information of an image and benefit the analysis when the data are damaged or insufficient 
activity recognition using mobile phone ha great potential in many application including mobile healthcare in order to let a person easily know whether he is in strict compliance with the doctor s exercise prescription and adjust his exercise amount accordingly we can use a smart phone based activity reporting system to accurately recognize a range of daily activity and report the duration of each activity a triaxial accelerometer embedded in the smart phone is used for the classification of several activity such a staying still walking running and going upstairs and downstairs the model learnt from a specific person often cannot yield accurate result when used on a different person to solve the cross people activity recognition problem we propose an algorithm known a transemdt transfer learning embedded decision tree that integrates a decision tree and the k mean clustering algorithm for personalized activity recognition model adaptation tested on a real world data set the result show that our algorithm outperforms several traditional baseline algorithm 
in this paper we present an approach aimed at enriching the open information extraction paradigm with semantic relation ontologization by integrating syntactic and semantic feature into it workflow to achieve this goal we combine deep syntactic analysis and distributional semantics using a shortest path kernel method and soft clustering the output of our system is a set of automatically discovered and ontologized semantic relation 
adopt and bnb adopt are two optimal dcop search algorithm that are similar except for their search strategy the former us best first search and the latter us depth first branch and bound search in this paper we present a new algorithm called adopt k that generalizes them it behavior depends on the k parameter it behaves like adopt when k like bnb adopt when k and like a hybrid of adopt and bnb adopt when k k is a correct and complete algorithm and experimentally show that adopt k outperforms adopt and bnb adopt on several benchmark across several metric 
we describe a generative bayesian model for action understanding in which inverse forward internal model pair are considered hypothesis of plausible action goal that are explored in parallel via an approximate inference mechanism based on sequential monte carlo method the reenactment of internal model pair can be considered a form of motor simulation which support both perceptual prediction and action understanding at the goal level however this procedure is generally considered to be computationally inefficient we present a model that dynamically reallocates computational resource to more accurate internal model depending on both the available prior information and the prediction error of the inverse forward model and which lead to successful action recognition we present experimental result that test the robustness and efficiency of our model in real world scenario 
it is widely acknowledged that modern information sytems require an ontological layer on top of data associated with advanced reasoning mechanism able to exploit the semantics encoded in ontology we focus here on ontology based data access obda a new paradigm that seek to take ontological knowledge into account when querying data this paradigm is currently the subject of intense research in the database knowledge representation and reasoning and semantic web community indeed it is expected to have a major impact in many application domain however some foundational issue need first to be adressed in this context we consider an emerging logical framework based on existential rule also known a datalog this framework can also be defined in graph term compared to the lighweight description logic currently developed for obda it is more powerful and flexible an important feature is that predicate arity is not restricted which allows for a natural coupling with database schema and facilitates the integration of additional information such a contextual knowledge on the other hand the existential rule framework extends the deductive database language datalog by enabling to infer the existence of entity that do not necessarily occur in the database hence the name existential rule a feature that ha been recognized a crucial in the context of incomplete information in this talk we will provide an introduction to this framework in the context of obda then present the main decidability and complexity result a well a algorithmic technique and discus some challenging research issue 
recently the low cost microsoft kinect sensor which can capture real time high resolution rgb and depth visual information ha attracted increasing attention for a wide range of application in computer vision existing technique extract hand tuned feature from the rgb and the depth data separately and heuristically fuse them which would not fully exploit the complementarity of both data source in this paper we introduce an adaptive learning methodology to automatically extract holistic spatio temporal feature simultaneously fusing the rgb and depth information from rgb d video data for visual recognition task we address this a an optimization problem using our proposed restricted graph based genetic programming rggp approach in which a group of primitive d operator are first randomly assembled a graph based combination and then evolved generation by generation by evaluating on a set of rgb d video sample finally the best performed combination is selected a the near optimal representation for a pre defined task the proposed method is systematically evaluated on a new hand gesture dataset skig that we collected ourselves and the public msr daily activity d dataset respectively extensive experimental result show that our approach lead to significant advantage compared with state of the art hand crafted and machine learned feature 
we compare the expressiveness of the fragment of halpern and shoham s interval logic h i e of all interval logic with modal operator associated with allen s relation between interval in linear order we establish a complete set of inter definability equation between these modal operator and thus obtain a complete classification of the family of fragment of h with respect to their expressiveness using that result and a computer program we have found that there are expressively different such interval logic over the class of all linear order 
most of the existing personalization system such a content recommenders or targeted ad focus on individual user and ignore the social situation in which the service are consumed however many human activity are social and involve several individual whose taste and expectation must be taken into account by the system when a group profile is not available different profile aggregation strategy can be applied to recommend adequate item to a group of user based on their individual profile we consider an approach intended to determine the factor that influence the choice of an aggregation strategy we present evaluation made on a large scale dataset of tv viewing where real group interest are compared to the prediction obtained by combining individual user profile according to different strategy 
there is a fundamental incompatibility between efficiency interim individual rationality and budget balance in mechanism design even for extremely simple setting yet it is possible to specify efficient mechanism that satisfy participation and budget balance constraint in expectation prior to type being realized we do so here in fact deriving mechanism that are individually rational for each agent even ex post of other agent type realization however participation must still bear some risk of loss for agent that are risk neutral we show how the center can extract the entire surplus in expectation or alternatively provide an equal expected share of the surplus for each participant without violating dominant strategy incentive compatibility efficiency or ex ante budget balance we compare these solution to a third efficient mechanism we design explicitly to address risk aversion in trade setting payment are defined to minimize the odds of loss satisfying ex ante participation constraint for agent with attitude toward risk ranging from neutrality to high loss aversion 
we present a novel unsupervised data analysis method multi feature information bottleneck mfib which is an extension of the information bottleneck ib in comparison with the original ib the proposed mfib method can analyze the data simultaneously from multiple feature variable which characterize the data from multiple cue to verify the effectiveness of mfib we apply the corresponding mfib algorithm to unsupervised image categorization in our experiment by taking into account multiple type of feature such a local shape color and texture the mfib algorithm is found to be consistently superior to the original ib algorithm which take only one source of feature into consideration besides the performance of mfib algorithm is also superior to the state of the art unsupervised image categorization method 
partially supervised text classification ha received great research attention since it only us positive and unlabeled example a training data this problem can be solved by automatically labeling some negative and more positive example from unlabeled example before training a text classifier but it is difficult to guarantee both high quality and quantity of the new labeled example in this paper a multi level example based learning method for partially supervised text classification is proposed which can make full use of all unlabeled example a heuristic method is proposed to assign possible label to unlabeled example and partition them into multiple level according to their labeling confidence a text classifier is trained on these multi level example using weighted support vector machine experiment show that the multi level example based learning method is effective for partially supervised text classification and outperforms the existing popular method such a biased svm roc svm s em and wl copyright association for the advancement of artificial intelligence all right reserved 
coherence that tie sentence of a text into a meaningfully connected structure is of great importance to text generation and translation in this paper we propose a topic based coherence model to produce coherence for document translation in term of the continuity of sentence topic in a text we automatically extract a coherence chain for each source text to be translated based on the extracted source coherence chain we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document the proposed topic based coherence model then us the predicted target coherence chain to help decoder select coherent word phrase translation our experiment show that incorporating the topic based coherence model into machine translation achieves substantial improvement over both the baseline and previous method that integrate document topic rather than coherence chain into machine translation copyright association for the advancement of artificial intelligence www aaai org all right reserved 
this paper address one of the key component of the mining process the geological prediction of natural resource from spatially distributed measurement we present a novel approach combining undirected graphical model with ensemble classifier to provide d geological model from multiple sensor installed in an autonomous drill rig drill sensor measurement used for drilling automation known a measurement while drilling mwd data have the potential to provide an estimate of the geological property of the rock being drilled the proposed method map mwd parameter to rock type while considering spatial relationship i e associating measurement obtained from neighboring region we use a conditional random field with local information provided by boosted decision tree to jointly reason about the rock category of neighboring measurement to validate the approach mwd data wa collected from a drill rig operating at an iron ore mine graphical model of the d structure present in real data set posse a high number of node edge and cycle making them intractable for exact inference we provide a comparison of three approximate inference method to calculate the most probable distribution of class label the empirical result demonstrate the benefit of spatial modeling through graphical model to improve classification performance 
information retrieval may suggest a document and information extraction may tell u what it say but which information source do we trust and which assertion do we believe when different author make conflicting claim trust algorithm known a fact finder attempt to answer these question but consider only which source make which claim ignoring a wealth of background knowledge and contextual detail such a the uncertainty in the information extraction of claim from document attribute of the source the degree of similarity among claim and the degree of certainty expressed by the source we introduce a new generalized fact finding framework able to incorporate this additional information into the fact finding process experiment using several state of theart fact finding algorithm demonstrate that generalized fact finder achieve significantly better performance than their original variant on both semi synthetic and real world problem 
we study a decidable fixpoint extension of temporal description logic to this end we employ and extend decidability result obtained for various temporally first order monodic extension of firstorder description logic using these technique we obtain decidability and tight complexity result for various fixpoint extension of temporal description logic 
most of the algorithm for inverse reinforcement learning irl assume that the reward function is a linear function of the pre defined state and action feature however it is often difficult to manually specify the set of feature that can make the true reward function representable a a linear function we propose a bayesian nonparametric approach to identifying useful composite feature for learning the reward function the composite feature are assumed to be the logical conjunction of the pre defined atomic feature so that we can represent the reward function a a linear function of the composite feature we empirically show that our approach is able to learn composite feature that capture important aspect of the reward function on synthetic domain and predict taxi driver behaviour with high accuracy on a real gps trace dataset 
this paper present a novel latent semantic learning algorithm for action recognition through efficient sparse coding we can learn latent semantics i e high level feature from a large vocabulary of abundant mid level feature i e visual keywords more importantly we can capture the manifold structure hidden among midlevel feature by incorporating hypergraph regularization into sparse coding the learnt latent semantics can further be readily used for action recognition by defining a histogram intersection kernel different from the traditional latent semantic analysis based on topic model our sparse coding method with hypergraph regularization can exploit the manifold structure hidden among mid level feature for latent semantic learning which result in compact but discriminative high level feature for action recognition we have tested our method on the commonly used kth action dataset and the unconstrained youtube action dataset the experimental result show the superior performance of our method copyright association for the advancement of artificial intelligence all right reserved 
image are usually associated with multiple label and comprised of multiple view due to each image containing several object e g a pedestrian bicycle and tree and multiple visual feature e g color texture and shape currently available tool tend to use either label or feature for classification but both are necessary to describe the image properly there have been recent success in using vector valued function which construct matrix valued kernel to explore the multi label structure in the output space this ha motivated u to develop multi view vector valued manifold regularization mv mr in order to integrate multiple feature mv mr exploit the complementary property of different feature and discovers the intrinsic local geometry of the compact support shared by different feature under the theme of manifold regularization we validate the effectiveness of the proposed mv mr methodology for image classification by conducting extensive experiment on two challenge datasets pascal voc and mir flickr copyright association for the advancement of artificial intelligence www aaai org all right reserved 
affinity propagation is a state of the art clustering method recently proposed by frey and dueck it ha been successfully applied to broad area of computer science research because it ha much better clustering performance than traditional clustering method such a k mean in order to obtain high quality set of cluster the original affinity propagation algorithm iteratively exchange real valued message between all pair of data point until convergence however this algorithm doe not scale for large datasets because it requires quadratic cpu time in the number of data point to compute the message this paper proposes an efficient affinity propagation algorithm that guarantee the same clustering result a the original algorithm after convergence the heart of our approach is to prune unnecessary message exchange in the iteration and to compute the convergence value of prunedmessages after the iteration to determine cluster experimental evaluation on several different datasets demonstrate the effectiveness of our algorithm 
ensemble learning with output from multiple supervised and unsupervised model aim to improve the classification accuracy of supervised model ensemble by jointly considering the grouping result from unsupervised model in this paper we cast this ensemble task a an unconstrained probabilistic embedding problem specifically we assume both object and class cluster have latent coordinate without constraint in a d dimensional euclidean space and consider the mapping from the embedded space into the space of result from supervised and unsupervised model a a probabilistic generative process the prediction of an object is then determined by the distance between the object and the class in the embedded space a solution of this embedding can be obtained using the quasi newton method resulting in the object and class cluster with high co occurrence weight being embedded close we demonstrate the benefit of this unconstrained embedding method by three real application 
in his seminal work plaza plaza proposed the public announcement logic pal which is considered a the pilot logic in the field of dynamic epistemic logic in the same paper plaza also introduced an interesting know value operator kv and listed a few valid formula of pal kv however it is unknown that whether these formula on top of the axiom for pal completely axiomatize pal kv in this paper we first give a negative answer to this open problem moreover we generalize the kv operator and show that in the setting of pal replacing the kv operator with it generalized version doe not increase the expressive power of the resulting logic this suggests that we can simply use the more flexible generalization instead of the original pal kv a the main result we give a complete proof system for pal plus the generalized operator based on a complete axiomatization of epistemic logic with the same operator in the single agent setting 
the high computational complexity of the expressive description logic dl that underlie the owl standard ha motivated the study of their horn fragment which are usually tractable in data complexity and can also have lower combined complexity particularly for query answering in this paper we provide algorithm for answering conjunctive way regular path query crpqs a non trivial generalization of plain conjunctive query in the horn fragment of the dl shoiq and sroiq underlying owl and owl we show that the combined complexity of the problem is exptime complete for horn shoiq and exptimecomplete for the more expressive horn sroiq but is ptime complete in data complexity for both in contrast even decidability of plain conjunctive query is still open for full shoiq and sroiq these are the first completeness result for query answering in dl with inverse nominal and counting and show that for the considered logic the problem is not more expensive than standard reasoning 
we propose a unified approach to plan execution and schedule dispatching that convert a plan which ha been augmented with temporal constraint into a policy for dispatching our approach generalizes the original plan and temporal constraint so that the executor need only consider the subset of state that is relevant to successful execution of valid plan fragment we can accommodate a variety of calamitous and serendipitous change to the state of the world by supporting the seamless re execution or omission of plan fragment without the need for costly replanning our methodology for plan generalization and online dispatching is a novel combination of plan execution and schedule dispatching technique we demonstrate the effectiveness of our method through a prototype implementation and a series of experiment 
event anaphora resolution play a critical role in discourse analysis this paper proposes a tree kernel based framework for event pronoun resolution in particular a new tree expansion scheme is introduced to automatically determine a proper parse tree structure for event pronoun resolution by considering various kind of competitive information related with the anaphor and the antecedent candidate evaluation on the ontonotes english corpus show the appropriateness of the tree kernel based framework and the effectiveness of competitive information for event pronoun resolution 
this paper introduces a novel multimodular method for reinforcement learning a multimodular system is one that partition the learning task among a set of expert module where each expert is incapable of solving the entire task by itself there are many advantage to splitting up large task in this way but existing method face difficulty when choosing which module s should contribute to the agent s action at any particular moment we introduce a novel selection mechanism where every module besides calculating a set of action value also estimate it own error for the current input the selection mechanism combine each module s estimate of long term reward and self error to produce a score by which the next module is chosen a a result the module can use their resource effectively and efficiently divide up the task the system is shown to learn complex task even when the individual module use only linear function approximators 
answer set programming modulo theory is a new framework of tight integration of answer set programming asp and satisfiability modulo theory smt similar to the relationship between first order logic and smt it is based on a recent proposal of the functional stable model semantics by fixing interpretation of background theory analogously to a known relationship between asp and sat tight aspmt program can be translated into smt instance we demonstrate the usefulness of aspmt by enhancing action language c to handle continuous change a well a discrete change we reformulate the semantics of c in term of aspmt and show that smt solver can be used to compute the language we also show how the language can represent cumulative effect on continuous resource 
imitation learning refers to the problem of learning how to behave by observing a teacher in action we consider imitation learning in relational domain in which there is a varying number of object and relation among them in prior work simple relational policy are learned by viewing imitation learning a supervised learning of a function from state to action for propositional world functional gradient method have been proved to be beneficial they are simpler to implement than most existing method more efficient more naturally satisfy common constraint on the cost function and better represent our prior belief about the form of the function building on recent generalization of functional gradient boosting to relational representation we implement a functional gradient boosting approach to imitation learning in relational domain in particular given a set of trace from the human teacher our system learns a policy in the form of a set of relational regression tree that additively approximate the functional gradient the use of multiple additive tree combined with relational representation allows for learning more expressive policy than what ha been done before we demonstrate the usefulness of our approach in several different domain 
combinatorial optimization is an important area of computer science that ha many theoretical and practical application in the thesis chu we present important contribution to several different area of combinatorial optimization including nogood learning symmetry breaking dominance relaxation and parallelization we develop a new nogood learning technique based on constraint projection that allows u to exploit subproblem dominance that arise when two different search path lead to subproblems which are identical on the remaining unfixed variable we present a new symmetry breaking technique called sbds uip which extends symmetry breaking during search sbds by using the more powerful uip nogoods generated by lazy clause generation lcg solver we present two new general method for exploiting almost symmetry by modifying sbds uip and by using conditional symmetry breaking constraint we solve the minimization of open stack problem the talent scheduling problem csplib prob and the maximum density still life problem csplib prob many order of magnitude faster than the previous state of the art by applying various powerful technique such a nogood learning dynamic programming dominance and relaxation we present cache aware data structure for sat solver which allows sequential and parallel version of sat solver to run more quickly and we present a new load balancing scheme for parallel search called confidence based work stealing which allows the parallel search to make use of the information contained in the branching heuristic 
we investigate the problem of influencing the preference of player within a boolean game so that if all player act rationally certain desirable outcome will result the way in which we influence preference is by overlaying game with taxation scheme in a boolean game each player ha unique control of a set of boolean variable and the choice available to the player correspond to the possible assignment that may be made to these variable each player also ha a goal represented by a boolean formula that they desire to see satisfied whether or not a player s goal is satisfied will depend both on their own choice and on the choice of others which give boolean game their strategic character we extend this basic framework by introducing an external principal who is able to levy a taxation scheme on the game which imposes a cost on every possible action that a player can choose by designing a taxation scheme appropriately it is possible to perturb the preference of the player so that they are incentivised to choose some equilibrium that would not otherwise be chosen after motivating and formally presenting our model we explore some issue surrounding it including the complexity of finding a taxation scheme that implement some socially desirable outcome and then discus desirable property of taxation scheme 
this paper identifies a widely existing phenomenon in web data which we call the word of few mouth phenomenon this phenomenon in the context of online review refers to the case that a large fraction of the review are each voted only by very few user we discus the challenge of word of few mouth in the development of recommender system based on user opinion and advocate probabilistic methodology to handle such challenge we develop a probabilistic model and correspondingly a logistic regression based learning algorithm for review helpfulness prediction our experimental result indicate that the proposed model outperforms the current state of the art algorithm not only in the presence of the word of few mouth phenomenon but also in the absence of such phenomenon 
the growth of social medium and on line social network ha opened up a set of fascinating new challenge and direction for researcher in both computing and the social science and an active interface is growing between these area we discus a set of basic question that arise in the design and analysis of system supporting on line social interaction focusing on two main issue the role of network structure in the dynamic of social medium site and the analysis of textual data a a way to study property of on line social interaction 
probabilistic abstract argumentation combine dung s abstract argumentation framework with probability theory in order to model uncertainty in argumentation in this setting we address the fundamental problem of computing the probability that a set of argument is an extension according to a given semantics we focus on the most popular semantics i e admissible stable complete grounded preferred ideal and show the following dichotomy result computing the probability that a set of argument is an extension is either ptime or fp p complete depending on the semantics adopted our ptime result are particularly interesting a they hold for some semantics for which no polynomial time technique wa known so far 
this paper proposes the integration of the resolution rule for max sat with unsatisfiability based max sat solver first we show that the resolution rule for max sat can be safely applied a dictated by the resolution proof associated with an unsatisfiable core when such proof is read once that is each clause is used at most once in the resolution process second we study how this property can be integrated in an unsatisfiability based solver in particular the resolution rule for max sat is applied to read once proof or to read once subpart of a general proof finally we perform an empirical investigation on structured instance from recent max sat evaluation preliminary result show that the use of read once resolution substantially improves the performance of the solver 
we consider the mechanism design problem for agent with single peaked preference over multi dimensional domain when multiple alternative can be chosen facility location and committee selection are classic embodiment of this problem we propose a class of percentile mechanism a form of generalized median mechanism that are strategy proof and derive worst case approximation ratio for social cost and maximum load for l and l cost model more importantly we propose a sample based framework for optimizing the choice of percentile relative to any prior distribution over preference while maintaining strategy proofness our empirical investigation using social cost and maximum load a objective demonstrate the viability of this approach and the value of such optimized mechanism vi vi mechanism derived through worst case analysis 
tournament solution i e function that associate with each complete and asymmetric relation on a set of alternative a non empty subset of the alternative play an important role within social choice theory and the mathematical social science at large laffond et al have shown that various tournament solution satisfy composition consistency a structural invariance property based on the similarity of alternative we define the decomposition degree of a tournament a a parameter that reflects it decomposability and show that computing any composition consistent tournament solution is fixed parameter tractable with respect to the decomposition degree furthermore we experimentally investigate the decomposition degree of two natural distribution of tournament and it impact on the running time of computing the tournament equilibrium set 
default reasoning and interpolation are two important form of commonsense rule based reasoning the former allows u to draw conclusion from incompletely specified state by making assumption on normality whereas the latter allows u to draw conclusion from state that are not explicitly covered by any of the available rule although both approach have received considerable attention in the literature it is at present not well understood how they can be combined to draw reasonable conclusion from incompletely specified state and incomplete rule base in this paper we introduce an inference system for interpolating default rule based on a geometric semantics in which normality is related to spatial density and interpolation is related to geometric betweenness we view default rule and information on the betweenness of natural category a particular type of constraint on qualitative representation of g rdenfors conceptual space we propose an axiomatization extending the well known system p and show it soundness and completeness w r t the proposed semantics subsequently we explore how our extension of preferential reasoning can be further refined by adapting two classical approach for handling the irrelevance problem in default reasoning rational closure and conditional entailment 
evolutionary algorithm ea are a large family of heuristic optimization algorithm inspired by natural phenomenon and are often used in practice to obtain satisficing instead of optimal solution in this work we investigate a largely underexplored issue the approximation performance of ea in term of how close the obtained solution is to an optimal solution we study an ea framework named simple ea with isolated population seip that can be implemented a a singleor multi objective ea we present general approximation result of seip and specifically on the minimum set cover problem we find that seip achieves the currently best achievable approximation ratio moreover on an instance class of the k set cover problem we disclose how seip can overcome the difficulty that limit the greedy algorithm 
we study the problem of diverse promoting recommendation task selecting a subset of diverse item that can better predict a given user s preference recommendation technique primarily based on user or item similarity can suffer from the risk that user cannot get expected information from the over specified recommendation list in this paper we propose an entropy regularizer to capture the notion of diversity the entropy regularizer ha good property in that it satisfies monotonicity and submodularity such that when we combine it with a modular rating set function we get submodular objective function which can be maximized approximately by efficient greedy algorithm with provable constant factor guarantee of optimality we apply our approach on the top k prediction problem and evaluate it performance on movie lens data set which is a standard database containing movie rating data collected from a popular online movie recommender system we compare our model with the state of the art recommendation algorithm our experiment show that entropy regularizer effectively capture diversity and hence improves the performance of recommendation task 
entity linking map name mention in the document to entry in a knowledge base through resolving the name variation and ambiguity in this paper we propose three advancement for entity linking firstly expanding acronym can effectively reduce the ambiguity of the acronym mention however only rule based approach relying heavily on the presence of text marker have been used for entity linking in this paper we propose a supervised learning algorithm to expand more complicated acronym encountered which lead to accuracy improvement over state of the art acronym expansion method secondly a entity linking annotation is expensive and labor intensive to automate the annotation process without compromise of accuracy we propose an instance selection strategy to effectively utilize the automatically generated annotation in our selection strategy an informative and diverse set of instance are selected for effective disambiguation lastly topic modeling is used to model the semantic topic of the article these advancement give statistical significant improvement to entity linking individually collectively they lead the highest performance on kbp task 
multi label learning method assign multiple label to one object in practice in addition to differentiating relevant label from irrelevant one it is often desired to rank the relevant label for an object whereas the ranking of irrelevant label are not important such a requirement however cannot be met because most existing method were designed to optimize existing criterion yet there is no criterion which encodes the aforementioned requirement in this paper we present a new criterion pro loss concerning the prediction on all label a well a the ranking of only relevant label we then propose prosvm which optimizes pro loss efficiently using alternating direction method of multiplier we further improve it efficiency with an upper approximation that reduces the number of constraint from o t to o t where t is the number of label experiment show that our proposal are not only superior on pro loss but also highly competitive on existing evaluation criterion copyright association for the advancement of artificial intelligence www aaai org all right reserved 
in this paper we propose an approach to build a multi way concept hierarchy from a text corpus which is based on wordnet and multi way hierarchical clustering in addition a new evaluation metric is presented and our approach is compared with kind of existing method on the amazon customer review data set copyright association for the advancement of artificial intelligence www aaai org all right reserved 
the counterfactual regret minimization cfr algorithm is state of the art for computing strategy in large game and other sequential decision making problem little is known however about cfr in game with more than player this extended abstract outline research towards a better understanding of cfr in multiplayer game and new procedure for computing even stronger multiplayer strategy we summarize work already completed that investigates technique for creating expert strategy for playing smaller sub game and work that prof cfr avoids class of undesirable strategy in addition we provide an outline of our future research direction our goal are to apply regret minimization to the problem of playing multiple game simultaneously and augment cfr to achieve effective on line opponent modelling of multiple opponent the objective of this research is to build a world class computer poker player for multiplayer limit texas hold em 
we present a reformulation of the stochastic optimal control problem in term of kl divergence minimisation not only providing a unifying perspective of previous approach in this area but also demonstrating that the formalism lead to novel practical approach to the control problem specifically a natural relaxation of the dual formulation give rise to exact iterative solution to the finite and infinite horizon stochastic optimal control problem while direct application of bayesian inference method yield instance of risk sensitive control 
the bin packing problem is to partition a multiset of n number into a few bin of capacity c a possible such that the sum of the number in each bin doe not exceed c we compare two existing algorithm for solving this problem bin completion bc and branch and cut and price bcp we show experimentally that the problem difficulty and dominant algorithm are a function of n the precision of the input element and the number of bin in an optimal solution we describe three improvement to bc which result in a speedup of up to five order of magnitude a compared to the original bc algorithm while the current belief is that bcp is the dominant bin packing algorithm we show that improved bc is up to five order of magnitude faster than a state of the art bcp algorithm on problem with relatively few bin we then explore a closely related problem the number partitioning problem and show that an algorithm based on improved bin packing is up to three order of magnitude faster than a bcp solver called dimm which claim to be state of the art finally we show how to use number partitioning to generate difficult bin packing instance 
recently tag recommendation tr ha become a very hot research topic in data mining and related area however neither co occurrence based method which only use the item tag matrix nor content based method which only use the item content information can achieve satisfactory performance in real tr application hence how to effectively combine the item tag matrix item content information and other auxiliary information into the same recommendation framework is the key challenge for tr in this paper we first adapt the collaborative topic regression ctr model which ha been successfully applied for article recommendation to combine both item tag matrix and item content information for tr furthermore by extending ctr we propose a novel hierarchical bayesian model called ctr with social regularization ctr sr to seamlessly integrate the item tag matrix item content information and social network between item into the same principled model experiment on real data demonstrate the effectiveness of our proposed model 
we study map matching the problem of estimating the route that is traveled by a vehicle where the point observed with the global positioning system are available a state of the art approach for this problem is a hidden markov model hmm we propose a particular transition probability between latent road segment by the use of the number of turn in addition to the travel distance between the latent road segment we use inverse reinforcement learning to estimate the importance of the number of turn relative to the travel distance this estimated importance is incorporated in the transition probability of the hmm we show through numerical experiment that the error of map matching can be reduced substantially with the proposed transition probability 
agent system based on the bdi paradigm need to make decision about which plan are used to achieve their goal usually the choice of which plan to use to achieve a particular goal is left up to the system to determine in this paper we show how preference which can be set by the user of the system can be incorporated into the bdi execution process and used to guide the choice made 
answer set programming is the most appreciated framework for non monotonic reasoning stable model semantics a the semantics behind this success ha been subject to many extension the two main such extension are equilibrium model and flp semantics despite their very interesting foundation they both have two problem they cannot guarantee either minimality or rationality of their intended model that is both these semantics allow model in which some atom are self justified i e the only possible reason for including those atom in the model are those atom themselves present paper extends stable model semantics to the full propositional language while guaranteeing both property above our extension is called supported because it guarantee the existence of noncircular justification for all atom in a supported model these goal are achieved through a form of completion in intuitionistic logic we also discus how supported model relate to other semantics for non monotonic reasoning such a equilibrium model finally we discus the complexity of reasoning about supported model and show that the complexity of brave cautious reasoning in supported semantics remains a before i e the rationality property come for no additional cost 
one of main difficulty of multi dimensional packing problem is the fragmentation of free space into several unusable small part after a few item are packed this study proposes a defragmentation technique to combine the fragmented space into a continuous usable space which potentially allows the packing of additional item we illustrate the effectiveness of this technique on the two and three dimensional bin packing problem in conjunction with a bin shuffling strategy for incremental improvement our resultant algorithm outperforms all leading meta heuristic approach 
we establish the unexpected power of conflict driven clause learning cdcl proof search by proving that the set of unsatisfiable clause obtained from the guarded graph tautology principle of alekhnovich johannsen pitassi and urquhart have polynomial size pool resolution refutation that use only input lemma a learned clause we further show that under the correct heuristic choice these refutation can be carried out in polynomial time by cdcl proof search without restarts even when restricted to greedy unit propagating search the guarded graph tautology had been conjectured to separate cdcl without restarts from resolution our result refute this conjecture 
the ipdb procedure by haslum et al is the state of the art method for computing additive abstraction heuristic for domain independent planning it performs a hill climbing search in the space of pattern collection combining information from multiple pattern in the so called canonical heuristic we show how stronger heuristic estimate can be obtained through linear programming an experimental evaluation demonstrates the strength of the new technique on the ipc benchmark suite 
binary aggregation study problem in which individual express yes no choice over a number of possibly correlated issue and these individual choice need to be aggregated into a collective choice we show how several classical framework of social choice theory particularly preference and judgment aggregation can be viewed a binary aggregation problem by designing an appropriate set of integrity constraint for each specific setting we explore the generality of this framework showing that it make available useful technique both to prove theoretical result such a a new impossibility theorem in preference aggregation and to analyse practical problem such a the characterisation of safe agenda in judgment aggregation in a syntactic way the framework also allows u to formulate a general definition of paradox that is independent of the domain under consideration which give rise to the study of the class of aggregation procedure of generalised dictatorship 
this paper introduces streaming half space tree h tree a fast one class anomaly detector for evolving data stream it requires only normal data for training and work well when anomalous data are rare the model feature an ensemble of random h tree and the tree structure is constructed without any data this make the method highly efficient because it requires no model restructuring when adapting to evolving data stream our analysis show that streaming h tree ha constant amortised time complexity and constant memory requirement when compared with a state of the art method our method performs favourably in term of detection accuracy and runtime performance our experimental result also show that the detection performance of streaming h tree is not sensitive to it parameter setting 
we analyze the foundation of cyclic causal model for discrete variable and compare structural equation model sems to an alternative semantics a the equilibrium stationary distribution of a markov chain we show under general condition discrete cyclic sems cannot have independent noise even in the simplest case cyclic structural equation model imply constraint on the noise we give a formalization of an alternative markov chain equilibrium semantics which requires not only the causal graph but also a sample order we show how the resulting equilibrium is a function of the sample ordering both theoretically and empirically 
the proposal of elisa marengo s thesis is to extend commitment protocol in order to i allow for expressing commitment to temporal regulation and ii to supply a tool for expressing law convention and the like in order to specify legal interaction these two aspect will be deeply investigated in the proposal of a unified framework this proposal is part of ongoing work that will be included in the thesis 
the usual representation of quantitative data is to formalize it a an information table which assumes the independence of attribute in real world data attribute are more or le interacted and coupled via explicit or implicit relationship limited research ha been conducted on analyzing such attribute interaction which only describe a local picture of attribute coupling in an implicit way this paper proposes a framework of the coupled attribute analysis to capture the global dependency of continuous attribute such global coupling integrate the intra coupled interaction within an attribute i e the correlation between attribute and their own power and inter coupled interaction among different attribute i e the correlation between attribute and the power of others to form a coupled representation for numerical object by the taylor like expansion this work make one step forward towards explicitly addressing the global interaction of continuous attribute verified by the application in data structure analysis data clustering and data classification substantial experiment on uci data set demonstrate that the coupled representation can effectively capture the global coupling of attribute and outperforms the traditional way supported by statistical analysis 
we define the class of e bounded theory in the epistemic situation calculus where the number of fluent atom that the agent think may be true is bounded by a constant such theory can still have an infinite domain and an infinite set of state we show that for them verification of an expressive class of first order calculus temporal epistemic property is decidable we also show that if the agent s knowledge in the initial situation is e bounded and the objective part of an action theory maintains boundedness then the entire epistemic theory is e bounded 
problem that require multiple agent to follow non interfering path from their current state to their respective goal state are called cooperative pathfinding problem we present the first complete algorithm for finding these path that is sufficiently fast for real time application furthermore our algorithm offer a trade off between running time and solution quality we then refine our algorithm into an anytime algorithm that first quickly find a solution and then us any remaining time to incrementally improve that solution until it is optimal or the algorithm is terminated we compare our algorithm to those in the literature and show that in addition to completeness our algorithm offer improved solution quality a well a competitive running time 
this thesis analyzes the performance of multiobjective heuristic graph search algorithm the analysis is focused on the influence of heuristic information correlation between objective and solution depth 
there are many complex combinatorial problem which involve searching for an undirected graph satisfying a certain property these problem are often highly challenging because of the large number of isomorphic representation of a possible solution in this paper we introduce novel effective and compact symmetry breaking constraint for undirected graph search while incomplete these prove highly beneficial in pruning the search for a graph we illustrate the application of symmetry breaking in graph representation to resolve several open instance in extremal graph theory 
recently there ha been an increasing interest in incorporating intensional function in answer set programming intensional function are those whose value can be described by other function and predicate rather than being pre defined a in the standard answer set programming we demonstrate that the functional stable model semantics play an important role in the framework of answer set programming modulo theory aspmt a tight integration of answer set programming and satisfiability modulo theory under which existing integration approach can be viewed a special case where the role of function is limited we show that tight aspmt program can be translated into smt instance which is similar to the known relationship between asp and sat 
the paper describes a method of relation extraction which is based on parsing the input text using a combination of a generic hpsg based grammar and a highly focused domain and relation specific lexicon we also show a method of unsupervised acquisition of such a lexicon from a large unlabeled corpus together the method introduce a novel approach to the open ie task which is superior in accuracy and in quality of relation identification to the existing approach 
one challenge in making online education more effective is to develop automatic grading software that can provide meaningful feedback this paper provides a solution to automatic grading of the standard computation theory problem that asks a student to construct a deterministic finite automaton dfa from the given description of it language we focus on how to assign partial grade for incorrect answer each student s answer is compared to the correct dfa using a hybrid of three technique devised to capture different class of error first in an attempt to catch syntactic mistake we compute the edit distance between the two dfa description second we consider the entropy of the symmetric difference of the language of the two dfas and compute a score that estimate the fraction of the number of string on which the student answer is wrong our third technique is aimed at capturing mistake in reading of the problem description for this purpose we consider a description language mosel which add syntactic sugar to the classical monadic second order logic and allows defining regular language in a concise and natural way we provide algorithm along with optimization for transforming mosel description into dfas and vice versa these allow u to compute the syntactic edit distance of the incorrect answer from the correct one in term of their logical representation we report an experimental study that evaluates hundred of answer submitted by real student by comparing grade feedback computed by our tool with human grader our conclusion is that the tool is able to assign partial grade in a meaningful way and should be preferred over the human grader for both scalability and consistency 
the partner unit problem is a specific type of configuration problem with important application in the area of surveillance and security in this work we show that a special case of the problem that is of great interest to our partner in industry can directly be tackled via a structural problem decompostion method combining these theoretical insight with general purpose ai technique such a constraint satisfaction and sat solving prof to be particularly effective in practice 
we define a family of epistemic extension of halpern shoham logic for reasoning about temporal epistemic property of multi agent system we exemplify their use and study the complexity of their model checking problem we show a range of result ranging from ptime to pspace hard depending on the logic considered 
error correcting output code ecoc are a successful technique to combine a set of binary classifier for multi class learning problem however in traditional ecoc framework all the base classifier are trained independently according to the defined ecoc matrix in this paper we reformulate the ecoc model from the perspective of multi task learning where the binary classifier are learned in a common subspace of data this novel model can be considered a an adaptive generalization of the traditional ecoc framework it simultaneously optimizes the representation of data a well a the binary classifier more importantly it build a bridge between the ecoc framework and multitask learning for multi class learning problem to deal with complex data we also present the kernel extension of the proposed model extensive empirical study on data set from uci machine learning repository and the usps handwritten digit recognition application demonstrates the effectiveness and efficiency of our model 
security is a critical concern around the world since resource for security are always limited lot of interest have arisen in using game theory to handle security resource allocation problem however most of the existing work doe not address adequately how a defender chooses his optimal strategy in a game with absent inaccurate uncertain and even ambiguous strategy profile payoff to address this issue we propose a general framework of security game under ambiguity based on dempster shafer theory and the ambiguity aversion principle of minimax regret then we reveal some property of this framework also we present two method to reduce the influence of complete ignorance our investigation show that this new framework is better in handling security resource allocation problem under ambiguity 
recently how to recommend celebrity to the public becomes an interesting problem on the social network website such a twitter and tencent weibo in this paper we proposed a unified hierarchical bayesian model to recommend celebrity to the general user specifically we proposed to leverage both social network and description of celebrity to improve the prediction ability and recommendation interpretability in our model we combine topic model with matrix factorization for both social network of celebrity and user following action matrix it work by regularizing celebrity factor through celebrity s social network and descriptive word associated with each celebrity we also proposed to incorporate different confidence for different dyadic context to handle the situation that only positive observation exist we conducted experiment on two real world datasets from twitter and tencent weibo which are the largest and second largest microblog website in usa and china respectively the experiment result show that our model achieves a higher performance and provide more effective result than the state of art method especially when recommending new celebrity we also show that our model capture user intertests more precisely and give better recommendation interpretability 
there is interest in artificial intelligence for principled technique to analyze inconsistent information this stem from the recognition that the dichotomy between consistent and inconsistent set of formula that come from classical logic is not sufficient for describing inconsistent information we review some existing proposal and make new proposal for measure of inconsistency and measure of information and then prove that they are all pairwise incompatible this show that the notion of inconsistency is a multi dimensional concept where different measure provide different insight we then explore relationship between measure of inconsistency and measure of information in term of the trade offs they identify when using them to guide resolution of inconsistency 
positive and unlabelled learning pu learning ha been investigated to deal with the situation where only the positive example and the unlabelled example are available most of the previous work focus on identifying some negative example from the unlabelled data so that the supervised learning method can be applied to build a classifier however for the remaining unlabelled data which can not be explicitly identified a positive or negative we call them ambiguous example they either exclude them from the training phase or simply enforce them to either class consequently their performance may be constrained this paper proposes a novel approach called similarity based pu learning spul method by associating the ambiguous example with two similarity weight which indicate the similarity of an ambiguous example towards the positive class and the negative class respectively the local similarity based and global similarity based mechanism are proposed to generate the similarity weight the ambiguous example and their similarity weight are thereafter incorporated into an svm based learning phase to build a more accurate classifier extensive experiment on real world datasets have shown that spul outperforms state of the art pu learning method 
we present a new sampling approach to bayesian learning of the bayesian network structure like some earlier sampling method we sample linear order on node rather than directed acyclic graph dag the key difference is that we replace the usual markov chain monte carlo mcmc method by the method of annealed importance sampling ai we show that ai is not only competitive to mcmc in exploring the posterior but also superior to mcmc in two way it enables easy and efficient parallelization due to the independence of the sample and lower bounding of the marginal likelihood of the model with good probabilistic guarantee we also provide a principled way to correct the bias due to order based sampling by implementing a fast algorithm for counting the linear extension of a given partial order 
parsing human pose in image is fundamental in extracting critical visual information for artificial intelligent agent our goal is to learn self contained body part representation from image which we call visual symbol and their symbolwise geometric context in this parsing process each symbol is individually learned by categorizing visual feature leveraged by geometric information in the categorization we use latent support vector machine followed by an efficient cross validation procedure then these symbol naturally define geometric context of body part in a fine granularity when the structure of the compositional part is a tree we derive an efficient approach to estimating human pose in image experiment on two large datasets suggest our approach outperforms state of the art method 
the ever growing literature in biomedicine make it virtually impossible for individual to grasp all the information relevant to their interest since even expert knowledge is limited important association among key biomedical concept may remain unnoticed in the flood of information discovering those hidden association is called hypothesis discovery this paper report our approach to this problem taking advantage of a triangular chain of relation extracted from published knowledge we consider such chain of relation a implicit rule to generate potential hypothesis the generated hypothesis are then compared with newer knowledge for assessing their validity and if validated they are served a positive example for learning a regression model to rank hypothesis this framework called supervised hypothesis discovery is tested on real world knowledge from the biomedical literature to demonstrate it effectiveness 
active learning ha been extensively studied and shown to be useful in solving real problem the typical setting of traditional active learning method is querying label from an oracle this is only possible if an expert exists which may not be the case in many real world application in this paper we focus on designing easier question that can be answered by a non expert these question poll relative information a opposed to absolute information and can be even generated from sideinformation we propose an active learning approach that query the ordering of the importance of an instance s neighbor rather than it label we explore our approach on real datasets and make several interesting discovery including that querying neighborhood information can be an effective question to ask and sometimes can even yield better performance than querying label 
the action description language b and c have significant common core nevertheless some expressive possibility of b are difficult or impossible to simulate in c and the other way around the main advantage of b is that it allows the user to give prolog style recursive definition which is important in application on the other hand b solves the frame problem by incorporating the commonsense law of inertia in it semantics which make it difficult to talk about fluents whose behavior is described by default other than inertia in c and in it extension c the inertia assumption is expressed by axiom that the user is free to include or not to include and other default can be postulated a well this paper defines a new action description language called bc that combine the attractive feature of b and c example of formalizing commonsense domain discussed in the paper illustrate the expressive capability of bc and the use of answer set solver for the automation of reasoning about action described in this language 
we review recent result on inferencing for sroel a description logic that subsumes the main feature of the w c recommendation owl el rule based deduction system are developed for various reasoning task and logical sublanguages certain feature combination lead to increased space upper bound for materialisation suggesting that efficient implementation are easier to obtain for suitable fragment of owl el 
consider random hypergraphs on n vertex where each k element subset of vertex is selected with probability p independently and randomly a a hyperedge by sparse we mean that the total number of hyperedges is o n or o n ln n when k these are exactly the classical erd s r nyi random graph g n p we prove that with high probability hinge width on these sparse random hypergraphs can grow linearly with the expected number of hyperedges some random constraint satisfaction problem such a model rb and model rd have satisfiability threshold on these sparse constraint hypergraphs thus the large hinge width result provide some theoretical evidence for random instance around satisfiability threshold to be hard for a standard hinge decomposition based algorithm we also conduct experiment on these and other kind of random graph with several hundred vertex including regular random graph and power law random graph the experimental result also show that hinge width can grow linearly with the number of edge on these different random graph these result may be of further interest 
user may ask a service robot to accomplish various task so that the designer of the robot cannot program each of the task beforehand a more and more open source knowledge resource become available it is worthwhile trying to make use of open source knowledge resource for service robot the challenge lie in the autonomous identification acquisition and utilization of missing knowledge about a user task at hand in this paper the core problem is formalized and the complexity result of the main reasoning issue are provided a mechanism for task planning with open knowledge rule which are provided by non expert in semi structured natural language and thus generally underspecified are introduced technique for translating the semi structured knowledge from a large open source knowledge base are also presented experiment showed a remarkable improvement of the system performance on a test set consisting of hundred of user desire from the open source knowledge base 
learning from demonstration lfd is a popular technique for building decision making agent from human help traditional lfd method use demonstration a training example for supervised learning but complex task can require more example than is practical to obtain we present abstraction from demonstration afd a novel form of lfd that us demonstration to infer state abstraction and reinforcement learning rl method in those abstract state space to build a policy empirical result show that afd is greater than an order of magnitude more sample efficient than just using demonstration a training example and exponentially faster than rl alone 
in this paper we conceptualize abstract argumentation in term of successful and unsuccessful attack such that argument are accepted when there are no successful attack on them we characterize the relation between attack semantics and dung s approach and we define an scc recursive algorithm for attack semantics using attack labelings 
this paper present a new monte carlo search algorithm for very large sequential decision making problem we apply non linear regression within monte carlo search online to estimate a state action value function from the outcome of random roll out this value function generalizes between related state and action and can therefore provide more accurate evaluation after fewer rollouts a further significant advantage of this approach is it ability to automatically extract and leverage domain knowledge from external source such a game manual we apply our algorithm to the game of civilization ii a challenging multiagent strategy game with an enormous state space and around joint action we approximate the value function by a neural network augmented by linguistic knowledge that is extracted automatically from the official game manual we show that this non linear value function is significantly more efficient than a linear value function which is itself more efficient than monte carlo tree search our non linear monte carlo search win over of game against the built in ai of civilization ii 
a novel contextual logic is presented that combine feature of both multi context system and logic of context broadly contextual logic are those with a formal notion of context knowledge that is true only under specific assumption multicontext system use discrete logistic system a individual context related by meta level rule whereas logic of context partition a single knowledge base into context related using object level rule the contextual logic presented here is strongly local in that knowledge and inference is discrete for individual context but which are nevertheless part of a single logistic system that relates context at the object level so combining advantage of both a deductive system of contextual inference and a possible world based semantics is given with formal result including soundness and completeness and a number of property are examined 
the minimax concave plus penalty mcp ha been demonstrated to be effective in nonconvex penalization for feature selection in this paper we propose a novel construction approach for mcp in particular we show that mcp can be derived from a concave conjugate of the euclidean distance function this construction approach in turn lead u to an augmented lagrange multiplier method for solving the penalized regression problem with mcp in our method each tuning parameter corresponds to a feature and these tuning parameter can be automatically updated we also develop a d c difference of convex function programming approach for the penalized regression problem we find that the augmented lagrange multiplier method degenerate into the d c programming method under specific condition experimental analysis is conducted on a set of simulated data the result is encouraging copyright association for the advancement of artificial intelligence www aaai org all right reserved 
we introduce a game theoretic framework to address the community detection problem based on the social network structure the dynamic of community formation is framed a a strategic game called community formation game given a social network each node is selfish and selects community to join or leave based on her own utility measurement a community structure can be interpreted a an equilibrium of this game we formulate the agent utility by the combination of a gain function and a loss function each agent can select multiple community which naturally capture the concept of overlapping community we propose a gain function based on newman s modularity function and a simple loss function that reflects the intrinsic cost incurred when people join the community we conduct extensive experiment under this framework our result show that our algorithm is effective in identifying overlapping community and is often better than other algorithm we evaluated especially when many people belong to multiple community 
selecting a clustering algorithm is a perplexing task yet since different algorithm may yield dramatically different output on the same data the choice of algorithm is crucial when selecting a clustering algorithm user tend to focus on cost related consideration software purchasing cost running time etc difference concerning the output of the algorithm are not usually considered recently a formal approach for selecting a clustering algorithm ha been proposed the approach involves distilling abstract property of the input output behavior of different clustering paradigm and classifying algorithm based on these property in this paper we extend the approach in into the hierarchical setting the class of linkage based algorithm is perhaps the most popular class of hierarchical algorithm we identify two property of hierarchical algorithm and prove that linkage based algorithm are the only one that satisfy both of these property our characterization clearly delineates the difference between linkage based algorithm and other hierarchical algorithm we formulate an intuitive notion of locality of a hierarchical algorithm that distinguishes between linkage based and global hierarchical algorithm like bisecting k mean and prove that popular divisive hierarchical algorithm produce clustering that cannot be produced by any linkage based algorithm 
in this paper we propose a novel robust and pragmatic feature selection approach unlike those sparse learning based feature selection method which tackle the approximate problem by imposing sparsity regularization in the objective function the proposed method only ha one l norm loss term with an explicit l norm equality constraint an efficient algorithm based on augmented lagrangian method will be derived to solve the above constrained optimization problem to find out the stable local solution extensive experiment on four biological datasets show that although our proposed model is not a convex problem it outperforms the approximate convex counterpart and state of art feature selection method evaluated in term of classification accuracy by two popular classifier what is more since the regularization parameter of our method ha the explicit meaning i e the number of feature selected it avoids the burden of tuning the parameter making it a pragmatic feature selection method 
we describe an innovative application of a novel game theoretic approach for a national scale security deployment working with the united state transportation security administration tsa we have developed a new application called guard to allocate the tsa s limited resource across hundred of security activity to provide protection at over united state airport similar security application e g armor and iris have focused on one off tailored application and one security activity e g checkpoint per application guard on the other hand face three new key issue i reasoning about hundred of heterogeneous security activity ii reasoning over diverse potential threat iii developing a system designed for hundred of end user since a national deployment precludes tailoring to specific airport our key idea are i creating a new game theoretic framework that allows for heterogeneous defender activity and compact modeling of a large number of threat ii developing an efficient solution technique based on general purpose stackelberg game solver iii taking a partially centralized approach for knowledge acquisition the scheduling assistant ha been delivered to the tsa and is currently undergoing evaluation for scheduling practice at an undisclosed airport if successful the tsa intends to incorporate the system into their unpredictable scheduling practice nationwide 
in their groundbreaking paper bartholdi tovey and trick argued that many well known voting rule such a plurality borda copeland and maximin are easy to manipulate an important assumption made in that paper is that the manipulator s goal is to ensure that his preferred candidate is among the candidate with the maximum score or equivalently that tie are broken in favor of the manipulator s preferred candidate in this paper we examine the role of this assumption in the easiness result of we observe that the algorithm presented in extends to all rule that break tie according to a fixed ordering over the candidate we then show that all scoring rule are easy to manipulate if the winner is selected from all tied candidate uniformly at random this result extends to maximin under an additional assumption on the manipulator s utility function that is inspired by the original model of in contrast we show that manipulation becomes hard when arbitrary polynomial time tie breaking rule are allowed both for the rule considered in and for a large class of scoring rule 
defeasible inheritance network are a nonmonotonic framework that deal with hierarchical knowledge on the other hand rational closure is acknowledged a a landmark of the preferential approach we will combine these two approach and define a new non monotonic closure operation for propositional knowledge base that combine the advantage of both then we redefine such a procedure for description logic a family of logic well suited to model structured information in both case we will provide a simple reasoning method that is build on top of the classical entailment relation 
in this paper we introduce a probabilistic classification model to address the task of semi supervised learning the major novelty of our proposal stem from measuring distributional relationship between the labeled and unlabeled data this is achieved from a stochastic translation model between data distribution that is estimated from a mixture model the proposed classifier is defined from the combination of both the translation model and a kernel logistic regression on labeled data experimental result obtained over synthetic and real world data set validate the usefulness of our proposal 
uncertainty and vagueness are pervasive phenomenon in real life knowledge they are supported in extended description logic that adapt classical description logic to deal with numerical probability or fuzzy truth degree while the two concept are distinguished for good reason they combine in the notion of probably which is ultimately a fuzzy qualification of probability here we develop existing propositional logic of fuzzy probability into a full blown description logic and we show decidability of several variant of this logic under ukasiewicz semantics we obtain these result in a novel generic framework of fuzzy coalgebraic logic this enables u to extend our result to logic that combine crisp ingredient including standard crisp role and crisp numerical probability with fuzzy role and fuzzy probability 
speaker of many different language use the internet a common activity among these user is uploading image and associating these image with word in their own language a caption filename or surrounding text we use these explicit monolingual image to word connection to successfully learn implicit bilingual word to word translation bilingual pair of word are proposed a translation if their corresponding image have similar visual feature we generate bilingual lexicon in language pair focusing on word that have been automatically identified a physical object the use of visual similarity substantially improves performance over standard approach based on string similarity for generated lexicon with translation including visual information lead to an absolute improvement in accuracy of over string edit distance alone 
the rapid construction of supervised text classification model is becoming a pervasive need across many modern application to reduce human labeling bottleneck many new statistical paradigm e g active semi supervised transfer and multi task learning have been vigorously pursued in recent literature with varying degree of empirical success concurrently the emergence of web platform in the last decade ha enabled a world wide collaborative human effort to construct a massive ontology of concept with very rich detailed and accurate description in this paper we propose a new framework to extract supervisory information from such ontology and complement it with a shift in human effort from direct labeling of example in the domain of interest to the much more efficient identification of concept class association through empirical study on text categorization problem using the wikipedia ontology we show that this shift allows very high quality model to be immediately induced at virtually no cost 
multi agent plan recognition mapr seek to identify the dynamic team structure and team behavior from the observed activity sequence team trace of a set of intelligent agent based on a library of known team activity sequence team plan previous mapr system require that team trace and team plan are fully observed in this paper we relax this constraint i e team trace and team plan are allowed to be partial this is an important task in applying mapr to real world domain since in many application it is often difficult to collect full team trace or team plan due to environment limitation e g military operation this is also a hard problem since the information available is limited we propose a novel approach to recognizing team plan from partial team trace and team plan we encode the mapr problem a a satisfaction problem and solve the problem using a state of the art weighted max sat solver we empirically show that our algorithm is both effective and efficient 
the variability of human behavior during plan execution pose a difficult challenge for human robot team in this paper we use the concept of theory of mind to enable robot to account for two source of human variability during team operation when faced with an unexpected action by a human teammate a robot us a simulation analysis of different hypothetical cognitive model of the human to identify the most likely cause for the human s behavior this allows the cognitive robot to account for variance due to both different knowledge and belief about the world a well a different possible path the human could take with a given set of knowledge and belief an experiment showed that cognitive robot equipped with this functionality are viewed a both more natural and intelligent teammate compared to both robot who either say nothing when presented with human variability and robot who simply point out any discrepancy between the human s expected and actual behavior overall this analysis lead to an effective general approach for determining what thought process is leading to a human s action 
we present the first polynomial time construction procedure for generating graceful double wheel graph a graph is graceful if it vertex can be labeled with distinct integer value from e where e is the number of edge such that each edge ha a unique value corresponding to the absolute difference of it endpoint graceful graph have a range of practical application domain including in radio astronomy x ray crystallography cryptography and experimental design various family of graph have been proven to be graceful while others have only been conjectured to be in particular it ha been conjectured that so called double wheel graph are graceful a double wheel graph consists of two cycle of n node connected to a common hub we prove this conjecture by providing the first construction for graceful double wheel graph for any n using a framework that combine streamlined constraint reasoning with insight from human computation we also use this framework to provide a polynomial time construction for diagonally ordered magic square 
the field of deterministic ai planning can roughly be divided into two approach classical state based planning and hierarchical task network htn planning the plan existence problem of the former is known to be decidable while it ha been proved undecidable for the latter when extending htn planning by allowing the unrestricted insertion of task and ordering constraint one obtains a form of planning which is often referred to a hybrid planning we present a simplified formalization of htn planning with and without task insertion we show that the plan existence problem is undecidable for the htn setting without task insertion and that it becomes decidable when allowing task insertion in the course of the proof we obtain an upper complexity bound of expspace for the plan existence problem for propositional htn planning with task insertion 
while function symbol are widely acknowledged a an important feature in logic programming they make common inference task undecidable to cope with this problem recent research ha focused on identifying class of logic program imposing restriction on the use of function symbol but guaranteeing decidability of common inference task this ha led to several criterion called termination criterion providing sufficient condition for a program to have finitely many stable model each of finite size this paper introduces the new class of bounded program which guarantee the aforementioned property and strictly includes the class of program determined by current termination criterion different result on the correctness the expressiveness and the complexity of the class of bounded program are presented 
the game description language is a high level rule based formalism for communicating the rule of arbitrary game to general game playing system whose challenging task is to learn to play previously unknown game without human intervention originally designed for deterministic game with complete information about the game state the language wa recently extended to include randomness and imperfect information however determining the extent to which this enhancement allows to describe truly arbitrary game wa left a an open problem we provide a positive answer to this question by relating the extended game description language to the universal mathematical concept of extensive form game proving that indeed just any such game can be described faithfully 
persistent homology is a mathematical tool from topological data analysis it performs multi scale analysis on a set of point and identifies cluster hole and void therein these latter topological structure complement standard feature representation making persistent homology an attractive feature extractor for artificial intelligence research on persistent homology for ai is in it infancy and is currently hindered by two issue the lack of an accessible introduction to ai researcher and the paucity of application in response the first part of this paper present a tutorial on persistent homology specifically aimed at a broader audience without sacrificing mathematical rigor the second part contains one of the first application of persistent homology to natural language processing specifically our similarity filtration with time skeleton sifts algorithm identifies hole that can be interpreted a semantic tie back in a text document providing a new document structure representation we illustrate our algorithm on document ranging from nursery rhyme to novel and on a corpus with child and adolescent writing 
we address the problem of optimal path finding for multiple agent where agent must not collide and their total travel cost should be minimized previous work used traditional single agent search variant of the a algorithm we present a novel formalization for this problem which includes a search tree called the increasing cost tree ict and a corresponding search algorithm that find optimal solution we analyze this new formalization and compare it to the previous state of the art a based approach experimental result on various domain show the benefit and drawback of this approach a speedup of up to order of magnitude wa obtained in a number of case 
we study the existential closure of several propositional language l considered recently a target language for knowledge compilation kc namely the incomplete fragment krom c horn c k h c renh c aff and the corresponding disjunction closure krom c v horn c v k h c v renh c v and aff v we analyze the query transformation expressiveness and succinctness of the resulting language l in order to locate them in the kc map a a by product we also address several issue concerning disjunction closure that were left open so far from our investigation the language horn c v where disjunction and existential quantification can be applied to horn cnf formula appears a an interesting target language for the kc purpose challenging the influential dnnf language 
a key decision facing autonomous system with access to stream of sensory data is whether to act based on current evidence or to wait for additional information that might enhance the utility of taking an action computing the value of information is particularly difficult with streaming high dimensional sensory evidence we describe a belief projection approach to reasoning about information value in these setting using model for inferring future belief over state given streaming evidence these belief projection model can be learned from data or constructed via direct assessment of parameter and they fit naturally in modular hierarchical state inference architecture we describe principle of using belief projection and present result drawn from an implementation of the methodology within a conversational system 
continuous state dec mdps are critical for agent team in domain involving resource such a time but scaling them up is a significant challenge to meet this challenge we first introduce a novel continuous time dec mdp model that exploit transition independence in domain with temporal constraint more importantly we present a new locally optimal algorithm called spac compared to the best previous algorithm spac find solution of comparable quality substantially faster spac also scale to larger team of agent 
the last two decade ha seen the emergence of many different probabilistic logic that use logical language to specify and sometimes reason with probability distribution probabilistic logic that support reasoning with probability distribution such a problog use an implicit definition of an interaction rule to combine probabilistic evidence about atom in this paper we show that this interaction rule is an example of a more general class of interaction that can be described by nonmonotonic logic we furthermore show that such local interaction about the probability of an atom can be described by convolution the resulting extended probabilistic logic support nonmonotonic reasoning with probabilistic information 
schulze voting is a recently introduced voting system enjoying unusual popularity and a high degree of real world use with user including the wikimedia foundation several branch of the pirate party and mtv it is a condorcet voting system that determines the winner of an election using information about path in a graph representation of the election we resolve the complexity of many electoral control case for schulze voting we find that it fall short of the best known voting system in term of control resistance demonstrating vulnerability of concern to some prospective user of the system 
this paper present a framework for relevance based belief change in propositional horn logic we firstly establish a parallel interpolation theorem for horn logic and show that parikh s finest splitting theorem hold with horn formula by reformulating parikh s relevance criterion in the setting of horn belief change we construct a relevance based partial meet horn contraction operator and provide a representation theorem for the operator interestingly we find that this contraction operator can be fully characterised by delgrande and wassermann s postulate for partial meet horn contraction a well a parikh s relevance postulate without requiring any change on the postulate which is qualitatively different from the case in classical propositional logic copyright association for the advancement of artificial intelligence all right reserved 
recently several method have been proposed for optimal delete free planning we present an incremental compilation approach that enables these method to be applied to problem with conditional effect which none of them support natively with an h solver for problem with conditional effect in hand we also consider adapting the h anytime lower bound function to use the more space efficient pcec compilation this avoids the memory limitation of the original h caused by it reliance on an exponential space compilation it also lead to improvement on some problem where memory is not an issue 
we consider the problem of planning in environment where the state is fully observable action have non deterministic effect and plan must generate infinite state trajectory for achieving a large class of ltl goal more formally we focus on the control synthesis problem under the assumption that the ltl formula to be realized can be mapped into a deterministic b chi automaton we show that by assuming that action nondeterminism is fair namely that infinite execution of a nondeterministic action in the same state yield each possible successor state an infinite number of time the fair synthesis problem can be reduced to a standard strong cyclic planning task over reachability goal since strong cyclic planner are built on top of efficient classical planner the transformation reduces the non deterministic fully observable temporally extended planning task into the solution of classical planning problem a number of experiment are reported showing the potential benefit of this approach to synthesis in comparison with state of the art symbolic method 
we learn constraint network by asking the user partial query that is we ask the user to classify assignment to subset of the variable a positive or negative we provide an algorithm that given a negative example focus onto a constraint of the target network in a number of query logarithmic in the size of the example we give information theoretic lower bound for learning some simple class of constraint network and show that our generic algorithm is optimal in some case finally we evaluate our algorithm on some benchmark 
structured web search incorporating data from structured source into search engine result ha attracted much attention from both academic and industrial community to understand user s intent query structure interpretation is proposed to analyze the structure of query in a query log and map query term to the semantically relevant attribute of data source in a target domain existing method assume all query should be classified to the target domain and thus they are limited when interpreting query from different domain in real query log to address the problem we introduce a human machine hybrid method by utilizing crowdsourcing platform our method selects a small number of query term and asks the crowdsourcing worker to interpret them and then infers the interpretation based on the crowdsourcing result to improve the performance we propose an iterative probabilistic inference method based on a similarity graph of query term and select the most useful query term for crowdsourcing by considering their domain relevance and gained benefit we evaluate our method on a real query log and the experimental result show that our method outperforms the state of the art method 
we introduce a new representation scheme for coalitional game called coalition flow network cf net where the formation of effective coalition in a task based setting is reduced to the problem of directing flow through a network we show that our representation is intuitive fully expressive and capture certain pattern in a significantly more concise manner compared to the conventional approach furthermore our representation ha the flexibility to express various class of game such a characteristic function game coalitional game with overlapping coalition and coalitional game with agent type a such to the best of our knowledge cf net is the first representation that allows for switching conveniently and efficiently between overlapping non overlapping coalition with without agent type we demonstrate the efficiency of our scheme on the coalition structure generation problem where near optimal solution for large instance can be found in a matter of second 
this thesis investigates the generation of new concept from combination of existing concept a a language evolves we give a method for combining concept and will be investigating the utility of composite concept in language evolution and thence the utility of concept generation 
in this work we propose an approach for generalization in continuous domain reinforcement learning that instead of using a single function approximator try many different function approximators in parallel each one defined in a different region of the domain associated with each approximator is a relevance function that locally quantifies the quality of it approximation so that at each input point the approximator with highest relevance can be selected the relevance function is defined using parametric estimation of the variance of the q value and the density of sample in the input space which are used to quantify the accuracy and the confidence in the approximation respectively these parametric estimation are obtained from a probability density distribution represented a a gaussian mixture model embedded in the input output space of each approximator in our experiment the proposed approach required a lesser number of experience for learning and produced more stable convergence profile than when using a single function approximator 
one class collaborative filtering or collaborative ranking with implicit feedback ha been steadily receiving more attention mostly due to the one class characteristic of data in various service e g like in facebook and bought in amazon previous work for solving this problem include pointwise regression method based on absolute rating assumption and pairwise ranking method with relative score assumption where the latter wa empirically found performing much better because it model user ranking related preference more directly however the two fundamental assumption made in the pairwise ranking method individual pairwise preference over two item and independence between two user may not always hold a a response we propose a new and improved assumption group bayesian personalized ranking gbpr via introducing richer interaction among user in particular we introduce group preference to relax the aforementioned individual and independence assumption we then design a novel algorithm correspondingly which can recommend item more accurately a shown by various ranking oriented evaluation metric on four real world datasets in our experiment 
temporal reasoning problem arise in many area of ai including planning natural language understanding and reasoning about physical system the computational complexity of continuous time temporal constraint reasoning is fairly well understood there are however many different case where discrete time must be considered various scheduling problem and reasoning about sampled physical system are two example here the complexity of temporal reasoning is not a well studied nor a well understood in order to get a better understanding we consider the powerful horn dlr formalism adapted for discrete time and study it computational complexity we show that the full formalism is np hard and identify several maximal tractable subclass we also lift the maximality result to obtain hardness result for other family of constraint finally we discus how the result and technique presented in this paper can be used for studying even more expressive class of temporal constraint 
the problem of detecting the direction of time in vector autoregressive var process using statistical technique is considered by analogy to causal ar process with non gaussian noise we conjecture that the distribution of the time reversed residual of a linear var model is closer to a gaussian than the distribution of actual residual in the forward direction experiment with simulated data illustrate the validity of the conjecture based on these result we design a decision rule for detecting the direction of var process the correct direction in time forward is the one in which the residual of the time series are le gaussian a series of experiment illustrate the superior result of the proposed rule when compared with other method based on independence test 
just a conventional institution are organisational structure for coordinating the activity of multiple interacting individual electronic institution provide a computational analogue for coordinating the activity of multiple interacting software agent in this paper we argue that open multi agent system can be effectively designed and implemented a electronic institution for which we provide a comprehensive computational model more specifically the paper provides an operational semantics for electronic institution specifying the essential data structure the state representation and the key operation necessary to implement them 
the behavior composition problem involves the automatic synthesis of a controller able to realize i e implement a desired target behavior specification by suitably coordinating a set of already available behavior while the problem ha been thoroughly studied one open issue ha resisted a principled solution if the target specification is not fully realizable is there a way to realize it at best in this paper we answer positively by showing that there exists a unique supremal realizable target behavior satisfying the specification more importantly we give an effective procedure to compute such a target then we introduce exogenous event and show that the supremal can again be computed though this time into two variant depending on the ability to observe such event 
inconsistency and partial information is the norm in knowledge base used in many real world application that support among other thing human decision making process in this work we argue that the management of this kind of data need to be context sensitive creating a synergy with the user to build useful flexible data management system 
learning from streaming data represents an important and challenging task maintaining an accurate model while the stream go by requires a smart way for tracking data change through time originating concept drift one way to treat this kind of problem is to resort to ensemble based technique in this context the advent of new technology related to web and ubiquitous service call for the need of new learning approach able to deal with structured complex information such a tree kernel method enable the modeling of structured data in learning algorithm however they are computationally demanding the contribute of this work is to show how an effective ensemble based approach can be deviced for stream of tree by optimizing the kernel based model representation both efficacy and efficiency of the proposed approach are assessed for different model by using data set exhibiting different level and type of concept drift 
since their popularity began to rise in the mid s there ha been significant growth in the number of multi core and multi processor computer available knowledge representation system using logical inference have been slow to embrace this new technology we present the concept of inference graph a natural deduction inference system which scale well on multi core and multi processor machine inference graph enhance propositional graph by treating propositional node a task which can be scheduled to operate upon message sent between node via the arc that already exist a part of the propositional graph representation the use of scheduling heuristic within a prioritized message passing architecture allows inference graph to perform very well in forward backward bi directional and focused reasoning test demonstrate the usefulness of our scheduling heuristic and show significant speedup in both best case and worst case inference scenario a the number of processor increase 
linearly solvable markov decision process mdp model are a powerful subclass of problem with a simple structure that allow the policy to be written directly in term of the uncontrolled passive dynamic of the environment and the goal of the agent however there have been no learning algorithm for this class of model in this research we develop a robust learning approach to linearly solvable mdps to exploit the simple solution for general problem we show how to construct passive dynamic from any transition matrix use bayesian updating to estimate the model parameter and apply approximate and efficient bayesian exploration to speed learning in addition we reduce the computational cost of learning using intermittent bayesian updating and policy solving we also gave a polynomial theoretical time complexity bound for the convergence of our learning algorithm and demonstrate a linear bound for the subclass of the reinforcement learning problem with the property that the transition error depends only on the agent itself test result for our algorithm in a grid world are presented comparing our algorithm with the beb algorithm the result showed that our algorithm learned more than the beb algorithm without losing convergence speed so that the advantage of our algorithm increased a the environment got more complex we also showed that our algorithm s performance is more stable after convergence finally we show how to apply our approach to the cellular telephone problem by defining the passive dynamic 
querying large database while taking ontology into account is currently a very active domain research in this paper we consider ontology described by existential rule also known a datalog a framework that generalizes lightweight description logic a common approach is to rewrite a conjunctive query w r t an ontology into a union of conjunctive query ucq which can be directly evaluated against a database however the practicability of this approach is questionable due to the weak expressivity of class for which efficient rewriter have been implemented the large size of optimal rewriting using ucq we propose to use semi conjunctive query scq which are a restricted form of positive existential formula and compute sound and complete rewriting which are union of scq uscq a novel algorithm for query rewriting compact is presented it computes sound and complete rewriting for large class of ontology first experiment show that uscq are both efficiently computable and more efficiently evaluable than their equivalent ucq 
the decentralized partially observable markov decision process dec pomdp is a powerful model for multiagent planning under uncertainty but it applicability is hindered by it high complexity solving dec pomdps optimally is nexp hard recently kumar et al introduced the value factorization vf framework which exploit decomposable value function that can be factored into subfunctions this framework ha been shown to be a generalization of several model that leverage sparse agent interaction such a ti dec mdps ndpomdps and td pomdps existing algorithm for these model assume that the interaction graph of the problem is given in this paper we introduce three algorithm to automatically generate interaction graph for model within the vf framework and establish lower and upper bound on the expected reward of an optimal joint policy we illustrate experimentally the benefit of these technique for sensor placement in a decentralized tracking application 
we show that different semantics of ability in atl give rise to different validity set a a consequence different notion of ability induce different strategic logic and different general property of game moreover the study can be seen a the first systematic step towards satisfiability checking algorithm for atl with imperfect information 
the number partitioning problem is to divide a given set of n positive integer into k subset so that the sum of the number in each subset are a nearly equal a possible while effective algorithm for two way partitioning exist multiway partitioning is much more challenging we introduce an improved algorithm for optimal multiway partitioning by combining several existing algorithm with some new extension we test our algorithm for partitioning bit integer from three to ten way and demonstrate order of magnitude speedup over the previous state of the art 
g del s proof of his famous first incompleteness theorem g ha quite understandably long been a tantalizing target for those wanting to engineer impressively intelligent computational system after all in establishing g g del did something that by any metric must be classified a stunningly intelligent we observe that it ha long been understood that there is some sort of analogical relationship between the liar paradox lp and g and that g del himself appreciated and exploited the relationship yet the exact nature of the relationship ha hitherto not been uncovered by which we mean that the following question ha not been answered given a description of lp and the suspicion that it may somehow be used by a suitably programmed computing machine to find a proof of the incompleteness of peano arithmetic can such a machine provided this description a input produce a output a complete and verifiably correct proof of g in this paper we summarize engineering that entail an affirmative answer to this question our approach us what we call analogico deductive reasoning adr which combine analogical and deductive reasoning to produce a full deductive proof of g from lp our engineering us a form of adr based on our meta r system and a connection between the liar sentence in lp and g del s fixed point lemma from which g follows quickly 
the robot programming and plan language indigolog allows for on line execution of action and offline projection of program in dynamic and partly unknown environment basic assumption are that the outcome of primitive and sensing action are correctly modeled and that the agent is informed about all exogenous event beyond it control in real world application however such assumption do not hold in fact an action s outcome is error prone and sensing result are noisy in this paper we present a belief management system in indigolog that is able to detect inconsistency between a robot s modeled belief and what happened in reality the system furthermore derives explanation and maintains a consistent belief our main contribution are a belief management system following a history based diagnosis approach that allows an agent to actively cope with faulty action and the occurrence of exogenous event and an implementation in indigolog and experimental result from a delivery domain 
constraint satisfaction problem may be nearly tractable for instance most of the relation in a problem might belong to a tractable language we introduce a method to take advantage of this fact by computing a backdoor to this tractable language the method can be applied to many tractable class for which the membership test is itself tractable we introduce therefore two polynomial membership testing algorithm to check if a language is closed under a majority or conservative mal tsev polymorphism respectively then we show that computing a minimal backdoor for such class is fixed parameter tractable fpt if the tractable subset of relation is given and w complete otherwise finally we report experimental result on the xcsp benchmark set we identified a few promising problem class where problem were nearly closed under a majority polymorphism and small backdoor could be computed 
consistency based feature selection is an important category of feature selection research yet is defined only intuitively in the literature first we formally define a consistency measure and then using this definition evaluate feature selection measure from the literature while only of these were labeled a consistency measure by their original author by our definition an additional measure should be classified a consistency measure to compare these consistency measure in term of sensitivity we introduce the concept of quasi linear compatibility order and partially determine the order among the measure next we propose a new fast algorithm for consistency based feature selection we ran experiment using eleven large datasets to compare the performance of our algorithm against interact and lcc the only two instance of consistency based algorithm with potential real world application our algorithm show vast improvement in time efficiency while it performance in accuracy is comparable with that of interact and lcc 
record linkage is the process of matching record between two or multiple data set that represent the same real world entity an exhaustive record linkage process involves computing the similarity between all pair of record which can be very expensive for large data set blocking technique alleviate this problem by dividing the record into block and only comparing record within the same block to be adaptive from domain to domain one category of blocking technique formalizes construction of blocking scheme a a machine learning problem in the process of learning the best blocking scheme previous learning based technique utilize only a set of labeled data however since the set of labeled data is usually not large enough to well characterize the unseen unlabeled data the resultant blocking scheme may poorly perform on the unseen data by generating too many candidate match to address that in this paper we propose to utilize unlabeled data in addition to labeled data for learning blocking scheme our experimental result show that using unlabeled data in learning can remarkably reduce the number of candidate match while keeping the same level of coverage for true match 
a variable elimination rule allows the polynomial time identification of certain variable whose elimination doe not affect the satisfiability of an instance variable elimination in the constraint satisfaction problem csp can be used in preprocessing or during search to reduce search space size we show that there are essentially just four variable elimination rule defined by forbidding generic sub instance known a irreducible pattern in arc consistent csp instance one of these rule is the broken triangle property whereas the other three are novel 
consistent word selection in machine translation is currently realized by resolving word sense ambiguity through the context of a single sentence or neighboring sentence however consistent word selection over the whole article ha yet to be achieved consistency over the whole article is extremely important when applying machine translation to collectively developed document like wikipedia in this paper we propose to consider constraint between word in the whole article based on their semantic relatedness and contextual distance the proposed method is successfully implemented in both statistical and rule based translator we evaluate those system by translating article in the englishwikipedia into japanese the result show that the ratio of appropriate word selection for common noun increased to around with our method while it wa around without our method 
much of opinion mining research focus on product review because review are opinion rich and contain little irrelevant information however this cannot be said about online discussion and comment in such posting the discussion can get highly emotional and heated with many emotional statement and even personal attack a a result many of the posting and sentence do not express positive or negative opinion about the topic being discussed to find people s opinion on a topic and it different aspect which we call evaluative opinion those irrelevant sentence should be removed the goal of this research is to identify evaluative opinion sentence a novel unsupervised approach is proposed to solve the problem and our experimental result show that it performs well copyright association for the advancement of artificial intelligence all right reserved 
this paper describes a novel framework to jointly learn data dependent label and locality preserving projection given a set of data instance from multiple class the proposed approach can automatically learn which class are more similar to each other and construct discriminative feature using both labeled and unlabeled data to map similar class to similar location in a lower dimensional space in contrast to linear discriminant analysis lda and it variant which can only return c feature for a problem with c class the proposed approach can generate d feature where d is bounded only by the number of the input feature we describe and evaluate the new approach both theoretically and experimentally and compare it performance with other state of the art method 
institution also called normative framework provide an effective mechanism to govern agent in open distributed system an institution specifies a set of norm with respect to the achievement of a goal or goal that regulate agent behaviour in term of permission empowerment and obligation however in most real circumstance several institution probably have to cooperate to govern the same entity simultaneously which is very likely to give rise to norm conflict simply if institution will be designed independently and typically with different goal in this thesis we aim i to identify the different way to combine institution ii to model those way formally and computationally by extending an existing model for single institution iii to detect conflict in different type of combined institution automatically and iv to resolve those conflict via automatic norm revision using an approach based on inductive learning 
prediction model for multivariate spatio temporal function in geosciences are typically developed using supervised learning from attribute collected by remote sensing instrument collocated with the outcome variable provided at sparsely located site in such collocated data there are often large temporal gap due to missing attribute value at site where outcome label are available our objective is to develop more accurate spatio temporal predictor by using enlarged collocated data obtained by imputing missing attribute at time and location where outcome label are available the proposed method for large gap estimation in space and time called largest exploit temporal correlation of attribute correlation among multiple attribute collected at the same time and space and spatial correlation among attribute from multiple site largest outperformed alternative method in imputing up to of randomly missing observation at a synthetic spatio temporal signal and at a model of fluoride content in a water distribution system largest wa also applied for imputing of nonrandom missing value in data from one of the most challenging earth science problem related to aerosol property using such enlarged data a predictor of aerosol optical depth is developed that wa much more accurate than predictor based on alternative imputation method when tested rigorously over entire continental u in year 
landmark heuristic are perhaps the most accurate current known admissible heuristic for optimal planning a disjunctive action landmark can be seen a form of at least one constraint on the action it contains in many domain some critical proposition have to be established for a number of time propositional landmark are too weak to express this kind of constraint in this paper we propose to generalize landmark to multi valued landmark to represent the more general cardinality constraint we present a class of local multi valued landmark that can be efficiently extracted from propositional landmark by encoding multi valued landmark into cnf formula we can also use sat solver to systematically extract multi valued landmark experiment evaluation show that multivalued landmark based heuristic are more close to h and compete favorably with the state of the art of admissible landmark heuristic on benchmark domain copyright association for the advancement of artificial intelligence www aaai org all right reserved 
a significant portion of the electricity network capacity is built to run only a few day a year when demand peak a a result expensive power generation plant and equipment costing million of dollar are sitting idle most of the time which increase cost for everyone we present randomized load control a simple distributed approach for scheduling smart appliance randomized load control schedule the start time of appliance that are programmed to run within a specified time window so that the aggregate load achieves a given ideal load our result show that we do achieve the given ideal load to a great extent this is remarkable a the approach is completely distributed and preserve customer privacy a the scheduling happens within each house or building separately 
due to the explosive growth of the internet online review we can easily collect a large amount of labeled review from different domain but only some of them are beneficial for training a desired target domain sentiment classifier therefore it is important for u to identify those sample that are the most relevant to the target domain and use them a training data to address this problem a novel approach based on instance selection and instance weighting via pu learning is proposed pu learning is used at first to learn an in target domain selector which assigns an in target domain probability to each sample in the training set for instance selection the sample with higher in target domain probability are used a training data for instance weighting the calibrated in target domain probability are used a sampling weight for training an instance weighted naive bayes model based on the principle of maximum weighted likelihood estimation the experimental result prove the necessity and effectiveness of the approach especially when the size of training data is large it is also proved that the larger the kullback leibler divergence between the training and test data is the more effective the proposed approach will be 
in this paper a novel method is developed for enabling multi kernel multi label learning interlabel dependency and similarity diversity are simultaneously leveraged in the proposed method a concept network is constructed to capture the inter label correlation for classifier training maximal margin approach is used to effectively formulate the feature label association and the labellabel correlation specific kernel are learned not only for each label but also for each pair of the inter related label by learning the eigenfunctions of the kernel the similarity between a new data point and the training sample can be computed in the online mode our experimental result on real datasets web page image music and bioinformatics have demonstrated the effectiveness of our method 
we investigate the computational complexity of two global constraint cumulative and interdistance these are key constraint in modeling and solving scheduling problem enforcing domain consistency on both is np hard however restricted version of these constraint are often sufficient in practice some example include scheduling problem with a large number of similar task or task sparsely distributed over time another example is runway sequencing problem in air traffic control where landing period have a regular pattern such case can be characterized in term of structural restriction on the constraint we identify a number of such structural restriction and investigate how they impact the computational complexity of propagating these global constraint in particular we prove that such restriction often make propagation tractable 
specifying the reward function of a markov decision process mdp can be demanding requiring human assessment of the precise quality of and tradeoff among various state and action however reward function often posse considerable structure which can be leveraged to streamline their specification we develop new decision theoretically sound heuristic for eliciting reward for factored mdps whose reward function exhibit additive independence since we can often find good policy without complete reward specification we also develop new exact and approximate algorithm for robust optimization of imprecise reward mdps with such additive reward our method are evaluated in two domain autonomic computing and assistive technology 
we discus a novel approach for dealing with single stage stochastic constraint satisfaction problem scsps that include random variable over a continuous or large discrete support our approach is based on two novel tool sampled scsps and solution instead of explicitly enumerating a very large or infinite set of future scenario we employ statistical estimation to determine if a given assignment is consistent for a scsp a in statistical estimation the quality of our estimate is determined via confidence interval analysis in contrast to existing approach based on sampling we provide likelihood guarantee for the quality of the solution found our approach can be used in concert with existing strategy for solving scsps 
we propose a variant of alternating time temporal logic atl grounded in the agent operational know how a defined by their library of abstract plan in our logic it is possible to refer to rational strategy for agent developed under the belief desire intention agent paradigm this allows u to express and verify property of bdi system using atl type logical framework 
to tackle the potentially hard task of defining the reward function in a markov decision process we propose a new approach based on value iteration which interweaves the elicitation and optimization phase we assume that reward whose numeric value are unknown can only be ordered and that a tutor is present to help comparing sequence of reward we first show how the set of possible reward function for a given preference relation can be represented a a polytope then our algorithm called interactive value iteration search for an optimal policy while refining it knowledge about the possible reward function by querying a tutor when necessary we prove that the number of query needed before finding an optimal policy is upperbounded by a polynomial in the size of the problem and we present experimental result which demonstrate that our approach is efficient in practice 
the generation of route description is a fundamental task of navigation system a particular problem in this context is to identify route that can easily be described and processed by user in this work we present a framework for representing route network with the qualitative information necessary to evaluate and optimize route description with regard to ambiguity in them we identify different agent model that differ in how agent are assumed to process route description while navigating through route network further we analyze the computational complexity of matching route description and path in route network in dependency of the agent model finally we empirically evaluate the influence of the agent model on the optimization and the processing of route instruction 
we consider the problem of learning to act in partially observable continuous state and action world where we have abstract prior knowledge about the structure of the optimal policy in the form of a distribution over policy using idea from planning a inference reduction and bayesian unsupervised learning we cast markov chain monte carlo a a stochastic hill climbing policy search algorithm importantly this algorithm s search bias is directly tied to the prior and it mcmc proposal kernel which mean we can draw on the full bayesian toolbox to express the search bias including nonparametric prior and structured recursive process like grammar over action sequence furthermore we can reason about uncertainty in the search bias itself by constructing a hierarchical prior and reasoning about latent variable that determine the abstract structure of the policy this yield an adaptive search algorithm our algorithm learns to learn a structured policy efficiently we show how inference over the latent variable in these policy prior enables intraand intertask transfer of abstract knowledge we demonstrate the flexibility of this approach by learning meta search bias by constructing a nonparametric finite state controller to model memory by discovering motor primitive using a simple grammar over primitive action and by combining all three 
many system use markov model to generate finite length sequence that imitate a given style these system often need to enforce specific control constraint on the sequence to generate unfortunately control constraint are not compatible with markov model a they induce long range dependency that violate the markov hypothesis of limited memory attempt to solve this issue using heuristic search do not give any guarantee on the nature and probability of the sequence generated we propose a novel and efficient approach to controlled markov generation for a specific class of control constraint that guarantee that generated sequence satisfy control constraint and follow the statistical distribution of the initial markov model revisiting markov generation in the framework of constraint satisfaction we show how constraint can be compiled into a nonhomogeneous markov model using arc consistency technique and renormalization we illustrate the approach on a melody generation problem and sketch some realtime application in which control constraint are given by gesture controller 
distributed electricity producer such a small wind farm and solar installation pose several technical and economic challenge in smart grid design one approach to addressing these challenge is through broker agent who buy electricity from distributed producer and also sell electricity to consumer via a tariff market a new market mechanism where broker agent publish concurrent bid and ask price we investigate the learning of pricing strategy for an autonomous broker agent to profitably participate in a tariff market we employ markov decision process mdps and reinforcement learning an important concern with this method is that even simple representation of the problem domain result in very large number of state in the mdp formulation because market price can take nearly arbitrary real value in this paper we present the use of derived state space feature computed using statistic on tariff market price and broker agent customer portfolio to obtain a scalable state representation we also contribute a set of pricing tactic that form building block in the learned broker agent strategy we further present a tariff market simulation model based on real world data and anticipated market dynamic we use this model to obtain experimental result that show the learned strategy performing vastly better than a random strategy and significantly better than two other non learning strategy 
probabilistic logical language provide powerful formalism for knowledge representation and learning yet performing inference in these language is extremely costly especially if it is done at the propositional level lifted inference algorithm which avoid repeated computation by treating indistinguishable group of object a one help mitigate this cost seeking inspiration from logical inference where lifted inference e g resolution is commonly performed we develop a model theoretic approach to probabilistic lifted inference our algorithm compiles a first order probabilistic theory into a first order deterministic decomposable negation normal form d dnnf circuit compilation offer the advantage that inference is polynomial in the size of the circuit furthermore by borrowing technique from the knowledge compilation literature our algorithm effectively exploit the logical structure e g context specific independency within the first order model which allows more computation to be done at the lifted level an empirical comparison demonstrates the utility of the proposed approach 
how should we manage a sensor network to optimally guard security critical infrastructure how should we coordinate search and rescue helicopter to best locate survivor after a major disaster in both application we would like to control sensing resource in uncertain adversarial environment in this paper we introduce rsense an efficient algorithm which guarantee near optimal randomized sensing strategy whenever the detection performance satisfies submodularity a natural diminishing return property for any fixed adversarial scenario our approach combine technique from game theory with submodular optimization the rsense algorithm applies to setting where the goal is to manage a deployed sensor network or to coordinate mobile sensing resource such a unmanned aerial vehicle we evaluate our algorithm on two real world sensing problem 
canonical correlation analysis cca is a useful technique for measuring relationship between two set of vector data for paired tensor data set we propose a multilinear cca mcca method unlike existing multilinear variation of cca mcca extract uncorrelated feature under two architecture while maximizing paired correlation through a pair of tensor to vector projection one architecture enforces zero correlation within each set while the other enforces zero correlation between different pair of the two set we take a successive and iterative approach to solve the problem experiment on matching face of different pose show that mcca outperforms cca and dcca while using much fewer feature in addition the fusion of two architecture lead to performance improvement indicating complementary information 
in this paper we propose a novel approach for learning regression rule by transforming the regression problem into a classification problem unlike previous approach to regression by classification in our approach the discretization of the class variable is tightly integrated into the rule learning algorithm the key idea is to dynamically define a region around the target value predicted by the rule and considering all example within that region a positive and all example outside that region a negative in this way conventional rule learning heuristic may be used for inducing regression rule our result show that our heuristic algorithm outperforms approach that use a static discretization of the target variable and performs en par with other comparable rule based approach albeit without reaching the performance of statistical approach 
carneades is a recently proposed formalism for structured argumentation with varying proof standard an open question is it relation with dung s seminal abstract approach to argumentation in this paper the two formalism are formally related by translating carneades into aspic another recently proposed formalism for structured argumentation since aspic is defined to generate dungstyle abstract argumentation framework this in effect translates carneades graph into abstract argumentation framework it is proven that carneades always induces a unique dung extension which is the same in all of dung s semantics 
dispersing a team of robot into an unknown and dangerous environment such a a collapsed building can provide information about structural damage and location of survivor and help rescuer plan their action we propose a rolling dispersion algorithm which make use of a small number of robot and achieves full exploration the robot disperse a much a possible while maintaining communication and then advance a a group leaving behind beacon to mark explored area and provide a path back to the entrance the novelty of this algorithm come from the manner in which the robot continue their exploration a a group after reaching the maximum dispersion possible while staying in contact with each other we use simulation to show that the algorithm work in multiple environment and for varying number of robot 
in this paper we propose a new unsupervised approach for word segmentation the core idea of our approach is a novel word induction criterion called wordrank which estimate the goodness of word hypothesis character or phoneme sequence we devise a method to derive exterior word boundary information from the link structure of adjacent word hypothesis and incorporate interior word boundary information to complete the model in light of wordrank word segmentation can be modeled a an optimization problem a viterbi styled algorithm is developed for the search of the optimal segmentation extensive experiment conducted on phonetic transcript a well a standard chinese and japanese data set demonstrate the effectiveness of our approach on the standard brent version of bernstein ratner corpus our approach outperforms the state of the art bayesian model by more than plus our approach is simpler and more efficient than the bayesian method consequently our approach is more suitable for real world application copyright association for the advancement of artificial intelligence all right reserved 
in this paper we look into the assumption of interpreting ltl over finite trace in particular we show that ltlf i e ltl under this assumption is le expressive than what might appear at first sight and that at essentially no computational cost one can make a significant increase in expressiveness while maintaining the same intuitiveness of ltlf indeed we propose a logic ldlf for linear dynamic logic over finite trace which borrows the syntax from propositional dynamic logic pdl but is interpreted over finite trace satisfiability validity and logical implication a well a model checking for ltlf are pspace complete a for ltlf and ltl 
a set of constraint that cannot be simultaneously satisfied is over constrained minimal relaxation and minimal explanation for over constrained problem find many practical us for boolean formula minimal relaxation of over constrained problem are referred to a minimal correction subset mc mc find many application including the enumeration of mus existing approach for computing mc either use a maximum satisfiability maxsat solver or iterative call to a boolean satisfiability sat solver this paper show that existing algorithm for mc computation can be inefficient and so inadequate in certain practical setting to address this problem this paper develops a number of novel technique for improving the performance of existing mc computation algorithm more importantly the paper proposes a novel algorithm for computing mc both the technique and the algorithm are evaluated empirically on representative problem instance and are shown to yield the most efficient and robust solution for mc computation 
feature extraction for activity recognition in context aware ubiquitous computing application is usually a heuristic process informed by underlying domain knowledge relying on such explicit knowledge is problematic when aiming to generalize across different application domain we investigate the potential of recent machine learning method for discovering universal feature for context aware application of activity recognition we also describe an alternative data representation based on the empirical cumulative distribution function of the raw data which effectively abstract from absolute value experiment on accelerometer data from four publicly available activity recognition datasets demonstrate the significant potential of our approach to address both contemporary activity recognition task and next generation problem such a skill assessment and the detection of novel activity 
question and answer pair in community question answering cqa service are organized into hierarchical structure or taxonomy to facilitate user to find the answer for their question conveniently we observe that different cqa service have their own knowledge focus and used different taxonomy to organize their question and answer pair in their archive a there are no simple semantic mapping between the taxonomy of the cqa service the integration of cqa service is a challenging task the existing approach on integrating taxonomy ignore the hierarchical structure of the source taxonomy in this paper we propose a novel approach that is capable of incorporating the parent child and sibling information in the hierarchical structure of the source taxonomy for accurate taxonomy integration our experimental result with real world cqa data demonstrate that the proposed method significantly outperforms state of the art method copyright association for the advancement of artificial intelligence all right reserved 
recent year have witnessed the tremendous development of social medium which attracts a vast number of internet user the high dimension content generated by these user provides an unique opportunity to understand their behavior deeply a one of the most fundamental topic location estimation attracts more and more research effort different from the previous literature we find that user s location is strongly related to user interest based on this we first build a detection model to mine user interest from short text we then establish the mapping between location function and user interest before presenting an efficient framework to predict the user s location with convincing fidelity thorough evaluation and comparison on an authentic data set show that our proposed model significantly outperforms the state of the art approach moreover the high efficiency of our model also guarantee it applicability in real world scenario copyright association for the advancement of artificial intelligence www aaai org all right reserved 
sustainable supply chain management ha been an increasingly important topic of research in recent year at the strategic level there are computational model which study supply and distribution network with environmental consideration at the operational level there are for example routing and scheduling model which are constrained by carbon emission our paper explores work in tactical planning with regard to vehicle resource allocation from distribution center to customer location in a multi echelon logistics network we formulate the bi objective optimization problem exactly and design a memetic algorithm to efficiently derive an approximate pareto front we illustrate the applicability of our approach with a large real world dataset 
we propose a generalmcmc method for bayesian inference in logic based probabilistic modeling it cover a broad class of generativemodels including bayesian network and pcfgs the idea is to generalize an mcmc method for pcfgs to the one for a turing complete probabilistic modeling language prism in the context of statistical abduction where parse tree are replaced with explanation we describe how to estimate the marginal probability of data from mcmc sample and how to perform bayesian viterbi inference using an example of naive bayesmodel augmentedwith a hidden variable 
one fundamental evaluation criterion of an ai technique is it performance in the worst case for static strategy in extensive game this can be computed using a best response computation conventionally this requires a full game tree traversal for very large game such a poker that traversal is infeasible to perform on modern hardware in this paper we detail a general technique for best response computation that can often avoid a full game tree traversal additionally our method is specifically well suited for parallel environment we apply this approach to computing the worst case performance of a number of strategy in head up limit texas hold em which prior to this work wa not possible we explore these result thoroughly a they provide insight into the effect of abstraction on worst case performance in large imperfect information game this is a topic that ha received much attention but could not previously be examined outside of toy domain 
with the fast development of social medium the information overload problem becomes increasingly severe and recommender system play an important role in helping online user find relevant information by suggesting information of potential interest social activity for online user produce abundant social relation social relation provide an independent source for recommendation presenting both opportunity and challenge for traditional recommender system user are likely to seek suggestion from both their local friend and user with high global reputation motivating u to exploit social relation from local and global perspective for online recommender system in this paper we develop approach to capture local and global social relation and propose a novel framework locabal taking advantage of both local and global social context for recommendation empirical result on real world datasets demonstrate the effectiveness of our proposed framework and further experiment are conducted to understand how local and global social context work for the proposed framework 
constrained partially observable markov decision process cpomdps extend the standard pomdps by allowing the specification of constraint on some aspect of the policy in addition to the optimality objective for the value function cpomdps have many practical advantage over standard pomdps since they naturally model problem involving limited resource or multiple objective in this paper we show that the optimal policy in cpomdps can be randomized and present exact and approximate dynamic programming method for computing randomized optimal policy while the exact method requires solving a minimax quadratically constrained program qcp in each dynamic programming update the approximate method utilizes the point based value update with a linear program lp we show that the randomized policy are significantly better than the deterministic one we also demonstrate that the approximate point based method is scalable to solve large problem 
a strategy is used by a participant in a persuasion dialogue to select locution most likely to achieve it objective of persuading it opponent such strategy often assume that the participant ha a model of it opponent which may be constructed on the basis of a participant s accumulated dialogue experience however in most case the fact that an agent s experience may encode additional information which if appropriately used could increase a strategy s efficiency is neglected in this work we rely on an agent s experience to define a mechanism for augmenting an opponent model with information likely to be dialectally related to information already contained in it precise computation of this likelihood is exponential in the volume of related information we thus describe and evaluate an approximate approach for computing these likelihood based on monte carlo simulation 
in this paper we investigate a very challenging task of automatically generating presentation slide for academic paper the generated presentation slide can be used a draft to help the presenter prepare their formal slide in a quicker way a novel system called ppsgen is proposed to address this task it first employ regression method to learn the importance of the sentence in an academic paper and then exploit the integer linear programming ilp method to generate well structured slide by selecting and aligning key phrase and sentence evaluation result on a test set of pair of paper and slide collected on the web demonstrate that our proposed ppsgen system can generate slide with better quality a user study is also illustrated to show that ppsgen ha a few evident advantage over baseline method 
non negative matrix factorization nmf is a traditional unsupervised machine learning technique for decomposing a matrix into a set of base and coefficient under the non negative constraint nmf with sparse constraint is also known for extracting reasonable component from noisy data however nmf tends to give undesired result in the case of highly sparse data because the information included in the data is insufficient to decompose our key idea is that we can ease this problem if complementary data are available that we could integrate into the estimation of the base and coefficient in this paper we propose a novel matrix factorization method called non negative multiple matrix factorization nmmf which utilizes complementary data a auxiliary matrix that share the row or column index of the target matrix the data sparseness is improved by decomposing the target and auxiliary matrix simultaneously since auxiliary matrix provide information about the base and coefficient we formulate nmmf a a generalization of nmf and then present a parameter estimation procedure derived from the multiplicative update rule we examined nmmf in both synthetic and real data experiment the effect of the auxiliary matrix appeared in the improved nmmf performance we also confirmed that the base that nmmf obtained from the real data were intuitive and reasonable thanks to the non negative constraint 
forming effective coalition is a major research challenge in ai and multi agent system thus coalitional game including coalition structure generation have been attracting considerable attention from the ai research community traditionally the input of a coalitional game is a black box function called a characteristic function in this paper we develop a new concise representation scheme for a characteristic function which is based on the idea of agent type this representation can be exponentially more concise than existing concise representation scheme furthermore this idea can be used in conjunction with existing scheme to further reduce the representation size 
this paper is concerned with the use of conversational agent a an interaction paradigm for accessing open domain encyclopedic knowledge by mean of wikipedia more precisely we describe a dialog based question answering system for german which utilizes wikipedia based topic model a a reference point for context detection and answer prediction we investigate two different perspective to the task of interfacing virtual agent with collaborative knowledge first we exploit the use of wikipedia category a a basis for identifying the broader topic of a spoken utterance second we describe how to enhance the conversational behavior of the virtual agent by mean of a wikipedia based question answering component which incorporates the question topic at large our approach identifies topic related focus term of a user s question which are subsequently mapped onto a category taxonomy thus we utilize the taxonomy a a reference point to derive topic label for a user s question the employed topic model is thereby based on explicitly given concept a represented by the document and category structure of the wikipedia knowledge base identified topic category are subsequently combined with different linguistic filtering method to improve answer candidate retrieval and reranking result show that the topic model approach contributes to an enhancement of the conversational behavior of virtual agent 
in this paper we propose a new framework for constructing text metric which can be used to compare and support inference among term and set of term our metric is derived from data driven kernel on graph that let u capture global relation among term and set of term regardless of their complexity and size to compute the metric efficiently for any two subset of term we develop an approximation technique that relies on the precompiled term term similarity to scale up the approach to problem with huge number of term we develop and experiment with a solution that sub sample the term space we demonstrate the benefit of the whole framework on two text inference task prediction of term in the article from it abstract and query expansion in information retrieval 
case based problem solving system reason by retrieving relevant prior case and adapting their solution to fit new circumstance the ability of case based reasoning cbr to reason from ungeneralized episode can benefit knowledge acquisition but acquiring the needed case adaptation knowledge ha proven challenging this paper present a method for alleviating this problem with justin time gathering of case adaptation knowledge based on introspective reasoning and mining of web knowledge source the approach combine knowledge planning with introspective reasoning to guide recovery from case adaptation failure and reinforcement learning to guide selection of knowledge source the failure recovery and knowledge source selection method have been tested in three highly different domain with encouraging result the paper close with a discussion of limitation and future step 
a robust system for ontology based data access should provide meaningful answer to query even when the data conflict with the ontology this can be accomplished by adopting an inconsistency tolerant semantics with the consistent query answering cqa semantics being the most prominent example unfortunately query answering under the cqa semantics ha been shown to be computationally intractable even when extremely simple ontology language are considered in this paper we address this problem by proposing two new family of inconsistency tolerant semantics which approximate the cqa semantics from above and from below and converge to it in the limit we study the data complexity of conjunctive query answering under these new semantics and show a general tractability result for all known first order rewritable ontology language we also analyze the combined complexity of query answering for ontology language of the dl lite family 
the common pool resource cpr game is a social dilemma where agent have to decide how to consume a shared cpr either they each take their cut completely destroying the cpr or they restrain themselves gaining le immediate profit but sustaining the resource and future profit when no consumption take place the cpr simply grows to it carrying capacity a such this dilemma provides a framework to study the evolution of social consumption strategy and the sustainability of resource whose size adjusts dynamically through consumption and their own implicit population dynamic the present study provides for the first time a detailed analysis of the evolutionary dynamic of consumption strategy in finite population focusing on the interplay between the resource level and preferred consumption strategy we show analytically which restrained consumer survive in relation to the growth rate of the resource and how this affect the resource carrying capacity second we show that population structure affect the sustainability of the resource and social welfare in the population current result provide an initial insight into the complexity of the cpr game showing potential for a variety of different study in the context of social welfare and resource sustainability 
complex network pervade in diverse area ranging from the natural world to the engineered world and from traditional application domain to new and emerging domain including web based social network of crucial importance to the understanding of many network phenomenon dynamic and function is the study of network structural property one important type of network structure is known a community structure which refers to the existence of community that are tightly knit local group with relatively dense connection among their member community detection is the problem of detecting these community automatically in this paper based on the modularity measure proposed previously for community detection we first propose a reformulation of an optimization problem for the partition problem based on this new formulation we can extend it naturally for tackling the general k partition problem directly without having to tackle multiple partition subproblems like what other method do we then propose a convex relaxation scheme to give an iterative algorithm which solves a simple quadratic program in each iteration we empirically compare our method with some related method and find that our method is both scalable and competitive in performance via maintaining a good tradeoff between efficiency and quality 
satisfiability modulo constraint handling rule smchr is the integration of the constraint handling rule chrs solver programming language into a satisfiability modulo theory smt solver framework constraint solver are implemented in chr a a set of high level rule that specify the simplification rewriting and constraint propagation behavior the traditional chr execution algorithm manipulates a global store representing a flat conjunction of constraint this paper introduces smchr a tight integration of chr with a modern boolean satisfiability sat solver unlike chr smchr can handle quantifier free formula with an arbitrary propositional structure smchr is essentially a satisfiability modulo theory smt solver where the theory t is implemented in chr 
the semantic web aim to offer an interoperable environment that will allow user to safely delegate complex action to intelligent agent much work ha been done for agent interoperability especially in the area of ontology based metadata and rule based reasoning nevertheless the sw proof layer ha been neglected so far although it is vital for agent and human to understand how a result came about in order to increase the trust in the interchanged information this paper focus on the implementation of third party sw reasoning and proofing service wrapped a agent in a multi agent framework this way agent can exchange and justify their argument without the need to conform to a common rule paradigm via external reasoning and proofing service the receiving agent can grasp the semantics of the received rule set and check the validity of the inferred result 
real world problem generally involve several antagonistic objective like quality and cost for design problem or makespan and cost for planning problem the only approach to multiobjective ai planning rely on metric that can incorporate several objective in some linear combination and metric sensitive planner that are able to give different plan for different metric and hence to eventually approximate the pareto front of the multiobjective problem i e the set of optimal trade offs between the antagonistic objective divide and evolve dae is an evolutionary planner that embeds a classical planner and feed it with a sequence of subproblems of the problem at hand like all evolutionary algorithm dae can be turned into a pareto based multiobjective solver even though using an embedded planner that is not metric sensitive the pareto based multiobjective planner mo dae thus avoids the drawback of the aggregation method furthermore using yahsp a the embedded planner it outperforms in many case the metric based approach using lpg metric sensitive planner a witnessed by experimental result on original multiobjective benchmark built upon ipc domain 
we review the challenge of bayesian network learning especially parameter learning and specify the problem of learning with sparse data we explain how it is possible to incorporate both qualitative knowledge and data with a multinomial parameter learning method to achieve more accurate prediction with sparse data 
the markov decision process model is a powerful tool in planing task and sequential decision making problem the randomness of state transition and reward implies that the performance of a policy is often stochastic in contrast to the standard approach that study the expected performance we consider the policy that maximizes the probability of achieving a pre determined target performance a criterion we term probabilistic goal markov decision process we show that this problem is np hard but can be solved using a pseudo polynomial algorithm we further consider a variant dubbed chance constraint markov decision problem that treat the probability of achieving target performance a a constraint instead of the maximizing objective this variant is np hard but can be solved in pseudo polynomial time 
the growing focus on sustainable and environmentally friendly energy production ha resulted in the proliferation of distributed energy resource ders mainly based on renewable source like wind and sunlight however their small size and the intermittent nature of their supply mean that such generator cannot easily be assimilated into the current electricity network grid like conventional generator against this background virtual power plant are fast emerging a a solution to this problem whereby a large number of small energy generator may be aggregated together such that they exhibit the characteristic like a traditional generator in term of predictability and robustness in this work we propose a method to promote the formation of such cooperative vpps cvpps using multi agent technology in particular we design a payment mechanism that encourages ders to join cvpps with large overall production our method is based on strictly proper scoring rule and elicits accurate probabilistic estimate of energy production from the cvpps and in turn the member ders which aid in the planning of the supply schedule at the grid we empirically evaluate our approach using the real world setting of commercial wind farm in the uk and we show that our mechanism incentivises real ders to form cvpps and moreover it outperforms the current state of the art payment mechanism developed for this problem 
in a pursuit evasion game one or more pursuer aim to discover the existence of and then capture an evader the paper study pursuit evasion game in which player may have incomplete information concerning the game state a methodology is presented for the application of a model checker for the logic of knowledge and time to verify epistemic property in such game experimental result are provided from a number of case study that validate the feasibility of the approach 
the problem of feature selection ha aroused considerable research interest in the past few year traditional learning based feature selection method separate embedding learning and feature ranking in this paper we introduce a novel unsupervised feature selection approach via joint embedding learning and sparse regression jelsr instead of simply employing the graph laplacian for embedding learning and then regression we use the weight via locally linear approximation to construct graph and unify embedding learning and sparse regression to perform feature selection by adding the l norm regularization we can learn a sparse matrix for feature ranking we also provide an effective method to solve the proposed problem compared with traditional unsupervised feature selection method our approach could integrate the merit of embedding learning and sparse regression simultaneously plenty of experimental result are provided to show the validity 
this paper present a plan recognition algorithm for inferring student behavior using virtual science laboratory the algorithm extends existing plan recognition technology and wa integrated with an existing educational application for chemistry automatic recognition of student activity in virtual laboratory can provide important information to teacher a well a serve a the basis for intelligent tutoring student use of virtual laboratory present several challenge student may repeat activity indefinitely interleave between activity and engage in exploratory behavior using trial and error the plan recognition algorithm us a recursive grammar that heuristically generates plan on the fly taking into account chemical reaction and effect to determine student intended high level action the algorithm wa evaluated empirically on data obtained from college student using virtual laboratory software for teaching chemistry result show that the algorithm wa able to infer the plan used by student to construct their model recognize such key process a titration and dilution when they occurred in student work identify partial solution isolate sequence of action that were part of a single error 
the concept of ideal semantics ha been promoted a an alternative basis for skeptical reasoning within abstract argumentation setting informally ideal acceptance not only requires an argument to be skeptically accepted in the traditional sense but further insists that the argument is in an admissible set all of whose argument are also skeptically accepted the original proposal wa couched in term of the so called preferred semantics for abstract argumentation we argue in this paper that the notion of ideal acceptability is applicable to arbitrary semantics and justify this claim by showing that standard property of classical ideal semantics e g unique status continue to hold in any reasonable extension based semantics we categorise the relationship between the diver concept of ideal extension wrt semantics that arise and we present a comprehensive analysis of algorithmic and complexity theoretic issue 
this paper is concerned with data selection for adapting language model lm in statistical machine translation smt and aim to find the lm training sentence that are topic similar to the translation task although the traditional approach have gained significant performance they ignore the topic information and the distribution information of word when selecting similar training sentence in this paper we present two bilingual topic model bltm joint and coupled bltm based sentence representation for cross lingual data selection we map the data selection task into cross lingual semantic representation that are language independent then rank and select sentence in the target language lm training corpus for a sentence in the translation task by the semanticsbased likelihood the semantic representation are learned from the parallel corpus with the assumption that the bilingual pair share the same or similar distribution over semantic topic large scale experimental result demonstrate that our approach significantly outperform the state of the art approach on both lm perplexity and translation performance respectively 
the distributed ontology language dol currently being standardized a iso wd within the ontoiop ontology integration and interoperability activity of iso tc provides a unified framework for ontology formalized in heterogeneous logic modular ontology link between ontology and ontology annotation a dol ontology consists of module formalized in language such a owl or common logic serialized in the existing syntax of these language on top dol s meta level allows for expressing heterogeneous ontology and link between ontology including heterogeneous import and alignment conservative extension and theory interpretation we present the abstract syntax of these meta level construct with three alternative semantics direct translational and collapsed semantics 
computational complexity of voting manipulation is one of the most actively studied topic in the area of computational social choice starting with the groundbreaking work of bartholdi et al most of the existing work in this area including that of bartholdi et al implicitly assumes that whenever several candidate receive the top score with respect to the given voting rule the resulting tie is broken according to a lexicographic ordering over the candidate however till recently an equally appealing method of tie breaking namely selecting the winner uniformly at random among all tied candidate ha not been considered in the computational social choice literature the first paper to analyze the complexity of voting manipulation under randomized tiebreaking is obraztsova et al where the author provide polynomial time algorithm for this problem under scoring rule and under an additional assumption on the manipulator s utility for maximin in this paper we extend the result of obraztsova et al by showing that finding an optimal vote under randomized tie breaking is computationally hard for copeland and maximin with general utility a well a for stv and ranked pair but easy for the bucklin rule and plurality with runoff 
in dynamic system state constraint are formula that hold in every reachable state it ha been shown that state constraint can be used to greatly reduce the planning search space they are also useful in program verification in this paper we propose a sound but incomplete method for automatic verification and discovery of state constraint for a class of action theory that include many planning benchmark our method is formulated in the situation calculus theoretically based on skolemization and herbrand theorem and implemented with sat solver basically we verify a state constraint by strengthening it in a novel and smart way so that it becomes a state invariant we experimented with the block world logistics and satellite domain and the result showed that almost all known state constraint can be verified in a reasonable amount of time and meanwhile succinct and intuitive related state constraint are discovered 
motivated by the recent development of nonconvex penalty in sparsity modeling we propose a nonconvex optimization model for handing the low rank matrix recovery problem different from the famous robust principal component analysis rpca we suggest recovering low rank and sparse matrix via a nonconvex loss function and a nonconvex penalty the advantage of the nonconvex approach lie in it stronger robustness to solve the model we devise a majorization minimization augmented lagrange multiplier mm alm algorithm which find the local optimal solution of the proposed nonconvex model we also provide an efficient strategy to speedup mm alm which make the running time comparable with the state of the art algorithm of solving rpca finally empirical result demonstrate the superiority of our nonconvex approach over rpca in term of matrix recovery accuracy 
the original yahoo search engine consists of manually organized topic hierarchy of webpage for easy browsing modern search engine such a google and bing on the other hand return a flat list of webpage based on keywords it would be ideal if hierarchical browsing and keyword search can be seamlessly combined the main difficulty in doing so is to automatically i e not manually classify and rank a massive number of webpage into various hierarchy such a topic medium type region of the world in this paper we report our attempt towards building this integrated search engine called see search engine with hierarchy we implement a hierarchical classification system based on supportvector machine and embed it in see we also design a novel user interface that allows user to dynamically adjust their desire for a higher accuracy v more result in any sub category of the hierarchy though our current search engine is still small indexing about million webpage the result including a small user study have shown a great promise for integrating such technique in the next generation search engine 
model checking is one of the most effective technique in automated system verification although this technique can handle complex verification model checking tool usually do not give any suggestion on how to repair inconsistent system model in this paper we show that approach developed to update model of computation tree logic ctl cannot deal with all kind of change we introduce the concept of ctl model revision an approach based on belief revision to handle system inconsistency in a static context 
the problem of finding the set of pareto optimals for constraint and qualitative preference together is of great interest to many application area it can be viewed a a preference constrained optimization problem where the goal is to find one or more feasible solution that are not dominated by other feasible outcome our work aim to enhance the current literature of the problem by providing solving method targeting the problem in a dynamic environment we target the problem with an eye on adopting and benefiting from the current constraint satisfaction technique 
network community detection the problem of dividing a network of interest into cluster for intelligent analysis ha recently attracted significant attention in diverse field of research to discover intrinsic community structure a quantitative measure called modularity ha been widely adopted a an optimization objective unfortunately modularity is inherently np hard to optimize and approximate solution must be sought if tractability is to be ensured in practice a spectral relaxationmethod is most often adopted after which a community partition is recovered from relaxed fractional value by a rounding process in this paper we propose an iterative rounding strategy for identifying the partition decision that is coupled with a fast constrained power method that sequentially achieves tighter spectral relaxation extensive evaluation with this coupled relaxation rounding method demonstrates consistent and sometimes dramatic improvement in the modularity of the community discovered 
we address the issue of manipulating game through communication in the specific setting we consider a variation of boolean game we assume there is some set of environment variable the value of which is not directly accessible to player each player ha their own belief about these variable and make decision about what action to perform based on these belief the communication we consider take the form of truthful announcement about the value of some environment variable the effect of an announcement about some variable is to modify the belief of the player who hear the announcement so that they accurately reflect the value of the announced variable by choosing announcement appropriately it is possible to perturb the game away from certain rational outcome and towards others we specifically focus on the issue of stabilisation making announcement that transform a game from having no stable state to one that ha stable configuration 
an elegant approach to learning temporal ordering from text is to formulate this problem a a constraint optimization problem which can be then given an exact solution using integer linear programming this work well for case where the number of possible relation between temporal entity is restricted to the mere precedence relation bramsen et al chamber and jurafsky but becomes impractical when considering all possible interval relation this paper proposes two innovation inspired from work on temporal reasoning that control this combinatorial blow up therefore rendering an exact ilp inference viable in the general case first we translate our network of constraint from temporal interval to their endpoint to handle a drastically smaller set of constraint while preserving the same temporal information second we show that additional efficiency is gained by enforcing coherence on particular subset of the entire temporal graph we evaluate these innovation through various experiment on timebank and compare our ilp formulation with various baseline and oracle system 
this paper proposes a novel algorithm for manifold alignment preserving global geometry this approach construct mapping function that project data instance from different input domain to a new lower dimensional space simultaneously matching the instance in correspondence and preserving global distance between instance within the original domain in contrast to previous approach which are largely based on preserving local geometry the proposed approach is suited to application where the global manifold geometry need to be respected we evaluate the effectiveness of our algorithm for transfer learning in two real world cross lingual information retrieval task 
subset space logic have been introduced and studied a a framework for reasoning about a notion of effort in epistemic logic the seminal subset space logic ssl by moss and parikh modeled a single agent and most work in this area ha focused on different extension of the language or different model class resulting from restriction on subset space while still keeping the single agent assumption in this paper we argue that the few existing attempt at multi agent version of ssl are unsatisfactory and propose a new multi agent subset space logic which is a natural extension of single agent ssl the main result are a sound and complete axiomatization of this logic a well a an alternative and equivalent relational semantics 
individual often express their opinion on social medium platform like twitter and facebook during public event such a the u s presidential debate and the oscar award ceremony gleaning insight from these post is of importance to analyzing the impact of the event in this work we consider the problem of identifying the segment and topic of an event that garnered praise or criticism according to aggregated twitter response we propose a flexible factorization framework socsent to learn factor about segment topic and sentiment to regulate the learning process several constraint based on prior knowledge on sentiment lexicon sentiment orientation on a few tweet a well a tweet alignment to the event are enforced we implement our approach using simple update rule to get the optimal solution we evaluate the proposed method both quantitatively and qualitatively on two large scale tweet datasets associated with two event from different domain to show that it improves significantly over baseline model 
monte carlo tree search mcts method have had recent success in game planning and optimization mcts us result from rollouts to guide search a rollout is a path that descends the tree with a randomized decision at each ply until reaching a leaf mcts result can be strongly influenced by the choice of appropriate policy to bias the rollouts most previous work on mcts us static uniform random or domain specific policy we describe a new mcts method that dynamically adapts the rollout policy during search in deterministic optimization problem our starting point is cazenave s original nested monte carlo search nmcs but rather than navigating the tree directly we instead use gradient ascent on the rollout policy at each level of the nested search we benchmark this new nested rollout policy adaptation nrpa algorithm and examine it behavior our test problem are instance of crossword puzzle construction and morpion solitaire over moderate time scale nrpa can substantially improve search efficiency compared to nmcs and over longer time scale nrpa improves upon all previous published solution for the test problem result include a new morpion solitaire solution that improves upon the previous human generated record that had stood for over year 
in the past few year sentiment analysis and opinion mining becomes a popular and important task these study all assume that their opinion resource are real and trustful however they may encounter the faked opinion or opinion spam problem in this paper we study this issue in the context of our product review mining system on product review site people may write faked review called review spam to promote their product or defame their competitor product it is important to identify and filter out the review spam previous work only focus on some heuristic rule such a helpfulness voting or rating deviation which limit the performance of this task in this paper we exploit machine learning method to identify review spam toward the end we manually build a spam collection from our crawled review we first analyze the effect of various feature in spam identification we also observe that the review spammer consistently writes spam this provides u another view to identify review spam we can identify if the author of the review is spammer based on this observation we provide a twoview semi supervised method co training to exploit the large amount of unlabeled data the experiment result show that our proposed method is effective our designed machine learning method achieve significant improvement in comparison to the heuristic baseline 
the boolean network is a mathematical model of biological system and ha attracted much attention a a qualitative tool for analyzing the regulatory system the stable state and dynamic of boolean network are characterized by their attractor whose property have been analyzed computationally yet not much work ha been done from the viewpoint of logical inference system in this paper we show direct translation of boolean network into logic program and propose new method to compute their trajectory and attractor based on inference on such logic program in particular point attractor of both synchronous and asynchronous boolean network are characterized a supported model of logic program so that sat technique can be applied to compute them investigation of these relationship suggests u to view boolean network a logic program and vice versa 
in virtually all machine learning application hyper parameter tuning is required to maximize predictive accuracy such tuning is computationally expensive and the cost is further exacerbated by the need for multiple evaluation via cross validation or bootstrap at each configuration setting to guarantee statistically significant result this paper present a simple general technique for improving the efficiency of hyper parameter tuning by minimizing the number of resampled evaluation at each configuration we exploit the fact that train test sample can easily be matched across candidate hyper parameter configuration this permit the use of paired hypothesis test and power analysis that allow for statistically sound early elimination of suboptimal candidate to minimize the number of evaluation result on synthetic and real world datasets demonstrate that our method improves over competitor for discrete parameter setting and enhances state of the art technique for continuous parameter setting 
most study were devoted to the design of efficient algorithm and the evaluation and application on diverse ranking problem whereas few work ha been paid to the theoretical study on ranking learnability in this paper we study the relation between uniform convergence stability and learnability of ranking in contrast to supervised learning where the learnability is equivalent to uniform convergence we show that the ranking uniform convergence is sufficient but not necessary for ranking learnability with aerm and we further present a sufficient condition for ranking uniform convergence with respect to bipartite ranking loss considering the ranking uniform convergence being unnecessary for ranking learnability we prove that the ranking average stability is a necessary and sufficient condition for ranking learnability 
multi winner social choice considers the problem of selecting a slate of k option to realize some social objective it ha found application in the construction of political legislature and committee product recommendation and related problem and ha recently attracted attention from a computational perspective we address the multi winner problem when facing incomplete voter preference using the notion of minimax regret to determine a robust slate of option in the presence of preference uncertainty we analyze the complexity of this problem and develop new exact and greedy robust optimization algorithm for it solution using these technique we also develop preference elicitation heuristic which in practice allow u to find near optimal slate with considerable saving in the preference information required vi vi complete vote 
in this work we present a new framework for large scale online kernel classification making kernel method efficient and scalable for large scale online learning task unlike the regular budget kernel online learning scheme that usually us different strategy to bound the number of support vector our framework explores a functional approximation approach to approximating a kernel function matrix in order to make the subsequent online learning task efficient and scalable specifically we present two different online kernel machine learning algorithm i the fourier online gradient descent fogd algorithm that applies the random fourier feature for approximating kernel function and ii the nystr m online gradient descent nogd algorithm that applies the nystr m method to approximate large kernel matrix we offer theoretical analysis of the proposed algorithm and conduct experiment for large scale online classification task with some data set of over million instance our encouraging result validate the effectiveness and efficiency of the proposed algorithm making them potentially more practical than the family of existing budget kernel online learning approach 
since the first principle of knowledge compilation kc most of the work ha been focused in finding a good compilation target language in term of compromise between compactness and expressiveness the central idea remains unchanged in the last fifteen year an off line very hard stage allows to compile the initial theory in order to guarantee theoretically an efficient on line stage on a set of predefined query and operation we propose a new just in time approach for kc here any knowledge base kb will be immediately available for query and the effort spent on past query will be partly amortized for future one to guarantee efficient answer we rely on the tremendous progress made in the practical solving of sat and incremental sat applicative problem even if each query may be theoretically hard we show that our approach outperforms previous kc approach on the set of classical problem used in the field and allows to handle problem that are out of the scope of current approach 
we propose a method to deform robot trajectory based on affine transformation at the heart of our approach is the concept of affine invariance trajectory are deformed in order to avoid unexpected obstacle or to attain new goal but at the same time certain precise feature of the original motion are preserved such feature include for instance trajectory smoothness periodicity affine velocity or more generally all affine invariant feature which are of particular importance in human centered application furthermore the proposed method is very efficient and easy to implement there is no need to re integrate even a part of the trajectory and in most case closed form solution can be worked out the method is also versatile optimization of geometric and dynamic parameter or satisfaction of inequality constraint can be taken into account in a very natural way a illustration we present a method for transferring human motion to humanoid robot while preserving equiaffine velocity building on the presented affine deformation framework we finally revisit the concept of trajectory redundancy from the viewpoint of group theory 
in this paper we provide a new axiomatization of the event model based dynamic epistemic logic based on the completeness proof method proposed in wang and cao this axiomatization doe not use any of the standard reduction axiom but naturally capture the essence of the update product we demonstrate the use of our new axiomatization and the corresponding proof technique by three set of result characterization theorem of the update operation representation theorem of the del generatable epistemic temporal structure given a fixed event model and a complete axiomatization of del on model with protocol 
the paper address the problem of learning to parse sentence to logical representation of their underlying meaning by inducing a syntactic semantic grammar the approach us a class of grammar which ha been proven to be learnable from representative example in this paper we introduce tractable learning algorithm for learning this class of grammar comparing them in term of a priori knowledge needed by the learner hypothesis space and algorithm complexity we present experimental result on learning tense aspect modality and negation of verbal construction 
in this paper we concentrate on finding a suitable metric to determine the flexibility of a simple temporal problem stp after reviewing some flexibility metric that have been proposed we conclude that these metric fail to capture the correlation between event specified in the stp resulting in an overestimation of the available flexibility in the system we propose to use an intuitively more acceptable flexibility metric based upon uncorrelated time interval for the allowed starting time of event in an stp this metric is shown to be computable in low polynomial time a a byproduct of the flexibility computation we get a decomposition of the stn almost for free for every possible k partitioning of the event space a decomposition can be computed in o k time even more importantly we show that contrary to popular belief such a decomposition doe not affect the flexibility of the original stp 
we introduce miningzinc a general framework for constraint based pattern mining one of the most popular task in data mining miningzinc consists of two key component a language component and a toolchain component the language allows for high level and natural modeling of mining problem such that miningzinc model closely resemble definition found in the data mining literature it is inspired by the zinc family of language and system and support user defined constraint and optimization criterion the toolchain allows for finding solution to the model it ensures the solver independence of the language and support both standard constraint solver and specialized data mining system automatic model transformation enable the efficient use of different solver and system the combination of both component allows one to rapidly model constraint based mining problem and execute these with a wide variety of method we demonstrate this experimentally for a number of well known solver and data mining task 
reputation is a crucial concept in dynamic multiagent environment despite the large body of work on reputation system no metric exist to directly and quantitatively evaluate and compare them we present a common conceptual interface for reputation system and a set of four measurable desideratum that are broadly applicable across multiple domain these desideratum employ concept from dynamical system theory to measure how a reputation system reacts to a strategic agent attempting to maximize it own utility we study a diverse set of well known reputation model from the literature in a moral hazard setting and identify a rich variety of characteristic that they support 
automatically identifying informative review is increasingly important given the rapid growth of user generated review on site like amazon and tripadvisor in this paper we describe and evaluate technique for identifying and recommending helpful product review using a combination of review feature including topical and sentiment information mined from a review corpus 
maximum entropy discrimination med is a general framework for discriminative estimation based on the well known maximum entropy principle which embodies the bayesian integration of prior information with large margin constraint on observation it is a successful combination of maximum entropy learning and maximum margin learning and can subsume support vector machine svms a a special case in this paper we present a multi view maximum entropy discrimination framework that is an extension of med to the scenario of learning with multiple feature set different from existing approach to exploiting multiple view such a co training style algorithm and co regularization style algorithm we propose a new method to make use of the distinct view where classification margin from these view are required to be identical we give the general form of the solution to the multi view maximum entropy discrimination and provide an instantiation under a specific prior formulation which is analogical to a multi view version of svms experimental result on real world data set show the effectiveness of the proposed multi view maximum entropy discrimination approach 
we study to incorporate multiple view of data in a perceptive transfer learning framework and propose a multi view discriminant transfer mdt learning approach for domain adaptation the main idea is to find the optimal discriminant weight vector for each view such that the correlation between the two view projected data is maximized while both the domain discrepancy and the view disagreement are minimized simultaneously furthermore we analyze mdt theoretically from discriminant analysis perspective to explain the condition and reason under which the proposed method is not applicable the analytical result allow u to investigate whether there exist within view and or between view conflict and thus provides a deep insight into whether the transfer learning algorithm work properly or not in the view based problem and the combined learning problem experiment show that mdt significantly outperforms the state of the art baseline including some typical multi view learning approach in single or cross domain 
data sparsity due to missing rating is a major challenge for collaborative filtering cf technique in recommender system this is especially true for cf domain where the rating are expressed numerically we observe that while we may lack the information in numerical rating we may have more data in the form of binary rating this is especially true when user can easily express themselves with their like and dislike for certain item in this paper we explore how to use the binary preference data expressed in the form of like dislike to help reduce the impact of data sparsity of more expressive numerical rating we do this by transferring the rating knowledge from some auxiliary data source in binary form that is like or dislike to a target numerical rating matrix our solution is to model both numerical rating and like dislike in a principled way using a novel framework of transfer by collective factorization tcf in particular we construct the shared latent space collectively and learn the data dependent effect separately a major advantage of the tcf approach over previous collective matrix factorization or bifactorization method is that we are able to capture the data dependent effect when sharing the data independent knowledge so a to increase the over all quality of knowledge transfer experimental result demonstrate the effectiveness of tcf at various sparsity level a compared to several state of the art method 
persuasion is a common social and economic activity it usually arises when conflicting interest among agent exist and one of the agent wish to sway the opinion of others this paper considers the problem of an automated agent that need to influence the decision of a group of self interested agent that must reach an agreement on a joint action for example consider an automated agent that aim to reduce the energy consumption of a nonresidential building by convincing a group of people who share an office to agree on an economy mode of the air conditioning and low light intensity in this paper we present four problem that address issue of minimality and safety of the persuasion process we discus the relationship to similar problem from social choice and show that if the agent are using plurality or veto a their voting rule all of our problem are in p we also show that with k approval bucklin and borda voting rule some problem become intractable we thus present heuristic for efficient persuasion with borda and evaluate them through simulation 
we define solving technique for the minimum satisfiability problem minsat propose an efficient branch and bound algorithm to solve the weighted partial minsat problem and report on an empirical evaluation of the algorithm on min sat max clique and combinatorial auction problem technique solving minsat are substantially different from those for the maximum satisfiability problem maxsat our result provide empirical evidence that solving combinatorial optimization problem by reducing them to minsat may be substantially faster than reducing them to maxsat and even competitive with specific algorithm we also use minsat to study an interesting correlation between the minimum number and the maximum number of satisfied clause of a sat instance 
with the proliferation of it application in various industry sentiment analysis by using publicly available web data ha become an active research area in text classification during these year it is argued by researcher that semi supervised learning is an effective approach to this problem since it is capable to mitigate the manual labeling effort which is usually expensive and timeconsuming however there wa a long term debate on the effectiveness of unlabeled data in text classification this wa partially caused by the fact that many assumption in theoretic analysis often do not hold in practice we argue that this problem may be further understood by adding an additional dimension in the experiment this allows u to address this problem in the perspective of bias and variance in a broader view we show that the well known performance degradation issue caused by unlabeled data can be reproduced a a subset of the whole scenario we argue that if the bias variance tradeoff is to be better balanced by a more effective feature selection method unlabeled data is very likely to boost the classification performance we then propose a feature selection framework in which labeled and unlabeled training sample are both considered we discus it potential in achieving such a balance besides the application in financial sentiment analysis is chosen because it not only exemplifies an important application the data posse better illustrative power a well the implication of this study in text classification and financial sentiment analysis are both discussed copyright association for the advancement of artificial intelligence www aaai org all right reserved 
a serious and ubiquitous issue in machine learning is the lack of sufficient training data in a domain of interest domain adaptation is an effective approach to dealing with this problem by transferring information or model learned from related albeit distinct domain to the target domain we develop a novel domain adaptation method for text document classification under the framework of non negative matrix factorization two key idea of our method are to construct a latent topic space where a topic is decomposed into common word shared by all domain and word specific to individual domain and then to establish association between word in different domain through the common word a a bridge for knowledge transfer the correspondence between cross domain topic lead to more coherent distribution of source and target domain in the new representation while preserving the predictive power our new method outperformed several state of the art domain adaptation method on several benchmark datasets 
multi modal data is dramatically increasing with the fast growth of social medium learning a good distance measure for data with multiple modality is of vital importance for many application including retrieval clustering classification and recommendation in this paper we propose an effective and scalable multi modal distance metric learning framework based on the multi wing harmonium model our method provides a principled way to embed data of arbitrary modality into a single latent space of which an optimal distance metric can be learned under proper supervision i e by minimizing the distance between similar pair whereas maximizing the distance between dissimilar pair the parameter are learned by jointly optimizing the data likelihood under the latent space model and the loss induced by distance supervision thereby our method seek a balance between explaining the data and providing an effective distance metric which naturally avoids overfitting we apply our general framework to text image data and present empirical result on retrieval and classification to demonstrate the effectiveness and scalability 
there is increasing awareness in the planning community that depending on complete model impedes the applicability of planning technology in many real world domain where the burden of specifying complete domain model is too high in this paper we consider a novel solution for this challenge that combine generative planning on incomplete domain model with a library of plan case that are known to be correct while this wa arguably the original motivation for case based planning most existing case based planner assume and depend on from scratch planner that work on complete domain model in contrast our approach view the plan generated with respect to the incomplete model a a skeletal plan and augments it with directed mining of plan fragment from library case we will present the detail of our approach and present an empirical evaluation of our method in comparison to a state of the art case based planner that depends on complete domain model copyright association for the advancement of artificial intelligence www aaai org all right reserved 
multi level logic synthesis is a problem of immense practical significance and is a key to developing circuit that optimize a number of parameter such a depth energy dissipation reliability etc the problem can be defined a the task of taking a collection of component from which one want to synthesize a circuit that optimizes a particular objective function this problem is computationally hard and there are very few automated approach for it solution to solve this problem we propose an algorithm called circuit decomposition engine cde that is based on learning decision tree and us a greedy approach for function learning we empirically demonstrate that cde when given a library of different component type can learn the function of disjunctive normal form dnf boolean representation and synthesize circuit structure using the input library we compare the structure of the synthesized circuit with that of well known circuit using a range of circuit similarity metric 
given an argumentation framework and a group of agent the individual may have divergent opinion on the status of the argument if the group need to reach a common position on the argumentation framework the question is how the individual evaluation can be mapped into a collective one this problem ha been recently investigated in in this paper we study under which condition these operator are pareto optimal and whether they are manipulable 
in heterogeneous multi agent system trust is necessary to improve interaction by enabling agent to choose good partner most trust model work by taking in addition to direct experience other agent communicated evaluation into account however in an open ma other agent may use different trust model and the evaluation they communicate are based on different principle a such they are meaningless without some form of alignment my doctoral research give a formal definition of this problem and proposes two method of achieving an alignment 
we study a pursuit evasion game in which one or more cop try to capture a robber by moving onto the robber s current location all player have equal maximum velocity they can observe each other at all time we show that three cop can capture the robber in any polygonal environment which can contain any finite number of hole 
in this paper we propose a novel method for object discovery and dense modelling in rgb d image sequence using motion cue we develop our method a a building block for active object perception such that robot can learn about the environment through perceiving the effect of action our approach simultaneously segment rigid body motion within key view and discovers object and hierarchical relation between object part the pose of the key view are optimized in a graph of spatial relation to recover the rigid body motion trajectory of the camera with respect to the object in experiment we demonstrate that our approach find moving object aligns partial view on the object and retrieves hierarchical relation between the object 
measuring the semantic meaning between word is an important issue because it is the basis for many application such a word sense disambiguation document summarization and so forth although it ha been explored for several decade most of the study focus on improving the effectiveness of the problem i e precision and recall in this paper we propose to address the efficiency issue that given a collection of word how to efficiently discover the top k most semantic similar word to the query this issue is very important for real application yet the existing state of the art strategy cannot satisfy user with reasonable performance efficient strategy on searching top k semantic similar word are proposed we provide an extensive comparative experimental evaluation demonstrating the advantage of the introduced strategy over the state of the art approach 
this paper present a meta heuristic algorithm for solving the flexible job shop scheduling problem fjssp this strategy known a iterative flattening search ifs iteratively applies a relaxation step in which a subset of scheduling decision are randomly retracted from the current solution and a solving step in which a new solution is incrementally recomputed from this partial schedule this work contributes two separate result it proposes a constraint based procedure extending an existing approach previously used for classical job shop scheduling problem it proposes an original relaxation strategy on feasible fjssp solution based on the idea of randomly breaking the execution order of the activity on the machine and opening the resource option for some activity selected at random the efficacy of the overall heuristic optimization algorithm is demonstrated on a set of well known benchmark 
we study hybrid online batch matching problem where agent arrive continuously but are only matched in periodic round when many of them can be considered simultaneously agent not getting matched in a given round remain in the market for the next round this setting model several scenario of interest including many job market a well a kidney exchange mechanism we consider the social utility of two commonly used mechanism for such market one that aim for stability in each round greedy and one that attempt to maximize social utility in each round max weight surprisingly we find that in the long term the social utility of the greedy mechanism can be higher than that of the max weight mechanism we hypothesize that this is because the greedy mechanism behaves similarly to a soft threshold mechanism where all connection below a certain threshold are rejected by the participant in favor of waiting until the next round motivated by this observation we propose a method to approximately calculate the optimal threshold for an individual agent based on characteristic of the other agent and demonstrate empirically that social utility is high when all agent use this strategy 
in automatic summarization centrality a relevance mean that the most important content of an information source or of a collection of information source corresponds to the most central passage considering a representation where such notion make sense graph spatial etc we ass the main paradigm and introduce a new centrality based relevance model for automatic summarization that relies on the use of support set to better estimate the relevant content geometric proximity is used to compute semantic relatedness centrality relevance is determined by considering the whole input source and not only local information and by taking into account the existence of minor topic or lateral subject in the information source to be summarized the method consists in creating for each passage of the input source a support set consisting only of the most semantically related passage then the determination of the most relevant content is achieved by selecting the passage that occur in the largest number of support set this model produce extractive summary that are generic and languageand domain independent thorough automatic evaluation show that the method achieves state of the art performance both in written text and automatically transcribed speech summarization even when compared to considerably more complex approach 
in this paper we propose to extract a compact yet discriminative visual descriptor directly on the mobile device which tackle the wireless query transmission latency in mobile landmark search this descriptor originates from offline learning the location context of geo tagged web photo from both flickr and panoramio with two phrase first we segment the landmark photo collection into discrete geographical region using a gaussian mixture model stauffer et al second a ranking sensitive vocabulary boosting is introduced to learn a compact codebook within each region to tackle the locally optimal descriptor learning caused by imprecise geographical segmentation we further iterate above phrase incorporating the feedback of an entropy based descriptor compactness into a prior distribution to constrain the gaussian mixture modeling consequently when entering a specific geographical region the codebook in the mobile device is downstream adapted which ensures efficient extraction of compact descriptor it low bit rate transmission a well a promising discrimination ability we descriptor to both htc and iphone mobile phone testing landmark search over one million image in typical area like beijing new york and barcelona etc our descriptor outperforms alternative compact descriptor chen et al chen et al chandrasekhar et al a chandrasekhar et al b with a large margin 
constraint propagation is one of the key technique in constraint programming and a large body of work ha built up around it special purpose constraint propagation algorithm frequently make implicit use of short support by examining a subset of the variable they can infer support a justification that a variable value pair still form part of a solution to the constraint for all other variable and value and save substantial work recently short support have been used in general purpose propagator and when the constraint is amenable to short support speed ups of more than three order of magnitude have been demonstrated in this paper we present shortstr a development of the simple tabular reduction algorithm str we show that shortstr is complementary to the existing algorithm shortgac and haggisgac that exploit short support while being much simpler when a constraint is amenable to short support the short support set can be exponentially smaller than the full length support set therefore shortstr can efficiently propagate many constraint that str cannot even load into memory we also show that shortstr can be combined with a simple algorithm to identify short support from full length support to provide a superior drop in replacement for str 
we propose a novel approach to semantic segmentation using weakly supervised label in traditional fully supervised method superpixel label are available for training however it is not easy to obtain enough labeled superpixels to learn a satisfying model for semantic segmentation by contrast only image level label are necessary in weakly supervised method which make them more practical in real application in this paper we develop a new way of evaluating classification model for semantic segmentation given weekly supervised label for a certain category provided the classification model parameter we firstly learn the basis superpixels by sparse reconstruction and then evaluate the parameter by measuring the reconstruction error among negative and positive superpixels based on gaussian mixture model we use iterative merging update imu algorithm to obtain the best parameter for the classification model experimental result on two real world datasets show that the proposed approach outperforms the existing weakly supervised method and it also competes with state of the art fully supervised method 
nondeterminism is pervasive in all but the simplest action domain an agent may flip a coin or pick up a different object than intended or an action may fail and may fail in different way in this paper we provide a qualitative theory of nondeterminism the account is based on an epistemic extension to the situation calculus that accommodates sensing action our position is that nondeterminism is an epistemic phenomenon and that the world is most usefully regarded a deterministic nondeterminism arises from an agent s limited awareness and perception the account offer several advantage an agent ha a set of categorical a opposed to probabilistic belief yet can deal with equally likely outcome such a in flipping a fair coin or with outcome of differing plausibility such a an action that may on rare occasion fail 
trust prediction which explores the unobserved relationship between online community user is an emerging and important research topic in social network analysis and many web application similar to other social based recommender system trust relationship between user can be also modeled in the form of matrix recent study show user generally establish friendship due to a few latent factor it is therefore reasonable to assume the trust matrix are of low rank a a result many recommendation system strategy can be applied here in particular trace norm minimization which us matrix s trace norm to approximate it rank is especially appealing however recent article cast doubt on the validity of trace norm approximation in this paper instead of using trace norm minimization we propose a new robust rank k matrix completion method which explicitly seek a matrix with exact rank moreover our method is robust to noise or corrupted observation we optimize the new objective function in an alternative manner based on a combination of ancillary variable and augmented lagrangian multiplier alm method we perform the experiment on three real world data set and all empirical result demonstrate the effectiveness of our method 
consensus clustering emerges a a promising solution to find cluster structure from data a an efficient approach for consensus clustering the k mean based method ha garnered attention in the literature but the existing research is still preliminary and fragmented in this paper we provide a systematic study on the framework of k mean based consensus clustering kcc we first formulate the general definition of kcc and then reveal a necessary and sufficient condition for utility function that work for kcc on both complete and incomplete basic partitioning experimental result on various real world data set demonstrate that kcc is highly efficient and is comparable to the state of the art method in term of clustering quality in addition kcc show high robustness to incomplete basic partitioning with substantial missing value 
recent year have witnessed the success of hashing technique in approximate nearest neighbor search in practice multiple hash table are usually employed to retrieve more desired result from all hit bucket of each table however there are rare work studying the unified approach to constructing multiple informative hash table except the widely used random way in this paper we regard the table construction a a selection problem over a set of candidate hash function with the graph representation of the function set we propose an efficient solution that sequentially applies normalized dominant set to finding the most informative and independent hash function for each table to further reduce the redundancy between table we explore the reciprocal hash table in a boosting manner where the hash function graph is updated with high weight emphasized on the misclassified neighbor pair of previous hash table the construction method is general and compatible with different type of hashing algorithm using different feature space and or parameter setting extensive experiment on two large scale benchmark demonstrate that the proposed method outperforms both naive construction method and state of the art hashing algorithm with up to accuracy gain copyright association for the advancement of artificial intelligence www aaai org all right reserved 
pose variation is one of the challenging factor for face recognition in this paper we propose a novel cross pose face recognition method named a regularized latent least square regression rllsr the basic assumption is that the image captured under different pose of one person can be viewed a pose specific transforms of a single ideal object we treat the observed image a regressor the ideal object a response and then formulate this assumption in the least square regression framework so a to learn the multiple pose specific transforms specifically we incorporate some prior knowledge a two regularization term into the least square approach the smoothness regularization a the transforms for nearby pose should not differ too much the local consistency constraint a the distribution of the latent ideal object should preserve the geometric structure of the observed image space we develop an alternating algorithm to simultaneously solve for the ideal object of the training individual and a set of pose specific transforms the experimental result on the multi pie dataset demonstrate the effectiveness of the proposed method and superiority over the previous method 
we address the issue of ontology based data access which consists of exploiting the semantics expressed in ontology while querying data ontology are represented in the framework of existential rule also known a datalog we focus on the backward chaining paradigm which involves rewriting the query assumed to be a conjunctive query cq into a set of cqs seen a a union of cqs the proposed algorithm accepts any set of existential rule a input and stop for so called finite unification set of rule fus the rewriting step relies on a graph notion called a piece which allows to identify subset of atom from the query that must be processed together we first show that our rewriting method computes a minimal set of cqs when this set is finite i e the set of rule is a fus we then focus on optimizing the rewriting step first experiment are reported in the associated technical report 
salient object detection provides an alternative solution to various image semantic understanding task such a object recognition adaptive compression and image retrieval recently low rank matrix recovery lr theory ha been introduced into saliency detection and achieves impressed result however the existing lr based model neglect the underlying structure of image and inevitably degrade the associated performance in this paper we propose a low rank and structured sparse matrix decomposition lsmd model for salient object detection in the model a tree structured sparsity inducing norm regularization is firstly introduced to provide a hierarchical description of the image structure to ensure the completeness of the extracted salient object the similarity of saliency value within the salient object is then guaranteed by the norm finally high level prior are integrated to guide the matrix decomposition and enhance the saliency detection experimental result on the largest public benchmark database show that our model outperforms existing lrbased approach and other state of the art method which verifies the effectiveness and robustness of the structure cue in our model copyright association for the advancement of artificial intelligence www aaai org all right reserved 
because of the inevitable impact factor such a pose expression lighting and aging on face identity verification through face is still an unsolved problem research on biometrics raise an even challenging problem is it possible to determine the kinship merely based on face image a critical observation that face of parent captured while they were young are more alike their child s compared with image captured when they are old ha been revealed by genetics study this enlightens u the following research first a new kinship database named ub kinface composed of child young parent and old parent face image is collected from internet second an extended transfer subspace learning method is proposed aiming at mitigating the enormous divergence of distribution between child and old parent the key idea is to utilize an intermediate distribution close to both the source and target distribution to bridge them and reduce the divergence naturally the young parent set is suitable for this task through this learning process the large gap between distribution can be significantly reduced and kinship verification problem becomesmore discriminative experimental result show that our hypothesis on the role of young parent is valid and transfer learning is effective to enhance the verification accuracy 
thresholding a measure in conditional independence ci test using a fixed value enables learning and removing edge a part of learning a bayesian network structure however the learned structure is sensitive to the threshold that is commonly selected arbitrarily irrespective of characteristic of the domain and fixed for all ci test we analyze the impact on mutual information a ci measure of factor such a sample size degree of variable dependence and variable cardinality following we suggest to adaptively threshold individual test based on the factor we show that adaptive threshold better distinguish between pair of dependent variable and pair of independent variable and enable learning structure more accurately and quickly than when using fixed threshold 
labeling training data is quite time consuming but essential for supervised learning model to solve this problem the active learning ha been studied and applied to select the informative and representative data point for labeling however during the early stage of experiment only a small number or none of labeled data point exist thus the most representative sample should be selected first in this paper we propose a novel robust active learning method to handle the early stage experimental design problem and select the most representative data point selecting the representative sample is an np hard problem thus we employ the structured sparsity inducing norm to relax the objective to an efficient convex formulation meanwhile the robust sparse representation loss function is utilized to reduce the effect of outlier a new efficient optimization algorithm is introduced to solve our non smooth objective with low computational cost and proved global convergence empirical result on both single label and multi label classification benchmark data set show the promising result of our method 
robot must perform task efficiently and reliably while acting under uncertainty one way to achieve efficiency is to give the robot common sense knowledge about the structure of the world reliable robot behaviour can be achieved by modelling the uncertainty in the world probabilistically we present a robot system that combine these two approach and demonstrate the improvement in efficiency and reliability that result our first contribution is a probabilistic relational model integrating common sense knowledge about the world in general with observation of a particular environment our second contribution is a continual planning system which is able to plan in the large problem posed by that model by automatically switching between decision theoretic and classical procedure we evaluate our system on object search task in two different real world indoor environment by reasoning about the trade offs between possible course of action with different informational effect and exploiting the cue and general structure of those environment our robot is able to consistently demonstrate efficient and reliable goal directed behaviour 
in this paper a general maximum k min approach for classification is proposed which focus on maximizing the gain obtained by the k worst classified instance while ignoring the remaining one to make the original optimization problem with combinational constraint computationally tractable the optimization technique are adopted and a general compact representation lemma is summarized based on the lemma a nonlinear maximum k min nmkm classifier is presented and the experiment result demonstrate the superior performance of the maximum k min approach copyright association for the advancement of artificial intelligence www aaai org all right reserved 
we present a novel approach for identifying exact and approximate behavioral equivalence between model of agent this is significant because both decision making and game play in multiagent setting must contend with behavioral model of other agent in order to predict their action one approach that reduces the complexity of the model space is to group model that are behaviorally equivalent identifying equivalence between model requires solving them and comparing entire policy tree because the tree grow exponentially with the horizon our approach is to focus on partial policy tree for comparison and determining the distance between updated belief at the leaf of the tree we propose a principled way to determine how much of the policy tree to consider which trade off solution quality for efficiency we investigate this approach in the context of the interactive dynamic influence diagram and evaluate it performance copyright association for the advancement of artificial intelligence all right reserved 
semantic role labeling srl for tweet is a meaningful task that can benefit a wide range of application such a fine grained information extraction and retrieval from tweet one main challenge of the task is the lack of annotated tweet which is required to train a statistical model we introduce self training to srl leveraging abundant unlabeled tweet to alleviate it depending on annotated tweet a novel strategy of tweet selection is presented ensuring the chosen tweet are both correct and informative more specifically the correctness is estimated according to the labeling confidence and agreement of two conditional random field based labelers which are trained on the randomly evenly spitted labeled data while the informativeness is in proportion to the maximum distance between the tweet and the already selected tweet we evaluate our method on a human annotated data set and show that bootstrapping improve a baseline by fl copyright association for the advancement of artificial intelligence all right reserved 
predictive model play a key role for inference and decision making in crowdsourcing we present method that can be used to guide the collection of data for enhancing the competency of such predictive model while using the model to provide a base crowdsourcing service we focus on the challenge of ideally balancing the goal of collecting data over time for learning and for improving task performance with the cost of worker contribution over the lifetime of the operation of a system we introduce the use of distribution over a set of predictive model to represent uncertainty about the dynamic of the world we employ a novel monte carlo algorithm to reason simultaneously about uncertainty about the world dynamic and the progression of task solution a worker are hired over time to optimize hiring decision we evaluate the methodology with experiment on a challenging citizen science problem demonstrating how it balance exploration and exploitation over the lifetime of a crowdsourcing system 
we propose a method to predict user interest in social medium using time evolving multinomial relational data we exploit various action performed by user and their preference to predict user interest action performed by user in social medium such a twitter delicious and facebook have two fundamental property a user action can be represented a high dimensional or multinomial relation e g referring url bookmarking and tagging clicking a favorite button on a post etc b user action are time varying and user specific each user ha unique preference that change over time consequently it is appropriate to represent each user s action at some point in time a a multinomial relational data we propose actiongraph a novel graph representation for modeling user multinomial time varying action each user s action at some time point is represented by an action node actiongraph is a bipartite graph whose edge connect an action node to it involving entity referred to a object node using real world social medium data we empirically justify the proposed graph structure our experimental result show that the proposed actiongraph improves the accuracy in a user interest prediction task by outperforming several baseline including standard tensor analysis a previously proposed state of the art lda based method and other graph based variant moreover the proposed method show robust performance in the presence of sparse data 
in this work we describe the theoretical foundation and the implementation of a new automaton based technique for reasoning over expressive description logic that is worst case optimal and lends itself to an efficient implementation in order to show the feasibility of the approach we have realized a working prototype of a reasoner based upon these technique an experimental evaluation of this prototype show encouraging result 
understanding the rapidly growing short text is very important short text is different from traditional document in it shortness and sparsity which hinders the application of conventional machine learning and text mining algorithm two major approach have been exploited to enrich the representation of short text one is to fetch contextual information of a short text to directly add more text the other is to derive latent topic from existing large corpus which are used a feature to enrich the representation of short text the latter approach is elegant and efficient in most case the major trend along this direction is to derive latent topic of certain granularity through well known topic model such a latent dirichlet allocation lda however topic of certain granularity are usually not sufficient to set up effective feature space in this paper we move forward along this direction by proposing an method to leverage topic at multiple granularity which can model the short text more precisely taking short text classification a an example we compared our proposed method with the state of the art baseline over one open data set our method reduced the classification error by and respectively on two classifier 
facing a large number of clustering solution cluster ensemble method provides an effective approach to aggregating them into a better one in this paper we propose a novel cluster ensemble method from probabilistic perspective it assumes that each clustering solution is generated from a latent cluster model under the control of two probabilistic parameter thus the cluster ensemble problem is reformulated into an optimization problem of maximum likelihood an em style algorithm is designed to solve this problem it can determine the number of cluster automatically experimenal result have shown that the proposed algorithm outperforms the state of the art method including eac al cspa hgpa and mcla furthermore it ha been shown that our algorithm is stable in the predicted number of cluster 
we present online nested expectation maximization for model free reinforcement learning in a pomdp the algorithm evaluates the policy only in the current learning episode discarding the episode after the evaluation and memorizing the sufficient statistic from which the policy is computed in closed form a a result the online algorithm ha a time complexity o n and a memory complexity o compared to o n and o n for the corresponding batch mode algorithm where n is the number of learning episode the online algorithm which ha a provable convergence is demonstrated on five benchmark pomdp problem 
a baby experiencing the world for the first time face a considerable challenging sorting through what william james called the blooming buzzing confusion of the sens with the increasing capacity of modern sensor and the complexity of modern robot body a robot in an unknown or unfamiliar body face a similar and equally daunting challenge addressing this challenge directly by designing robot agent capable of resolving the confusion of sensory experience in an autonomous manner would substantially reduce the engineering required to program robot and the improve the robustness of resulting robot capability working towards a general solution to this problem this work us distinctive state abstraction and sensorimotor embedding to generate basic knowledge of sensor structure local geometry and object geometry starting with uninterpreted sensor and effector 
this paper considers a constraint based scheduling approach to the flexible jobshop a generalization of the traditional jobshop scheduling where activity have a choice of machine it study both large neighborhood lns and adaptive randomized decomposition ard scheme using random temporal and machine decomposition empirical result on standard benchmark show that within minute both lns and ard produce many new best solution and are about in average from the best known solution moreover over longer runtimes they improve of the best known solution and match the remaining one the empirical result also show the importance of hybrid decomposition in lns and ard 
we present yago an extension of the yago knowledge base in which entity fact and event are anchored in both time and space yago is built automatically from wikipedia geonames and wordnet it contains million fact about million entity human evaluation confirmed an accuracy of of the fact in yago in this paper we present the extraction methodology and the integration of the spatio temporal dimension 
sequence classification is central to many practical problem within machine learning distance metric between arbitrary pair of sequence can be hard to define because sequence can vary in length and the information contained in the order of sequence element is lost when standard metric such a euclidean distance are applied we present a scheme that employ a hidden markov model variant to produce a set of fixed length description vector from a set of sequence we then define three inference algorithm a baum welch variant a gibbs sampling algorithm and a variational algorithm to infer model parameter finally we show experimentally that the fixed length representation produced by these inference method is useful for classifying sequence of amino acid into structural class 
we consider a framework for preference aggregation on multiple binary issue where agent preference are represented by possibly cyclic cp net we focus on the majority aggregation of the individual cp net which is the cp net where the direction of each edge of the hypercube is decided according to the majority rule first we focus on hypercube condorcet winner hcws in particular we show that assuming a uniform distribution for the cp net the probability that there exists at least one hcwis at least e and the expected number of hcws is our experimental result confirm these result we also show experimental result under the impartial culture assumption we then generalize a few tournament solution to select winner from weighted majoritarian cp net namely copeland maximin and kemeny for each of these we address some social choice theoretic and computational issue 
while voting scheme provide an effective mean for aggregating preference method for the effective elicitation of voter preference have received little attention we address this problem by first considering approximate winner determination when incomplete voter preference are provided exploiting natural scoring metric we use max regret to measure the quality or robustness of proposed winner and develop polynomial time algorithm for computing the alternative with minimax regret for several popular voting rule we then show how minimax regret can be used to effectively drive incremental preference vote elicitation and devise several heuristic for this process despite worst case theoretical result showing that most voting protocol require nearly complete voter preference to determine winner we demonstrate the practical effectiveness of regret based elicitation for determining both approximate and exact winner on several real world data set 
many collective decision making problem have a combinatorial structure the agent involved must decide on multiple issue and their preference over one issue may depend on the choice adopted for some of the others voting is an attractive method for making collective decision but conducting a multi issue election is challenging on the one hand requiring agent to vote by expressing their preference over all combination of issue is computationally infeasible on the other decomposing the problem into several election on smaller set of issue can lead to paradoxical outcome any pragmatic method for running a multi issue election will have to balance these two concern we identify and analyse the problem of generating an agenda for a given election specifying which issue to vote on together in local election and in which order to schedule those local election 
spectral clustering is one of the most popular clustering approach despite it good performance it is limited in it applicability to large scale problem due to it high computational complexity recently many approach have been proposed to accelerate the spectral clustering unfortunately these method usually sacrifice quite a lot information of the original data thus result in a degradation of performance in this paper we propose a novel approach called landmark based spectral clustering lsc for large scale clustering problem specifically we select p n representative data point a the landmark and represent the original data point a the linear combination of these landmark the spectral embedding of the data can then be efficiently computed with the landmark based representation the proposed algorithm scale linearly with the problem size extensive experiment show the effectiveness and efficiency of our approach comparing to the state of the art method copyright association for the advancement of artificial intelligence all right reserved 
topic model have been used successfully for a variety of problem often in the form of application specific extension of the basic latent dirichlet allocation lda model because deriving these new model in order to encode domain knowledge can be difficult and time consuming we propose the fold all model which allows the user to specify general domain knowledge in first order logic fol however combining topic modeling with fol can result in inference problem beyond the capability of existing technique we have therefore developed a scalable inference technique using stochastic gradient descent which may also be useful to the markov logic network mln research community experiment demonstrate the expressive power of fold all a well a the scalability of our proposed inference method 
difference in social response of individual can often be related to difference in functioning of neurological mechanism this paper present a cognitive agent model capable of showing different type of social response pattern based on such mechanism adopted from theory on mirror neuron system emotion regulation empathy and autism spectrum disorder the presented agent model provides a basis for human like social response pattern of virtual agent in the context of simulation based training e g for training of therapist gaming or for agent based generation of virtual story 
with the emergence of large scale evolving time varying network dynamic network analysis dna ha become a very hot research topic in recent year although a lot of dna method have been proposed by researcher from different community most of them can only model snapshot data recorded at a very rough temporal granularity recently some model have been proposed for dna which can be used to model large scale citation network at a fine temporal granularity however they suffer from a significant decrease of accuracy over time because the learned parameter or node feature are static fixed during the prediction process for evolving citation network in this paper we propose a novel model called online egocentric model oem to learn time varying parameter and node feature for evolving citation network experimental result on real world citation network show that our oem can not only prevent the prediction accuracy from decreasing over time but also uncover the evolution of topic in citation network 
the extended global cardinality constraint egcc is an important component of constraint solving system since it is very widely used to model diverse problem the literature contains many different version of this constraint which trade strength of inference against computational cost in this paper i focus on the highest strength of inference usually considered enforcing generalized arc consistency gac on the target variable this work is an extensive empirical survey of algorithm and optimization considering both gac on the target variable and tightening the bound of the cardinality variable i evaluate a number of key technique from the literature and report important implementation detail of those technique which have often not been described in published paper two new optimization are proposed for egcc one of the novel optimization dynamic partitioning generalized from alldifferent wa found to speed up search by time in the best case and time on average while exploring the same search tree the empirical work represents by far the most extensive set of experiment on variant of algorithm for egcc overall the best combination of optimization give a mean speedup of time compared to the same implementation without the optimization 
we examine the utility of a curriculum a mean of presenting training sample in a meaningful order in unsupervised learning of probabilistic grammar we introduce the incremental construction hypothesis that explains the benefit of a curriculum in learning grammar and offer some useful insight into the design of curriculum a well a learning algorithm we present result of experiment with a carefully crafted synthetic data that provide support for our hypothesis and b natural language corpus that demonstrate the utility of curriculum in unsupervised learning of probabilistic grammar 
multitask bregman clustering mbc alternatively update cluster and learns relationship between cluster of different task and the two phase boost each other however the boosting doe not always have positive effect it may also cause negative effect another issue of mbc is that it cannot deal with nonlinear separable data in this paper we show that mbc s process of using cluster relationship to boost the updating cluster phase may cause negative effect i e cluster centroid may be skewed under some condition we propose a smart multi task bregman clustering s mbc algorithm which identifies negative effect of the boosting and avoids the negative effect if it occurs we then extend the framework of s mbc to a smart multi task kernel clustering s mkc framework to deal with nonlinear separable data we also propose a specific implementation of the framework which could be applied to any mercer kernel experimental result confirm our analysis and demonstrate the superiority of our proposed method copyright association for the advancement of artificial intelligence www aaai org all right reserved 
in traditional topic model such a lda a word is generated by choosing a topic from a collection however existing topic model do not identify different type of topic in a document such a topic that represent the content and topic that represent the sentiment in this paper our goal is to discover such different type of topic if they exist we represent our model a several parallel topic model called topic factor where each word is generated from topic from these factor jointly since the latent membership of the word is now a vector the learning algorithm become challenging we show that using a variational approximation still allows u to keep the algorithm tractable our experiment over several datasets show that our approach consistently outperforms many classic topic model while also discovering fewer more meaningful topic 
rank aggregation which combine multiple individual rank list to obtain a better one is a fundamental technique in various application such a meta search and recommendation system most existing rank aggregation method blindly combine multiple rank list with possibly considerable noise which often degrades their performance in this paper we propose a new model for robust rank aggregation rra via matrix learning which recovers a latent rank list from the possibly incomplete and noisy input rank list in our model we construct a pairwise comparison matrix to encode the order information in each input rank list based on our observation each comparison matrix can be naturally decomposed into a shared low rank matrix combined with a deviation error matrix which is the sum of a column sparse matrix and a row sparse one the latent rank list can be easily extracted from the learned lowrank matrix the optimization formulation of rra ha an element wise multiplication operator to handle missing value a symmetric constraint on the noise structure and a factorization trick to restrict the maximum rank of the low rank matrix to solve this challenging optimization problem we propose a novel procedure based on the augmented lagrangian multiplier scheme we conduct extensive experiment on metasearch and collaborative filtering benchmark datasets the result show that the proposed rra ha superior performance gain over several state of the art algorithm for rank aggregation copyright association for the advancement of artificial intelligence www aaai org all right reserved 
recent year have seen a lot of work towards extending the established agm belief revision theory with respect to iterating revision preserving conditional belief and handling set of proposition a new information in particular novel postulate like independence and evidence retainment have been brought forth a new standard for revising epistemic state by set of propositional information in this paper we propose a constructive approach for revising epistemic state by set of propositional and conditional belief that combine idea from nonmonotonic reasoning with conditional belief revision we also propose a novel principle called enforcement that cover both independence and evidence retainment and we show our revision operator to comply with major postulate from the literature moreover we point out the relevance of our approach for default reasoning 
the task of explanatory diagnosis conjecture action to explain observation this is a common task in real life and an essential ability of intelligent agent it becomes more complicated in multi agent scenario since agent action may be partially observable to other agent and observation might involve agent knowledge about the world or other agent knowledge or even common knowledge of a group of agent for example we might want to explain the observation that p doe not hold but ann belief p or the observation that ann bob and carl commonly believe p in this paper we formalize the multi agent explanatory diagnosis task in the framework of dynamic epistemic logic where kripke model of action are used to represent agent partial observability of action since this task is undecidable in general we identify important decidable fragment via technique of reducing the potentially infinite search space to finite one of epistemic state or action sequence 
complex network describe a wide range of system in nature and society to understand the complex network it is crucial to investigate their internal structure in this paper we propose an online community detection method for large complex network which make it possible to process network edge by edge in a serial fashion we investigate the generative mechanism of complex network and propose a split mechanism based on the degree of the node to create new community our method ha linear time complexity the method ha been applied to six real world network datasets and the experimental result show that it is comparable to existing method in modularity with much le running time 
this paper present an algorithm of adaptation for a case based reasoning system with case and domain knowledge represented in the expressive description logic alc the principle is to first pretend that the source case to be adapted solves the current target case this may raise some contradiction with the specification of the target case and with the domain knowledge the adaptation consists then in repairing these contradiction this adaptation algorithm is based on an extension of the classical tableau method used for deductive inference in alc 
one of the main difficulty in facial age estimation is that the learning algorithm cannot expect sufficient and complete training data fortunately the face at close age look quite similar since aging is a slow and smooth process inspired by this observation instead of considering each face image a an instance with one label age this paper regard each face image a an instance associated with a label distribution the label distribution cover a certain number of class label representing the degree that each label describes the instance through this way one face image can contribute to not only the learning of it chronological age but also the learning of it adjacent age two algorithm named ii lld and cpnn are proposed to learn from such label distribution experimental result on two aging face database show remarkable advantage of the proposed label distribution learning algorithm over the compared single label learning algorithm either specially designed for age estimation or for general purpose 
we propose a new representation for coalitional game called the coalitional skill vector model where there is a set of skill in the system and each agent ha a skill vector a vector consisting of value that reflect the agent level in different skill furthermore there is a set of goal each with requirement expressed in term of the minimum skill level necessary to achieve the goal agent can form coalition to aggregate their skill and achieve goal otherwise unachievable we show that this representation is fully expressive that is it can represent any characteristic function game we also show that for some interesting class of game our representation is significantly more compact than the classical representation and facilitates the development of efficient algorithm to solve the coalition structure generation problem a well a the problem of computing the core and or the least core we also demonstrate that by using the coalitional skill vector representation our solver can handle up to agent 
interactive narrative is a form of digital entertainment heavily based on ai technique to support narrative generation and user interaction significant progress arriving with the adoption of planning technique however there is a lack of unified model that integrate generation user response and interaction this paper address this by revisiting existing interactive narrative paradigm granting explicit status to user disposition towards story character a part of narrative generation a well a adding support for new form of interaction we demonstrate this with a novel brain computer interface bci design incorporating empathy for a main character derived from brain signal within filmic conception of narrative which drive generation using planning technique result from an experimental study with a fully implemented system demonstrate the effectiveness of a eeg neurofeedback based approach showing that subject can successfully modulate empathic support of a character in a medical drama mri analysis also show activation in associated region of the brain during expression of support 
learning distance metric is a fundamental problem in machine learning previous distance metric learning research assumes that the training and test data are drawn from the same distribution which may be violated in practical application when the distribution differ a situation referred to a covariate shift the metric learned from training data may not work well on the test data in this case the metric is said to be inconsistent in this paper we address this problem by proposing a novel metric learning framework known a consistent distance metric learning cdml which solves the problem under covariate shift situation we theoretically analyze the condition when the metric learned under covariate shift are consistent based on the analysis a convex optimization problem is proposed to deal with the cdml problem an importance sampling method is proposed for metric learning and two importance weighting strategy are proposed and compared in this work experiment are carried out on synthetic and real world datasets to show the effectiveness of the proposed method 
the most successful recent approach to partially observable markov decision problem pomdp solving have largely been point based approximation algorithm these work by selecting a finite number of belief point computing alpha vector for those point and using the resulting policy everywhere however if during execution the belief state is far from the point there is no guarantee that the policy will be good this case occurs either when the point are chosen poorly or there are too few point to capture the whole optimal policy for example in domain where there are many low probability transition such a fault or exogenous event in this paper we explore the use of an on line plan repair approach to overcome this difficulty the idea is to split computation between off line plan creation and if necessary on line plan repair we evaluate a variety of heuristic used to determine when plan repair might be useful and then repair the plan by sampling a small number of additional belief point and recomputing the policy we show in several domain that the approach is more effective than either off line planning alone even with much more computation time or a purely on line planning based on forward search we also show that the overhead of checking the heuristic is very small when replanning is unnecessary 
human action recognition from video is a challenging machine vision task with multiple important application domain such a human robot machine interaction interactive entertainment multimedia information retrieval and surveillance in this paper we present a novel approach to human action recognition from d skeleton sequence extracted from depth data we use the covariance matrix for skeleton joint location over time a a discriminative descriptor for a sequence to encode the relationship between joint movement and time we deploy multiple covariance matrix over sub sequence in a hierarchical fashion the descriptor ha a fixed length that is independent from the length of the described sequence our experiment show that using the covariance descriptor with an off the shelf classification algorithm outperforms the state of the art in action recognition on multiple datasets captured either via a kinect type sensor or a sophisticated motion capture system we also include an evaluation on a novel large dataset using our own annotation 
many formalism discussed in the literature on qualitative spatial reasoning are designed for expressing static spatial constraint only however dynamic situation arise in virtually all application of these formalism which make it necessary to study variant and extension dealing with change this paper present a study on the computational complexity of qualitative change more precisely we discus the reasoning task of finding a solution to a temporal sequence of static reasoning problem where this sequence is subject to additional transition constraint our focus is primarily on smoothness and continuity constraint we show how such transition can be defined a relation and expressed within qualitative constraint formalism our result demonstrate that for point based constraint formalism the interesting fragment are np complete in the presence of continuity constraint even if the satisfiability problem of it static description is tractable 
topic model have been widely used to identify topic in text corpus it is also known that purely unsupervised model often result in topic that are not comprehensible in application in recent year a number of knowledge based model have been proposed which allow the user to input prior knowledge of the domain to produce more coherent and meaningful topic in this paper we go one step further to study how the prior knowledge from other domain can be exploited to help topic modeling in the new domain this problem setting is important from both the application and the learning perspective because knowledge is inherently accumulative we human being gain knowledge gradually and use the old knowledge to help solve new problem to achieve this objective existing model have some major difficulty in this paper we propose a novel knowledge based model called mdk lda which is capable of using prior knowledge from multiple domain our evaluation result will demonstrate it effectiveness 
loganswer is a question answering qa system for the german language aimed at providing concise and correct answer to arbitrary question for this purpose loganswer is designed a an embedded artificial intelligence system which integrates method from several field of ai namely natural language processing machine learning knowledge representation and automated theorem proving we intend to employ loganswer a a virtual user within internet based qa forum where it must be able to identify the question that it cannot answer correctly a task that normally receives little attention in qa research compared to the actual answer derivation the paper present a machine learning solution to the wrong answer avoidance waa problem applying a meta classifier to the output of simple term based classifier and a rich set of other waa feature experiment with a large set of real world question from a qa forum show that the proposed method significantly improves the waa characteristic of our system 
we present unsupervised approach to the problem of modeling dialog act in asynchronous conversation i e conversation where participant collaborate with each other at different time in particular we investigate a graph theoretic deterministic framework and two probabilistic conversation model i e hmm and hmm mix for modeling dialog act in email and forum we train and test our conversation model on a temporal order and b graph structural order of the datasets empirical evaluation suggests i the graph theoretic framework that relies on lexical and structural similarity metric is not the right model for this task ii conversation model perform better on the graph structural order than the temporal order of the datasets and iii hmm mix is a better conversation model than the simple hmm model 
we study security game with multiple defender to achieve maximum security defender must perfectly synchronize their randomized allocation of resource however in real life scenario such a protection of the port of boston this is not the case our goal is to quantify the loss incurred by miscoordination between defender both theoretically and empirically we introduce two notion that capture this loss under different assumption the price of miscoordination and the price of sequential commitment generally speaking our theoretical bound indicate that the loss may be extremely high in the worst case while our simulation establish a smaller yet significant loss in practice 
cepstral feature have been widely used in audio application domain knowledge ha played an important role in designing different type of cepstral feature proposed in the literature in this paper we present a novel approach for learning optimized cepstral feature directly from audio data to better discriminate between different category of signal in classification task we employ multi layer feed forward neural network to model the cepstral feature extraction process the network weight are initialized to replicate a reference cepstral feature like the mel frequency cepstral coefficient we then propose a embedded approach that integrates feature learning with the training of a support vector machine svm classifier a single optimization problem is formulated where the feature and classifier variable are optimized simultaneously so a to refine the initial feature and minimize the classification risk experimental result have demonstrated the effectiveness of the proposed feature learning approach outperforming competing method by a large margin on benchmark data 
we describe an approach to computing upper bound on the length of solution to reachability problem in transition system it is based on a decomposition of state variable dependency graph causal graph our approach is able to find practical upper bound in a number of planning benchmark computing the bound is computationally cheap in practice and in a number of benchmark our algorithm run in polynomial time in the number of action and propositional variable that characterize the problem 
semi supervised support vector machine s vms are a kind of popular approach which try to improve learning performance by exploiting unlabeled data though s vms have been found helpful in many situation they may degenerate performance and the resultant generalization ability may be even worse than using the labeled data only in this paper we try to reduce the chance of performance degeneration of s vms our basic idea is that rather than exploiting all unlabeled data the unlabeled instance should be selected such that only the one which are very likely to be helpful are exploited while some highly risky unlabeled instance are avoided we propose the s vm u method by using hier archical clustering to select the unlabeled instance experiment on a broad range of data set over eighty eight different setting show that the chance of performance degeneration of s vm u is much smaller than that of existing s vms copyright association for the advancement of artificial intelligence all right reserved 
one drawback with using plan recognition in adversarial game is that often player must commit to a plan before it is possible to infer the opponent s intention in such case it is valuable to couple plan recognition with plan repair particularly in multi agent domain where complete replanning is not computationally feasible this paper present a method for learning plan repair policy in real time using upper confidence bound applied to tree uct we demonstrate how these policy can be coupled with plan recognition in an american football game rush to create an autonomous offensive team capable of responding to unexpected change in defensive strategy our real time version of uct learns play modification that result in a significantly higher average yardage and fewer interception than either the baseline game or domain specific heuristic although it is possible to use the actual game simulator to measure reward offline to execute uct in real time demand a different approach here we describe two module for reusing data from offline uct search to learn accurate state and reward estimator 
aerosol optical depth aod recognized a one of the most important quantity in understanding and predicting the earth s climate is estimated daily on a global scale by several earth observing satellite instrument each instrument ha different coverage and sensitivity to atmospheric and surface condition and a a result the quality of aod estimated by different instrument varies across the globe we present a method for learning how to aggregate aod estimation from multiple satellite instrument into a more accurate estimation the proposed method is semi supervised a it is able to learn from a small number of labeled data where label come from a few accurate and expensive ground based instrument and a large number of unlabeled data the method us a latent variable to partition the data so that in each partition the expert aod estimation are aggregated in a different optimal way we applied the method to combine aod estimation from instrument aboard satellite and the result indicate that it can successfully exploit labeled and unlabeled data to produce accurate aggregated aod estimation 
en route charging station allow electric vehicle to greatly extend their range however a a full charge take a considerable amount of time there may be significant waiting time at peak hour to address this problem we propose a novel navigation system which communicates it intention i e routing policy to other driver using these intention our system accurately predicts congestion at charging station and suggests the most efficient route to it user we achieve this by extending existing time dependent stochastic routing algorithm to include the battery s state of charge and charging station furthermore we describe a novel technique for combining historical information with agent intention to predict the queue at charging station through simulation we show that our system lead to a significant increase in utility compared to existing approach that do not explicitly model waiting time or use intention in some case reducing waiting time by over and achieving near optimal overall journey time 
a usual strategy to select the final answer in factoid question answering qa relies on redundancy a score is given to each candidate answer a a function of it frequency of occurrence and the final answer is selected from the set of candidate sorted in decreasing order of score for that purpose system often try to group together semantically equivalent answer however they hold several other semantic relation such a inclusion which are not considered and candidate are mostly seen independently a competitor our hypothesis is that not just equivalence but other relation between candidate answer have impact on the performance of a redundancy based qa system in this paper we describe experimental study to back up this hypothesis our finding show that with relatively simple technique to recognize relation system accuracy can be improved for answer of category number date and entity 
trust is an important mechanism enabling agent to self police open and dynamic multi agent system odmass truster evaluate the reputation of trustee based on their past observed performance and use this information to guide their future interaction decision existing trust model tend to concentrate truster interaction on a small number of highly reputable trustee to minimize risk exposure when a trustee s servicing capacity is limited such an approach may cause long delay for truster and subsequently damage the reputation of trustee to mitigate this problem we propose a reputation management approach for trustee agent based on distributed constraint optimization it help a trustee to make situation aware decision on which incoming request to serve and prevent the resulting reputation score from being affected by factor out of the trustee s control the approach is evaluated through theoretical analysis and within a simulated highly dynamic multi agent environment the result show that it can achieve close to optimally efficient utilization of the trustee agent collective capacity in an odmas promotes fair treatment of trustee agent based on their behavior and significantly outperforms related work in enhancing social welfare 
in today s service driven economic environment it is imperative for organization to provide better quality service experience to differentiate and grow their business customer satisfaction c sat is the key driver for retention and growth in retail banking wait time the time spent by a customer at the branch before getting serviced contributes significantly to c sat due to high footfall it is improbable to improve the wait time of every customer walking in the branch therefore bank in developing country are strategically looking to segment it customer and service and offer differentiated qos based service delivery in this work we present a system for customer segmentation and scheduling based on historic value of the customer and characteristic of current service request we describe the system and give mathematical formulation of the scheduling problem and the associated heuristic we present result and experience of deployment of this solution in multiple branch of a leading bank in india 
this paper present an algorithm for finding approximately optimal policy in very large markov decision process by constructing a hierarchical model and then solving it approximately it exploit factored representation to achieve compactness and efficiency and to discover connectivity property of the domain we provide a bound on the quality of the solution and give asymptotic analysis of the runtimes in addition we demonstrate performance on a collection of very large domain result show that the quality of resulting policy is very good and the total running time for both creating and solving the hierarchy are significantly le than for an optimal factored mdp solver 
hierarchical classification hc play an significant role in machine learning and data mining however most of the state of the art hc algorithm suffer from high computational cost to improve the performance of solving we propose a stochastic perceptron sp algorithm in the large margin framework in particular a stochastic choice procedure is devised to decide the direction of next iteration we prove that after finite iteration the sp algorithm yield a sub optimal solution with high probability if the input instance are separable for large scale and high dimensional data set we reform sp to the kernel version ksp which dramatically reduces the memory space needed the ksp algorithm ha the merit of low space complexity a well a low time complexity the experimental result show that our ksp approach achieves almost the same accuracy a the contemporary algorithm on the real world data set but with much le cpu running time copyright association for the advancement of artificial intelligence www aaai org all right reserved 
artificial intelligence ha a long history of learning from domain problem ranging from chess to jeopardy in this work we look at a problem stemming from social science namely how do social relationship influence communication content and vice versa the tool used to study communication content content analysis have rarely been combined with those used to study social relationship social network analysis furthermore there is even le work addressing the longitudinal characteristic of such a combination this paper present a general framework for measuring the dynamic bi directional influence between communication content and social network the framework leverage the idea that knowledge about both kind of network can be represented using the same knowledge representation in particular through the use of semantic web standard the extraction of network is made easier the framework is applied to two use case online forum discussion and conference publication the result provide a new perspective over the dynamic involving both social network and communication content 
formulating knowledge for use in ai planning engine is currently something of an ad hoc process where the skill of knowledge engineer and the tool they use may significantly influence the quality of the resulting planning application there is little in the way of guideline or standard procedure however for knowledge engineer to use when formulating knowledge into planning domain language such a pddl this paper seek to investigate this process using a a case study a road traffic accident management domain managing road accident requires systematic sound planning and coordination of resource to improve outcome for accident victim we have derived a set of requirement in consultation with stakeholder for the resource coordination part of managing accident we evaluate two separate knowledge engineering strategy for encoding the resulting planning domain from the set of requirement a the traditional method of pddl expert and text editor and b a leading planning gui with built in uml modelling tool these strategy are evaluated using process and product metric where the domain model the product wa tested extensively with a range of planning engine the result give insight into the strength and weakness of the approach highlight lesson learned regarding knowledge encoding and point to important line of research for knowledge engineering for planning 
product defect and rework effort due to flawed specification represent major issue for a project s performance so that there is a high motivation for providing effective mean that assist designer in assessing and ensuring a specification s quality recent research in the context of formal specification e g on coverage and vacuity offer important mean to tackle related issue in the currently underrepresented research direction of diagnostic reasoning on a specification we propose a scenario based diagnosis at a specification s operator level using weak or strong fault model drawing on efficient sat encoding we show in this paper how to achieve that effectively for specification in ltl our experimental result illustrate our approach s validity and attractiveness 
this work is motivated by the following concern suppose we have a game exhibiting multiple nash equilibrium with little to distinguish them except that one of them can be verified while the others cannot that is one of these equilibrium carry sufficient information that if this is the outcome then the player can tell that an equilibrium ha been played this provides an argument for this equilibrium being played instead of the alternative verifiability can thus serve to make an equilibrium a focal point in the game we formalise and investigate this concept using a model of boolean game with incomplete information we define and investigate three increasingly strong type of verifiable equilibrium characterise the complexity of checking these and show how checking their existence can be captured in a variant of modal epistemic logic 
combining graph regularization with nonnegative matrix tri factorization nmf ha shown great performance improvement compared with traditional nonnegative matrix tri factorization model due to it ability to utilize the geometric structure of the document and word in this paper we show that these model are not well defined and suffering from trivial solution and scale transfer problem in order to solve these common problem we propose two model for graph regularized non negative matrix tri factorization which can be applied for document clustering and co clustering respectively in the proposed model a normalized cut like constraint is imposed on the cluster assignment matrix to make the optimization problem well defined we derive a multiplicative updating algorithm for the proposed model and prove it convergence experiment of clustering and coclustering on benchmark text data set demonstrate that the proposed model outperform the original model a well a many other state of the art clustering method 
motivated by consideration in quantum mechanic we introduce the class of robust constraint satisfaction problem in which the question is whether every partial assignment of a certain length can be extended to a solution provided the partial assignment doe not violate any of the constraint of the given instance we explore the complexity of specific robust colorability and robust satisfiability problem and show that they are np complete we then use these result to establish the computational intractability of detecting local hidden variable model in quantum mechanic 
this paper present a new dcop algorithm called deqed decomposition with quadratic encoding to decentralize deqed is based on the divide and coordinate dac framework where the agent repeat solving their updated local sub problem the divide stage and exchanging coordination information that cause to update their local sub problem the coordinate stage unlike other dac based dcop algorithm deqed doe not essentially increase the complexity of local sub problem and allows agent to avoid exchanging variable value in the coordinate stage our experimental result show that deqed significantly outperformed other incomplete dcop algorithm for both random and structured instance 
recent research on external memory search ha shown that disk can be effectively used a secondary storage when performing large breadth first search we introduce the write minimizing breadth first search wmbfs algorithm which is designed to minimize the number of writes performed in an external memory bfs wmbfs is also designed to store the result of the bfs for later use we present the result of a bfs on a single agent version of chinese checker and the rubik s cube edge cube state space with about trillion state each in evaluating against a comparable approach wmbfs reduces the i o for the chinese checker domain by over an order of magnitude in rubik s cube in addition to reducing i o the search is also time faster analysis of the result suggests the machine and state space property necessary for wmbfs to perform well 
when solving extensive form game with large action space typically significant abstraction is needed to make the problem manageable from a modeling or computational perspective when this occurs a procedure is needed to interpret action of the opponent that fall outside of our abstraction by mapping them to action in our abstraction this is called an action translation mapping prior action translation mapping have been based on heuristic without theoretical justification we show that the prior mapping are highly exploitable and that most of them violate certain natural desideratum we present a new mapping that satisfies these desideratum and ha significantly lower exploitability than the prior mapping furthermore we observe that the cost of this worst case performance benefit low exploitability is not high in practice our mapping performs competitively with the prior mapping against no limit texas hold em agent submitted to the annual computer poker competition we also observe several paradox that can arise when performing action abstraction and translation for example we show that it is possible to improve performance by including suboptimal action in our abstraction and excluding optimal action 
motivated by application from computer network security and software engineering we study the problem of reducing reachability on a graph with unknown edge cost when the cost are known reachability reduction can be solved using a linear relaxation of sparsest cut problem arise however when edge cost are unknown in this case blindly applying sparsest cut with incorrect edge cost can result in suboptimal or infeasible solution instead we propose to solve the problem via edge classification using feedback on individual edge we show that this approach outperforms competing approach in accuracy and efficiency on our target application 
the nystr m method is a well known sampling based low rank matrix approximation approach it is usually considered to be originated from the numerical treatment of integral equation and eigendecomposition of matrix in this paper we present a novel point of view for the nystr m approximation we show that theoretically the nystr m method can be regraded a a set of point wise ordinary least square linear regression of the kernel matrix sharing the same design matrix with the new interpretation we are able to analyze the approximation quality based on the fulfillment of the homoscedasticity assumption and explain the success and deficiency of various sampling method we also empirically show that positively skewed explanatory variable distribution can lead to heteroscedasticity based on this discovery we propose to use non symmetric explanatory function to improve the quality of the nystr m approximation with almost no extra computational cost experiment show that positively skewed datasets widely exist and our method exhibit good improvement on these datasets 
in order for agent to be able to act intelligently in an environment a first necessary step is to become aware of the current situation in the environment forming such awareness is not a trivial matter appropriate observation should be selected by the agent and the observation result should be interpreted and combined into one coherent picture human use dedicated mental model which represent the relationship between various observation and the formation of belief about the environment which then again direct the further observation to be performed in this paper a generic agent model for situation awareness is proposed that is able to take a mental model a input and utilize this model to create a picture of the current situation in order to show the suitability of the approach it ha been applied within the domain of f fighter pilot training for which a dedicated mental model ha been specified and simulation experiment have been conducted 
the accuracy of subchloroplast location prediction algorithm often depends on predictive and succinct feature derived from protein thus to improve the prediction accuracy this paper proposes a novel subchloroplast location prediction method called schots which integrates the homolog knowledge transfer and feature selection method schots contains two stage first discriminating feature are generated by w lchi a weighted gene ontology go transfer model based on bitscore of protein and logarithmic transformation of chisquare second the more informative go term are selected from the feature extensive study conducted on three real datasets demonstrate that schots outperforms three offtheshelf subchloroplast prediction method copyright association for the advancement of artificial intelligence www aaai org all right reserved 
the next generation of power system face significant challenge both in coping with increased loading of an aging infrastructure and incorporating renewable energy source meeting these challenge requires a fundamental change in the operation of power system by replacing human in the loop operation with autonomous system this is especially acute in distribution system where renewable integration often occurs this paper investigates the automation of power supply restoration psr that is the process of optimally reconfiguring a faulty distribution grid to resupply customer the key contribution of the paper are a flexible mixed integer programming framework for solving psr a model decomposition to obtain high quality solution within the required time constraint and an experimental validation of the potential benefit of the proposed psr operation 
a tweet have become a comprehensive repository of fresh information semantic role labeling srl for tweet ha aroused great research interest because of it central role in a wide range of tweet related study such a fine grained information extraction sentiment analysis and summarization however the fact that a tweet is often too short and informal to provide sufficient information pose a major challenge to tackle this challenge we propose a new method to collectively label similar tweet the underlying idea is to exploit similar tweet to make up for the lack of information in a tweet specifically similar tweet are first grouped together by clustering then for each cluster a two stage labeling is conducted one labeler conduct srl to get statistical information such a the predicate argument role triple that occur frequently from it highly confidently labeled result then in the second stage another labeler performs srl with such statistical information to refine the result experimental result on a human annotated dataset show that our approach remarkably improves srl by f 
car pollution is one of the major cause of greenhouse emission and traffic congestion is rapidly becoming a social plague dynamic ride sharing drs system have the potential to mitigate this problem by computing plan for car driver e g commuter allowing them to share their ride existing effort in drs are suffering from the problem that participant are abandoning the system after repeatedly failing to get a shared ride in this paper we present an incentive compatible drs solution based on auction while existing drs system are mainly focusing on fixed assignment that minimize the totally travelled distance the presented approach is adaptive to individual preference of the participant furthermore our system allows to tradeoff the minimization of vehicle kilometer travelled vkt with the overall probability of successful ride share which is an important feature when bootstrapping the system to the best of our knowledge we are the first to present a drs solution based on auction using a sealed bid second price scheme 
a tensor provide a natural and efficient representation of multidimensional structured data in this paper we consider probabilistic multinomial probit classification for tensor variate input with gaussian process gp prior placed over the latent function in order to take into account the underlying multimodes structure information within the model we propose a framework of probabilistic product kernel for tensorial data based on a generative model assumption more specifically it can be interpreted a mapping tensor to probability density function space and measuring similarity by an information divergence since tensor kernel enable u to model input tensor observation the proposed tensor variate gp is considered a both a generative and discriminative model furthermore a fully variational bayesian treatment for multiclass gp classification with multinomial probit likelihood is employed to estimate the hyperparameters and infer the predictive distribution simulation result on both synthetic data and a real world application of human action recognition in video demonstrate the effectiveness and advantage of the proposed approach for classification of multiway tensor data especially in the case that the underlying structure information among multimodes is discriminative for the classification task copyright association for the advancement of artificial intelligence www aaai org all right reserved 
what is structure of water surrounding protein remains a one of fundamental unsolved problem of science method in biophysics only provide qualitative description of the structure and thus clarifying the collective phenomenon of a huge number of water molecule is still beyond intuition in biophysics we introduce a simulation based data mining approach that quantitatively model the structure of water surrounding a protein a cluster of water molecule having similar moving behavior the paper present and explains how the advance of ai technique can potentially solve this challenging data intensive problem 
online rating system are now ubiquitous due to the success of recommender system in such system user are allowed to rate the item movie song commodity in a predefined range of value the rating collected can be used to infer user preference a well a item intrinsic feature which are then matched to perform personalized recommendation most previous work focus on improving the prediction accuracy or ranking capability little attention ha been paid to the problem of spammer or low reputed user in such system spammer contaminate the rating system by assigning unreasonable score to item which may affect the accuracy of a recommender system there are evidence supporting the existence of spammer in online rating system reputation estimation method can be employed to keep track of user reputation and detect spammer in such system in this paper we propose a unified framework for computing the reputation score of a user given only user rating on item we show that previously proposed reputation estimation method can be captured a special case of our framework we propose a new low rank matrix factorization based reputation estimation method and demonstrate it superior discrimination ability 
previous text processing technique focus on text itself while neglecting human reading process therefore they are limited in special application this paper proposes a text scanning mechanism for generating the dynamic impression of word in text by simulating recall association and forget process during reading experiment show that the mechanism is suitable for multiple text processing application 
the design of multiagent simulation devoted to complex system address the issue of modeling behavior that are involved at different space time behavior scale each one being relevant so a to represent a feature of the phenomenon we propose here a generic formalismintended to represent multiple environment endowed with their own spatiotemporal scale and with behavioral rule for the agent they contain an environment can be nested inside any agent which itself is situated in one or more environment this lead to a lattice decomposition of the global system which appears to be necessary for an accurate design of multi scale system this uniform representation of entity and behavior at each abstraction level relies upon an interaction oriented approach for the design of agent simulation which clearly separate agent from interaction from the modeling to the code we also explain the implementation of our formalism within an existing interaction based platform 
one of the main research problem in structural bioinformatics is the analysis and prediction of three dimensional structure d of polypeptide or protein the s genome project resulted in a large increase in the number of protein sequence however the number of identified d protein structure ha not followed the same trend the determination of protein structure is experimentally expensive and time consuming this make scientist largely dependent on computational method that can predict correct d protein structure only from extended and full amino acid sequence several computational methodology and algorithm have been proposed a a solution to the protein structure prediction psp problem we briefly describe the ai technique we have been used to tackle this problem 
directed acyclic graph can be used across many application domain in this paper we study a new pattern domain for supporting their analysis therefore we propose the pattern language of weighted path primitive constraint that enable to specify their relevancy e g frequency and compactness constraint and algorithm that can compute the specified collection it lead to a condensed representation setting whose efficiency and scalability are empirically studied 
the effectiveness of machine learning model can often be improved by feature selection a a preprocessing step often this is a data driven process only and can result in model that may not correspond to true relationship present in the data set due to overfitting in this work we propose leveraging known relationship between variable to constrain and guide feature selection using commonality across domain we provide a framework for the user to express model constraint while still making the feature selection process data driven and sensitive to actual relationship in the data 
the ability to recognize analogy is an important factor that is closely related to human intelligence verbal analogy have been used for evaluating both examinee at university entrance exam a well a algorithm for measuring relational similarity however relational similarity measure proposed so far are confined to measuring the similarity between pair of word unfortunately such pairwise approach ignore the rich relational structure that exists in real world knowledge base containing million of entity and semantic relation we propose a method to efficiently identify analogous entity tuples from a given entity relation graph first we present an efficient approach for extracting potential analogous tuples from a given entity relation graph second to measure the structural similarity between two tuples we propose two type of kernel function vertex feature kernel and edge feature kernel moreover we combine those kernel to construct composite kernel that simultaneously consider both vertex and edge feature experimental result show that our proposed method accurately identifies analogous tuples and significantly outperforms a state of the art pairwise relational similarity measure extended to tuples 
how do we scale information extraction to the massive size and unprecedented heterogeneity of the web corpus beginning in our knowitall project ha sought to extract high quality knowledge from the web in we introduced the open information extraction open ie paradigm which eschews hand labeled training example and avoids domain specific verb and noun to develop unlexicalized domain independent extractor that scale to the web corpus open ie system have extracted billion of assertion a the basis for both common sense knowledge and novel question answering system this paper describes the second generation of open ie system which rely on a novel model of how relation and their argument are expressed in english sentence to double precision recall compared with previous system such a textrunner and woe 
despite recent successful real world deployment of stackelberg security game ssgs scale up remains a fundamental challenge in this field the latest technique do not scale up to domain where multiple defender must coordinate time dependent joint activity to address this challenge this paper present two branch and price algorithm for solving ssgs smarto and smarth with three novel feature i a column generation approach that us an ordered network of node determined by solving the traveling salesman problem to generate individual defender strategy ii exploitation of iterative reward shaping of multiple coordinating defender unit to generate coordinated strategy iii generation of tighter upper bound for pruning by solving security game that only abide by key scheduling constraint we provide extensive experimental result and formal analysis 
we propose a machine learning approach to geophysical inversion problem for the exploration of earth resource our approach is based on nonparametric bayesian method specifically gaussian process and provides a full distribution over the predicted geophysical property whilst enabling the incorporation of data from different modality we ass our method both qualitatively and quantitatively using a real dataset from south australia containing gravity and drill hole data and through simulated experiment involving gravity drill hole and magnetics with the goal of characterizing rock density the significance of our probabilistic inversion extends to general exploration problem with potential to dramatically benefit the industry 
this paper present a novel symmetric graph regularization framework for pairwise constraint propagation we first decompose the challenging problem of pairwise constraint propagation into a series of two class label propagation subproblems and then deal with these subproblems by quadratic optimization with symmetric graph regularization more importantly we clearly show that pairwise constraint propagation is actually equivalent to solving a lyapunov matrix equation which is widely used in control theory a a standard continuous time equation different from most previous constraint propagation method that suffer from severe limitation our method can directly be applied to multi class problem and also can effectively exploit both must link and cannot link constraint the propagated constraint are further used to adjust the similarity between data point so that they can be incorporated into subsequent clustering the proposed method ha been tested in clustering task on six real life data set and then shown to achieve significant improvement with respect to the state of the art copyright association for the advancement of artificial intelligence all right reserved 
trust is crucial in dynamic multi agent system where agent may frequently join and leave and the structure of the society may often change in these environment it may be difficult for agent to form stable trust relationship necessary for confident interaction society may break down when trust between agent is too low to motivate interaction in such setting agent should make decision about who to interact with given their degree of trust in the available partner we propose a decision theoretic model of trust decision making allows control to be used a well a trust to increase confidence in initial interaction we consider explicit incentive monitoring and reputation a example of such control we evaluate our approach within a simulated highly dynamic multi agent environment and show how this model support the making of delegation decision when trust is low 
suppose a learner is faced with a domain of problem about which it know nearly nothing it doe not know the distribution of problem the space of solution is not smooth and the reward signal is uninformative providing perhaps a few bit of information but not enough to steer the learner effectively how can such a learner ever get off the ground a common intuition is that if the solution to these problem share a common structure and the learner can solve some simple problem by brute force it should be able to extract useful component from these solution and by composing them explore the solution space more efficiently here we formalize this intuition where the solution space is that of typed functional program and the gained information is stored a a stochastic grammar over program we propose an iterative procedure for exploring such space in the first step of each iteration the learner explores a finite subset of the domain guided by a stochastic grammar in the second step the learner compress the successful solution from the first step to estimate a new stochastic grammar we test this procedure on symbolic regression and boolean circuit learning and show that the learner discovers modular concept for these domain whereas the learner is able to solve almost none of the posed problem in the procedure s first iteration it rapidly becomes able to solve a large number by gaining abstract knowledge of the structure of the solution space 
in complex dynamic system accurate forecasting of extreme event such a hurricane is a highly underdetermined yet very important sustainability problem while physic based model deserve their own merit they often provide unreliable prediction for variable highly related to extreme event in this paper we propose a new supervised machine learning problem which we call a forecast oriented classification of spatio temporal extreme event we formulate three important real world extreme event classification task including seasonal forecasting of a tropical cyclone in northern hemisphere b hurricane and landfalling hurricane in north atlantic and c north african rainfall corresponding predictor and predictand data set are constructed these data present unique characteristic and challenge that could potentially motivate future artificial intelligent and data mining research 
activity recognition aim to discover one or more user action and goal based on sensor reading in the real world a single user s data are often insufficient for training an activity recognition model due to the data sparsity problem this is especially true when we are interested in obtaining a personalized model in this paper we study how to collaboratively use different user sensor data to train a model that can provide personalized activity recognition for each user we propose a user dependent aspect model for this collaborative activity recognition task our model introduces user aspect variable to capture the user grouping information so that a target user can also benefit from her similar user in the same group to train the recognition model in this way we can greatly reduce the need for much valuable and expensive labeled data required in training the recognition model for each user our model is also capable of incorporating time information and handling new user in activity recognition we evaluate our model on a real world wifi data set obtained from an indoor environment and show that the proposed model can outperform several state of art baseline algorithm 
a the major component of big data unstructured heterogeneous multimedia content such a text image audio video and d increasing rapidly on the internet user demand a new type of cross medium retrieval where user can search result across various medium by submitting query of any medium since the query and the retrieved result can be of different medium how to learn a heterogeneous metric is the key challenge most existing metric learning algorithm only focus on a single medium where all of the medium object share the same data representation in this paper we propose a joint graph regularized heterogeneous metric learning jgrhml algorithm which integrates the structure of different medium into a joint graph regularization in jgrhml different medium are complementary to each other and optimizing them simultaneously can make the solution smoother for both medium and further improve the accuracy of the final metric based on the heterogeneous metric we further learn a high level semantic metric through label propagation jgrhml is effective to explore the semantic relationship hidden across different modality the experimental result on two datasets with up to five medium type show the effectiveness of our proposed approach association for the advancement of artificial intelligence www aaai org all right reserved 
to accurately and actively provide user with their potentially interested information or service is the main task of a recommender system collaborative filtering is one of the most widely adopted recommender algorithm whereas it is suffering the issue of data sparsity and cold start that will severely degrade quality of recommendation to address such issue this article proposes a novel method trying to improve the performance of collaborative filtering recommendation by mean of elaborately integrating twofold sparse information the conventional rating data given by user and the social trust network among the same user it is a model based method adopting matrix factorization technique to map user into low dimensional latent feature space in term of their trust relationship aiming to reflect user reciprocal influence on their own opinion more reasonably the validation against a real world dataset show that the proposed method performs much better than state of the art recommendation algorithm for social collaborative filtering by trust 
lazy clause generation is a powerful approach to reducing search in constraint programming this is achieved by recording set of domain restriction that previously led to failure a new clausal propagator symmetry breaking approach are also powerful method for reducing search by recognizing that part of the search tree are symmetric and do not need to be explored in this paper we show how we can successfully combine symmetry breaking method with lazy clause generation further we show that the more precise nogoods generated by a lazy clause solver allow our combined approach to exploit redundancy that cannot be exploited via any previous symmetry breaking method be it static or dynamic 
metaphorical interpretation and affect detection using context profile from open ended text input are challenging in affective language processing field in this paper we explore recognition of a few typical affective metaphorical phenomenon and context based affect sensing using the modeling of speaker improvisational mood and other participant emotional influence to the speaking character under the improvisation of loose scenario the overall updated affect detection module is embedded in an ai agent the new development have enabled the ai agent to perform generally better in affect sensing task the work emphasizes the conference theme on affective dialogue processing human agent interaction and intelligent user interface 
in this paper we study the problem of learning a metric and propose a loss function based metric learning framework in which the metric is estimated by minimizing an empirical risk over a training set with mild condition on the instance distribution and the used loss function we prove that the empirical risk converges to it expected counterpart at rate of root n in addition with the assumption that the best metric that minimizes the expected risk is bounded we prove that the learned metric is consistent two example algorithm are presented by using the proposed loss function based metric learning framework each of which us a log loss function and a smoothed hinge loss function respectively experimental result suggest the effectiveness of the proposed algorithm 
in the area of description logic the least common subsumer lcs and the most specific concept msc are inference that generalize a set of concept or an individual respectively into a single concept if computed w r t a general el tbox neither the lcs nor the msc need to exist so far in this setting no exact condition for the existence of lcs or msc concept are known this paper provides necessary and sufficient condition for the existence of these two kind of concept for the lcs of a fixed number of concept and the msc we show decidability of the existence in ptime and polynomial bound on the maximal role depth of the lcs and msc concept this bound allows to compute the lcs and the msc respectively 
bounded suboptimal search algorithm offer shorter solving time by sacrificing optimality and instead guaranteeing solution cost within a desired factor of optimal typically these algorithm use a single admissible heuristic both for guiding search and bounding solution cost in this paper we present a new approach to bounded suboptimal search explicit estimation search that separate these role consulting potentially inadmissible information to determine search order and using admissible information to guarantee the cost bound unlike previous proposal it successfully combine estimate of solution length and solution cost to predict which node will lead most quickly to a solution within the suboptimality bound an empirical evaluation across six diverse benchmark domain show that explicit estimation search is competitive with the previous state of the art in domain with unit cost action and substantially outperforms previously proposed technique for domain in which solution cost and length can differ 
over constrained temporal problem are commonly encountered while operating autonomous and decision support system an intelligent system must learn a human s preference over a problem in order to generate preferred resolution that minimize perturbation we present the best first conflict directed relaxation bcdr algorithm for enumerating the best continuous relaxation for an over constrained conditional temporal problem with controllable choice bcdr reformulates such a problem by making it temporal constraint relaxable and solves the problem using a conflict directed approach it extends the conflict directed a cd a algorithm to conditional temporal problem by first generalizing the conflict learning process to include all discrete variable assignment and continuous temporal constraint and then by guiding the forward search away from known infeasible region using conflict resolution when evaluated empirically on a range of coordinated car sharing network problem bcdr demonstrates a substantial improvement in performance and solution quality compared to previous conflict directed approach 
human vision system actively seek salient region and movement in video sequence to reduce the search effort modeling computational visual saliency map provides important information for semantic understanding in many real world application in this paper we propose a novel video saliency detection model for detecting the attended region that correspond to both interesting object and dominant motion in video sequence in spatial saliency map we inherit the classical bottom up spatial saliency map in temporal saliency map a novel optical flow model is proposed based on the dynamic consistency of motion the spatial and the temporal saliency map are constructed and further fused together to create a novel attention model the proposed attention model is evaluated on three video datasets empirical validation demonstrate the salient region detected by our dynamic consistent saliency map highlight the interesting object effectively and efficiency more importantly the automatically video attended region detected by proposed attention model are consistent with the ground truth saliency map of eye movement data association for the advancement of artificial intelligence www aaai org all right reserved 
most text mining task including clustering and topic detection are based on statistical method that treat text a bag of word semantics in the text is largely ignored in the mining process and mining result often have low interpretability one particular challenge faced by such approach lie in short text understanding a short text lack enough content from which statistical conclusion can be drawn easily in this paper we improve text understanding by using a probabilistic knowledgebase that is a rich a our mental world in term of the concept of worldly fact it contains we then develop a bayesian inference mechanism to conceptualize word and short text we conducted comprehensive experiment on conceptualizing textual term and clustering short piece of text such a twitter message compared to purely statistical method such a latent semantic topic modeling or method that use existing knowledge base e g wordnet freebase and wikipedia our approach brings significant improvement in short text understanding a reflected by the clustering accuracy 
constructing a strong heuristic function is a central problem in heuristic search a common approach is to combine a number of heuristic by maximizing over the value from each if a limit is placed on this number then a subset selection problem arises we treat this a an optimization problem and proceed by translating a natural loss function into a submodular and monotonic utility function under which greedy selection is guaranteed to be near optimal we then extend this approach with a sampling scheme that retains provable optimality our empirical result show large improvement over existing method and give new insight into building heuristic for directed domain 
the availability of microblogging like twitter and sina weibo make it a popular platform for spammer to unfairly overpower normal user with unwanted content via social network known a social spamming the rise of social spamming can significantly hinder the use of microblogging system for effective information dissemination and sharing distinct feature of microblogging system present new challenge for social spammer detection first unlike traditional social network microblogging allows to establish some connection between two party without mutual consent which make it easier for spammer to imitate normal user by quickly accumulating a large number of human friend second microblogging message are short noisy and unstructured traditional social spammer detection method are not directly applicable to microblogging in this paper we investigate how to collectively use network and content information to perform effective social spammer detection in microblogging in particular we present an optimization formulation that model the social network and content information in a unified framework experiment on a real world twitter dataset demonstrate that our proposed method can effectively utilize both kind of information for social spammer detection 
transfer learning address the problem that labeled training data are insufficient to produce a high performance model typically given a target learning task most transfer learning approach require to select one or more auxiliary task a source by the designer however how to select the right source data to enable effective knowledge transfer automatically is still an unsolved problem which limit the applicability of transfer learning in this paper we take one step ahead and propose a novel transfer learning framework known a source selection free transfer learning ssftl to free user from the need to select source domain instead of asking the user for source and target data pair a traditional transfer learning doe ssftl turn to some online information source such a world wide web or the wikipedia for help the source data for transfer learning can be hidden somewhere within this large online information source but the user do not know where they are based on the online information source we train a large number of classifier then given a target task a bridge is built for label of the potential source candidate and the target domain data in ssftl via some large online social medium with tag cloud a a label translator an added advantage of ssftl is that unlike many previous transfer learning approach which are difficult to scale up to the web scale ssftl is highly scalable and can offset much of the training work to offline stage we demonstrate the effectiveness and efficiency of ssftl through extensive experiment on several real world datasets in text classification 
a major challenge of query language design is the combination of expressivity with effective static analysis such a query containment in the setting of xml document are seen a finite tree whose structure may additionally be constrained by type constraint such a those described by an xml schema we consider the problem of query containment in the presence of type constraint for a class of regular path query extended with counting and interleaving operator the counting operator restricts the number of occurrence of child node satisfying a given logical property the interleaving operator provides a succinct notation for describing the absence of order between node satisfying a logical property we provide a logic based framework supporting these operator which can be used to solve common query reasoning problem such a satisfiability and containment of query in exponential time 
cake cutting is a playful name for the problem of fairly dividing a heterogeneous divisible good among a set of agent the agent valuation for different piece of cake are typically assumed to be additive however in certain practical setting this assumption is invalid because agent may not have positive value for arbitrarily small crumb of cake in this paper we propose a new more expressive model of agent valuation that capture this feature we present an approximately proportional algorithm for any number of agent that have such expressive valuation the algorithm is optimal in the sense that no other algorithm can guarantee a greater worst case degree of proportionality we also design an optimal approximately proportional and fully envy free algorithm for two agent 
we consider the problem of how enormous database of common sense knowledge can be both learned and utilized in reasoning in a computationally efficient manner we propose that this is possible if the learning only occurs implicitly i e without generating an explicit representation we show that it is feasible to invoke such implicitly learned knowledge in essentially all natural tractable reasoning problem this implicit learning also turn out to be provably robust to occasional counterexample a appropriate for such common sense knowledge 
we introduce the multi inter distance constraint that ensures no more than m variable are assigned to value lying in a window of p consecutive value this constraint is useful for modeling scheduling problem where task of processing time p compete for m identical resource we present a propagator that achieves bound consistency in cubic time experiment show that this new constraint offer a much stronger filtering than an edge finder and that it allows to solve larger instance of the runway scheduling problem 
side information is highly useful in the learning of a nonparametric kernel matrix however this often lead to an expensive semidefinite program sdp in recent year a number of dedicated solver have been proposed though much better than off the shelf sdp solver they still cannot scale to large data set in this paper we propose a novel solver based on the alternating direction method of multiplier admm the key idea is to use a low rank decomposition of the kernel matrix k vtu with the constraint that v u the resultant optimization problem though non convex ha favorable convergence property and can be efficiently solved without requiring eigen decomposition in each iteration experimental result on a number of real world data set demonstrate that the proposed method is a accurate a directly solving the sdp but can be one to two order of magnitude faster 
my thesis contributes to the field of multi agent system by proposing a novel trust based decision model for supply chain management 
diagnosis i e the identification of root cause for failing or unexpected system behavior is an important task in practice within the last three decade many different ai based solution for solving the diagnosis problem have been presented and have been gaining in attraction this leaf u with the question of which algorithm to prefer in a certain situation in this paper we contribute to answering this question in particular we compare two class of diagnosis algorithm one class exploit conflict in their search i e set of system component whose correct behavior contradicts given observation the other class ignores conflict and derives diagnosis from observation and the underlying model directly in our study we use different reasoning engine ranging from an optimized horn clause theorem prover to general sat and constraint solver thus we also address the question whether publicly available general reasoning engine can be used for an efficient diagnosis 
this paper deal with the issue of strategic argumentation in the setting of dung style abstract argumentation theory such reasoning take place through the use of opponent model recursive representation of an agent s knowledge and belief regarding the opponent s knowledge using such model we present three approach to reasoning the first directly utilises the opponent model to identify the best move to advance in a dialogue the second extends our basic approach through the use of quantitative uncertainty over the opponent s model the final extension introduces virtual argument into the opponent s reasoning process such argument are unknown to the agent but presumed to exist and interact with known argument they are therefore used to add a primitive notion of risk to the agent s reasoning we have implemented our model and we have performed an empirical analysis that show that this added expressivity improves the performance of an agent in a dialogue 
multi instance multi label learning miml deal with data object that are represented by a bag of instance and associated with a set of class label simultaneously previous study typically assume that for every training example all positive label are tagged whereas the untagged label are all negative in many real application such a image annotation however the learning problem often suffers from weak label that is user usually tag only a part of positive label and the untagged label are not necessarily negative in this paper we propose the mimlwel approach which work by assuming that highly relevant label share some common instance and the underlying class mean of bag for each label are with a large margin experiment validate the effectiveness of mimlwel in handling the weak label problem 
transfer learning a a new machine learning paradigm ha gained increasing attention lately in situation where the training data in a target domain are not sufficient to learn predictive model effectively transfer learning leverage auxiliary source data from other related source domain for learning while most of the existing work in this area only focused on using the source data with the same structure a the target data in this paper we push this boundary further by proposing a heterogeneous transfer learning framework for knowledge transfer between text and image we observe that for a target domain classification problem some annotated image can be found on many social web site which can serve a a bridge to transfer knowledge from the abundant text document available over the web a key question is how to effectively transfer the knowledge in the source data even though the text can be arbitrarily found our solution is to enrich the representation of the target image with semantic concept extracted from the auxiliary source data through a novel matrix factorization method by using the latent semantic feature generated by the auxiliary data we are able to build a better integrated image classifier we empirically demonstrate the effectiveness of our algorithm on the caltech image dataset copyright association for the advancement of artificial intelligence all right reserved 
we investigate the complexity of satisfiability for one agent refinement modal logic text sffamily rml a known extension of basic modal logic text sffamily ml obtained by adding refinement quantifier on structure it is known that text sffamily rml ha the same expressiveness a text sffamily ml but the translation of text sffamily rml into text sffamily ml is of non elementary complexity and text sffamily rml is at least doubly exponentially more succinct than text sffamily ml in this paper we show that text sffamily rml satisfiability is only singly exponentially harder than text sffamily ml satisfiability the latter being a well known pspace complete problem more precisely we establish that text sffamily rml satisfiability is complete for the complexity class aexp text sffamily pol i e the class of problem solvable by alternating turing machine running in single exponential time but only with a polynomial number of alternation note that nexptime aexp text sffamily pol expspace 
graph based semi supervised learning gssl play an important role in machine learning system the most crucial step in gssl is graph construction although several interesting graph construction method have been proposed in recent year how to construct an effective graph is still an open problem in this paper we develop a novel approach to constructing graph which is based on low rank coding and b matching constraint by virtue of recent advance in low rank subspace recovery theory compact encoding using low rank representation coefficient allows u to obtain a robust similarity metric between all pair of sample meanwhile the b matching constraint help in obtaining a sparse and balanced graph which benefit label propagation in gssl we build a joint optimization model to learn low rank code and balanced graph simultaneously after using a graph re weighting strategy we present a semi supervised learning algorithm by incorporating our sparse and balanced graph with gaussian harmonic function ghf experimental result on the extended yaleb pie orl and usps database demonstrate that our graph outperforms several state of the art graph especially when the labeled sample are very scarce 
the goal of controlling a gene regulatory network grn is to generate an intervention strategy i e a control policy such that by applying the policy the system will avoid undesirable state in this work we propose a method to control grns by using batch mode reinforcement learning batch rl our idea is based on the fact that time series gene expression data can actually be interpreted a a sequence of experience tuples collected from the environment existing study on this control task try to infer a model using gene expression data and then calculate a control policy over the constructed model however we propose a method that can directly use the available gene expression data to obtain an approximated control policy for gene regulation that avoids the time consuming model building phase result show that we can obtain policy for gene regulation system of several thousand of gene just in several second while existing solution get stuck for even ten of gene interestingly the reported result also show that our method produce policy that are almost a good a the one generated by existing model dependent method 
this paper present a new application of logic programming to a real life problem in hydraulic engineering the work is developed a a collaboration of computer scientist and hydraulic engineer and applies constraint logic programming to solve a hard combinatorial problem this application deal with one aspect of the design of a water distribution network i e the valve isolation system design we take the formulation of the problem by giustolisi and savic and show how thanks to constraint propagation we can get better solution than the best solution known in the literature for the apulian distribution network 
computational narrative is a complex and interesting domain for exploring ai technique that algorithmically analyze understand and most importantly generate story this paper study the importance of domain knowledge in story generation and particularly in analogy based story generation asg based on the construct of knowledge container in case based reasoning we present a theoretical framework for incorporating domain knowledge in asg we complement the framework with empirical result in our existing system riu 
in this paper we consider the inclusion exclusion rule a known yet seldom used rule of probabilistic inference unlike the widely used sum rule which requires easy access to all joint probability value the inclusion exclusion rule requires easy access to several marginal probability value we therefore develop a new representation of the joint distribution that is amenable to the inclusion exclusion rule we compare the relative strength and weakness of the inclusion exclusion rule with the sum rule and develop a hybrid rule called the inclusion exclusion sum y rule which combine their power we apply the y rule to junction tree treating the latter a a target for knowledge compilation and show that in many case it greatly reduces the time required to answer query our experiment demonstrate the power of our approach in particular at query time on several network our new scheme wa an order of magnitude faster than the junction tree algorithm 
in this paper we propose a new classification framework for image matrix the approach is realized by learning two group of classification vector for each dimension of the image matrix one novelty is that we utilize compound regression model in the learning process which endows the algorithm increased degree of freedom on top of that we extend the two dimensional classification method to a semi supervised classifier which leverage both labeled and unlabeled data a fast iterative solution is then proposed to solve the objective function the proposed method is evaluated by several different application the experimental result show that our method outperforms several classification approach in addition we observe that our method attains respectable classification performance even when only few labeled training sample are provided this advantage is especially desirable for real world problem since precisely annotated image are scarce 
there are many hard shortest path search problem that cannot be solved because best first search run out of memory space and depth first search run out of time we propose forward perimeter search fps a heuristic search with controlled use of memory it build a perimeter around the root node and test each perimeter node for a shortest path to the goal the perimeter is adaptively extended towards the goal during the search process we show that fps expands in random puzzle fewer node than bf ida while requiring several order of magnitude le memory additionally we present a hard problem instance of the puzzle that need at least move to solve i e more move than the previously published hardest instance 
we focus on solving two player zero sum extensive form game with perfect information and simultaneous move in these game both player fully observe the current state of the game where they simultaneously make a move determining the next state of the game we solve these game by a novel algorithm that relies on two component it iteratively solves the game that correspond to a single simultaneous move using a double oracle method and it prune the state of the game using bound on the sub game value obtained by the classical alpha beta search on a serialized variant of the game we experimentally evaluate our algorithm on the goofspiel card game a pursuit evasion game and randomly generated game the result show that our novel algorithm typically provides significant running time improvement and reduction in the number of evaluated node compared to the full search algorithm 
typical solution for agent assessing trust relies on the circulation of information on the individual level i e reputational image subjective experience statistical analysis etc this work present an alternative approach inspired to the cognitive heuristic enabling human to reason at a categorial level the approach is envisaged a a crucial ability for agent in order to estimate trustworthiness of unknown trustee based on an ascribed membership to category learn a series of emergent relation between trustee observable property and their effective ability to fulfill task in situated condition on such a basis categorization is provided to recognize sign manifesta through which hidden capability kripta can be inferred learning is provided to refine reasoning attitude needed to ascribe task to category a series of architecture combining categorization ability individual experience and context awareness are evaluated and compared in simulated experiment 
the linked data paradigm ha evolved into a powerful enabler for the transition from the document oriented web into the semantic web while the amount of data published a linked data grows steadily and ha surpassed billion triple le than of these triple are link between knowledge base link discovery framework provide the functionality necessary to discover missing link between knowledge base yet this task requires a significant amount of time especially when it is carried out on large data set this paper present and evaluates lime a novel time efficient approach for link discovery in metric space our approach utilizes the mathematical characteristic of metric space during the mapping process to filter out a large number of those instance pair that do not suffice the mapping condition we present the mathematical foundation and the core algorithm employed in lime we evaluate our algorithm with synthetic data to elucidate their behavior on small and large data set with different configuration and compare the runtime of lime with another state of the art link discovery tool 
both sat and sat can represent difficult problem in seemingly dissimilar area such a planning verification and probabilistic inference here we examine an expressive new language sat that generalizes both of these language sat problem require counting the number of satisfiable formula in a concisely describable set of existentially quantified propositional formula we characterize the expressiveness and worst case difficulty of sat by proving it is complete for the complexity class pnp and relating this class to more familiar complexity class we also experiment with three new general purpose sat solver on a battery of problem distribution including a simple logistics domain our experiment show that despite the formidable worst case complexity of pnp many of the instance can be solved efficiently by noticing and exploiting a particular type of frequent structure 
personalized point of interest poi recommendation is a significant task in location based social network lbsns a it can help provide better user experience a well a enable third party service e g launching advertisement to provide a good recommendation various research ha been conducted in the literature however pervious effort mainly consider the check in in a whole and omit their temporal relation they can only recommend poi globally and cannot know where a user would like to go tomorrow or in the next few day in this paper we consider the task of successive personalized poi recommendation in lbsns which is a much harder task than standard personalized poi recommendation or prediction to solve this task we observe two prominent property in the check in sequence personalized markov chain and region localization hence we propose a novel matrix factorization method namely fpmc lr to embed the personalized markov chain and the localized region our proposed fpmc lr not only exploit the personalized markov chain in the check in sequence but also take into account user movement constraint i e moving around a localized region more importantly utilizing the information of localized region we not only reduce the computation cost largely but also discard the noisy information to boost recommendation result on two real world lbsns datasets demonstrate the merit of our proposed fpmc lr 
we advocate the use of an explicit time representation in syntactic pattern recognition because it can result in more succinct model and easier learning problem we apply this approach to the real world problem of learning model for the driving behavior of truck driver we discretize the value of onboard sensor into simple event instead of the common syntactic pattern recognition approach of sampling the signal value at a fixed rate we model the time constraint using timed model we learn these model using the rti algorithm from grammatical inference and show how to use computational mechanic and a form of semi supervised classification to construct a real time automaton classifier for driving behavior promising result are shown using this new approach 
null 
we present the notion of social instrument a mechanism that facilitate the emergence of convention from repeated interaction between member of a society specifically we focus on two social instrument rewiring and observation our main goal is to provide agent with tool that allow them to leverage their social network of interaction when effectively addressing coordination and learning problem paying special attention to dissolving metastable subconventions our initial experiment throw some light on how self reinforcing substructure sr in the network prevent full convergence to society wide convention resulting in reduced convergence rate the use of an effective composed social instrument observation rewiring allow agent to achieve convergence by eliminating the subconventions that otherwise remained meta stable 
existential rule i e datalog extended with existential quantifier in rule head are currently studied under a variety of name such a datalog rule and tuple generating dependency the renewed interest in this formalism is fuelled by a wealth of recently discovered language fragment for which query answering is decidable this paper extends and consolidates two of the main approach in this field acyclicity and guardedness by providing complexity preserving generalisation of weakly acyclic and weakly frontier guarded rule and a novel formalism of glut frontier guarded rule that subsumes both this build on an insight that acyclicity can be used to extend any existential rule language while retaining decidability besides decidability combined query complexity are established in all case 
null 
bayesian optimization technique have been successfully applied to robotics planning sensor placement recommendation advertising intelligent user interface and automatic algorithm configuration despite these success the approach is restricted to problem of moderate dimension and several workshop on bayesian optimization have identified it scaling to high dimension a one of the holy grail of the field in this paper we introduce a novel random embedding idea to attack this problem the resulting random embedding bayesian optimization rembo algorithm is very simple and applies to domain with both categorical and continuous variable the experiment demonstrate that rembo can effectively solve high dimensional problem including automatic parameter configuration of a popular mixed integer linear programming solver 
hartigan s method for k mean clustering hold several potential advantage compared to the classical and prevalent optimization heuristic known a lloyd s algorithm e g it wa recently shown that the set of local minimum of hartigan s algorithm is a subset of those of lloyd s method we develop a closed form expression that allows to establish hartigan s method for k mean clustering with any bregman divergence and further strengthen the case of preferring hartigan s algorithm over lloyd s algorithm specifically we characterize a range of problem with various noise level of the input for which any random partition represents a local minimum for lloyd s algorithm while hartigan s algorithm easily converges to the correct solution extensive experiment on synthetic and real world data further support our theoretical analysis 
complex adaptive system ca are composed of interacting agent exhibit nonlinear property such a positive and negative feedback and tend to produce emergent behavior that cannot be wholly explained by deconstructing the system into it constituent part both system dynamic equation based approach and agent based approach have been used to model such system and each ha it benefit and drawback in this paper we introduce a class of agent based model with an embedded system dynamic model and detail the semantics of a simulation framework for these model this model definition along with the simulation framework combine agent based and system dynamic approach in a way that retains the strength of both paradigm we show the applicability of our model by instantiating it for two example complex adaptive system in the field of computational sustainability drawn from ecology and epidemiology we then present a more detailed application in epidemiology in which we compare a previously unstudied intervention strategy to established one our experimental result unattainable using previous method yield insight into the effectiveness of these intervention strategy 
in many application the data may be high dimensional represented by multiple feature and associated with more than one label embedding learning is an effective strategy for dimensionality reduction and for nearest neighbor search in massive datasets we propose a novel method to seek compact embedding that allows efficient retrieval with incompletely labeled multi view data based on multi graph laplacian we achieve the optimal combination of heterogeneous feature to effectively describe data which exploit the feature correlation between different view we learn the embedding that preserve the neighborhood context in the original space and obtain the complete label simultaneously inter label correlation are sufficiently leveraged in the proposed framework our goal is to find the map from multiple input space to the compact embedding space and to the semantic concept space at the same time there is semantic gap between the input multi view feature space and the semantic concept space and the compact embedding space can be looked on a the bridge between the above space experimental evaluation on three real world datasets demonstrates the effectiveness of the proposed method 
recent year have witnessed the growing popularity of hash function learning for large scale data search although most existing hashing based method have been proven to obtain high accuracy they are regarded a passive hashing and assume that the labelled point are provided in advance in this paper we consider updating a hashing model upon gradually increased labelled data in a fast response to user called smart hashing update shu in order to get a fast response to user shu aim to select a small set of hash function to relearn and only update the corresponding hash bit of all data point more specifically we put forward two selection method for performing efficient and effective update in order to reduce the response time for acquiring a stable hashing algorithm we also propose an accelerated method in order to further reduce interaction between user and the computer we evaluate our proposal on two benchmark data set our experimental result show it is not necessary to update all hash bit in order to adapt the model to new input data and meanwhile we obtain better or similar performance without sacrificing much accuracy against the batch mode update 
coalition formation is a fundamental research topic in multi agent system in this context while it is desirable to generate a coalition structure that maximizes the sum of the value of the coalition the space of possible solution is often too large to allow exhaustive search thus a fundamental open question in this area is the following can we search through only a subset of coalition structure and be guaranteed to find a solution that is within a desirable bound from optimum if so what is the minimum such subset to date the above question ha only been partially answered by sandholm et al in their seminal work on anytime coalition structure generation sandholm et al more specifically they identified minimum subset to be searched for two particular bound n and n nevertheless the question remained open for other value of in this paper we provide the complete answer to this question 
this work develops an approach to efficient reasoning in first order knowledge base with incomplete information we build on levesque s proper knowledge base approach which support limited incomplete knowledge in the form of a possibly infinite set of positive or negative ground fact we propose a generalization which allows these fact to involve unknown individual a in the work on labeled null value in database dealing with such unknown individual ha been shown to be a key feature in the database literature on data integration and data exchange in this way we obtain one of the most expressive first order open world setting for which reasoning can still be done efficiently by evaluation a in relational database we show the soundness of the reasoning procedure and it completeness for query in a certain normal form 
this extended research abstract describes an argumentation based approach to modelling articulated decision making context the approach encompasses a variety of argument and attack scheme aimed at representing basic knowledge and reasoning pattern for decision support 
in election an alternative is said to be a condorcet winner if it is preferred to any other alternative by a majority of voter while this is a very attractive solution concept many election do not have a condorcet winner in this paper we propose a setvalued relaxation of this concept which we call a condorcet winning set such set consist of alternative that collectively dominate any other alternative we also consider a more general version of this concept where instead of domination by a majority of voter we require domination by a given fraction of voter we refer to this concept a winning set we explore social choice theoretic and algorithmic aspect of these solution concept both theoretically and empirically 
we consider scenario where several agent must aggregate their preference over a large set of candidate with a combinatorial structure that is each candidate is an element of the cartesian product of the domain of some variable we assume agent compactly express their preference over the candidate via soft constraint we consider a sequential procedure that chooses one candidate by asking the agent to vote on one variable at a time while some property of this procedure have been already studied here we focus on independence of irrelevant alternative non dictatorship and strategy proofness also we perform an experimental study that show that the proposed sequential procedure yield a considerable saving in time with respect to a non sequential approach while the winner satisfy the agent just a well independently of the variable ordering and of the presence of coalition of agent 
algorithm for finding game theoretic solution are now used in several real world security application this work ha generally assumed a stackelberg model where the defender commits to a mixed strategy first in general two player normal form game stackelberg strategy are easier to compute than nash equilibrium though it ha recently been shown that in many security game stackelberg strategy are also nash strategy for the defender however the work on security game so far assumes that the attacker attack only a single target in this paper we generalize to the case where the attacker attack multiple target simultaneously here stackelberg and nash strategy for the defender can be truly different we provide a polynomial time algorithm for finding a nash equilibrium the algorithm gradually increase the number of defender resource and maintains an equilibrium throughout this process moreover we prove that nash equilibrium in security game with multiple attacker satisfy the interchange property which resolve the problem of equilibrium selection in such game on the other hand we show that stackelberg strategy are actually np hard to compute in this context finally we provide experimental result 
fages introduces the notion of well supportedness a a key requirement for the semantics of normal logic program and characterizes the standard answer set semantics in term of the well supportedness condition with the property of well supportedness answer set are guaranteed to be free of circular justification in this paper we extend fages work to description logic program or dl program we introduce two form of well supportedness for dl program the first one defines weakly well supported model that are free of circular justification caused by positive literal in rule body the second one defines strongly well supported model that are free of circular justification caused by either positive or negative literal we then define two new answer set semantics for dl program and characterize them in term of the weakly and strongly well supported model respectively the first semantics is based on an extended gelfond lifschitz transformation and defines weakly well supported answer set that are free of circular justification for the class of dl program without negative dlatoms the second semantics defines strongly well supported answer set which are free of circular justification for all dl program we show that the existing answer set semantics for dl program such a the weak answer set semantics the strong answer set semantics and the flp based answer set semantics satisfy neither the weak nor the strong well supportedness condition even for dl program without negative dl atom this explains why their answer set incur circular justification 
query containment ha been studied extensively in kr and database for different kind of query language and domain constraint we address the longstanding open problem of containment under expressive description logic dl constraint for two way regular path query rpqs and their conjunction which generalize conjunctive query with the ability to express regular navigation we show that surprisingly functionality constraint alone make containment of rpqs already exptime hard by employing automaton theoretic technique we also provide a matching upper bound that extends to very expressive dl constraint for conjunctive rpqs we prove a further exponential jump in complexity and provide again a matching upper bound for expressive dl our technique provide also a solution to the problem of query entailment over dl knowledge base in which individual in the abox may be related through regular role path 
interaction among agent are complicated since in order to make the best decision each agent ha to take into account not only the strategy used by other agent but also how those strategy might change in the future and what cause these change the objective of my work will be to develop a framework for learning agent model opponent or teammate more accurately and with le interaction with a special focus on fast learning non stationary strategy a preliminary work we have proposed an initial approach for learning nonstationary strategy in repeated game we use decision tree to learn a model of the agent and we transform the learned tree into a mdp and solve it to obtain the optimal policy 
in real world application the effective integration of learning and reasoning in a cognitive agent model is a difficult task however such integration may lead to a better understanding use and construction of more realistic model unfortunately existing model are either oversimplified or require much processing time which is unsuitable for online learning and reasoning currently controlled environment like training simulator do not effectively integrate learning and reasoning in particular higher order concept and cognitive ability have many unknown temporal relation with the data making it impossible to represent such relationship by hand we introduce a novel cognitive agent model and architecture for online learning and reasoning that seek to effectively represent learn and reason in complex training environment the agent architecture of the model combine neural learning with symbolic knowledge representation it is capable of learning new hypothesis from observed data and infer new belief based on these hypothesis furthermore it deal with uncertainty and error in the data using a bayesian inference model the validation of the model on real time simulation and the result presented here indicate the promise of the approach when performing online learning and reasoning in real world scenario with possible application in a range of area 
road traffic prediction is a critical component in modern smart transportation system it provides the basis for traffic management agency to generate proactive traffic operation strategy for alleviating congestion existing work on near term traffic prediction forecasting horizon in the range of minute to hour relies on the past and current traffic condition however once the forecasting horizon is beyond hour i e in longer term traffic prediction these technique do not work well since additional factor other than the past and current traffic condition start to play important role to address this problem in this paper for the first time we examine whether it is possible to use the rich information in online social medium to improve longer term traffic prediction to this end we first analyze the correlation between traffic volume and tweet count with various granularity then we propose an optimization framework to extract traffic indicator based on tweet semantics using a transformation matrix and incorporate them into traffic prediction via linear regression experimental result using traffic and twitter data originated from the san francisco bay area of california demonstrate the effectiveness of our proposed framework 
in this paper we propose the first heuristic approach for the vertex separator problem vsp based on breakout local search bls bls is a recent meta heuristic that follows the general framework of the popular iterated local search il with a particular focus on the perturbation strategy based on some relevant information on search history it try to introduce the most suitable degree of diversification by determining adaptively the number and type of move for the next perturbation phase the proposed heuristic is highly competitive with the exact state of art approach from the literature on the current vsp benchmark moreover we present for the first time computational result for a set of large graph with up to vertex which constitutes a new challenging benchmark for vsp approach 
slinko and white have recently introduced a new model of coalitional manipulation of voting rule under limited communication which they call safe strategic voting the computational aspect of this model were first studied by hazon and elkind who provide polynomial time algorithm for finding a safe strategic vote under kapproval and the bucklin rule in this paper we answer an open question of hazon and elkind by presenting a polynomial time algorithm for finding a safe strategic vote under the borda rule our result for borda generalize to several interesting class of scoring rule 
hashtags can be viewed a an indication to the context of the tweet or a the core idea expressed in the tweet they provide valuable information for many application such a information retrieval opinion mining text classification and so on however only a small number of microblogs are manually tagged to address this problem in this work we propose a topical translation model for microblog hashtag suggestion we assume that the content and hashtags of the tweet are talking about the same theme but written in different language under the assumption hashtag suggestion is modeled a a translation process from content to hashtags moreover in order to cover the topic of tweet the proposed model regard the translation probability to be topic specific it us topic specific word trigger to bridge the vocabulary gap between the word in tweet and hashtags and discovers the topic of tweet by a topic model designed for microblogs experimental result on the dataset crawled from real world microblogging service demonstrate that the proposed method outperforms state of the art method 
creating descriptor for trajectory ha many application in robotics human motion analysis and video copy detection here we propose a novel descriptor for d trajectory histogram of oriented displacement hod each displacement in the trajectory vote with it length in a histogram of orientation angle d trajectory are described by the hod of their three projection we use hod to describe the d trajectory of body joint to recognize human action which is a challenging machine vision task with application in human robot machine interaction interactive entertainment multimedia information retrieval and surveillance the descriptor is fixed length scale invariant and speed invariant experiment on msr action d and hdm datasets show that the descriptor outperforms the state of the art when using off the shelf classification tool 
mobile manipulation robot are envisioned to provide many useful service both in domestic environment a well a in the industrial context in this paper we present novel approach to allow mobile maniplation system to autonomously adapt to new or changing situation the approach developed in this paper cover the following four topic learning the robot s kinematic structure and property using actuation and visual feedback learning about articulated object in the environment in which the robot is operating using tactile feedback to augment visual perception and learning novel manipulation task from human demonstration 
with the rapid proliferation of social medium more and more people freely express their opinion or comment on news product and movie through online service such a forum discussion group and microblogs those comment may be concerned with different aspect topic of the target web document e g a news page it would be interesting to align the social comment to the corresponding subtopics contained in the web document in this paper we propose a novel framework that is able to automatically detect the subtopics from a given web document and also align the associated social comment with the detected subtopics this provides a new view of the web standard document and it associated user generated content through topic which facilitates the reader to quickly focus on those hot topic or grasp topic that they are interested in extensive experiment show that our proposed framework significantly outperforms the existing state of the art method in social content alignment 
it is widely acknowledged that stochastic local search sl algorithm can efficiently find model of satisfiable formula for the boolean satisfiability sat problem there ha been much interest in studying sl algorithm on random k sat instance compared to random sat instance which have special statistical property rendering them easy to solve random k sat instance with long clause are similar to structured one and remain very difficult this paper is devoted to efficient sl algorithm for random k sat instance with long clause by combining a novel variable property subscore with the commonly used property score we design a scoring function named comprehensive score which is utilized to develop a new sl algorithm called cscoresat the experiment show that cscoresat outperforms state of the art sl solver including the winner of recent sat competition by one to two order of magnitude on large random sat and sat instance in addition cscoresat significantly outperforms it competitor on random k sat instance for each k from sat challenge which indicates it robustness 
this paper focus on computing first order theory under either stable model semantics or circumscription a reduction from first order theory to logic program under stable model semantics over finite structure is proposed and an embedding of circumscription into stable model semantics is also given having such reduction and embedding reasoning problem represented by first order theory under these two semantics can then be handled by using existing answer set solver the effectiveness of this approach in computing hard problem beyond np is demonstrated by some experiment 
online social network continue to witness a tremendous growth both in term of the number of registered user and their mutual interaction in this paper we focus on online signed social network where positive interaction among the user signify friendship or approval whereas negative interaction indicate antagonism or disapproval we introduce a novel problem which we call the link label prediction problem given the information about sign of certain link in a social network we want to learn the nature of relationship that exist among the user by predicting the sign positive or negative of the remaining link we propose a matrix factorization based technique mf lisp that exhibit strong generalization guarantee we also investigate the applicability of logistic regression in this setting our experiment on wiki vote epinions and slashdot data set strongly corroborate the efficacy of these approach 
we define an extension of stit logic that encompasses subjective probability representing belief about simultaneous choice exertion of other agent the formalism enables u to express the notion of attempt a a choice exertion that maximizes the chance of success with respect to an action effect the notion of attempt or effort is central in philosophical and legal discussion on responsibility and liability 
the fact that bird have feather and ice is cold seems trivially true yet most machine readable source of knowledge either lack such common sense fact entirely or have only limited coverage prior work on automated knowledge base construction ha largely focused on relation between named entity and on taxonomic knowledge while disregarding common sense property in this paper we show how to gather large amount of common sense fact from web n gram data using seed from the conceptnet collection our novel contribution include scalable method for tapping onto web scale data and a new scoring model to determine which pattern and fact are most reliable the experimental result show that this approach extends conceptnet by many order of magnitude at comparable level of precision 
counting the number of social medium post on a target phenomenon ha become a popular method to monitor a spatiotemporal signal however such counting is plagued by biased missing or scarce data we address these issue by formulating signal recovery a a poisson point process estimation problem we explicitly incorporate human population bias time delay and spatial distortion and spatiotemporal regularization into the model to address the data quality issue our model produce qualitatively convincing result in a case study on wildlife roadkill monitoring 
among the many approach for reasoning about degree of belief in the presence of noisy sensing and acting the logical account proposed by bacchus halpern and levesque is perhaps the most expressive while their formalism is quite general it is restricted to fluents whose value are drawn from discrete countable domain a opposed to the continuous domain seen in many robotic application in this paper we show how this limitation in their approach can be lifted by dealing seamlessly with both discrete distribution and continuous density within a rich theory of action we provide a very general logical specification of how belief should change after acting and sensing in complex noisy domain 
tensor are increasingly common in several area such a data mining computer graphic and computer vision tensor clustering is a fundamental tool for data analysis and pattern discovery however there usually exist outlying data point in real world datasets which will reduce the performance of clustering this motivates u to develop a tensor clustering algorithm that is robust to the outlier in this paper we propose an algorithm of robust tensor clustering rtc the rtc firstly find a lower rank approximation of the original tensor data using a l norm optimization function because the l norm doesn t exaggerate the effect of outlier compared with l norm the minimization of the l norm approximation function make rtc robust to outlier then we compute the hosvd decomposition of this approximate tensor to obtain the final clustering result different from the traditional algorithm solving the approximation function with a greedy strategy we utilize a non greedy strategy to obtain a better solution experiment demonstrate that rtc ha better performance than the state of the art algorithm and is more robust to outlier 
the recent proliferation of smart phone and other wearable device ha lead to a surge of new mobile application partially observable markov decision process provide a natural framework to design application that continuously make decision based on noisy sensor measurement however given the limited battery life there is a need to minimize the amount of online computation this can be achieved by compiling a policy into a finite state controller since there is no need for belief monitoring or online search in this paper we propose a new branch and bound technique to search for a good controller in contrast to many existing algorithm for controller our search technique is not subject to local optimum we also show how to reduce the amount of search by avoiding the enumeration of isomorphic controller and by taking advantage of suitable upper and lower bound the approach is demonstrated on several benchmark problem a well a a smart phone application to assist person with alzheimer s to wayfind 
this paper present a novel semantic regularized matrix factorization method for learning descriptive visual bag of word bow representation although very influential in image classification the traditional visual bow representation ha one distinct drawback that is for efficiency purpose this visual representation is often generated by directly clustering the low level visual feature vector extracted from local keypoints or region without considering the high level semantics of image in other word this visual representation still suffers from the semantic gap and may lead to significant performance degradation in more challenging task e g classification of community contributed image with large intra class variation to overcome this drawback we develop a semantic regularized matrix factorization method for learning descriptive visual bow representation by adding laplacian regularization defined with the tag easy to access although noisy of community contributed image into matrix factorization experimental result on two benchmark datasets show the promising performance of the proposed method 
opinion leader play an important role in influencing people s belief action and behavior although a number of method have been proposed for identifying influentials using secondary source of information the use of primary source such a survey is still favored in many domain in this work we present a new surveying method which combine secondary data with partial knowledge from primary source to guide the information gathering process we apply our proposed active surveying method to the problem of identifying key opinion leader in the medical field and show how we are able to accurately identify the opinion leader while minimizing the amount of primary data required which result in significant cost reduction in data acquisition without sacrificing it integrity 
extracting the relation that exist between two entity is an important step in numerous web related task such a information extraction a supervised relation extraction system that is trained to extract a particular relation type might not accurately extract a new type of a relation for which it ha not been trained however it is costly to create training data manually for every new relation type that one might want to extract we propose a method to adapt an existing relation extraction system to extract new relation type with minimum supervision our proposed method comprises two stage learning a lower dimensional projection between different relation and learning a relational classifier for the target relation type with instance sampling we evaluate the proposed method using a dataset that contains instance for different relation type our experimental result show that the proposed method achieves a statistically significant macro average f score of moreover the proposed method outperforms numerous baseline and a previously proposed weakly supervised relation extraction method 
we present a fast fully parameterizable gpu implementation of convolutional neural network variant our feature extractor are neither carefully designed nor pre wired but rather learned in a supervised way our deep hierarchical architecture achieve the best published result on benchmark for object classification norb cifar and handwritten digit recognition mnist with error rate of respectively deep net trained by simple back propagation perform better than more shallow one learning is surprisingly rapid norb is completely trained within five epoch test error rate on mnist drop to and after and epoch respectively 
vast amount of video data are available on the web and are being generated daily using surveillance camera or other source being able to efficiently analyse and process this data is essential for a number of different application we want to be able to efficiently detect activity in these video or be able to extract and store essential information contained in these video for future use and easy search and access cohn et al proposed a comprehensive representation of spatial feature that can be efficiently extracted from video and used for these purpose in this paper we present a modified version of this approach that is equally efficient and allows u to extract spatial information with much higher accuracy than previously possible we present efficient algorithm both for extracting and storing spatial information from video a well a for processing this information in order to obtain useful spatial feature we evaluate our approach and demonstrate that the extracted spatial information is considerably more accurate than that obtained from existing approach 
eliciting user preference constitutes a major step towards developing recommender system and decision support tool assuming that preference are ceteris paribus allows for their concise representation a conditional preference network cp net this work present the first empirical investigation of an algorithm for reliably and efficiently learning cp net in a manner that is minimally intrusive at the same time it introduces a novel process for efficiently reasoning with the learned preference 
we introduce a new approach to disjunctive asp solving that aim at an equitable interplay between generating and testing solver unit to this end we develop novel characterization of answer set and unfounded set allowing for a bidirectional dynamic information exchange between solver unit for orthogonal task this result in the new multithreaded disjunctive asp solver claspd greatly improving the performance of existing system 
advanced e application require comprehensive knowledge about their user preference in order to provide accurate personalized service in this paper we propose to learn user preference to product brand from their implicit feedback such a their searching and browsing behavior in user web browsing log data the user brand preference learning problem is challenge since the user implicit feedback are extremely sparse in various product domain and we can only observe positive feedback from user behavior in this paper we propose a latent factor model to collaboratively mine user brand preference across multiple domain simultaneously by collective learning the learning process in all the domain are mutually enhanced and hence the problem of data scarcity in each single domain can be effectively addressed on the other hand we learn our model with an adaption of the bayesian personalized ranking bpr optimization criterion which is a general learning framework for collaborative filtering from implicit feedback experiment with both synthetic and real world datasets show that our proposed model significantly outperforms the baseline copyright association for the advancement of artificial intelligence www aaai org all right reserved 
relative direction information is very commonly used observer typically describe their environment by specifying the relative direction in which they see other object or other people from their point of view or they receive navigation instruction with respect to their point of view for example turn left at the next intersection however it is surprisingly hard to integrate relative direction information obtained from different observer and to reconstruct a model of the environment or the location of the observer based on this information despite intensive research there is currently no algorithm that can effectively integrate this information this problem is np hard but not known to be in np even if we only use left and right relation in this paper we present a novel qualitative representation starvars that can solve these problem it is an extension of the star calculus renz and mitra by a variable interpretation of the orientation of observer we show that reasoning in starvars is in np and present the first algorithm that allows u to effectively integrate relative direction information from different observer 
in weighted voting game each agent ha a weight and a coalition of player is deemed to be winning if it weight meet or exceeds the given quota an agent s power in such game is usually measured by her shapley value which depends both on the agent s weight and the quota zuckerman et al show that one can alter a player s power significantly by modifying the quota and investigate some of the related algorithmic issue in this paper we answer a number of question that were left open by zuckerman et al we show that even though deciding whether a quota maximizes or minimizes an agent s shapley value is conp hard finding a shapley value maximizing quota is easy minimizing a player s power appears to be more difficult however we propose and evaluate a heuristic for this problem which take into account the voter s rank and the overall weight distribution we also explore a number of other algorithmic issue related to quota manipulation 
in this paper we address the column based low rank matrix approximation problem using a novel parallel approach our approach is based on the divide and combine idea we first perform column selection on submatrices of an original data matrix in parallel and then combine the selected column into the final output our approach enjoys a theoretical relative error upper bound in addition our column based low rank approximation partition data in a deterministic way and make no assumption about matrix coherence compared with other traditional method our approach is scalable on large scale matrix finally experiment on both simulated and real world data show that our approach is both efficient and effective 
the interval algebra ia and a subset of the region connection calculus rcc namely rcc are the dominant artificial intelligence approach for representing and reasoning about qualitative temporal and topological relation respectively such qualitative information can be formulated a a qualitative constraint network qcn in this paper we focus on the minimal labeling problem mlp and we propose an algorithm to efficiently derive all the feasible base relation of a qcn our algorithm considers chordal qcns and a new form of partial consistency which we define a g consistency further the proposed algorithm us tractable subclass of relation having a specific patchwork property for which consistency implies the consistency of the input qcn experimentation with qcns of ia and rcc show the importance and efficiency of this new approach 
we propose a personalized re ranking algorithm through mining user dwell time derived from a user s previously online reading or browsing activity we acquire document level user dwell time via a customized web browser from which we then infer conceptword level user dwell time in order to understand a user s personal interest according to the estimated concept word level user dwell time our algorithm can estimate a user s potential dwell time over a new document based on which personalized webpage re ranking can be carried out we compare the ranking produced by our algorithm with ranking generated by popular commercial search engine and a recently proposed personalized ranking algorithm the result clearly show the superiority of our method 
in this paper we tackle the challenge of multilabel classification by developing a general conditional dependency network model the proposed model is a cyclic directed graphical model which provides an intuitive representation for the dependency among multiple label variable and a well integrated framework for efficient model training using binary classifier and label prediction using gibbs sampling inference our experiment show the proposed conditional model can effectively exploit the label dependency to improve multilabel classification performance 
in multiagent scheduling each agent ha to schedule it activity to respect it local internal temporal constraint and also to satisfy external constraint between it activity and activity of other agent a scheduling problem is decoupled if each agent can independently and thus privately autonomously etc form a solution to it local problem such that agent combined solution are guaranteed to satisfy all external constraint we expand previous work that decouples multiagent scheduling problem containing strictly conjunctive temporal constraint to more general problem containing disjunctive constraint while this raise a host of challenging issue agent can leverage shared information a early and a often a possible to quickly adopt additional temporal constraint within their local problem that sacrifice some local scheduling flexibility in favor of decoupled independent and rapid local scheduling 
a explained by axelrod in his seminal work an evolutionary approach to norm punishment is a key mechanism to achieve the necessary social control and to impose social norm in a self regulated society in this paper we distinguish between two enforcing mechanism i e punishment and sanction focusing on the specific way in which they favor the emergence and maintenance of cooperation the key research question is to find more stable and cheaper mechanism for norm compliance in hybrid social environment populated by human and computational agent to achieve this task we have developed a normative agent able to punish and sanction defector and to dynamically choose the right amount of punishment and sanction to impose on them dynamic adaptation heuristic the result obtained through agent based simulation show u that sanction is more effective and le costly than punishment in the achievement and maintenance of cooperation and it make the population more resilient to sudden change than if it were enforced only by mere punishment 
in the aviation safety research domain cause identification refers to the task of identifying the possible cause responsible for the incident described in an aviation safety incident report this task present a number of challenge including the scarcity of labeled data and the difficulty in finding the relevant portion of the text we investigate the use of annotator rationale to overcome these challenge proposing several new way of utilizing rationale and showing that through judicious use of the rationale it is possible to achieve significant improvement over a unigram svm baseline 
in a seminal paper lin and reiter introduced the notion of progression of basic action theory unfortunately progression is second order in general recently liu and lakemeyer improve on earlier result and show that for the local effect and normal action case progression is computable but may lead to an exponential blow up nevertheless they show that for certain kind of expressive first order knowledge base with disjunctive information called proper it is efficient however answering query about the resulting state is still undecidable in this paper we continue this line of research and extend proper kb to include function we prove that their progression wrt local effect normal action and range restricted theory is first order definable and efficiently computable we then provide a new logically sound and complete decision procedure for certain kind of query 
we conjecture that the distribution of the time reversed residual of a causal linear process is closer to a gaussian than the distribution of the noise used to generate the process in the forward direction this property is demonstrated for causal ar process assuming that all the cumulants of the distribution of the noise are defined based on this observation it is possible to design a decision rule for detecting the direction of time series that can be described a linear process the correct direction forward in time is the one in which the residual from a linear fit to the time series are le gaussian a series of experiment with simulated and real world data illustrate the superior result of the proposed rule when compared with other state of the art method based on independence test 
in multidimensional classification the goal is to assign an instance to a set of different class this task is normally addressed either by defining a compound class variable with all the possible combination of class label power set method lpms or by building independent classifier for each class binary relevance method brms however lpms do not scale well and brms ignore the dependency relation between class we introduce a method for chaining binary bayesian classifier that combine the strength of classifier chain and bayesian network for multidimensional classification the method consists of two phase in the first phase a bayesian network bn that represents the dependency relation between the class variable is learned from data in the second phase several chain classifier are built such that the order of the class variable in the chain is consistent with the class bn at the end we combine the result of the different generated order our method considers the dependency between class variable and take advantage of the conditional independence relation to build simplified model we perform experiment with a chain of na ve bayes classifier on different benchmark multidimensional datasets and show that our approach outperforms other state of the art method 
how obliged can we be to ai and how much danger doe it pose u a surprising proportion of our society hold exaggerated fear or hope for ai such a the fear of robot world conquest or the hope that ai will indefinitely perpetuate our culture these misapprehension are symptomatic of a larger problem a confusion about the nature and origin of ethic and it role in society while ai technology do pose promise and threat these are not qualitatively different from those posed by other artifact of our culture which are largely ignored from factory to advertising weapon to political system ethical system are based on notion of identity and the exaggerated hope and fear of ai derive from our culture having not yet accommodated the fact that language and reasoning are no longer uniquely human the experience of ai may improve our ethical intuition and self understanding potentially helping our society make better informed decision on serious ethical dilemma 
model based diagnosis mbd us an abstraction of system to diagnose possible faulty function of an underlying system to improve the solution efficiency for multi fault diagnosis problem especially for large scale system this paper proposes a method to induce reasonable diagnosis solution under coarse diagnosis by using the relationship between system output and component compared to existing diagnosis method the proposed framework only need to consider association between output and component by using an assumption based truth maintenance system atm de kleer to obtain correlation component for every output node a a result our method significantly reduces the number of variable required for model diagnosis which make it suitable for large scale circuit system copyright association for the advancement of artificial intelligence all right reserved 
stream of object that are associated with one or more label at the same time appear in many application however stream classification of multi label data is largely unexplored existing approach try to tackle the problem by transferring traditional single label stream classification practice to the multi label domain nevertheless they fail to consider some of the unique property of the problem such a within and between class imbalance and multiple concept drift to deal with these challenge this paper proposes a novel multilabel stream classification approach that employ two window for each label one for positive and one for negative example instance sharing is exploited for space efficiency while a time efficient instantiation based on the k nearest neighbor algorithm is also proposed finally a batch incremental thresholding technique is proposed to further deal with the class imbalance problem result of an empirical comparison against two other method on three real world datasets are in favor of the proposed approach 
monitoring and forecast of global spread of infectious disease is difficult mainly due to lack of fine grained and timely data previous work in computational epidemiology ha shown that mining data from the web can improve the predictability of high level aggregate pattern of epidemic by contrast this paper explores how individual contribute to the global spread of disease we consider the important task of predicting the prevalence of flu like illness in a given city based on interpersonal interaction of the city s resident with the outside world we use the geo tagged status update of traveling twitter user to infer property of the flow of individual between city while previous research considered only the raw volume of passenger we estimate a number of latent variable including the number of sick symptomatic traveler and the number of sick individual to whom each traveler wa exposed we show that ai technique provide insight into the mechanism of disease spread and significantly improve predictability of future flu outbreak our experiment involve over individual traveling between city prior and during a severe ongoing flu epidemic october january our model leverage the text and interpersonal interaction recorded in over million online status update without any active user participation enabling scalable public health application 
existing belief merging operator take advantage of all the model from the base including those contradicting the integrity constraint in this paper we show that this is not suited to every merging scenario we study the case when the base are rationalized with respect to the integrity constraint during the merging process we define in formal term several independence condition for merging operator and show how they interact with the standard ic postulate for belief merging especially we give an independence based axiomatic characterization of a distance based operator 
multiagent planning ha seen much progress with the development of formal model such a dec pomdps however the complexity of these model nexp complete even for two agent ha limited scalability we identify certain mild condition that are sufficient to make multiagent planning amenable to a scalable approximation w r t the number of agent this is achieved by constructing a graphical model in which likelihood maximization is equivalent to plan optimization using the expectation maximization framework for likelihood maximization we show that the necessary inference can be decomposed into process that often involve a small subset of agent thereby facilitating scalability we derive a global update rule that combine these local inference to monotonically increase the overall solution quality experiment on a large multiagent planning benchmark confirm the benefit of the new approach in term of runtime and scalability 
decision theory focus on the problem of making decision under uncertainty this uncertainty arises from the unknown aspect of the state of the world the decision maker is in or the unknown utility function of performing action the uncertainty can be modeled a a probability distribution capturing our belief about the world the decision maker is in upon making new observation the decision maker becomes more confident about this model in addition if there is a prior belief on this uncertainty that may have obtained from similar experiment the bayesian method may be employed the loss incurred by the decision maker can also be utilized for the optimal action selection most machine learning algorithm developed though focus on one of these aspect for learning and prediction either learning the probabilistic model or minimizing the loss in probabilistic model approximate inference the process of obtaining the desired model from the observation when it is not tractable doe not consider the task loss on the other end of the spectrum the common practice in learning is to minimize the task loss without considering the uncertainty of prediction model therefore we investigate the intersection of decision theory and machine learning considering both uncertainty in prediction model and the task loss 
the semi supervised learning usually only predict label for unlabeled data appearing in training data and cannot effectively predict label for testing data never appearing in training set to handle this out of sample problem many inductive method make a constraint such that the predicted label matrix should be exactly equal to a linear model in practice this constraint is too rigid to capture the manifold structure of data motivated by this deficiency we relax the rigid linear embedding constraint and propose to use an elastic embedding constraint on the predicted label matrix such that the manifold structure can be better explored to solve our new objective and also a more general optimization problem we study a novel adaptive loss with efficient optimization algorithm our new adaptive loss minimization method take the advantage of both l norm and l norm and is robust to the data outlier under laplacian distribution and can efficiently learn the normal data under gaussian distribution experiment have been performed on image classification task and our approach outperforms other state of the art method 
the cognitive agent model presented in this paper generates prior and retrospective ownership state for an action based on principle from recent neurological theory a prior ownership state is affected by prediction of the effect of a prepared action and exerts control by strengthening or suppressing actual execution of the action a retrospective ownership state depends on whether the sensed consequence co occur with the predicted consequence and is the basis for acknowledging authorship of action for example in social context it is shown how poor action effect prediction capability can lead to reduced retrospective ownership state a in person suffering from schizophrenia 
in the agm framework alchourr n and makinson a revision function can be defined directly through construction like system of sphere epistemic entrenchment etc or indirectly through a contraction operation via the levi identity a recent trend is to construct agm style contraction and revision function that operate under horn logic a direct construction of horn revision is given in delgrande and peppas however it is unknown whether horn revision can be defined indirectly from horn contraction in this paper we address this problem by obtaining a model based horn revision through the model based horn contraction studied in zhuang and pagnucco our result show that under proper restriction horn revision is definable through horn contraction via the levi identity 
in a seminal paper lin and reiter introduced the notion of progression for basic action theory in the situation calculus earlier work by moore scherl and levesque extended the situation calculus to account for knowledge in this paper we study progression of knowledge in the situation calculus we first adapt the concept of bisimulation from modal logic and extend lin and reiter s notion of progression to accommodate knowledge we show that for physical action progression of knowledge reduces to forgetting predicate in first order modal logic we identify a class of first order modal formula for which forgetting an atom is definable in first order modal logic this class of formula go beyond formula without quantifyingin we also identify a simple case where forgetting a predicate reduces to forgetting a finite number of atom thus we are able to show that for local effect physical action when the initial kb is a formula in this class progression of knowledge is definable in first order modal logic finally we extend our result to the multi agent case 
we propose ccrank the first parallel algorithm for learning to rank targeting simultaneous improvement in learning accuracy and efficiency ccrank is based on cooperative coevolution cc a divide and conquer framework that ha demonstrated high promise in function optimization for problem with large search space and complex structure moreover cc naturally allows parallelization of sub solution to the decomposed subproblems which can substantially boost learning efficiency with ccrank we investigate parallel cc in the context of learning to rank extensive experiment on benchmark in comparison with the state of the art algorithm show that ccrank gain in both accuracy and efficiency copyright association for the advancement of artificial intelligence all right reserved 
recently the use of social and human computing ha witnessed increasing interest in the ai community however in order to harness the true potential of social computing human subject must play an active role in achieving computation in social network and related medium our work proposes an initial desideratum for effective social computing drawing inspiration from artificial intelligence extensive experimentation reveals that several open issue and research question have to be answered before the true potential of social and human computing is achieved we however take a somewhat novel approach by implementing a social network environment where human subject cooperate towards computational problem solving in our social environment human and artificial agent cooperate in their computation task which may lead to a single problem solving social network that potentially allows seamless cooperation among human and machine agent 
the ability to cluster high dimensional categorical data is essential for many machine learning application such a bioinfomatics currently central clustering of categorical data is a difficult problem due to the lack of a geometrically interpretable definition of a cluster center in this paper we propose a novel kernel density based definition using a bayes type probability estimator then a new algorithm called k center is proposed for central clustering of categorical data incorporating a new feature weighting scheme by which each attribute is automatically assigned with a weight measuring it individual contribution for the cluster experimental result on real world data show outstanding performance of the proposed algorithm especially in recognizing the biological pattern in dna sequence 
part of the long lasting cultural heritage of china is the classical ancient chinese poem which follow strict format and complicated linguistic rule automatic chinese poetry composition by program is considered a a challenging problem in computational linguistics and requires high artificial intelligence assistance and ha not been well addressed in this paper we formulate the poetry composition task a an optimization problem based on a generative summarization framework under several constraint given the user specified writing intent the system retrieves candidate term out of a large poem corpus and then order these term to fit into poetry format satisfying tonal and rhythm requirement the optimization process under constraint is conducted via iterative term substitution till convergence and output the subset with the highest utility a the generated poem for experiment we perform generation on large datasets of classic poem from tang and song dynasty of china a comprehensive evaluation using both human judgment and rouge score ha demonstrated the effectiveness of our proposed approach 
online content have become an important medium to disseminate information and express opinion with their proliferation user are faced with the problem of missing the big picture in a sea of irrelevant and or diverse content in this paper we address the problem of information organization of online document collection and provide algorithm that create a structured representation of the otherwise unstructured content we leverage the expressiveness of latent probabilistic model e g topic model and non parametric bayes technique e g dirichlet process and give online and distributed inference algorithm that scale to terabyte datasets and adapt the inferred representation with the arrival of new document this paper is an extended abstract of the acm sigkdd best doctoral dissertation award of ahmed 
it is well known that cheating occurs in sport in cup competition a common type of sport competition one method of cheating is in manipulating the seeding to unfairly advantage a particular team previous empirical and theoretical study of seeding manipulation have focused on competition with unrestricted seeding however real cup competition often place restriction on seedings to ensure fairness wide geographic interest and so on in this paper we perform an extensive empirical study of seeding manipulation under comprehensive and realistic set of restriction a generalized random model of competition problem is proposed this model creates a realistic range of problem instance that are used to identify the set of seeding restriction that are hard to manipulate in practice we end with a discussion of the implication of this work and recommendation for organizing competition so a to prevent or reduce the opportunity for manipulating the seeding 
in this paper we propose a general dimensionality reduction method for data generated from a very broad family of distribution and nonlinear function based on the generalized linear model called generalized linear principal component analysis glpca data of different domain often have very different structure these data can be modeled by different distribution and reconstruction function for example real valued data can be modeled by the gaussian distribution with a linear reconstruction function whereas binary valued data may be more appropriately modeled by the bernoulli distribution with a logit or probit function based on general linear model we propose a unified framework for extracting feature from data of different domain a general optimization algorithm based on natural gradient ascent on distribution manifold is proposed for obtaining the maximum likelihood solution we also present some specific algorithm derived from this framework to deal with specific data modeling problem such a document modeling experimental result of these algorithm on several data set are shown for the validation of glpca 
we propose a framework that add learning for improving plan selection in the popular bdi agent programming paradigm in contrast with previous proposal the approach given here is able to scale up well with the complexity of the agent s plan library technically we develop a novel confidence measure which allows the agent to adjust it reliance on the learning dynamically facilitating in principle infinitely many re learning phase we demonstrate the benefit of the approach in an example controller for energy management 
image annotation datasets are becoming larger and larger with ten of million of image and ten of thousand of possible annotation we propose a strongly performing method that scale to such datasets by simultaneously learning to optimize precision at the top of the ranked list of annotation for a given image and learning a low dimensional joint embedding space for both image and annotation our method called wsabie both outperforms several baseline method and is faster and consumes le memory 
one criticism often advanced against abstract argumentation framework afs is that these consider only one form of interaction between atomic argument specifically that an argument attack another attempt to broaden the class of relationship include bipolar framework where argument support others and abstract dialectical framework adfs the latter allow acceptance of an argument x to be predicated on a given propositional function cx dependent on the corresponding acceptance of it parent i e those y for which y x occurs although offering a richly expressive formalism subsuming both standard and bipolar afs an issue that arises with adfs is whether this expressiveness is achieved in a manner that would be infeasible within standard afs can the semantics used in adfs be mapped to some af semantics how many argument are needed in an af to simulate an adf we show that in a formally defined sense any adf can be simulated by an af of similar size and that this translation can be realised by a polynomial time algorithm 
the growing number of statistical topic model led to the need to better evaluate their output traditional evaluation mean estimate the model s fitness to unseen data it ha recently been proven than the output of human judgment can greatly differ from these measure thus the need for method that better emulate human judgment is stringent in this paper we present a system that computes the conceptual relevance of individual topic from a given model on the basis of information drawn from a given concept hierarchy in this case wordnet the notion of conceptual relevance is regarded a the ability to attribute a concept to each topic and separate word related to the topic from the unrelated one based on that concept in multiple experiment we prove the correlation between the automatic evaluation method and the answer received from human evaluator for various corpus and difficulty level by changing the evaluation focus from a statistical one to a conceptual one we were able to detect which topic are conceptually meaningful and rank them accordingly 
we present a new approach to token level causal reasoning that we call sequence of mechanism som which model causality a a dynamic sequence of active mechanism that chain together to propagate causal influence through time we motivate this approach by using example from ai and robotics and show why existing approach are inadequate we present an algorithm for causal reasoning based on som which take a input a knowledge base of first order mechanism and a set of observation and it hypothesizes which mechanism are active at what time we show empirically that our algorithm produce plausible causal explanation of simulated observation generated from a causal model we argue that the som approach is qualitatively closer to the human causal reasoning process for example it will only include relevant variable in explanation we present new insight about causal reasoning that become apparent with this view one such insight is that observation and manipulation do not commute in causal model a fact which we show to be a generalization of the equilibration manipulation commutability of dash 
in many problem setting for example on graph domain online learning algorithm on stream of data need to respect strict time constraint dictated by the throughput on which the data arrive when only a limited amount of memory budget is available a learning algorithm will eventually need to discard some of the information used to represent the current solution thus negatively affecting it classification performance more importantly the overhead due to budget management may significantly increase the computational burden of the learning algorithm in this paper we present a novel approach inspired by the passive aggressive and the lossy counting algorithm our algorithm us a fast procedure for deleting the le influential feature moreover it is able to estimate the weighted frequency of each feature and use it for prediction 
we propose a new clustering based low rank matrix approximation method cluster indicator decomposition cid which yield more accurate low rank approximation than previous commonly used singular value decomposition and other nystr m style decomposition our model utilizes the intrinsic structure of data and theoretically be more compact and accurate than the traditional low rank approximation approach the reconstruction in cid is extremely fast leading to a desirable advantage of our method in large scale kernel machine like support vector machine in which the reconstruction of the kernel need to be frequently computed experimental result indicate that our approach compress image much more efficiently than other factorization based method we show that combining our method with support vector machine obtains more accurate approximation and more accurate prediction while consuming much le computation resource 
qualitative model are predictive model that describe how change in value of input variable affect the output variable in qualitative term e g increasing or decreasing we describe pad a new method for qualitative learning which estimate partial derivative of the target function from training data and us them to induce qualitative model of the target function we formulated three method for computation of derivative all based on using linear regression on local neighbourhood the method were empirically tested on artificial and real world data we also provide a case study which show how the developed method can be used in practice 
human robot interaction hri is an active field of integrating and embedding different technique in artificial intelligence this paper describes my research topic on control of robotic system for safe interaction with human operator it consists of online motion generation for robotic manipulator interactingwith dynamic obstacle and human using a moving horizon scheme modeling and long term prediction of human motion using probabilistic model and reachability analysis and development of an hri demonstration platform 
nonmonotonic description logic dl program support rule based reasoning on top of description logic ontology using a well defined query interface however the interaction of the rule and the ontology may cause inconsistency such that no answer set i e model exists we thus consider repairing dl program i e changing formula to obtain consistency viewing the data part of the ontology a the source of inconsistency we define program repair and repair answer set based on change to it we analyze the complexity of the notion and we extend an algorithm for evaluating dl program to compute repair answer set under optional selection of preferred repair the extension involves a generalized ontology repair problem in which the entailment and non entailment of set of query with update to the ontology must be achieved while this is intractable in general we identify for the description logic dl litea some tractable class of preferred repair that are useful in practice 
most existing work on learning planning model assumes that the entire model need to be learned from scratch a more realistic situation is that the planning agent ha an incomplete model which it need to refine through learning in this paper we propose and evaluate a method for doing this our method take a input an incomplete model with missing precondition and effect in the action a well a a set of plan trace that are known to be correct it output a refined model that not only capture additional precondition effect knowledge about the given action but also macro action we use a max sat framework for learning where the constraint are derived from the executability of the given plan trace a well a the precondition effect of the given incomplete model unlike traditional macro action learner which use macro to increase the efficiency of planning in the context of a complete model our motivation for learning macro is to increase the accuracy robustness of the plan generated with the refined model we demonstrate the effectiveness of our approach through a systematic empirical evaluation 
recently a euclidean heuristic eh ha been proposed for a search eh exploit manifold learning method to construct an embedding of the state space graph and derives an admissible heuristic distance between two state from the euclidean distance between their respective embedded point eh ha shown good performance and memory efficiency in comparison to other existing heuristic such a differential heuristic however it potential ha not been fully explored in this paper we propose a number of technique that can significantly improve the quality of eh we propose a goal oriented manifold learning scheme that optimizes the euclidean distance to goal in the embedding while maintaining admissibility and consistency we also propose a state heuristic enhancement technique to reduce the gap between heuristic and true distance the enhanced heuristic is admissible but no longer consistent we then employ a modified search algorithm known a b algorithm that achieves optimality with inconsistent heuristic using consistency check and propagation we demonstrate the effectiveness of the above technique and report un matched reduction in search cost across several non trivial benchmark search problem copyright association for the advancement of artificial intelligence www aaai org all right reserved 
occlusion are a central phenomenon in multiobject computer vision however formal analysis los roc proposed in the spatial reasoning literature ignore many distinction crucial to computer vision a a result of which these algebra have been largely ignored in vision application two distinction of relevance to visual computation are a whether the occluder is a moving object or part of the static background and b whether the visible part of an object is a connected blob or fragmented in this work we develop a formal model of occlusion state that combine these criterion with overlap distinction modeled in spatial reasoning to come up with a comprehensive set of fourteen occlusion state which we define a ocs transition between these occlusion state are an important source of information on visual activity e g split and merges we show that the resulting formalism is representationally complete in the sense that these state constitute a partition of all possible occlusion situation based on these criterion finally we show result from implementation of this approach in a test application involving static camera based scene analysis where occlusion state analysis and multiple object tracking can be used for two task a identifying static occluders and b modeling a class of interaction represented a transition of occlusion state thus the formalism is shown to have direct relevance to actual vision application 
in this paper we model the pair wise similarity of a set of document a a weighted network with a single cutoff parameter such a network can be thought of an ensemble of unweighted graph each consisting of edge with weight greater than the cutoff value we look at this network ensemble a a complex system with a temperature parameter and refer to it a a latent network our experiment on a number of datasets from two different domain show that certain property of latent network like clustering coefficient average shortest path and connected component exhibit pattern that are significantly divergent from randomized network we explain that these pattern reflect the network phase transition a well a the existence of a community structure in document collection using numerical analysis we show that we can use the aforementioned network property to predicts the clustering normalized mutual information nmi with high correlation finally we show that our clustering method significantly outperforms other baseline method nmi 
the knowledge compilation map introduced by darwiche and marquis take advantage of a number of concept mainly query transformation expressiveness and succinctness to compare the relative adequacy of representation language to some ai problem however the framework is limited to the comparison of language that are interpreted in a homogeneous way formul are interpreted a boolean function this prevents one from comparing on a formal basis language that are close in essence such a obdd mdd and add to fill the gap we present a generalized framework into which comparing formally heterogeneous representation language becomes feasible in particular we explain how the key notion of query and transformation expressiveness and succinctness can be lifted to the generalized setting 
we introduce a novel algorithm called upper confidence weighted learning ucwl for online multiclass learning from binary feedback ucwl combine the upper confidence bound ucb framework with the soft confidence weighted scw online learning scheme ucwl achieves state of the art performance especially on noisy and nonseparable data with low computational cost estimated confidence interval are used for informed exploration which enables faster learning than the uninformed exploration case or the case where exploration is not used the targeted application setting is human robot interaction hri in which a robot is learning to classify it observation while a human teach it by providing only binary feedback e g right wrong result in an hri experiment and with two benchmark datasets show ucwl outperforms other algorithm in the online binary feedback setting and surprisingly even sometimes beat state of the art algorithm that get full feedback while ucwl get only binary feedback on the same data 
intensification and diversification are the key factor that control the performance of stochastic local search in satisfiability sat recently novelty walk ha become a popular method for improving diversification of the search and so ha been integrated in many well known sat solver such a tnm and gnovelty in this paper we introduce new heuristic to improve the effectiveness of novelty walk in term of reducing search stagnation in particular we use weight based on statistical information collected during the search to focus the diversification phase onto specific area of interest with a given probability we select the most frequently unsatisfied clause instead of a totally random one a novelty walk doe amongst all the variable appearing in the selected clause we then select the least flipped variable for the next move our experimental result show that the new weight enhanced diversification method significantly improves the performance of gnovelty and thus outperforms other local search sat solver on a wide range of structured and random satisfiability benchmark 
authoring tutorial for complex software application is a time consuming process it also highly depends on the tutorial designer s skill level and experience this paper introduces an approach which automatically generates software tutorial using the digital artifact produced by the user of a software program we model this process a an optimal planning problem using software produced artifact software specification and the human computer interaction keystroke level model klm we present tutorialplan an automated tutorial generator which creates step by step text and image instruction from cad drawing and help user learn autocad a complex design and drafting software in our tutorial generator the optimal planning problem is represented and solved using dlv a general answer set programming asp system dlv offer a natural representation of both the problem and the heuristic needed to solve it efficiently a user study show that the tutorial generated by our system are comparable to those generated by experienced autocad user 
the beth definability property a well known property from classical logic is investigated in the context of description logic dl if a general ltbox implicitly defines an l concept in term of a given signature where l is a dl then doe there always exist over this signature an explicit definition in l for the concept this property ha been studied before and used to optimize reasoning in dl in this paper a complete classification of beth definability is provided for extension of the basic dl alc with transitive role inverse role role hierarchy and or functionality restriction both on arbitrary and on finite structure moreover we present a tableau based algorithm which computes explicit definition of at most double exponential size this algorithm is optimal because it is also shown that the smallest explicit definition of an implicitly defined concept may be double exponentially long in the size of the input tbox finally if explicit definition are allowed to be expressed in first order logic then we show how to compute them in exptime 
in an introductory engineering course with an annual enrollment of over student a professor ha little option but to rely on multiple choice exam for midterm and final furthermore the teaching assistant are too overloaded to give detailed feedback on submitted homework assignment we introduce mechanix a computer assisted tutoring system for engineering student mechanix us recognition of freehand sketch to provide instant detailed and formative feedback a the student progress through each homework assignment quiz or exam free sketch recognition technique allow student to solve free body diagram and static truss problem a if they were using a pen and paper the same recognition algorithm enable professor to add new unique problem simply by sketching out the correct answer mechanix is able to ease the burden of grading so that instructor can assign more free response question which provide a better measure of student progress than multiple choice question do 
we argue for an alternative paradigm in evaluating machine translation quality that is strongly empirical but more accurately reflects the utility of translation by returning to a representational foundation based on ai oriented lexical semantics rather than the superficial flat n gram and string representation recently dominating the field driven by such metric a bleu and wer current smt frequently produce unusable translation where the semantic event structure is mistranslated who did what to whom when where why and how we argue that it is time for a new generation of more intelligent automatic and semi automatic metric based clearly on getting the structure right at the lexical semantics level we show empirically that it is possible to use simple propbank style semantic frame representation to surpass all currently widespread metric correlation to human adequacy judgment including even hter we also show that replacing human annotator with automatic semantic role labeling still yield much of the advantage of the approach we combine the best of both world from an smt perspective we provide superior yet low cost quantitative objective function for translation quality and yet from an ai perspective we regain the representational transparency and clear reflection of semantic utility of structural frame based knowledge representation 
the possibility that influenza activity can be generally detected through search log analysis ha been explored in recent year however previous study have mainly focused on influenza and little attention ha been paid to other epidemic with an analysis of web user behavior data we consider the problem of predicting the tendency of hand foot and mouth disease hfmd whose outbreak in resulted in a great panic in china in addition to search query we consider user interaction with search engine given the collected search log we cluster hfmd related search query medical page and news report into the following set epidemic related query erqs epidemic related page erps and epidemic related news ern furthermore we count their own frequency a different feature and we conduct a regression analysis with current hfmd occurrence the experimental result show that these feature exhibit good performance on both accuracy and time line 
in many of the possible application a well a the theoretical model of computational social choice the agent preference are represented a partial order in this paper we extend the maximum likelihood approach for defining optimal voting rule to this setting we consider distribution in which the pairwise comparison incomparabilities between alternative are drawn i i d we call such model pairwise independent model and show that they correspond to a class of voting rule that we call pairwise scoring rule this generalizes rule such a kemeny and borda moreover we show that borda is the only pairwise scoring rule that satisfies neutrality when the outcome space is the set of all alternative we then study which voting rule defined for linear order can be extended to partial order via our mle model we show that any weakly neutral outcome scoring rule including any ranking candidate scoring rule based on the weighted majority graph can be represented a the mle of a weakly neutral pairwise independent model therefore all such rule admit natural extension to profile of partial order finally we propose a specific mle model k for generating a set of k winning alternative and study the computational complexity of winner determination for the mle of k 
one approach for artificially intelligent agent wishing to maximise some performance metric in a given domain is to learn from a collection of training data that consists of action or decision made by some expert in an attempt to imitate that expert s style we refer to this type of agent a an expert imitator in this paper we investigate whether performance can be improved by combining decision from multiple expert imitator in particular we investigate two existing approach for combining decision the first approach combine decision by employing ensemble voting between multiple expert imitator the second approach dynamically selects the best imitator to use at runtime given the performance of the imitator in the current environment we investigate these approach in the domain of computer poker in particular we create expert imitator for limit and no limit texas hold em and determine whether their performance can be improved by combining their decision using the two approach listed above 
in recent year the summarisation and decomposition of social network ha become increasingly popular from community finding to role equivalence however these approach concentrate on one type of model only generalised blockmodelling decomposes a network into independent interpretable labeled block where the block label summarise the relationship between two set of user existing algorithm for fitting generalised blockmodels do not scale beyond network of vertex in this paper we introduce two new algorithm one based on genetic algorithm and the other on simulated annealing that is at least two order of magnitude faster than existing algorithm and obtaining similar accuracy using synthetic and real datasets we demonstrate their efficiency and accuracy and show how generalised block modelling and our new approach enable tractable network summarisation and modelling of medium sized network 
a challenging problem in machine learning information retrieval and computer vision research is how to recover a low rank representation of the given data in the presence of outlier and missing entry the l norm low rank matrix factorization lrmf ha been a popular approach to solving this problem however l norm lrmf is difficult to achieve due to it non convexity and non smoothness and existing method are often inefficient and fail to converge to a desired solution in this paper we propose a novel cyclic weighted median cwm method which is intrinsically a coordinate decent algorithm for l norm lrmf the cwm method minimizes the objective by solving a sequence of scalar minimization sub problem each of which is convex and can be easily solved by the weighted median filter the extensive experimental result validate that the cwm method outperforms state of the art in term of both accuracy and computational efficiency copyright association for the advancement of artificial intelligence www aaai org all right reserved 
a key issue for the realization of the smart grid vision is the implementation of effective demand side management one possible approach involves exposing dynamic energy price to end user in this paper we consider a resulting problem on the user s side how to adaptively heat a home given dynamic price the user face the challenge of having to react to dynamic price in real time trading off his comfort with the cost of heating his home to a certain temperature we propose an active learning approach to adjust the home temperature in a semi automatic way our algorithm learns the user s preference over time and automatically adjusts the temperature in real time a price change in addition the algorithm asks the user for feedback once a day to find the best query time the algorithm solves an optimal stopping problem via simulation we show that our algorithm learns user preference quickly and that using the expected utility loss a the query criterion outperforms standard approach from the active learning literature 
multi label classification where each instance is assigned to multiple category is a prevalent problem in data analysis however annotation of multi label instance are typically more time consuming or expensive to obtain than annotation of single label instance though active learning ha been widely studied on reducing labeling effort for single label problem current research on multi label active learning remains in a preliminary state in this paper we first propose two novel multi label active learning strategy a max margin prediction uncertainty strategy and a label cardinality inconsistency strategy and then integrate them into an adaptive framework of multi label active learning our empirical result on multiple multilabel data set demonstrate the efficacy of the proposed active instance selection strategy and the integrated active learning approach 
uncertainty is unavoidable when modeling most application domain in medicine for example symptom such a pain dizziness or nausea are always subjective and hence imprecise and incomparable additionally concept and their relationship may be inexpressible in a crisp clear cut manner we extend the description logic alc with multi valued semantics based on lattice that can handle uncertainty on concept a well a on the axiom of the ontology we introduce reasoning method for this logic w r t general concept inclusion and show that the complexity of reasoning is not increased by this new semantics 
heuristic used for solving hard real time search problem have region with depression such region are bounded area of the search space in which the heuristic function is exceedingly low compared to the actual cost to reach a solution real time search algorithm easily become trapped in those region since the heuristic value of state in them may need to be updated multiple time which result in costly solution state of theart real time search algorithm like l lrta lrta k etc improve lrta s mechanism to update the heuristic resulting in improved performance those algorithm however do not guide search towards avoiding or escaping depressed region this paper present depression avoidance a simple real time search principle to guide search towards avoiding state that have been marked a part of a heuristic depression we apply the principle to l lrta producing als lrta a new real time search algorithm whose search is guided towards exiting region with heuristic depression we show our algorithm outperforms l lrta in standard real time benchmark in addition we prove als lrta ha most of the good theoretical property of l lrta 
thanks to the idea of social collaboration wikipedia ha accumulated vast amount of semi structured knowledge in which the link structure reflects human s cognition on semantic relationship to some extent in this paper we proposed a novel method rcrank to jointly compute concept concept relatedness and concept category relatedness base on the assumption that information carried in concept concept link and concept category link can mutually reinforce each other different from previous work rcrank can not only find semantically related concept but also interpret their relation by category experimental result on concept recommendation and relation interpretation show that our method substantially outperforms classical method 
during the past decade finite mixture modeling ha become a well established technique in data analysis and clustering this paper focus on developing a variational inference framework to learn finite beta liouville mixture model that have been proposed recently a an efficient way for proportional data clustering in contrast to the conventional expectation maximization em algorithm commonly used for learning finite mixture model the proposed algorithm ha the advantage that it is more efficient from a computational point of view and by preventing over and under fitting problem moreover the complexity of the mixture model i e the number of component can be determined automatically and simultaneously with the parameter estimation in a closed form a part of the bayesian inference procedure the merit of the proposed approach are shown using both artificial data set and two interesting and challenging real application namely dynamic texture clustering and facial expression recognition 
the paper introduces a logical framework for negotiation among dishonest agent the framework relies on the use of abductive logic programming a a knowledge representation language for agent to deal with incomplete information and preference the paper show how intentionally false or inaccurate information of agent could be encoded in the agent knowledge base such disinformation can be effectively used in the process of negotiation to have desired outcome by agent the negotiation process are formulated under the answer set semantics of abductive logic programming and enable the exploration of various strategy that agent can employ in their negotiation 
activity recognition aim to identify and predict human activity based on a series of sensor reading in recent year machine learning method have become popular in solving activity recognition problem a special difficulty for adopting machine learning method is the workload to annotate a large number of sensor reading a training data labeling sensor reading for their corresponding activity is a time consuming task in practice we often have a set of labeled training instance ready for an activity recognition task if we can transfer such knowledge to a new activity recognition scenario that is different from but related to the source domain it will ease our effort to perform manual labeling of training data for the new scenario in this paper we propose a transfer learning framework based on automatically learning a correspondence between different set of sensor to solve this transfer learning in activity recognition problem we validate our framework on two different datasets and compare it against previous approach of activity recognition and demonstrate it effectiveness 
we propose a novel probabilistic model for collaborative filtering cf called srmcofi which seamlessly integrates both linear and bilinear random effect into a principled framework the formulation of srmcofi is supported by both social psychological experiment and statistical theory not only can many existing cf method be seen a special case of srmcofi but it also integrates their advantage while simultaneously overcoming their disadvantage the solid theoretical foundation of srmcofi is further supported by promising empirical result obtained in extensive experiment using real cf data set on movie rating copyright association for the advancement of artificial intelligence all right reserved 
recommender system especially the newly launched one have to deal with the data sparsity issue where little existing rating information is available recently transfer learning ha been proposed to address this problem by leveraging the knowledge from related recommender system where rich collaborative data are available however most previous transfer learning model assume that entity correspondence across different system are given a input which mean that for any entity e g a user or an item in a target system it corresponding entity in a source system is known this assumption can hardly be satisfied in real world scenario where entity correspondence across system are usually unknown and the cost of identifying them can be expensive for example it is extremely difficult to identify whether a user a from facebook and a user b from twitter are the same person in this paper we propose a framework to construct entity correspondence with limited budget by using active learning to facilitate knowledge transfer across recommender system specifically for the purpose of maximizing knowledge transfer we first iteratively select entity in the target system based on our proposed criterion to query their correspondence in the source system we then plug the actively constructed entity correspondence mapping into a general transferred collaborative filtering model to improve recommendation quality we perform extensive experiment on real world datasets to verify the effectiveness of our proposed framework for this crosssystem recommendation problem copyright association for the advancement of artificial intelligence www aaai org all right reserved 
my thesis research focus on developing tool and technique in the robotic science to study and understand large scale dynamic coastal process that are driven by global climate change a a first step my work target harmful algal bloom habs which have significant societal and economic impact to coastal community yet are poorly understood ecologically because of undersampling 
in this paper we propose the new ball ranking machine brms to address the supervised ranking problem in previous work supervised ranking method have been successfully applied in various information retrieval task among these methodology the ranking support vector machine rank svms are well investigated however one major fact limiting their application is that ranking svms need optimize a margin based objective function over all possible document pair within all query on the training set in consequence ranking svms need select a large number of support vector among a huge number of support vector candidate this paper introduces a new model of of ranking svms and develops an efficient approximation algorithm which decrease the training time and generates much fewer support vector empirical study on synthetic data and content based image video retrieval data show that our method is comparable to ranking svms in accuracy but use much fewer ranking support vector and significantly le training time 
lifted message passing algorithm exploit repeated structure within a given graphical model to answer query efficiently given evidence they construct a lifted network of supernodes and superpotentials corresponding to set of node and potential that are indistinguishable given the evidence recently efficient algorithm were presented for updating the structure of an existing lifted network with incremental change to the evidence in the inference stage however current algorithm need to construct a separate lifted network for each evidence case and run a modified message passing algorithm on each lifted network separately consequently symmetry across the inference task are not exploited in this paper we present a novel lifted message passing technique that exploit symmetry across multiple evidence case the benefit of this multi evidence lifted inference are shown for several important ai task such a computing personalized pageranks and kalman filter via multievidence lifted gaussian belief propagation 
automated planning is the process of automatically selecting action that achieve a desired outcome this paper summarises several contribution that improve the efficiency of automated planning via heuristic search we discus novel heuristic based on landmark and a search algorithm for anytime planning furthermore we analyse various search enhancement technique and show how the combination of these technique lead to a planning system that proved highly successful in the and international planning competition 
we propose a new approach based on model relaxation to compute minimum cardinality diagnosis of a faulty system we obtain a relaxed model of the system by splitting node in the system and compile the abstraction of the relaxed model into dnnf abstraction is obtained by treating self contained sub system called cone a single component we then use a novel branch and bound search algorithm and compute the abstract minimum cardinality diagnosis of the system which are later refined hierarchically in a careful manner to get all minimum cardinality diagnosis of the system experiment on iscas benchmark circuit show that the new approach is faster than the previous state of the art hierarchical approach and scale to all circuit in the suite for the first time 
we study an interesting phenomenon of social influence locality in a large microblogging network which suggests that user behavior are mainly influenced by close friend in their ego network we provide a formal definition for the notion of social influence locality and develop two instantiation function based on pairwise influence and structural diversity the defined influence locality function have strong predictive power without any additional feature we can obtain a f score of for predicting user retweet behavior by training a logistic regression classifier based on the defined function our analysis also reveals several intriguing discovery for example though the probability of a user retweeting a microblog is positively correlated with the number of friend who have retweeted the microblog it is surprisingly negatively correlated with the number of connected circle that are formed by those friend 
existing language in the valued decision diagram vdds family including add aadd and those of the sldd family prove to be valuable target language for compiling multivariate function however their efficiency is directly related to the size of the compiled formula in practice the existence of canonical form may have a major impact on the size of the compiled vdds while efficient normalization procedure have been pointed out for add and aadd the canonicity issue for sldd formula ha not been addressed so far in this paper the sldd family is revisited we modify the algebraic requirement imposed on the valuation structure so a to ensure tractable conditioning optimization and normalization for some language of the revisited sldd family we show that aadd is captured by this family finally we compare the spatial efficiency of some language of this family from both the theoretical side and the practical side 
in this paper we address the relation between domain difference and domain adaptation for dependency parsing our quantitative analysis showed that it is the inconsistent behavior of same feature cross domain rather than word or feature coverage that is the major cause of performance decrease of out domain model we further studied those ambiguous feature in depth and found that the set of ambiguous feature is small and ha concentric distribution based on the analysis we proposed a da method the da method can automatically learn which feature are ambiguous cross domain according to error made by out domain model on in domain training data our method is also extended to utilize multiple out domain model the result of dependency parser adaptation from wsj to genia and question bank showed that our method achieved significant improvement on small in domain datasets where da is mostly in need additionally we achieved improvement on the published best result of conll shared task on domain adaptation which confirms the significance of our analysis and our method 
localization of a mobile robot is crucial for autonomous navigation using laser scanner this can be facilitated by the pairwise alignment of consecutive scan in this paper we are interested in improving this scan alignment in challenging natural environment for this purpose local descriptor are generally effective a they facilitate point matching however we show that in some natural environment many of them are likely to be unreliable which affect the accuracy and robustness of the result therefore we propose to filter the unreliable descriptor a a prior step to alignment our approach us a fast machine learning algorithm trained on the fly under the positive and unlabeled learning paradigm without the need for human intervention our result show that the number of descriptor can be significantly reduced while increasing the proportion of reliable one thus speeding up and improving the robustness of the scan alignment process 
in human conversation meaning is transported through several channel such a verbal and nonverbal behavior certain of these behavioral aspect are culturally dependent mutual understanding or acceptance is thus amongst others depended on the cultural background of the interlocutor when designing virtual character behavior culture should be considered a it may improve the character s acceptance by user of certain cultural background this paper proposes a hybrid approach for the generation of culture specific behavior in a multiagent system a computational model ha been established by refining theoretical knowledge of culture specific behavior with statistical data extracted from a video corpus of german and japanese first time meeting evaluation study of such culturally enhanced virtual character were conducted in both targeted culture result indicate that human observer tend to prefer character behavior that wa designed to resemble their own cultural background 
we establish complexity of the conjunctive query entailment problem for class of existential rule i e tuple generating dependency or datalog rule our contribution is twofold first we introduce the class of greedy bounded treewidth set gbts which cover guarded rule and their known generalization namely weakly frontier guarded rule we provide a generic algorithm for query entailment with gbts which is worst case optimal for combined complexity with bounded predicate arity a well a for data complexity second we classify several gbts class whose complexity wa unknown namely frontier one frontier guarded and weakly frontier guarded rule with respect to combined complexity with bounded and unbounded predicate arity and data complexity 
this paper show how to learn general finite state machine representation of activity that function a recognizers of previously unseen instance of activity the central problem is to tell which difference between instance of activity are unimportant and may be safely ignored for the purpose of learning generalized representation of activity we develop a novel way to find the essential part of activity by a greedy kind of multiple sequence alignment and a method to transform the resulting alignment into finite state machine that will accept novel instance of activity with high accuracy 
in this paper we present the exact online updating formula for the generalized inverse of centered matrix the computational cost is o mn for matrix of size m x n experimental result validate the proposed method s accuracy and efficiency copyright association for the advancement of artificial intelligence all right reserved 
markov decision process arise a a natural model for many renewable resource allocation problem in many such problem high stake decision with potentially catastrophic outcome such a the collapse of an entire ecosystem need to be taken by carefully balancing social economic and ecologic goal we introduce a broad class of such mdp model with a risk averse attitude of the decision maker in order to obtain policy that are more balanced with respect to the welfare of future generation we prove that they admit a closed form solution that can be efficiently computed we show an application of the proposed framework to the pacific halibut marine fishery obtaining new and more cautious policy our result strengthen finding of related policy from the literature by providing new evidence that a policy based on periodic closure of the fishery should be employed in place of the one traditionally used that harvest a constant proportion of the stock every year 
we address two significant drawback of state of the art solver of decentralized pomdps decpomdps the reliance on complete knowledge of the model and limited scalability a the complexity of the domain grows we extend a recently proposed approach for solving dec pomdps via a reduction to the maximum likelihood problem which in turn can be solved using em we introduce a model free version of this approach that employ monte carlo em mcem while a na ve implementation of mcem is inadequate in multiagent setting we introduce several improvement in sampling that produce high quality result on a variety of dec pomdp benchmark including large problem with thousand of agent 
several kernel based method for multi task learning have been proposed which leverage relation among task a regularization to enhance the overall learning accuracy these method assume that the task share the same kernel which could limit their application because in practice different task may need different kernel the main challenge of introducing multiple kernel into multiple task is that model from different reproducing kernel hilbert space rkhss are not comparable making it difficult to exploit relation among task this paper address the challenge by formalizing the problem in the square integrable space si specially it proposes a kernel based method which make use of a regularization term defined in the si to represent task relation we prove a new representer theorem for the proposed approach in si we further derive a practical method for solving the learning problem and conduct consistency analysis of the method we discus the relation between our method and an existing method we also give an svm based implementation of our method for multi label classification experiment on two real world data set show that the proposed method performs better than the existing method copyright association for the advancement of artificial intelligence all right reserved 
many ai related reasoning problem are based on the problem of satisfiability sat while sat itself becomes easy when restricting the structure of the formula in a certain way this is not guaranteed for more involved reasoning problem in this work we focus on reasoning task in the area of belief revision and logic based abduction and show that in some case the restriction to krom formula i e formula in cnf where clause have at most two literal decrease the complexity while in others it doe not we thus also consider additional restriction to krom formula towards a better identification of the tractability frontier of such problem 
in a document network such a a citation network of scientific document web log etc the content produced by author exhibit their interest in certain topic in addition some author influence other author interest in this work we propose to model the influence of cited author along with the interest of citing author moreover we hypothesize that apart from the citation present in document the context surrounding the citation mention provides extra topical information about the cited author however associating term in the context to the cited author remains an open problem we propose novel document generation scheme that incorporate the context while simultaneously modeling the interest of citing author and influence of the cited author our experiment show significant improvement over baseline model for various evaluation criterion such a link prediction between document and cited author and quantitatively explaining unseen text 
many ai application are based on some underlying logic that tolerates inconsistent information in a non trivial way however it is not always clear what should be the exact nature of such a logic and how to choose one for a specific application in this paper we formulate a list of desirable property of ideal logic for reasoning with inconsistency identify a variety of logic that have these property and provide a systematic way of constructing for every n a family of such n valued logic 
there are two key issue for information diffusion in blogosphere blog post are usually short noisy and contain multiple theme information diffusion through blogosphere is primarily driven by the word of mouth effect thus making topic evolve very fast this paper present a novel topic tracking approach to deal with these issue by modeling a topic a a semantic graph in which the semantic relatedness between term are learned from wikipedia for a given topic post the name entity wikipedia concept and the semantic relatedness are extracted to generate the graph model noise are filtered out through the graph clustering algorithm to handle topic evolution the topic model is enriched by using wikipedia a background knowledge furthermore graph edit distance is used to measure the similarity between a topic and it post the proposed method is tested by using the real world blog data experimental result show the advantage of the proposed method on tracking the topic in short noisy text 
building on recent research on preference handling in artificial intelligence and related field our goal is to develop a coherent and generic methodological framework for case based reasoning cbr on the basis of formal concept and method for knowledge representation and reasoning with preference a preference based approach to cbr appears to be appealing for several reason notably because case based experience naturally lend themselves to representation in term of preference or order relation moreover the flexibility and expressiveness of a preference based formalism well accommodate the uncertain and approximate nature of case based problem solving in this paper we outline the basic idea of preference based cbr and sketch a formal framework for realizing these idea 
multi task learning is a way of bringing inductive transfer studied in human learning to the machine learning community a central issue in multitask learning is to model the relationship between task appropriately and exploit them to aid the simultaneous learning of multiple task effectively while some recent method model and learn the task relationship from data automatically only pairwise relationship can be represented by them in this paper we propose a new model called multi task high order relationship learning mthol which extends in a novel way the use of pairwise task relationship to high order task relationship we first propose an alternative formulation of an existing multi task learning method based on the new formulation we propose a high order generalization leading to a new prior for the model parameter of different task we then propose a new probabilistic model for multi task learning and validate it empirically on some benchmark datasets 
in this paper i present my ongoing research on temporal defeasible argumentation based multi agent planning in multi agent planning a team of agent share a set of goal but have diverse ability and temporal belief which vary over time in order to plan for these goal agent start a stepwise dialogue consisting of exchange of temporal plan proposal plus temporal argument against them where both action with different duration and temporal defeasible argument need to be integrated this thesis proposes a computational framework for this research on multi agent planning 
the area under the roc curve auc metric ha achieved a big success in binary classification problem since they measure the performance of classifier without making any specific assumption about the class distribution and misclassification cost this is desirable because the class distribution and misclassification cost may be unknown during training process or even change in environment mauc the extension of auc to multi class problem ha also attracted a lot of attention however despite the emergence of approach for training classifier with large auc little ha been done for mauc this paper analyzes mauc in depth and reveals that the maximization of mauc can be achieved by decomposing the multi class problem into a number of independent sub problem these sub problem are formulated in the form of a learning to rank problem for which well established method already exist based on the analysis a method that employ rankboost algorithm a the sub problem solver is proposed to achieve classification system with maximum mauc empirical study have shown the advantage of the proposed method over other eight relevant method due to the importance of mauc to multi class cost sensitive learning and class imbalanced learning problem the proposed method is a general technique for both problem it can also be generalized to accommodate other learning algorithm a the subproblem solver copyright association for the advancement of artificial intelligence www aaai org all right reserved 
goal driven autonomy gda is a reflective model of goal reasoning that control the focus of an agent s planning activity by dynamically resolving unexpected discrepancy in the world state which frequently arise when solving task in complex environment gda agent have performed well on such task by integrating method for discrepancy recognition explanation goal formulation and goal management however they require substantial domain knowledge including what constitutes a discrepancy and how to resolve it we introduce lgda a learning algorithm for acquiring this knowledge modeled a case that and integrates case based reasoning and reinforcement learning method we ass it utility on task from a complex video game environment we claim that for these task lgda can significantly outperform it ablation our evaluation provides evidence to support this claim lgda exemplifies a feasible design methodology for deployable gda agent 
we present probabilistic symbol refined tree substitution grammar sr tsg for statistical parsing of natural language sentence an sr tsg is an extension of the conventional tsg model where each nonterminal symbol can be refined subcategorized to fit the training data our probabilistic model is consistent based on the hierarchical pitman yor process to encode backoff smoothing from a fine grained sr tsg to simpler cfg rule thus all grammar rule can be learned from training data in a fully automatic fashion our sr tsg parser achieves the state of the art performance on the wall street journal wsj english penn tree bank data 
given it ubiquity scale and complexity few problem have created the combined interest of so many unrelated area a the evolution of cooperation using the tool of evolutionary game theory here we address for the first time the role played by intention recognition in the final outcome of cooperation in large population of self regarding individual by equipping individual with the capacity of assessing intention of others in the course of repeated prisoner s dilemma interaction we show how intention recognition open a window of opportunity for cooperation to thrive a it precludes the invasion of pure cooperator by random drift while remaining robust against defective strategy intention recognizers are able to assign an intention to the action of their opponent based on an acquired corpus of possible intention we show how intention recognizers can prevail against most famous strategy of repeated dilemma of cooperation even in the presence of error our approach invite the adoption of other classification and pattern recognition mechanism common among human to unveil the evolution of complex cognitive process in the context of social dilemma 
we address the problem of transferring information learned from experiment to a different environment in which only passive observation can be collected we introduce a formal representation called selection diagram for expressing knowledge about difference and commonality between environment and using this representation we derive procedure for deciding whether effect in the target environment can be inferred from experiment conducted elsewhere when the answer is affirmative the procedure identify the set of experiment and observation that need be conducted to license the transport we further discus how transportability analysis can guide the transfer of knowledge in non experimental learning to minimize re measurement cost and improve prediction power 
the problem of grasping is widely studied in the robotics community this project focus on the identification of object graspable feature using image and object structural information the primary aim is the creation of a framework in which the information gathered by the vision system can be integrated with automatically generated knowledge modelled by mean of fuzzy description logic 
the number partitioning problem seek to divide a set of n number across k distinct subset so a to minimize the sum of the largest partition in this work we develop a new optimal algorithm for multi way number partitioning a critical observation motivating our methodology is that a globally optimal k way partition may be recursively constructed by obtaining suboptimal solution to subproblems of size k we introduce a new principle of optimality that provides necessary and sufficient condition for this construction and use it to strengthen the relationship between sequential decomposition by enforcing upper and lower bound on intermediate solution we also demonstrate how to further prune unpromising partial assignment by detecting and eliminating dominated solution our approach outperforms the previous state of the art by up to four order of magnitude reducing average runtime on the largest benchmark from several hour to le than a second 
given the number of online source for news the volume of news generated are so daunting that gaining insight from these collection become impossible without some aid to link them semantic linking of news article facilitates grouping of similar or relevant news story together for ease of human consumption for example a political analyst may like to have a single view of all news article that report visit of state head of different country to a single country to make an in depth analytical report on the possible impact of all associated event it is likely that no news source link all the relevant news together in this paper we discus a multi resolution multi perspective news analysis system that can link news article collected from diverse source over a period of time the distinctive feature of the proposed news linking system is it capability to simultaneously link news article and story at multiple level of granularity at the lowest level several article reporting the same event are linked together higher level grouping are more contextual and semantic we have deployed a range of algorithm that use statistical text processing and natural language processing technique the system is incremental in nature and depicts how story have evolved over time along with main actor and activity it also illustrates how a single story diverges into multiple theme or multiple story converge due to conceptual similarity accuracy of linking thematically and conceptually linked news article are also presented 
non negative tensor factorization ntf ha attracted great attention in the machine learning community in this paper we extend traditional non negative tensor factorization into a supervised discriminative decomposition referred a supervised non negative tensor factorization with maximum margin constraint sntfm sntfm formulates the optimal discriminative factorization of non negative tensorial data a a coupled least square optimization problem via a maximum margin method a a result sntfm not only faithfully approximates the tensorial data by additive combination of the basis but also obtains a strong generalization power to discriminative analysis in particular for classification in this paper the experimental result show the superiority of our proposed model over state of the art technique on both toy and real world data set copyright association for the advancement of artificial intelligence www aaai org all right reserved 
change point detection is the problem of finding abrupt change in time series and it is attracting a lot of attention in the artificial intelligence and data mining community in this paper we present a supervised learning based change point detection approach in which we use the separability of past and future data at time t they are labeled a and a plausibility of change point based on this framework we propose a detection measure called the additive hilbert schmidt independence criterion ahsic which is defined a the weighted sum of the hsic score between feature and it corresponding binary label here the hsic is a kernel based independence measure a novel aspect of the ahsic score is that it can incorporate feature selection during it detection measure estimation more specifically we first select feature that are responsible for an abrupt change by using a supervised approach and then compute the ahsic score by employing the selected feature thus compared with traditional detection measure our approach tends to be robust a regard noise feature and so the ahsic is suitable for a use with high dimensional time series change point detection problem we demonstrate that the proposed change point detection method is promising through extensive experiment on synthetic data set and a real world human activity data set 
we identify a new representation of propositional knowledge base the sentential decision diagram sdd which is interesting for a number of reason first it is canonical in the presence of additional property that resemble reduction rule of obdds second sdds can be combined using any boolean operator in polytime third cnfs with n variable and treewidth w have canonical sdds of size o n w which is tighter than the bound on obdds based on pathwidth finally every obdd is an sdd hence working with the latter doe not preclude the former 
ontology that evolve through use to support new domain task can grow extremely large moreover large ontology require more resource to use and have slower response time than small one to help address this problem we present an on line semantic forgetting algorithm that remove ontology fragment containing infrequently used or cheap to relearn concept we situate our algorithm in an extension of the widely used robocup rescue platform which provides simulated task to agent we show that our agent send fewer message and complete more task and thus achieve a greater degree of success than other state of the art approach 
many trust model have been proposed to evaluate seller trustworthiness in multiagent e marketplace their performance varies highly depending on environment where they are applied however it is challenging to choose suitable model for environment where ground truth about seller trustworthiness is unknown called unknown environment we propose a novel framework to choose suitable trust model for unknown environment based on the intuition that if a model performs well in one environment it will do so in another similar environment specifically for an unknown environment we identify a similar simulated environment with known ground truth where the trust model performing the best will be chosen a the suitable solution evaluation result confirm the effectiveness of our framework in choosing suitable trust model for different environment 
in this paper we propose to combine three ai technique to speed up a reinforcement learning algorithm in a transfer learning problem case based reasoning heuristically accelerated reinforcement learning and neural network to do so we propose a new algorithm called l which work in stage in the first stage it us reinforcement learning to learn how to perform one task and store the optimal policy for this problem a a case base in the second stage it us a neural network to map action from one domain to action in the other domain and in the third stage it us the case base learned in the first stage a heuristic to speed up the learning performance in a related but different task the rl algorithm used in the first phase is the q learning and in the third phase is the recently proposed case based heuristically accelerated q learning a set of empirical evaluation were conducted in transferring the learning between two domain the acrobot and the robocup d the policy learned during the solution of the acrobot problem is transferred and used to speed up the learning of stability policy for a humanoid robot in the robocup d simulator the result show that the use of this algorithm can lead to a significant improvement in the performance of the agent 
in this paper we revisit the idea of splitting a planning problem into subproblems hopefully easier to solve with the help of landmark analysis while this technique initially proposed in the first approach related to landmark ha been outperformed by landmark based heuristic we believe that it is still a promising research direction to this end we propose a new method for problem splitting based on landmark which ha two advantage over the original technique it is complete if a solution exists the algorithm find it and it us the precedence relation over the landmark in a more flexible way we lay in this paper the foundation of a meta best first search algorithm which explores the landmark ordering to create subproblems and can use any embedded planner to solve subproblems it open up avenue for future research among them are new heuristic for guiding the meta search towards the most promising ordering different policy for generating subproblems and influence of the embedded subplanner 
there are two major us of abstraction in planning and search refinement where abstract solution are extended into concrete solution and heuristic where abstract solution are used to compute heuristic for the original search space these two approach are usually viewed a unrelated in the literature it is reasonable to believe though that they are related since they are both intrinsically based on the structure of abstract search space we take the first step towards formally investigating their relationship employing our recently introduced framework for analysing and comparing abstraction method by adding some mechanism for expressing metric property we can capture concept like admissibility and consistency of heuristic we present an extensive study of how such metric property relate to the property in the original framework revealing a number of connection between the refinement and heuristic approach this also provides new insight into for example valtorta s theorem and spurious state 
this paper describes the strategy used by astoncat plus the post tournament version of the specialist designed for the tac market design tournament it detail how astoncat plus accepts shout clear market set transaction price and charge fee through empirical evaluation we show that astoncat plus not only outperforms astoncat tournament version significantly but also achieves the second best overall score against some top entrant of the competition in particular it achieves the highest allocative efficiency transaction success rate and average trader profit among all the specialist in our controlled experiment 
location play an essential role in our life bridging our online and offline world this paper explores the interplay of people s location interaction and social tie within a large real world dataset we present and evaluate flap a system that solves two intimately related task link and location prediction in online social network for link prediction flap infers social tie by considering pattern in friendship formation the content of people s message and user location we show that while each component is a weak predictor of friendship alone combining them result in a strong model accurately identifying the majority of friendship for location prediction flap implement a scalable probabilistic model of human mobility where we treat user with known gps position a noisy sensor of the location of their friend we explore supervised and unsupervised learning scenario and focus on the efficiency of both learning and inference we evaluate flap on a large sample of highly active user from two distinct geographical area and show that it reconstructs the entire friendship graph with high accuracy even when no edge are given and infers people s fine grained location even when they keep their data private and we can only access the location of their friend our model significantly outperform current approach to either task 
in the last decade latent dirichlet allocation lda successfully discovers the statistical distribution of the topic over a unstructured text corpus meanwhile more and more document data come up with rich human provided tag information during the evolution of the internet which called semistructured data the semi structured data contain both unstructured data e g plain text and metadata such a paper with author and web page with tag in general different tag in a document play different role with their own weight to model such semi structured document is nontrivial in this paper we propose a novel method to model tagged document by a topic model called tag weighted topic model twtm twtm is a framework that leverage the tag in each document to infer the topic component for the document this allows not only to learn document topic distribution but also to infer the tag topic distribution for text mining e g classification clustering and recommendation moreover twtm automatically infers the probabilistic weight of tag for each document we present an efficient variational inference method with an em algorithm for estimating the model parameter the experimental result show that our twtm approach outperforms the baseline algorithm over three corpus in document modeling and text classification 
l regularized least square with the ability of discovering sparse representation is quite prevalent in the field of machine learning statistic and signal processing in this paper we propose a novel algorithm called dual projected newton method dpnm to solve the l regularized least square problem in dpnm we first derive a new dual problem a a box constrained quadratic programming then a projected newton method is utilized to solve the dual problem achieving a quadratic convergence rate moreover we propose to utilize some practical technique thus it greatly reduces the computational cost and make dpnm more efficient experimental result on six real world data set indicate that dpnm is very efficient for solving the l regularized least square problem by comparing it with state of the art method 
data set containing multi manifold structure are ubiquitous in real world task and effective grouping of such data is an important yet challenging problem though there were many study on this problem it is not clear on how to design principled method for the grouping of multiple hybrid manifold in this paper we show that spectral method are potentially helpful for hybrid manifold clustering when the neighborhood graph is constructed to connect the neighboring sample from the same manifold however traditional algorithm which identify neighbor according to euclidean distance will easily connect sample belonging to different manifold to handle this drawback we propose a new criterion i e local and structural consistency criterion which considers the neighboring information a well a the structural information implied by the sample based on this criterion we develop a simple yet effective algorithm named local and structural consistency lsc for clustering with multiple hybrid manifold experiment show that lsc achieves promising performance 
we propose a multi prototype based algorithm for online learning of soft pairwise preference over label the algorithm learns soft label preference via minimization of the proposed soft rank loss measure and can learn from total order a well a from various type of partial order the soft pairwise preference algorithm output are further aggregated to produce a total label ranking prediction using a novel aggregation algorithm that outperforms existing aggregation solution experiment on synthetic and real world data demonstrate state of the art performance of the proposed model 
random walk have become a popular component of recent planning system the increased exploration is a valuable addition to more exploitative search method such a greedy best first search gbfs a number of successful planner which incorporate random walk have been built the work presented here aim to exploit the experience gained from building those system it begin a systematic study of the design space and alternative choice for building such a system and develops a new random walk planner from scratch with careful experiment along the way four major insight are a high state evaluation frequency is usually superior to the endpoint only evaluation used in earlier system adjusting the restarting parameter according to the progress speed in the search space performs better than any fixed setting biasing the action selection towards preferred operator of only the current state is better than monte carlo helpful action which depend on the number of time an action ha been a preferred operator in previous walk and even simple form of random walk planning can compete with gbfs 
imprecise reward markov decision process irmdps are mdps in which the reward function is only partially specified e g by some elicitation process recent work using minimax regret to solve irmdps ha shown despite their theoretical intractability how the set of policy that are nondominated w r t reward uncertainty can be exploited to accelerate regret computation however the number of nondominated policy is generally so large a to undermine this leverage in this paper we show how the quality of the approximation can be improved online by pruning adding nondominated policy during reward elicitation while maintaining computational tractability drawing insight from the pomdp literature we also develop a new anytime algorithm for constructing the set of nondominated policy with provable anytime error bound these bound can be exploited to great effect in our online approximation scheme 
clause learning is a technique used by backtracking based propositional satisfiability solver where some clause obtained by analysis of conflict are added to the formula during backtracking it ha been observed empirically that clause learning doe not significantly improve the performance of a solver when restricted to learning clause of small width only this experience is supported by lower bound theorem it is shown that lower bound on the runtime of width restricted clause learning follow from lower bound on the width of resolution proof this yield the first lower bound on width restricted clause learning for formula in cnf 
we study the complexity of electing a committee under several variant of the chamberlin courant rule when the voter preference are single peaked on a tree we first show that this problem is easy for the egalitarian or minimax version of this problem for arbitrary tree and misrepresentation function for the standard utilitarian version of this problem we provide an algorithm for an arbitrary misrepresentation function whose running time is polynomial in the input size a long a the number of leaf of the underlying tree is bounded by a constant on the other hand we prove that our problem remains computationally hard on tree that have bounded degree diameter or pathwidth finally we show how to modify trick s algorithm to check whether an election is single peaked on a tree whose number of leaf doe not exceed a given parameter 
deep space mission are characterized by severely constrained communication link to meet the need of future mission and increase their scientific return future space system will require an increased level of autonomy on board in this work we propose a comprehensive approach to on board autonomy relying on model based reasoning and encompassing many important reasoning capability such a plan generation validation execution and monitoring fdir and run time diagnosis the controlled platform is represented symbolically and the reasoning capability are seen a symbolic manipulation of such formal model we have developed a prototype of our framework implemented within an on board autonomous reasoning engine we have evaluated our approach on two case study inspired by real world ongoing project and characterized it in term of reliability availability and performance 
one of the most advanced approach to querying data in the presence of ontology is to make use of relational database system rewriting the original query and the ontology into a new query that is formulated in sql or equivalently in first order logic fo for ontology written in many standard description logic dl however such fo rewriting are not guaranteed to exist we study fo rewriting and their existence for a basic class of query and for ontology formulated in horn dl such a horn shi and el our result include characterization of the existence of fo rewriting tight complexity bound for deciding whether an fo rewriting exists exptime and pspace and tight bound on the worst case size of fo rewriting when presented a a union of conjunctive query 
text classification is widely used in many real world application to obtain satisfied classification performance most traditional data mining method require lot of labeled data which can be costly in term of both time and human effort in reality there are plenty of such resource in english since it ha the largest population in the internet world which is not true in many other language in this paper we present a novel transfer learning approach to tackle the cross language text classification problem we first align the feature space in both domain utilizing some on line translation service which make the two feature space under the same coordinate although the feature set in both domain are the same the distribution of the instance in both domain are different which violates the i i d assumption in most traditional machine learning method for this issue we propose an iterative feature and instance weighting bi weighting method for domain adaptation we empirically evaluate the effectiveness and efficiency of our approach the experimental result show that our approach outperforms some baseline including four transfer learning algorithm 
we study the multi armed bandit problem with budget constraint and variable cost mab bv in this setting pulling an arm will receive a random reward together with a random cost and the objective of an algorithm is to pull a sequence of arm in order to maximize the expected total reward with the cost of pulling those arm complying with a budget constraint this new setting model many internet application e g ad exchange sponsored search and cloud computing in a more accurate manner than previous setting where the pulling of arm is either costless or with a fixed cost we propose two ucb based algorithm for the new setting the first algorithm need prior knowledge about the lower bound of the expected cost when computing the exploration term the second algorithm eliminates this need by estimating the minimal expected cost from empirical observation and therefore can be applied to more real world application where prior knowledge is not available we prove that both algorithm have nice learning ability with regret bound of o lnb furthermore we show that when applying our proposed algorithm to a previous setting with fixed cost which can be regarded a our special case one can improve the previously obtained regret bound our simulation result on real time bidding in ad exchange verify the effectiveness of the algorithm and are consistent with our theoretical analysis association for the advancement of artificial intelligence www aaai org all right reserved 
the standard gaussian process gp regression is often intractable when a data set is large or spatially nonstationary in this paper we address these challenging data property by designing a novel k nearest neighbor based kalman filter gaussian process knn kfgp regression based on a state space model established by the knn driven data grouping our knn kfgp recursively filter out the latent function value in a computationally efficient and accurate kalman filtering framework moreover knn allows each test point to find it strongly correlated local training subset so our knn kfgp provides a suitable way to deal with spatial nonstationary problem we evaluate the performance of our knn kfgp on several synthetic and real data set to show it validity 
the social lending market with over a billion dollar in loan is a two sided matching market where borrower specify demand and lender specify total budget and their desired interest rate from each acceptable borrower because different borrower correspond to different risk return profile lender have preference over acceptable borrower a borrower prefers lender in order of the interest rate they offer to her we investigate the question of what is a computationally feasible good allocation to clear this market we design a strongly polynomial time algorithm for computing a pareto efficient stable outcome in a two sided many to many matching market with indifference and use this to compute an allocation for the social lending market that satisfies the property of stability a standard notion of fairness in two sided matching market and pareto efficiency and additionally address envy freeness amongst similar borrower and risk diversification for lender 
object detection is a basic skill for a robot to perform task in human environment in order to build a good object classifier a large training set of labeled image is required this is typically collected and labeled often painstakingly by a human this method is not scalable and therefore limit the robot s detection performance we propose an algorithm for a robot to collect more data in the environment during it training phase so that in the future it could detect object more reliably the first step is to plan a path for collecting additional training image which is hard because a previously visited location affect the decision for the future location one key component of our work is path planning by building a sparse graph that capture these dependency the other key component is our learning algorithm that weighs the error made in robot s data collection process while updating the classifier in our experiment we show that our algorithm enable the robot to improve it object classifier significantly 
natural deduction which is a method for establishing validity of propositional type argument help develop important reasoning skill and is thus a key ingredient in a course on introductory logic we present two core component namely solution generation and practice problem generation for enabling computer aided education for this important subject domain the key enabling technology is use of an offline computed data structure called universal proof graph upg that encodes all possible application of inference rule over all small proposition abstracted using their bitvector based truth table representation this allows an efficient forward search for solution generation more interestingly this allows generating fresh practice problem that have given solution characteristic by performing a backward search in upg we obtained around natural deduction problem from various textbook our solution generation procedure can solve many more problem than the traditional forward chaining based procedure while our problem generation procedure can efficiently generate several variant with desired characteristic 
several recent work have focused on harvesting html table from the web and recovering their semantics cafarella et al a elmeleegy et al limaye et al venetis et al a a result hundred of million of high quality structured data table can now be explored by the user in this paper we argue that those effort only scratch the surface of the true value of structured data on the web and study the challenging problem of synthesizing table from the web i e producing never before seen table from raw table on the web table synthesis offer an important semantic advantage when a set of related table are combined into a single union table powerful mechanism such a temporal or geographical comparison and visualization can be employed to understand and mine the underlying data holistically we focus on one fundamental task of table synthesis namely table stitching within a given site many table with identical schema can be scattered across many page the task of table stitching involves combining such table into a single meaningful union table and identifying extra attribute and value for it row so that row from different original table can be distinguished specifically we first define the notion of stitchable table and identify collection of table that can be stitched second we design an effective algorithm for extracting hidden attribute that are essential for the stitching process and for aligning value of those attribute across table to synthesize new column we also assign meaningful name to these synthesized column experiment on real world table demonstrate the effectiveness of our approach 
we have developed a novel hybrid representation for music information retrieval our representation is built by incorporating audio content into the tag space in a tag track matrix and then learning hybrid concept using latent semantic analysis we apply this representation to the task of music recommendation using similarity based retrieval from a query music track we also develop a new approach to evaluating music recommender system which is based upon the relationship of user liking track we are interested in measuring the recommendation quality and the rate at which cold start track are recommended our hybrid representation is able to outperform a tag only representation in term of both recommendation quality and the rate that cold start track are included a recommendation 
we study repeated game in which player have imperfect execution skill and one player s true skill is not common knowledge in these setting the possibility arises of a player hustling or pretending to have lower execution skill than they actually have focusing on repeated zero sum game we provide a hustle proof strategy this strategy maximizes a player s payoff regardless of the true skill level of the other player 
monitoring team activity is beneficial when human team cooperate in the enactment of a joint plan monitoring allows team to maintain awareness of each other s progress within the plan and it enables anticipation of information need human find this difficult particularly in time stressed and uncertain environment in this paper we introduce a probabilistic model based on conditional random field to automatically recognise the composition of team and the team activity in relation to a plan the team composition and activity are recognised incrementally by interpreting a stream of spatio temporal observation 
researcher are becoming aware of the importance of other information source besides visual data in robot learning by demonstration lbd forcebased perception are shown to convey very relevant information missed by visual and position sensor for learning specific task in this paper we review some recent work using force a input data in lbd and human robot interaction hri scenario and propose a complete learning framework for teaching force based manipulation skill to a robot through a haptic device we suggest to use haptic interface not only a a demonstration tool but also a a communication channel between the human and the robot getting the teacher more involved in the teaching process by experiencing the force signal sensed by the robot within the proposed framework we provide solution for treating force signal extracting relevant information about the task encoding the training data and generalizing to perform successfully under unknown condition 
when answering query in the presence of ontology adopting the closed world assumption for some predicate easily result in intractability we analyze this situation on the level of individual ontology formulated in the description logic dl lite and el and show that in all case where answering conjunctive query cqs with open and closed predicate is tractable it coincides with answering cqs with all predicate assumed open in this sense cq answering with closed predicate is inherently intractable our analysis also yield a dichotomy between ac and conp for cq answering w r t ontology formulated in dl lite and a dichotomy between ptime and conp for el interestingly the situation is le dramatic in the more expressive description logic eli where we find ontology for which cq answering is in ptime but doe not coincide with cq answering where all predicate are open 
in the existing method for solving matrix completion such a singular value thresholding svt soft impute and fixed point continuation fpca algorithm it is typically required to repeatedly implement singular value decomposition svd of matrix when the size of the matrix in question is large the computational complexity of finding a solution is costly to reduce this expensive computational complexity we apply kronecker product to handle the matrix completion problem in particular we propose using kronecker factorization which approximates a matrix by the kronecker product of several matrix of smaller size we introduce kronecker factorization into the soft impute framework and devise an effective matrix completion algorithm especially when the factorized matrix have about the same size the computational complexity of our algorithm is improved substantially copyright association for the advancement of artificial intelligence all right reserved 
we investigate an interactive teaching scenario where a human teach a robot symbol which abstract the geometric property of object there are multiple motivation for this scenario first state of the art method for relational reinforcement learning demonstrate that we can learn and employ strongly generalizing abstract model with great success for goal directed object manipulation however these method rely on given grounded action and state symbol and raise the classical question where do the symbol come from second existing research on learning from human robot interaction ha focused mostly on the motion level e g imitation learning however if the goal of teaching is to enable the robot to autonomously solve sequential manipulation task in a goal directed manner the human should have the possibility to teach the relevant abstraction to describe the task and let the robot eventually leverage powerful relational rl method in this paper we formalize human robot teaching of grounded symbol a an active learning problem where the robot actively generates pick and place geometric situation that maximize it information gain about the symbol to be learned we demonstrate that the learned symbol can be used by a robot in a relational rl framework to learn probabilistic relational rule and use them to solve object manipulation task in a goal directed manner 
many people read online review written by other user to learn more about a product or venue however the overwhelming amount of user generated review and variance in length detail and quality across the review make it difficult to glean useful information in this paper we present a summarization system called review spotlight it provides a brief overview of review by using adjective noun word pair extracted from the review text the system also allows the user to click any word pair to read the original sentence from which the word pair wa extracted we present our system implementation a a google chrome browser extension and an evaluation on how two word pair scoring method tf and tf idf affect the identification of useful word pair 
we present a new recommender system for online dating using a large dataset from a major online dating website we first show that similar people a defined by a set of personal attribute like and dislike similar people and are liked and disliked by similar people this analysis provides the foundation for our content collaborative reciprocal ccr recommender approach the content based part us selected user profile feature and similarity measure to generate a set of similar user the collaborative filtering part us the interaction of the similar user including the people they like dislike and are liked disliked by to produce reciprocal recommendation ccr address the cold start problem of new user joining the site by being able to provide recommendation immediately based on their profile evaluation result show that the success rate of the recommendation is compared with a baseline of for the top ranked recommendation 
when using graphical model for decision making the presence of unobserved variable may hinder our ability to reach the correct decision a fundamental question here is whether or not one is ready to make a decision stopping criterion and if not what additional observation should be made in order to better prepare for a decision selection criterion a recently introduced notion the same decision probability sdp ha been shown to be useful a both a stopping and a selection criterion this query ha been shown to be highly intractable being pppp complete and is exemplary of a class of query which correspond to the computation of certain expectation we propose the first exact algorithm for computing the sdp in this paper and demonstrate it effectiveness on several real and synthetic network we also present a new complexity result for computing the sdp on model with a naive bayes structure 
most semi supervised method in natural language processing capitalize on unannotated resource in a single language however information can be gained from using parallel resource in more than one language since translation of the same utterance in different language can help to disambiguate each other we demonstrate a method that make effective use of vast amount of bilingual text a k a bitext to improve monolingual system we propose a factored probabilistic sequence model that encourages both cross language and intra document consistency a simple gibbs sampling algorithm is introduced for performing approximate inference experiment on english chinese named entity recognition ner using the ontonotes dataset demonstrate that our method is significantly more accurate than state of the art monolingual crf model in a bilingual test setting our model also improves on previous work by burkett et al achieving a relative error reduction of and in chinese and english respectively further more by annotating a moderate amount of unlabeled bi text with our bilingual model and using the tagged data for uptraining we achieve a error reduction in chinese over the state of the art stanford monolingual ner system copyright association for the advancement of artificial intelligence www aaai org all right reserved 
this paper present new result for the partial maximum a posteriori map problem in bayesian network which is the problem of querying the most probable state configuration of some of the network variable given evidence it is demonstrated that the problem remains hard even in network with very simple topology such a binary polytrees and simple tree including the naive bayes structure which extends previous complexity result furthermore a fully polynomial time approximation scheme for map in network with bounded treewidth and bounded number of state per variable is developed approximation scheme were thought to be impossible but here it is shown otherwise under the assumption just mentioned which are adopted in most application 
the paper generalizes abstract argument game to cope with case where proponent and opponent argue in front of an audience whose type is known only with uncertainty the generalization which make use of basic tool from probability theory is motivated by several example and delivers a class of abstract argument game whose adequacy is proven robust against uncertainty 
the objective of the thesis is to explore how complex data can be treated using unsupervised machine learning technique in which additional information is injected to guide the exploratory process starting from specific problem our contribution take into account the different dimension of the complex data their nature image text the additional information attached to the data label structure concept ontology and the temporal dimension a special attention is given to data representation and how additional information can be leveraged to improve this representation 
continuous time series data often comprise or contain repeated motif pattern that have similar shape and yet exhibit nontrivial variability identifying these motif even in the presence of variation is an important subtask in both unsupervised knowledge discovery and constructing useful feature for discriminative task this paper address this task using a probabilistic framework that model generation of data a switching between a random walk state and state that generate motif a motif is generated from a continuous shape template that can undergo non linear transformation such a temporal warping and additive noise we propose an unsupervised algorithm that simultaneously discovers both the set of canonical shape template and a template specific model of variability manifested in the data experimental result on three real world data set demonstrate that our model is able to recover template in data where repeated instance show large variability the recovered template provide higher classification accuracy and coverage when compared to those from alternative such a random projection based method and simpler generative model that do not model variability moreover in analyzing physiological signal from infant in the icu we discover both known signature a well a novel physiomarkers 
in this paper we use the kripke semantics characterization of dummett logic to introduce a new way of handling non forced formula in tableau proof system we pursue the aim of reducing the search space by strictly increasing the number of forced propositional variable after the application of noninvertible rule the focus of the paper is on a new tableau system for dummett logic for which we have an implementation 
there is increasing awareness in the planning community that the burden of specifying complete domain model is too high which impedes the applicability of planning technology in many real world domain although there have been many learning approach that help automatically creating domain model they all assume plan trace training data are correct in this paper we aim to remove this assumption allowing plan trace to be with noise compared to collecting large amount of correct plan trace it is much easier to collect noisy plan trace e g we can directly exploit sensor to help collect noisy plan trace we consider a novel solution for this challenge that can learn action model from noisy plan trace we create a set of random variable to capture the possible correct plan trace behind the observed noisy one and build a graphical model to describe the physic of the domain we then learn the parameter of the graphical model and acquire the domain model based on the learnt parameter in the experiment we empirically show that our approach is effective in several planning domain 
we show how finite model computation fmc of first order theory can efficiently and transparently be solved by taking advantage of an extension of answer set programming called incremental answer set programming iasp the idea is to use the incremental parameter in iasp program to account for the domain size of a model the fmc problem is then successively addressed for increasing domain size until an answer set representing a finite model of the original first order theory is found we developed a system based on the iasp solver iclingo and demonstrate it competitiveness 
we present a novel approach for domain adaptation based on feature grouping and re weighting our algorithm operates by creating an ensemble of multiple classifier where each classifier is trained on one particular feature group faced with the distribution change involved in domain change different feature group exhibit different cross domain prediction ability herein ensemble model provide u the flexibility of tuning the weight of corresponding classifier in order to adapt to the new domain our approach is supported by a solid theoretical analysis based on the expressiveness of ensemble classifier which allows trading off error across source and target domain moreover experimental result on sentiment classification and spam detection show that our approach not only outperforms the baseline method but is also superior to other state of the art method 
ranking scientific article is an important but challenging task partly due to the dynamic nature of the evolving publication network in this paper we mainly focus on two problem how to rank article in the heterogeneous network and how to use time information in the dynamic network in order to obtain a better ranking result to tackle the problem we propose a graph based ranking method which utilizes citation author journal conference and the publication time information collaboratively the experiment were carried out on two public datasets the result show that our approach is practical and rank scientific article more accurately than the state of art method copyright association for the advancement of artificial intelligence www aaai org all right reserved 
in this paper to support more precise chinese out of vocabulary oov term detection and part of speech po guessing a unified mechanism is proposed and formulated based on the fusion of multiple feature and supervised learning besides all the traditional feature the new feature for statistical information and global context are introduced a well a some constraint and heuristic rule which reveal the relationship among oov term candidate our experiment on the chinese corpus from both people s daily and sighan have achieved the consistent result which are better than those acquired by pure rule based or statistic based model from the experimental result for combining our model with chinese monolingual retrieval on the data set of trec it is found that the obvious improvement for the retrieval performance can also be obtained 
understanding extreme event such a hurricane or forest fire is of paramount importance because of their adverse impact on human being such event often propagate in space and time predicting even a few day in advance what location will get affected by the event track could benefit our society in many way arguably simulation from first principle where underlying physic based model are described by a system of equation provide least reliable prediction for variable characterizing the dynamic of these extreme event data driven model building ha been recently emerging a a complementary approach that could learn the relationship between historically observed or simulated multiple spatio temporal ancillary variable and the dynamic behavior of extreme event of interest while promising the methodology for predictive learning from such complex data is still in it infancy in this paper we propose a dynamic network based methodology for in advance prediction of the dynamic track of emerging extreme event by associating a network model of the system with the known track our method is capable of learning the recurrent network motif that could be used a discriminatory signature for the event s behavioral class when applied to classifying the behavior of the hurricane track at their early formation stage inwestern africa region our method is able to predict whether hurricane track will hit the land of the north atlantic region at least day lead lag time in advance with more than accuracy using fold cross validation to the best of our knowledge no comparable methodology exists for solving this problem using data driven model 
coalition formation is a fundamental approach to multi agent coordination in this paper we address the specific problem of coalition structure generation and focus on providing good enough solution using a novel heuristic approach that is based on data clustering method in particular we propose a hierarchical agglomerative clustering approach c link which us a similarity criterion between coalition based on the gain that the system achieves if two coalition merge we empirically evaluate c link on a synthetic benchmark data set a well a in collective energy purchasing setting our result show that the c link approach performs very well against an optimal benchmark based on mixed integer programming achieving solution which are in the worst case about of the optimal in the synthetic data set and of the optimal in the energy data set thus we show that c link can return solution for problem involving thousand of agent within minute 
recent real world deployment of stackelberg security game make it critical that we address human adversary bounded rationality in computing optimal strategy to that end this paper provides three key contribution i new efficient algorithm for computing optimal strategic solution using prospect theory and quantal response equilibrium ii the most comprehensive experiment to date studying the effectiveness of different model against human subject for security game and iii new technique for generating representative payoff structure for behavioral experiment in generic class of game our result with human subject show that our new technique outperform the leading contender for modeling human behavior in security game 
