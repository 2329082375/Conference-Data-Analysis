we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown by making use of simple linguistically motivated state split which break down false independence assumption latent in a vanilla treebank grammar indeed it performance of lp lr f pcfg model and surprisingly close to the current state of the art this result ha potential us beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized model an unlexicalized pcfg is much more compact easier to replicate and easier to interpret than more complex lexical model and the parsing algorithm are simpler more widely understood of lower asymptotic complexity and easier to optimize 
generalized multitext grammar gmtg is a synchronous grammar formalism that is weakly equivalent to linear context free rewriting system lcfrs but retains much of the notational and intuitive simplicity of context free grammar cfg gmtg allows both synchronous and independent rewriting such flexibility facilitates more perspicuous modeling of parallel text than what is possible with other synchronous formalism this paper investigates the generative capacity of gmtg prof that each component grammar of a gmtg retains it generative power and proposes a generalization of chomsky normal form which is necessary for synchronous cky style parsing 
we present new result on the relation between purely symbolic context free parsing strategy and their probabilistic counterpart such parsing strategy are seen a construction of push down device from grammar we show that preservation of probability distribution is possible under two condition viz the correct prefix property and the property of strong predictiveness these result generalize existing result in the literature that were obtained by considering parsing strategy in isolation from our general result we also derive negative result on so called generalized lr parsing 
abstract we present new result on the relation between context free parsing strategy and their probabilistic counter part we provide a necessary condition and a sufficient condition for the probabilistic extension of parsing strategy these result generalize existing result in the literature that were obtained by considering parsing strategy in isolation 
we present a novel discriminative approach to parsing inspired by the large margin criterion underlying support vector machine our formulation us a factorization analogous to the standard dynamic program for parsing in particular it allows one to efficiently learn a model which discriminates among the entire space of parse tree a opposed to reranking the top few candidate our model can condition on arbitrary feature of input sentence thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness we provide an efficient algorithm for learning such model and show experimental evidence of the model s improved performance over a natural baseline model and a lexicalized probabilistic context free grammar 
abstract we explore the descriptive power in term of syntactic phenomenon of a formalism that extends treeadjoining grammar tag by adding a fourth level of hierarchical decomposition to the three level tag already employ while extending the descriptive power minimally the additional level of decomposition allows u to obtain a uniform account of a range of phenomenon that ha heretofore been difficult to encompass an account that employ unitary elementary structure and eschews synchronized derivation operation and which is in many respect closer to the spirit of the intuition underlying tag based linguistic theory than previously considered extension to tag 
we present a detailed investigation of the challenge posed when applying parsing model developed against english corpus to chinese we develop a factored model statistical parser for the penn chinese treebank showing the implication of gross statistical difference between wsj and chinese tree bank for the most general method of parser adaptation we then provide a detailed analysis of the major source of statistical parse error for this corpus showing their cause and relative frequency and show that while some type of error are due to difficult ambiguity inherent in chinese grammar others arise due to treebank annotation practice we show how each type of error can be addressed with simple targeted change to the independence assumption of the maximum likelihood estimated pcfg factor of the parsing model which raise our f from to on our development set and achieves parse accuracy close to the best published figure for chinese parsing 
pipelined natural language generation nlg system have grown increasingly complex a architectural module were added to support language functionality such a referring expression lexical choice and revision this ha given rise to discussion about the relative placement of these new module in the overall architecture recent work on another aspect of multi paragraph text discourse marker indicates it is time to consider where a discourse marker insertion algorithm fit in we present example which suggest that in a pipelined nlg architecture the best approach is to strongly tie it to a revision component finally we evaluate the approach in a working multi page system 
we present an approach to bounded constraintrelaxation for entropy maximization that corresponds to using a double exponential prior or regularizer in likelihood maximization for log linear model we show that a combined incremental feature selection and regularization method can be established for maximum entropy modeling by a natural incorporation of the regularizer into gradientbased feature selection following perkins et al this provides an efficient alternative to standard regularization on the full feature set and a mathematical justification for thresholding technique used in likelihood based feature selection also we motivate an extension to n best feature selection for linguistic feature set with moderate redundancy and present experimental result showing it advantage over best regularization and over standard incremental feature selection for the task of maximum entropy parsing 
we present an approach using syntacto semantic rule for the extraction of relational information from biomedical abstract the result show that by overcoming the hurdle of technical terminology high precision result can be achieved from abstract related to baker s yeast we manage to extract a regulatory network comprised of pairwise relation from abstract with an accuracy of to achieve this we made use of a resource of gene protein name considerably larger than those used in most other biology related information extraction approach this list of name wa included in the lexicon of our retrained part of speech tagger for use on molecular biology abstract for the domain in question an accuracy of wa attained on po tag the method is easily adapted to other organism than yeast allowing u to extract many more biologically relevant relation 
in this paper we elucidate how korean temporal marker contribute to specifying the event time and formalize it in term of typed lambda calculus we also present a computational method for constructing temporal representation of korean sentence on the basis of g grammar proposed by renaud 
most statistical parser have used the grammar induction approach in which a stochastic grammar is induced from a treebank an alternative approach is to induce a controller for a given parsing automaton such controller may be stochastic here we focus on greedy controller which result in deterministic parser we use decision tree to learn the controller the resulting parser are surprisingly accurate and robust considering their speed and simplicity they are almost a fast a current part ofspeech tagger and considerably more accurate than a basic unlexicalized pcfg parser we also describe markov parsing model a general framework for parser modeling and control of which the parser reported here are a special case 
this paper proposes a novel method to compile statistical model for machine translation to achieve efficient decoding in our method each statistical submodel is represented by a weighted finite s tate transducer wfst and all of the submodels are expanded into a composition model beforehand furthermore the ambiguity of the composition model is reduced by the statistic of hypothesis while decoding the experimental result show that the proposed model representation drastically improves the efficienc y of decoding compared to the dynamic composition of the submodels which corresponds to conventional approach 
log of user query to an internet search engine p rovide a large amount of implicit and explicit inform ation about language in this paper we investigate their use in spelling correction of search query a task which pose many additional challenge beyond the traditional spelling correction problem we pre sent an approach that us an iterative transformat ion of the input query string into other string that correspond to more and more likely query according to statistic extracted from internet search query log s 
we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown by making use of simple linguistically motivated state split which break down false independence assumption latent in a vanilla treebank grammar indeed it performance of lp lr f pcfg model and surprisingly close to the current state of the art this result ha potential us beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized model an unlexicalized pcfg is much more compact easier to replicate and easier to interpret than more complex lexical model and the parsing algorithm are simpler more widely understood of lower asymptotic complexity and easier to optimize 
generalized multitext grammar gmtg is a synchronous grammar formalism that is weakly equivalent to linear context free rewriting system lcfrs but retains much of the notational and intuitive simplicity of context free grammar cfg gmtg allows both synchronous and independent rewriting such flexibility facilitates more perspicuous modeling of parallel text than what is possible with other synchronous formalism this paper investigates the generative capacity of gmtg prof that each component grammar of a gmtg retains it generative power and proposes a generalization of chomsky normal form which is necessary for synchronous cky style parsing 
we present new result on the relation between purely symbolic context free parsing strategy and their probabilistic counterpart such parsing strategy are seen a construction of push down device from grammar we show that preservation of probability distribution is possible under two condition viz the correct prefix property and the property of strong predictiveness these result generalize existing result in the literature that were obtained by considering parsing strategy in isolation from our general result we also derive negative result on so called generalized lr parsing 
abstract we present new result on the relation between context free parsing strategy and their probabilistic counter part we provide a necessary condition and a sufficient condition for the probabilistic extension of parsing strategy these result generalize existing result in the literature that were obtained by considering parsing strategy in isolation 
we present a novel discriminative approach to parsing inspired by the large margin criterion underlying support vector machine our formulation us a factorization analogous to the standard dynamic program for parsing in particular it allows one to efficiently learn a model which discriminates among the entire space of parse tree a opposed to reranking the top few candidate our model can condition on arbitrary feature of input sentence thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness we provide an efficient algorithm for learning such model and show experimental evidence of the model s improved performance over a natural baseline model and a lexicalized probabilistic context free grammar 
abstract we explore the descriptive power in term of syntactic phenomenon of a formalism that extends treeadjoining grammar tag by adding a fourth level of hierarchical decomposition to the three level tag already employ while extending the descriptive power minimally the additional level of decomposition allows u to obtain a uniform account of a range of phenomenon that ha heretofore been difficult to encompass an account that employ unitary elementary structure and eschews synchronized derivation operation and which is in many respect closer to the spirit of the intuition underlying tag based linguistic theory than previously considered extension to tag 
we present a detailed investigation of the challenge posed when applying parsing model developed against english corpus to chinese we develop a factored model statistical parser for the penn chinese treebank showing the implication of gross statistical difference between wsj and chinese tree bank for the most general method of parser adaptation we then provide a detailed analysis of the major source of statistical parse error for this corpus showing their cause and relative frequency and show that while some type of error are due to difficult ambiguity inherent in chinese grammar others arise due to treebank annotation practice we show how each type of error can be addressed with simple targeted change to the independence assumption of the maximum likelihood estimated pcfg factor of the parsing model which raise our f from to on our development set and achieves parse accuracy close to the best published figure for chinese parsing 
pipelined natural language generation nlg system have grown increasingly complex a architectural module were added to support language functionality such a referring expression lexical choice and revision this ha given rise to discussion about the relative placement of these new module in the overall architecture recent work on another aspect of multi paragraph text discourse marker indicates it is time to consider where a discourse marker insertion algorithm fit in we present example which suggest that in a pipelined nlg architecture the best approach is to strongly tie it to a revision component finally we evaluate the approach in a working multi page system 
we present an approach to bounded constraintrelaxation for entropy maximization that corresponds to using a double exponential prior or regularizer in likelihood maximization for log linear model we show that a combined incremental feature selection and regularization method can be established for maximum entropy modeling by a natural incorporation of the regularizer into gradientbased feature selection following perkins et al this provides an efficient alternative to standard regularization on the full feature set and a mathematical justification for thresholding technique used in likelihood based feature selection also we motivate an extension to n best feature selection for linguistic feature set with moderate redundancy and present experimental result showing it advantage over best regularization and over standard incremental feature selection for the task of maximum entropy parsing 
we present an approach using syntacto semantic rule for the extraction of relational information from biomedical abstract the result show that by overcoming the hurdle of technical terminology high precision result can be achieved from abstract related to baker s yeast we manage to extract a regulatory network comprised of pairwise relation from abstract with an accuracy of to achieve this we made use of a resource of gene protein name considerably larger than those used in most other biology related information extraction approach this list of name wa included in the lexicon of our retrained part of speech tagger for use on molecular biology abstract for the domain in question an accuracy of wa attained on po tag the method is easily adapted to other organism than yeast allowing u to extract many more biologically relevant relation 
in this paper we elucidate how korean temporal marker contribute to specifying the event time and formalize it in term of typed lambda calculus we also present a computational method for constructing temporal representation of korean sentence on the basis of g grammar proposed by renaud 
most statistical parser have used the grammar induction approach in which a stochastic grammar is induced from a treebank an alternative approach is to induce a controller for a given parsing automaton such controller may be stochastic here we focus on greedy controller which result in deterministic parser we use decision tree to learn the controller the resulting parser are surprisingly accurate and robust considering their speed and simplicity they are almost a fast a current part ofspeech tagger and considerably more accurate than a basic unlexicalized pcfg parser we also describe markov parsing model a general framework for parser modeling and control of which the parser reported here are a special case 
this paper proposes a novel method to compile statistical model for machine translation to achieve efficient decoding in our method each statistical submodel is represented by a weighted finite s tate transducer wfst and all of the submodels are expanded into a composition model beforehand furthermore the ambiguity of the composition model is reduced by the statistic of hypothesis while decoding the experimental result show that the proposed model representation drastically improves the efficienc y of decoding compared to the dynamic composition of the submodels which corresponds to conventional approach 
log of user query to an internet search engine p rovide a large amount of implicit and explicit inform ation about language in this paper we investigate their use in spelling correction of search query a task which pose many additional challenge beyond the traditional spelling correction problem we pre sent an approach that us an iterative transformat ion of the input query string into other string that correspond to more and more likely query according to statistic extracted from internet search query log s 
for sinequa s second participation to the senseval evaluation two system using contextual semantic have been proposed based on different approach they both share the same data preprocessing and enrichment the first system is a combined approach using semantic classification tree and information retrieval technique for the second system the word from the context are considered a clue the final sense is determined by summing the weight assigned to each clue for a given example 
we present the first algorithm that computes optimal ordering of sentence into a locally coherent discourse the algorithm run very efficiently on a variety of coherence measure from the literature we also show that the discourse ordering problem is np complete and cannot be approximated 
this paper describes an alternative translation model based on a text chunk under the framework of statistical machine translation the translation model suggested here first performs chunking then each word in a chunk is translated finally translated chunk are reordered under this scenario of translation modeling we have experimented on a broad coverage japanese english traveling corpus and achieved improved performance 
collocation translation is important for machine translation and many other nlp task unlike previous method using bilingual parallel corpus this paper present a new method for acquiring collocation translation by making use of monolingual corpus and linguistic knowledge first dependency triple are extracted from chinese and english corpus with dependency parser then a dependency triple translation model is estimated using the em algorithm based on a dependency correspondence assumption the generated triple translation model is used to extract collocation translation from two monolingual corpus experiment show that our approach outperforms the existing monolingual corpus based method in dependency triple translation and achieves promising result in collocation translation extraction 
this paper present a chinese word segmentation system which can adapt to different domain and standard we first present a statistical framework where domain specific word are identified in a unified approach to word segmentation based on linear model we explore several feature and describe how to create training data by sampling we then describe a transformation based learning method used to adapt our system to different word segmentation standard evaluation of the proposed system on five test set with different standard show that the system achieves stateof the art performance on all of them 
alias i threattrackers are an advanced information access application designed around the need of analyst working through a large daily data feed threattrackers help analyst decompose an information gathering topic like the unfolding political situation in iraq into specification including people place organization and relationship these specification are then used to collect and browse information on a daily basis the nearest related technology are information retrieval search engine document categorization information extraction and named entity detection threattrackers are currently being used in the total information awareness program 
word alignment play a crucial role in statistical machine translation word aligned corpus have been found to be an excellent source of translation related knowledge we present a statistical model for computing the probability of an alignment given a sentence pair this model allows easy integration of context specific feature our experiment show that this model can be an effective tool for improving an existing word alignment 
headline summarization is a difficult task because it requires maximizing text content in short summary length while maintaining grammaticality this paper describes our first attempt toward solving this problem with a sy stem that generates key headline cluster and fine tune them using template 
we present an unsupervised method for labelling the argument of verb with their semantic role our bootstrapping algorithm make initial unambiguous role assignment and then iteratively update the probability model on which future assignment are based a novel aspect of our approach is the use of verb slot and noun class information a the basis for backing off in our probability model we achieve reduction in the error rate over an informed baseline indicating the potential of our approach for a task that ha heretofore relied on large amount of manually generated training data 
in a multimodal conversation the way user communicate with a system depends on the available interaction channel and the situated context e g conversation focus visual feedback these dependency form a rich set of constraint from various perspective such a temporal alignment between different modality coherence of conversation and the domain semantics there is strong evidence that competition and ranking of these constraint is important to achieve an optimal interpretation thus we have developed an optimization approach for multimodal interpretation particularly for interpreting multimodal reference a preliminary evaluation indicates the effectiveness of this approach especially for complex user input that involve multiple referring expression in a speech utterance and multiple gesture 
we present a generative model for the unsupervised learning of dependency structure we also describe the multiplicative combination of this dependency model with a model of linear constituency the product model outperforms both component on their respective evaluation metric giving the best published figure for unsupervised dependency parsing and unsupervised constituency parsing we also demonstrate that the combined model work and is robust cross linguistically being able to exploit either attachment or distributional regularity that are salient in the data 
this paper present some of the first data visualization and analysis of distribution for a lexicalized statistical parsing model in order to better understand their nature in the course of this analysis we have paid particular attention to parameter that include bilexical dependency the prevailing view ha been that such statistic are very informative but su er greatly from sparse data problem by using a parser to constrain parse it own output and by hypothesizing and testing for distributional similarity with back o distribution we have evidence that finally explains that a bilexical statistic are actually getting used quite often but that b the distribution are so similar to those that do not include head word a to be nearly indistinguishable insofar a making parse decision finally our analysis ha provided for the first time an e ective way to do parameter selection for a generative lexicalized statistical parsing model 
we present an algorithm for generating referring expression in open domain existing algorithm work at the semantic level and assume the availability of a classification for attribute which is only feasible for restricted domain our alternative work at the realisation level relies on word net synonym and antonym set and give equivalent result on the example cited in the literature and improved result for example that prior approach cannot handle we believe that ours is also the first algorithm that allows for the incremental incorporation of relation we present a novel corpus evaluation using referring expression from the penn wall street journal treebank 
we present an unsupervised approach to recognizing discourse relation of contrast explanation evidence condition and elaboration that hold between arbitrary span of text we show that discourse relation classifier trained on example that are automatically extracted from massive amount of text can be used to distinguish between some of these relation with accuracy a high a even when the relation are not explicitly marked by cue phrase 
traditional vector based model use word co occurrence count from large corpus to represent lexical meaning in this paper we present a novel approach for constructing semantic space that take syntactic relation into account we introduce a formalisation for this class of model and evaluate their adequacy on two modelling task semantic priming and automatic discrimination of lexical relation 
this paper discus the application of the expectation maximization em clustering algorithm to the task of chinese verb sense discrimination the model utilized rich linguistic feature that capture predicate argument structure information of the target verb a semantic taxonomy for chinese noun which wa built semi automatically based on two electronic chinese semantic dictionary wa used to provide semantic feature for the model purity and normalized mutual information were used to evaluate the clustering performance on chinese verb the experimental result show that the em clustering model can learn sense or sense group distinction for most of the verb successfully we further enhanced the model with certain fine grained semantic category called lexical set our result indicate that these lexical set improve the model s performance for the three most challenging verb chosen from the first set of experiment 
we present a geometric view on bilingual lexicon extraction from comparable corpus which allows to re interpret the method proposed so far and identify unresolved problem this motivates three new method that aim at solving these problem empirical evaluation show the strength and weakness of these method a well a a significant gain in the accuracy of extracted lexicon 
this paper proposes a new approach for coreference resolution which us the bell tree to represent the search space and cast the coreference resolution problem a finding the best path from the root of the bell tree to the leaf node a maximum entropy model is used to rank these path the coreference performance on the and automatic content extraction ace data will be reported we also train a coreference system using the muc data and competitive result are obtained 
information graphic non pictorial graphic such a bar chart or line graph are an important component of multimedia document often such graphic convey information that is not contained elsewhere in the document thus document summarization must be extended to include summarization of information graphic this paper address our work on graphic summarization it argues that the message that the graphic designer intended to convey must play a major role in determining the content of the summary and it outline our approach to identifying this intended message and using it to construct the summary 
in statistical machine translation the generation of a translation hypothesis is computationally expensive if arbitrary word reordering are permitted the search problem is np hard on the other hand if we restrict the possible word reordering in an appropriate way we obtain a polynomial time search algorithm in this paper we compare two different reordering constraint namely the itg constraint and the ibm constraint this comparison includes a theoretical discussion on the permitted number of reordering for each of these constraint we show a connection between the itg constraint and the since known schr der number we evaluate these constraint on two task the verbmobil task and the canadian hansard task the evaluation consists of two part first we check how many of the viterbi alignment of the training corpus satisfy each of these constraint second we restrict the search to each of these constraint and compare the resulting translation hypothesis the experiment will show that the baseline itg constraint are not sufficient on the canadian hansard task therefore we present an extension to the itg constraint these extended itg constraint increase the alignment coverage from about to 
paraphrase recognition is a critical step for natural language interpretation accordingly many nlp application would benefit from high coverage knowledge base of paraphrase however the scalability of state of the art paraphrase acquisition approach is still limited we present a fully unsupervised learning algorithm for web based extraction of entailment relation an extended model of paraphrase we focus on increased scalability and generality with respect to prior work eventually aiming at a full scale knowledge base our current implementation of the algorithm take a it input a verb lexicon and for each verb search the web for related syntactic entailment template experiment show promising result with respect to the ultimate goal achieving much better scalability than prior web based method 
we describe experiment carried out with adaptive language and translation model in the context of an interactive computer assisted translation program we developed cache based language model which were then extended to the bilingual case for a cachebased translation model we present the improvement we obtained in two context in a theoretical setting we achieved a drop in perplexity for the new model and in a more practical situation simulating a user working with the system we showed that fewer keystroke would be needed to enter a translation 
we present a large scale meta evaluation of eight evaluation measure for both single document and multi document summarizers to this end we built a corpus consisting of a million automatic summary using six summarizers and baseline at ten summary length in both english and chinese b more than manual abstract and extract and c million automatic document and summary retrieval using query we present both qualitative and quantitative result showing the strength and draw back of all evaluation method and how they rank the different summarizers 
this paper present a method for unsupervised discovery of semantic pattern semantic pattern are useful for a variety of text understanding task in particular for locating event in text for information extraction the method build upon previously described approach to iterative unsupervised pattern acquisition one common characteristic of prior approach is that the output of the algorithm is a continuous stream of pattern with gradually degrading precision our method differs from the previous pattern acquisition algorithm in that it introduces competition among several scenario simultaneously this provides natural stopping criterion for the unsupervised learner while maintaining good precision level at termination we discus the result of experiment with several scenario and examine different aspect of the new procedure 
link detection ha been regarded a a core technology for the topic detection and tracking task of new event detection in this paper we formulate story link detection and new event detection a information retrieval task and hypothesize on the impact of precision and recall on both system motivated by these argument we introduce a number of new performance enhancing technique including part of speech tagging new similarity measure and expanded stop list experimental result validate our hypothesis 
it is well known that occurrence count of word in document are often modeled poorly by standard distribution like the binomial or poisson observed count vary more than simple model predict prompting the use of overdispersed model like gamma poisson or beta binomial mixture a robust alternative another deficiency of standard model is due to the fact that most word never occur in a given document resulting in large amount of zero count we propose using zero inflated model for dealing with this and evaluate competing model on a naive bayes text classification task simple zero inflated model can account for practically relevant variation and can be easier to work with than overdispersed model 
this paper describes an algorithm for detecting empty node in the penn treebank marcus et al finding their antecedent and assigning them function tag without access to lexical information such a valency unlike previous approach to this task the current method is not corpus based but rather make use of the principle of early government binding theory chomsky the syntactic theory that underlies the annotation using the evaluation metric proposed by johnson this approach outperforms previously published approach on both detection of empty category and antecedent identification given either annotated input stripped of empty category or the output of a parser some problem with this evaluation metric are noted and an alternative is proposed along with the result the paper considers the reason a principle based approach to this problem should outperform corpus based approach and speculates on the possibility of a hybrid approach 
we present an unsupervised methodfor word sense disambiguation thatexploits translation correspondencesin parallel corpus the techniquetakes advantage of the fact that crosslanguagelexicalizations of the sameconcept tend to be consistent preservingsome core element of it semantics and yet also variable reflecting differingtranslator preference and the influenceof context working with parallelcorpora introduces an extra complicationfor evaluation since it is 
this paper describes an empirical study of the information synthesis task defined a the process of given a complex information need extracting organizing and inter relating the piece of information contained in a set of relevant document in order to obtain a comprehensive non redundant report that satisfies the information need two main result are presented a the creation of an information synthesis testbed with report manually generated by nine subject for eight complex topic with relevant document each and b an empirical comparison of similarity metric between report under the hypothesis that the best metric is the one that best distinguishes between manual and automatically generated report a metric based on key concept overlap give better result than metric based on n gram overlap such a rouge or sentence overlap 
sentence ranking is a crucial part of generating text summary we compared human sentence ranking obtained in a psycholinguistic experiment to three different approach to sentence ranking a simple paragraph based approach intended a a baseline two word based approach and two coherence based approach in the paragraph based approach sentence in the beginning of paragraph received higher importance rating than other sentence the word based approach determined sentence ranking based on relative word frequency luhn salton buckley coherence based approach determined sentence ranking based on some property of the coherence structure of a text marcu page et al our result suggest poor performance for the simple paragraph based approach whereas word based approach perform remarkably well the best performance wa achieved by a coherence based approach where coherence structure are represented in a non tree structure most approach also outperformed the commercially available msword summarizer 
in this paper we describe two new objective automatic evaluation method for machine translation the first method is based on longest common subsequence between a candidate translation and a set of reference translation longest common subsequence take into account sentence level structure similarity naturally and identifies longest co occurring in sequence n gram automatically the second method relaxes strict n gram matching to skip bigram matching skip bigram is any pair of word in their sentence order skip bigram cooccurrence statistic measure the overlap of skip bigram between a candidate translation and a set of reference translation the empirical result show that both method correlate with human judgment very well in both adequacy and fluency 
this paper concern the discourse understanding process in spoken dialogue system this process enables the system to understand user utterance based on the context of a dialogue since multiple candidate for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding it is not appropriate to decide on a single understanding result after each user utterance by holding multiple candidate for understanding result and resolving the ambiguity a the dialogue progress the discourse understanding accuracy can be improved this paper proposes a method for resolving this ambiguity based on statistical information obtained from dialogue corpus unlike conventional method that use hand crafted rule the proposed method enables easy design of the discourse understanding process experiment result have shown that a system that exploit the proposed method performs sufficiently and that holding multiple candidate for understanding result is effective 
this paper present a method for incorporating word pronunciation information in a noisy channel model for spelling correction the proposed method build an explicit error model for word pronunciation by modeling pronunciation similarity between word we achieve a substantial performance improvement over the previous best performing model for spelling correction 
this paper proposes the use of uncertainty reduction in machine learning method such a co training and bilingual boot strapping which are referred to in a general term a collaborative bootstrapping the paper indicates that uncertainty reduction is an important factor for enhancing the performance of collaborative bootstrapping it proposes a new measure for representing the degree of uncertainty correlation of the two classifier in collaborative bootstrapping and us the measure in analysis of collaborative bootstrapping furthermore it proposes a new algorithm of collaborative bootstrapping on the basis of uncertainty reduction experimental result have verified the correctness of the analysis and have demonstrated the significance of the new algorithm 
this paper describes and evaluates mop an ie system for automatic extraction of metalinguistic information from technical and scientific document we claim that such a system can create special database to bootstrap compilation and facilitate update of the huge and dynamically changing glossary knowledge base and ontology that are vital to modern day research 
sentiment classification is the task of labeling a review document according to the polarity of it prevailing opinion favorable or unfavorable in approaching this problem a model builder often ha three source of information available a small collection of labeled document a large collection of unlabeled document and human understanding of language ideally a learning method will utilize all three source to accomplish this goal we generalize an existing procedure that us the latter two we extend this procedure by re interpreting it a a naive bayes model for document sentiment viewed a such it can also be seen to extract a pair of derived feature that are linearly combined to predict sentiment this perspective allows u to improve upon previous method primarily through two strategy incorporating additional derived feature into the model and where possible using labeled data to estimate their relative influence 
we define noun phrase translation a a subtask of machine translation this enables u to build a dedicated noun phrase translation subsystem that improves over the currently best general statistical machine translation method by incorporating special modeling and special feature we achieved translation accuracy in a german english translation task v with ibm model 
in most research on concept acquisition from corpus concept are modeled a vector of relation extracted from syntactic structure in the case of modifier these relation often specify value of attribute a in attr red this is unlike what typically proposed in theory of knowledge representation where concept are typically defined in term of their attribute e g color we compared model of concept based on value with model based on attribute using lexical clustering a the basis for comparison we find that attribute based model work better than value based one and result in shorter description but that mixed model including both the best attribute and the best value work best of all 
active learning al promise to reduce the cost of annotating labeled datasets for trainable human language technology contrary to expectation when creating labeled training material for hpsg parse selection and later reusing it with other model gain from al may be negligible or even negative this ha serious implication for using al showing that additional cost saving strategy may need to be adopted we explore one such strategy using a model during annotation to automate some of the decision our best result show an reduction in annotation cost compared with labeling randomly selected data with a single model 
in this paper we investigate the practical applicability of co training for the task of building a classifier for reference resolution we are concerned with the question if co training can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance 
previous approach to pronominalization have largely been theoretical rather than applied in nature frequently such method are based on centering theory which deal with the resolution of anaphoric pronoun but it is not clear that complex theoretical mechanism while having satisfying explanatory power are necessary for the actual generation of pronoun we first illustrate example of pronoun from various domain describe a simple method for generating pronoun in an implemented multi page generation system and present an evaluation of it performance 
convolution kernel such a sequence and tree kernel are advantageous for both the concept and accuracy of many natural language processing nlp task experiment have however shown that the over fitting problem often arises when these kernel are used in nlp task this paper discus this issue of convolution kernel and then proposes a new approach based on statistical feature selection that avoids this issue to enable the proposed method to be executed efficiently it is embedded into an original kernel calculation process by using sub structure mining algorithm experiment are undertaken on real nlp task to confirm the problem with a conventional method and to compare it performance with that of the proposed method 
we present the result of an experiment on extending the automatic method of machine translation evaluation blue with statistical weight for lexical item such a tf idf score we show that this extension give additional information about evaluated text in particular it allows u to measure translation adequacy which for statistical mt system is often overestimated by the baseline bleu method the proposed model us a single human reference translation which increase the usability of the proposed method for practical purpose the model suggests a linguistic interpretation which relates frequency weight and human intuition about translation adequacy and fluency 
this paper explores the large scale acquisition of sense tagged example for word sense disambiguation wsd we have applied the wordnet monosemous relative method to construct automatically a web corpus that we have used to train disambiguation system the corpus building process ha highlighted important factor such a the distribution of sens bias the corpus ha been used to train wsd algorithm that include supervised method combining automatic and manuallytagged example minimally supervised requiring sense bias information from hand tagged corpus and fully unsupervised these method were tested on the senseval lexical sample test set and compared successfully to other system with minimum or no supervision 
in this paper we propose a competition learning approach to coreference resolution traditionally supervised machine learning approach adopt the single candidate model nevertheless the preference relationship between the antecedent candidate cannot be determined accurately in this model by contrast our approach adopts a twin candidate learning model such a model can present the competition criterion for antecedent candidate reliably and ensure that the most preferred candidate is selected furthermore our approach applies a candidate filter to reduce the computational cost and data noise during training and resolution the experimental result on muc and muc data set show that our approach can outperform those based on the single candidate model 
we compare and contrast two different model for detecting sentence like unit in continuous speech using both acoustic and lexical information the first approach is based on hidden markov sequence model based on n gram us maximum likelihood estimation and model interpolation to combine different representation of the data the second approach model the posterior probability of the target class is therefore discriminative and integrates multiple knowledge source in the maximum entropy maxent framework both model combine lexical syntactic and prosodic information we develop a technique for integrating pretrained probability model into the maxent framework and show that this approach can improve if only slightly on an hmm based state of the art system for the sentence boundary detection task a much more substantial improvement is obtained by combining the posterior probability of the two system 
supertagging is the tagging process of assigning the correct elementary tree of ltag or the correct supertag to each word of an input sentence in this paper we propose to use supertags to expose syntactic dependency which are unavailable with po tag we first propose a novel method of applying sparse network of winnow snow to sequential model then we use it to construct a supertagger that us long distance syntactical dependency and the supertagger achieves an accuracy of we apply the supertagger to np chunking the use of supertags in np chunking give rise to almost absolute increase from to in f score under transformation based learning tbl frame the surpertagger described here provides an effective and efficient way to exploit syntactic information 
a crucial step toward the goal of automatic extraction of propositional information from natural language text is the identification of semantic relation between constituent in sentence we examine the problem of distinguishing among seven relation type that can occur between the entity treatment and disease in bioscience text and the problem of identifying such entity we compare five generative graphical model and a neural network using lexical syntactic and semantic feature finding that the latter help achieve high classification accuracy 
this paper discus the use of statistical word alignment over multiple parallel text for the identification of string span that cannot be constituent in one of the language this information is exploited in monolingual pcfg grammar induction for that language within an augmented version of the inside outside algorithm besides the aligned corpus no other resource are required we discus an implemented system and present experimental result with an evaluation against the penn tree bank 
this paper describes the user expertise model in athosmail a mobile speech based e mail system the model encodes the system s assumption about the user expertise and give recommendation on how the system should respond depending on the assumed competence level of the user the recommendation are realized a three type of explicitness in the system response the system monitor the user s competence with the help of parameter that describe e g the success of the user s interaction with the system the model consists of an online and an offline version the former taking care of the expertise level change during the same session the latter modelling the overall user expertise a a function of time and repeated interaction 
phrasal verb are an important feature of the english language properly identifying them provides the basis for an english parser to decode the related structure phrasal verb have been a challenge to natural language processing nlp because they sit at the borderline between lexicon and syntax traditional nlp framework that separate the lexicon module from the parser make it difficult to handle this problem properly this paper present a finite state approach that integrates a phrasal verb expert lexicon between shallow parsing and deep parsing to handle morpho syntactic interaction with precision recall combined performance benchmarked consistently at the phrasal verb identification problem ha basically been solved with the presented method 
automatically acquiring synonymous collocation pair such a and from corpus is a challenging task for this task we can in general have a large monolingual corpus and or a very limited bilingual corpus method that use monolingual corpus alone or use bilingual corpus alone are apparently inadequate because of low precision or low coverage in this paper we propose a method that us both these resource to get an optimal compromise of precision and coverage this method first get candidate of synonymous collocation pair based on a monolingual corpus and a word thesaurus and then selects the appropriate pair from the candidate using their translation in a second language the translation of the candidate are obtained with a statistical translation model which is trained with a small bilingual corpus and a large monolingual corpus the translation information is proved a effective to select synonymous collocation pair experimental result indicate that the average precision and recall of our approach are and respectively which outperform those method that only use monolingual corpus and those that only use bilingual corpus 
several approach have been described for the automatic unsupervised acquisition of pattern for information extraction each approach is based on a particular model for the pattern to be acquired such a a predicate argument structure or a dependency chain the effect of these alternative model ha not been previously studied in this paper we compare the prior model and introduce a new model the subtree model based on arbitrary subtrees of dependency tree we describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using subtree pattern 
the morphology of semitic language is unique in the sense that the major word formation mechanism is an inherently non concatenative process of interdigitation whereby two morpheme a root and a pattern are interwoven identifying the root of a given word in a semitic language is an important task in some case a crucial part of morphological analysis it is also a non trivial task which many human find challenging we present a machine learning approach to the problem of extracting root of hebrew word given the large number of potential root thousand we address the problem a one of combining several classifier each predicting the value of one of the root s consonant we show that when these predictor are combined by enforcing some fairly simple linguistics constraint high accuracy which compare favorably with human performance on this task can be achieved 
we approximate arabic s rich morphology by a model that a word consists of a sequence of morpheme in the pattern prefix stem suffix denotes zero or more occurrence of a morpheme our method is seeded by a small manually segmented arabic corpus and us it to bootstrap an unsupervised algorithm to build the arabic word segmenter from a large unsegmented arabic corpus the algorithm us a trigram language model to determine the most probable morpheme sequence for a given input the language model is initially estimated from a small manually segmented corpus of about word to improve the segmentation accuracy we use an unsupervised algorithm for automatically acquiring new stem from a million word unsegmented corpus and re estimate the model parameter with the expanded vocabulary and training corpus the resulting arabic word segmentation system achieves around exact match accuracy on a test corpus containing word token we believe this is a state of the art performance and the algorithm can be used for many highly inflected language provided that one can create a small manually segmented corpus of the language of interest 
when rule of transfer based machine translation mt are automatically acquired from bilingual corpus incorrect redundant rule are generated due to acquisition error or translation variety in the corpus a a new countermeasure to this problem we propose a feedback cleaning method using automatic evaluation of mt quality which remove incorrect redundant rule a a way to increase the evaluation score bleu is utilized for the automatic evaluation the hill climbing algorithm which involves feature of this task is applied to searching for the optimal combination of rule our experiment show that the mt quality improves by in test sentence according to a subjective evaluation this is considerable improvement over previous method 
this paper present a multi layered question answering q a architecture suitable for enhancing current q a capability with the possibility of processing complex question that is question whose answer need to be gathered from piece of factual information scattered in different document specifically we have designed a layer oriented to process the different type of temporal question complex temporal question are first decomposed into simpler one according to the temporal relationship expressed in the original question in the same way the answer of each simple question are re composed fulfilling the temporal restriction of the original complex question using this architecture a temporal q a system ha been developed in this paper we focus on explaining the first part of the process the decomposition of the complex question furthermore it ha been evaluated with the terqas question corpus of temporal question for the task of question splitting our system ha performed in term of precision and recall and respectively 
we present a data and error analysis for semantic role labelling in a first experiment we build a generic statistical model for semantic role assignment in the framenet paradigm and show that there is a high variance in performance across frame the main hypothesis of our paper is that this variance is to a large extent a result of difference in the underlying argument structure of the predicate in different frame in a second experiment we show that frame uniformity which measure argument structure variation correlate well with the performance figure effectively explaining the variance 
previous research ha demonstrated the utility of clustering in inducing semantic verb class from undisambiguated corpus data we describe a new approach which involves clustering subcategorization frame scf distribution using the information bottleneck and nearest neighbour method in contrast to previous work we particularly focus on clustering polysemic verb a novel evaluation scheme is proposed which account for the effect of polysemy on the cluster offering u a good insight into the potential and limitation of semantically classifying undisambiguated scf data 
in this paper we investigate whether paragraph can be identified automatically in different language and domain we propose a machine learning approach which exploit textual and discourse cue and we ass how well human perform on this task our best model achieve an accuracy that is significantly higher than the best baseline and for most data set come to within of human performance 
this paper discus the challenge and proposes a solution to performing information retrieval on the web using chinese natural language speech query the main contribution of this research is in devising a divide and conquer strategy to alleviate the speech recognition error it us the query model to facilitate the extraction of main core semantic string cs from the chinese natural language speech query it then break the cs into basic component corresponding to phrase and us a multi tier strategy to map the basic component to known phrase in order to further eliminate the error the resulting system ha been found to be effective 
this prototype system demonstrates a novel sentence alignment method for bilingual text based on adaptive learning and lexical information the system aligns bilingual text at the paragraph level first and acquires length related statistic for the subsequent sentence alignment process in addition to length a probabilistic translation lexicon is utilized to further enhance the precision the system is especially effective in the case of noisy translation produced in either translation direction that may involve different domain 
director musices is a program that transforms notated score into musical performance it implement the performance rule emerging from research project at the royal institute of technology kth rule in the program model performance aspect such a phrasing articulation and intonation and they operate on performance variable such a tone inter onset duration amplitude and pitch by manipulating rule parameter the user can act a a metaperformer controlling different feature of the performance leaving the technical execution to the computer different interpretation of the same piece can easily be obtained feature of director musices include midi file input and output rule palette graphical display of all performance variable along with the notation and userdefined performance rule the program is implemented in common lisp and is available free a a stand alone application both for macintosh and window platform further information including music example publication and the program itself is located online at http www speech kth se music performance this paper is a revised and updated version of a previous paper published in the computer music journal in year that wa mainly written by anders friberg friberg colombo fryd n and sundberg 
this paper take a critical look at the feature used in the semantic role tagging literature and show that the information in the input generally a syntactic parse tree ha yet to be fully exploited we propose an additional set of feature and our experiment show that these feature lead to fairly signicant improvement in the task we performed we further show that dierent feature are needed for dierent subtasks finally we show that by using a maximum entropy classier and fewer feature we achieved result comparable with the best previously reported result obtained with svm model we believe this is a clear indication that developing feature that capture the right kind of information is crucial to advancing the stateof the art in semantic analysis 
we propose a general model for joint inference in correlated natural language processing task when fully annotated training data is not available and apply this model to the dual task of word sense disambiguation and verb subcategorization frame determination the model us the em algorithm to simultaneously complete partially annotated training set and learn a generative probabilistic model over multiple annotation when applied to the word sense and verb subcategorization frame determination task the model learns sharp joint probability distribution which correspond to linguistic intuition about the correlation of the variable use of the joint model lead to error reduction over competitive independent model on these task 
a method for automatic plot analysis of narrative text that us component of both traditional symbolic analysis of natural language and statistical machine learning is presented for the story rewriting task in the story rewriting task an exemplar story is read to the pupil and the pupil rewrite the story in their own word this allows them to practice language skill such a spelling diction and grammar without being stymied by content creation often the pupil improperly recall the story our method of automatic plot analysis enables the tutoring system to automatically analyze the student s story for both general coherence and specific missing event 
one of the most robust finding of experimental psycholinguistics is that the context in which a word is presented influence the effort involved in processing that word we present a computational model of contextual facilitation based on word co occurrence vector and empirically validate the model through simulation of three representative type of context manipulation single word priming multiple priming and contextual constraint the aim of our study is to find out whether special purpose mechanism are necessary in order to capture the pattern of the experimental result 
the model used by the ccg parser of hockenmaier and steedman b would fail to capture the correct bilexical dependency in a language with freer word order such a dutch this paper argues that probabilistic parser should therefore model the dependency in the predicate argument structure a in the model of clark et al and defines a generative model for ccg derivation that capture these dependency including bounded and unbounded long range dependency 
this paper present japanese morphological analysis based on conditional random field crfs previous work in crfs assumed that observation sequence word boundary were fixed however word boundary are not clear in japanese and hence a straightforward application of crfs is not possible we show how crfs can be applied to situation where word boundary ambiguity exists crfs offer a solution to the long standing problem in corpus based or statistical japanese morphological analysis first flexible feature design for hierarchical tagsets become possible second influence of label and length bias are minimized we experiment crfs on the standard testbed corpus used for japanese morphological analysis and evaluate our result using the same experimental dataset a the hmms and memms previously reported in this task our result confirm that crfs not only solve the long standing problem but also improve the performance over hmms and memms 
this paper proposes a new method for word translation disambiguation using a machine learning technique called bilingual bootstrapping bilingual bootstrapping make use of in learning a small number of classified data and a large number of unclassified data in the source and the target language in translation it construct classifier in the two language in parallel and repeatedly boost the performance of the classifier by further classifying data in each of the two language and by exchanging between the two language information regarding the classified data experimental result indicate that word translation disambiguation based on bilingual bootstrapping consistently and significantly outperforms the existing method based on monolingual bootstrapping 
the paper report on progress in building computational model of a constructivist approach to language development it introduces a formalism for construction grammar and learning strategy based on invention abduction and induction example are drawn from experiment exercising the model in situated language game played by embodied artificial agent 
abstract introduction text summarizationtext summarization is the process of taking a textdocument and creating a compressed version thatconsists of the most useful information for the user one distinguishes between single document summarizers sd and multi document summarizers md multi document summarization is muchmore complicated than single document summarization factor that make multi document summarizationmore difficult include multiple article can be written by different 
this paper present a chinese word segmentation system that us improved source channel model of chinese sentence generation chinese word are defined a one of the following four type lexicon word morphologically derived word factoid and named entity our system provides a unified approach to the four fundamental feature of word level chinese language processing word segmentation morphological analysis factoid detection and named entity recognition the performance of the system is evaluated on a manually annotated test set and is also compared with several state of the art system taking into account the fact that the definition of chinese word often varies from system to system 
recent text and speech processing application such a speech mining raise new and more general problem related to the construction of language model we present and describe in detail several new and efficient algorithm to address these more general problem and report experimental result demonstrating their usefulness we give an algorithm for computing efficiently the expected count of any sequence in a word lattice output by a speech recognizer or any arbitrary weighted automaton describe a new technique for creating exact representation of n gram language model by weighted automaton whose size is practical for offline use even for a vocabulary size of about word and an n gram order n and present a simple and more general technique for constructing class based language model that allows each class to represent an arbitrary weighted automaton an efficient implementation of our algorithm and technique ha been incorporated in a general software library for language modeling the grm library that includes many other text and grammar processing functionality 
this paper aim at providing a view of text recycled within a short time by the author themselves we first present a simple and general method for extracting reused term sequence and then analyze several author identified text collection to compare the statistical quantity the ratio of recycling is also measured for each collection finally related research topic are introduced together with some discussion of future research direction 
introductionthe design of the askmsr question answering system ismotivated by recent observation in natural languageprocessing that for many application significantimprovements in accuracy can be attained simply byincreasing the amount of data used for learning e g banko amp brill by taking advantage of the vastamount of online text available via the worldwide web rather than relying on an approach that depends heavily onnatural language intensive technique we developed 
qa by dossier with constraint is a new approach to question answering whereby candidate answer confidence are adjusted by asking auxiliary question whose answer constrain the original answer these constraint emerge naturally from the domain of interest and enable application of real world knowledge to qa we show that our approach significantly improves system performance relative improvement in f measure on select question type and can create a dossier of information about the subject matter in the original question 
this paper is concerned with learning categorial grammar in gold s model in contrast to k valued classical categorial grammar k valued lambek grammar are not learnable from string this result wa shown for several variant but the question wa left open for the weakest one the non associative variant nl we show that the class of rigid and k valued nl grammar is unlearnable from string for each k this result is obtained by a specific construction of a limit point in the considered class that doe not use product operator another interest of our construction is that it provides limit point for the whole hierarchy of lambek grammar including the recent pregroup grammar such a result aim at clarifying the possible direction for future learning algorithm it express the difficulty of learning categorial grammar from string and the need for an adequate structure on example 
abstract awide range of supervised learning algorithm ha been applied to text categorization however the supervised learning approach have some problem one ofthem is that they require a large often prohibitive number of labeled training document for accurate learning generally acquiring class label for training data is costly while gathering a large quantity of unlabeled data is cheap we here propose a new automatic text categorization method for learning from only unlabeled data using a bootstrapping framework and a feature projection technique from result of our experiment our method showed reasonably comparable performance compared with a supervised method if our method is used in a text categorization task building text categorization system will become significantly faster and le expensive 
often the training procedure for statistical machine translation model is based on maximum likelihood or related criterion a general problem of this approach is that there is only a loose relation to the final translation quality on unseen text in this paper we analyze various training criterion which directly optimize translation quality these training criterion make use of recently proposed automatic evaluation metric we describe a new algorithm for efficient training an unsmoothed error count we show that significantly better result can often be obtained if the final evaluation criterion is taken directly into account a part of the training procedure 
we present a method capable of extracting parallel sentence from far more disparate very non parallel corpus than previous comparable corpus method by exploiting bootstrapping on top of ibm model em step of our method like previous method us similarity measure to find matching document in a corpus first and then extract parallel sentence a well a new word translation from these document but unlike previous method we extend this with an iterative bootstrapping framework based on the principle of find one get more which claim that document found to contain one pair of parallel sentence must contain others even if the document are judged to be of low similarity we re match document based on extracted sentence pair and refine the mining process iteratively until convergence this novel find one get more principle allows u to add more parallel sentence from dissimilar document to the baseline set experimental result show that our proposed method is nearly more effective than the baseline method without iteration we also show that our method is effective in boosting the performance of the ibm model em lexical learner a the latter though stronger than model used in previous work doe not perform well on data from very non parallel corpus 
we apply a novel variant of random forest breiman to the shallow semantic parsing problem and show extremely promising result the final system ha a semantic role classification accuracy of using propbank gold standard par these result are better than all others published except those of the support vector machine svm approach implemented by pradhan et al and random forest have numerous advantage over svms including simplicity faster training and classification easier multi class classification and easier problem specific customization we also present new feature which result in a gain in classification accuracy and describe a technique that result in a reduction in the feature space with no significant degradation in accuracy 
we present the first application of the head driven statistical parsing model of collins a a simultaneous language model and parser for large vocabulary speech recognition the model is adapted to an online left to right chart parser for word lattice integrating acoustic n gram and parser probability the parser us structural and lexical dependency not considered by n gram model conditioning recognition on more linguistically grounded relationship experiment on the wall street journal treebank and lattice corpus show word error rate competitive with the standard n gram language model while extracting additional structural information useful for speech understanding 
we train a decision tree inducer cart and a memory based classifier mbl on predicting prosodic pitch accent and break in dutch text on the basis of shallow easy to compute feature we train the algorithm on both task individually and on the two task simultaneously the parameter of both algorithm and the selection of feature are optimized per task with iterative deepening an efficient wrapper procedure that us progressive sampling of training data result show a consistent significant advantage of mbl over cart and also indicate that task combination can be done at the cost of little generalization score loss test on cross validated data and on held out data yield f score of mbl on accent placement of and respectively and on break of and respectively accent placement is shown to outperform an informed baseline rule reliably predicting break other than those already indicated by intra sentential punctuation however appears to be more challenging 
this paper compare a number of generative probability model for a wide coverage combinatory categorial grammar ccg parser these model are trained and tested on a corpus obtained by translating the penn treebank tree into ccg normal form derivation according to an evaluation of unlabeled word word dependency our best model achieves a performance of comparable to the figure given by collins for a linguistically le expressive grammar in contrast to gildea we find a significant improvement from modeling word word dependency 
it is often useful to classify email according to the intent of the sender e g propose a meeting deliver information we present experimental result in learning to classify email in this fashion where each class corresponds to a verb noun pair taken from a predefined ontology describing typical email speech act we demonstrate that although this categorization problem is quite different from topical text classification certain category of message can nonetheless be detected with high precision above and reasonable recall above using existing text classification learning method this result suggests that useful task tracking tool could be constructed based on automatic classification into this taxonomy 
most foreign name are transliterated into chinese japanese or korean with approximate phonetic equivalent the transliteration is usually achieved through intermediate phonemic mapping this paper present a new framework that allows direct orthographical mapping dom between two different language through a joint source channel model also called n gram transliteration model tm with the n gram tm model we automate the orthographic alignment process to derive the aligned transliteration unit from a bilingual dictionary the n gram tm under the dom framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state of the art machine learning algorithm the modeling framework is validated through several experiment for english chinese language pair 
a central problem of word sense disambiguation wsd is the lack of manually sense tagged data required for supervised learning in this paper we evaluate an approach to automatically acquire sense tagged training data from english chinese parallel corpus which are then used for disambiguating the noun in the senseval english lexical sample task our investigation reveals that this method of acquiring sense tagged data is promising on a subset of the most difficult senseval noun the accuracy difference between the two approach is only and the difference could narrow further to if we disregard the advantage that manually sense tagged data have in their sense coverage our analysis also highlight the importance of the issue of domain dependence in evaluating wsd program 
we present an unsupervised methodfor word sense disambiguation thatexploits translation correspondencesin parallel corpus the techniquetakes advantage of the fact that crosslanguagelexicalizations of the sameconcept tend to be consistent preservingsome core element of it semantics and yet also variable reflecting differingtranslator preference and the influenceof context working with parallelcorpora introduces an extra complicationfor evaluation since it is 
in this paper we present the rwth fsa toolkit an efficient implementation of algorithm for creating and manipulating weighted finite state automaton the toolkit ha been designed using the principle of on demand computation and offer a large range of widely used algorithm to prove the superior efficiency of the toolkit we compare the implementation to that of other publically available toolkits we also show that on demand computation help to reduce memory requirement significantly without any loss in speed to increase it flexibility the rwth fsa toolkit support high level interface to the programming language python a well a a command line tool for interactive manipulation of fsas furthermore we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system future extensibility of the toolkit is ensured a it will be publically available a open source software 
we describe a method for enriching the output of a parser with information available in a corpus the method is based on graph rewriting using memory based learning applied to dependency structure this general framework allows u to accurately recover both grammatical and semantic information a well a non local dependency it also facilitates dependency based evaluation of phrase structure parser our method is largely independent of the choice of parser and corpus and show state of the art performance 
coreferential information of a candidate such a the property of it antecedent is important for pronoun resolution because it reflects the salience of the candidate in the local discourse such information however is usually ignored in previous learning based system in this paper we present a trainable model which incorporates coreferential information of candidate into pronoun resolution preliminary experiment show that our model will boost the resolution performance given the right antecedent of the candidate we further discus how to apply our model in real resolution where the antecedent of the candidate are found by a separate noun phrase resolution module the experimental result show that our model still achieves better performance than the baseline 
we augment a model of translation based on re ordering node in syntactic tree in order to allow alignment not conforming to the original tree structure while keeping computational complexity polynomial in the sentence length this is done by adding a new subtree cloning operation to either tree to string or tree to tree alignment algorithm 
we show that a practical translation of mr description into normal dominance constraint is feasible we start from a recent theoretical translation and verify it assumption on the output of the english resource grammar erg on the redwood corpus the main assumption of the translation that all relevant underspecified description are net is validated for a large majority of case all non net computed by the erg seem to be systematically incomplete 
this paper proposes the application of finite state approximation technique on a unification based grammar of word formation for a language like german a refinement of an rtn based approximation algorithm is proposed which extends the state space of the automaton by selectively adding distinction based on the parsing history at the point of entering a context free rule the selection of history item exploit the specific linguistic nature of word formation a experiment show this algorithm avoids an explosion of the size of the automaton in the approximation construction 
in this paper we have designed and experimented novel convolution kernel for automatic classification of predicate argument their main property is the ability to process structured representation support vector machine svms using a combination of such kernel and the flat feature kernel classify prop bank predicate argument with accuracy higher than the current argument classification state of the art additionally experiment on framenet data have shown that svms are appealing for the classification of semantic role even if the proposed kernel do not produce any improvement 
this paper present an unsupervised learning approach to building a non english arabic stemmer the stemming model is based on statistical machine translation and it us an english stemmer and a small k sentence parallel corpus a it sole training resource no parallel text is needed after the training phase monolingual unannotated text can be used to further improve the stemmer by allowing it to adapt to a desired domain or genre example and result will be given for arabic but the approach is applicable to any language that need affix removal our resource frugal approach result in agreement with a state of the art proprietary arabic stemmer built using rule affix list and human annotated text in addition to an unsupervised component task based evaluation using arabic information retrieval indicates an improvement of in average precision over unstemmed text and of the performance of the proprietary stemmer above 
we present a new approach to intrinsic summary evaluation based on initial experiment in van halteren and teufel which combine two novel aspect comparison of information content rather than string similarity in gold standard and system summary measured in shared atomic information unit which we call factoid and comparison to more than one gold standard summary in our data and summary respectively in this paper we show that factoid annotation is highly reproducible introduce a weighted factoid score estimate how many summary are required for stable system ranking and show that the factoid score cannot be su ciently approximated by unigrams and the duc information overlap measure 
a mobile computing and communication have become popular predictive text entry system have become an increasingly important technology existing method still need refinement though with respect to personalization especially how to acquire vocabulary not pre registered in the system dictionary in this paper we report on an automatic method that dynamically obtains a user specific vocabulary from the user s unanalyzed document when a user make an entry the system dynamically extract the corresponding chunk from the user text and suggests them along with word suggested by the dictionary with our method text in a particular style or concerning a specific domain can be entered using a predictive text entry system we verified that a large amount of word not registered in the dictionary can be entered using our method 
we are interested in the problem of modeling and evaluating spoken language system in the context of human machine dialog spoken dialog corpus allow for a multidimensional analysis of speech recognition and language understanding model of dialog system therefore language model can be directly trained based either on the dialog history or it equivalence class or cluster in this paper we propose an algorithm to mine dialog trace which exhibit similar pattern and are identied by the same class for this purpose we apply data clustering method to large human machine spoken dialogue corpus the resulting cluster can be used for system evaluation and language modeling by clustering dialog trace we expect to learn about the behavior of the system with regard to not only the automation rate but the nature of the interaction e g easy v dicult dialog the equivalence class can also be used in order to automatically adapt the language model the understanding module and the dialogue strategy to better t the kind of interaction detected this paper investigates different way for encoding dialogue into multidimensional structure and dieren t clustering method preliminary result are given for cluster interpretation and dynamic model adaptation using the cluster obtained 
abstract robustness is a key requirement in spoken languageunderstanding slu system humanspeech is often ungrammatical and ill formed and there will frequently be a mismatch betweentraining and test data this paper discussesrobustness and adaptation issue in astatistically based slu system which is entirelydata driven to test robustness the systemhas been tested on data from the air travelinformation service atis domain which hasbeen artificially corrupted with varying 
we describe a statistical approach for modeling agreement and disagreement in conversational interaction our approach first identifies adjacency pair using maximum entropy ranking based on a set of lexical durational and structural feature that look both forward and backward in the discourse we then classify utterance a agreement or disagreement using these adjacency pair and feature that represent various pragmatic influence of previous agreement or disagreement on the current utterance our approach achieves accuracy a increase over previous work 
the paper present an approach to ellipsisresolution in a framework of scope underspecification underspecified discourserepresentation theory it is argued thatthe approach improves on previous proposalsto integrate ellipsis resolution andscope underspecification crouch egg et al in that application processeslike anaphora resolution do not requirefull disambiguation but can workdirectly on the underspecified representation 
we investigate global index grammar gig a grammar formalism that us a stack of index associated with production and ha restricted context sensitive power we discus some of the structural description that gig can generate compared with those generated by ligs we show also how gig can represent structural description corresponding to hpsgs pollard and sag schema 
abstract this paper summarizes the author s experience implementing somethingresembling abstract type in acl it assumes some experiencewith the acl theorem prover 
standard ir system can process query such a web not internet enabling user who are interested in arachnid to avoid document about computing the document retrieved for such a query should be irrelevant to the negated query term most system implement this by reprocessing result after retrieval to remove document containing the unwanted string of letter this paper describes and evaluates a theoretically motivated method for removing unwanted meaning directly from the original query in vector model with the same vector negation operator a used in quantum logic irrelevance in vector space is modelled using orthogonality so query vector are made orthogonal to the negated term or term a well a removing unwanted term this form of vector negation reduces the occurrence of synonym and neighbour of the negated term by a much a compared with standard boolean method by altering the query vector itself vector negation remove not only unwanted string but unwanted meaning 
this paper proposes the hierarchical directed acyclic graph hdag kernel for structured natural language data the hdag kernel directly accepts several level of both chunk and their relation and then efficiently computes the weighed sum of the number of common attribute sequence of the hdags we applied the proposed method to question classification and sentence alignment task to evaluate it performance a a similarity measure and a kernel function the result of the experiment demonstrate that the hdag kernel is superior to other kernel function and baseline method 
we describe how simple commonly understood statistical model such a statistical dependency parser probabilistic context free grammar and word to word translation model can be effectively combined into a unified bilingual parser that jointly search for the best english parse korean parse and word alignment where these hidden structure all constrain each other the model used for parsing is completely factored into the two parser and the tm allowing separate parameter estimation we evaluate our bilingual parser on the penn korean treebank and against several baseline system and show improvement parsing korean with very limited labeled data 
we describe ineats an interactive multi document summarization system that integrates a state of the art summarization engine with an advanced user interface three main goal of the system are provide a user with control over the summarization process support exploration of the document set with the summary a the staring point and combine text summary with alternative presentation such a a map based visualization of document 
this paper describes and evaluates log linear parsing model for combinatory categorial grammar ccg a parallel implementation of the l bfgs optimisation algorithm is described which run on a beowulf cluster allowing the complete penn treebank to be used for estimation we also develop a new efficient parsing algorithm for ccg which maximises expected recall of dependency we compare model which use all ccg derivation including non standard derivation with normal form model the performance of the two model are comparable and the result are competitive with existing wide coverage ccg parser 
this paper describes two method for detecting word segment and their morphological information in a japanese spontaneous speech corpus and describes how to tag a large spontaneous speech corpus accurately by using the two method the first method is used to detect any type of word segment the second method is used when there are several definition for word segment and their po category and when one type of word segment includes another type of word segment in this paper we show that by using semi automatic analysis we achieve a precision of better than for detecting and tagging short word and for long word the two type of word that comprise the corpus we also show that better accuracy is achieved by using both method than by using only the first 
we describe the ongoing construction of a large semantically annotated corpus resource a reliable basis for the large scale acquisition of word semantic information e g the construction of domain independent lexica the backbone of the annotation are semantic role in the frame semantics paradigm we report experience and evaluate the annotated data from the first project stage on this basis we discus the problem of vagueness and ambiguity in semantic annotation 
we apply statistical machine translation smt tool to generate novel paraphrase of input sentence in the same language the system is trained on large volume of sentence pair automatically extracted from clustered news article available on the world wide web alignment error rate aer is measured to gauge the quality of the resulting corpus a monotone phrasal decoder generates contextual replacement human evaluation show that this system outperforms baseline paraphrase generation technique and in a departure from previous work offer better coverage and scalability than the current best of breed paraphrasing approach 
in word sense disambiguation wsd the heuristic of choosing the most common sense is extremely powerful because the distribution of the sens of a word is often skewed the problem with using the predominant or first sense heuristic aside from the fact that it doe not take surrounding context into account is that it assumes some quantity of hand tagged data whilst there are a few hand tagged corpus available for some language one would expect the frequency distribution of the sens of word particularly topical word to depend on the genre and domain of the text under consideration we present work on the use of a thesaurus acquired from raw textual corpus and the wordnet similarity package to find predominant noun sens automatically the acquired predominant sens give a precision of on the noun of the senseval english all word task this is a very promising result given that our method doe not require any hand tagged text such a semcor furthermore we demonstrate that our method discovers appropriate predominant sens for word from two domain specific corpus 
some document genre contain a large number of figure this position paper outline approach to diagram summarization that can augment the many well developed technique of text summarization we discus figure a surrogate for entire document thumbnail extraction the relation between text and figure a well a how automation might be achieved the focus is on diagram line drawing because they allow parsing technique to be used in contrast to the difficulty of general image understanding we describe the advance in raster image vectorization and parsing needed to produce corpus for diagram summarization 
we describe two probabilistic model for unsupervised word sense disambiguation using parallel corpus the first model which we call the sense model build on the work of diab and resnik that us both parallel text and a sense inventory for the target language and recasts their approach in a probabilistic framework the second model which we call the concept model is a hierarchical model that us a concept latent variable to relate different language specific sense label we show that both model improve performance on the word sense disambiguation task over previous unsupervised approach with the concept model showing the largest improvement furthermore in learning the concept model a a by product we learn a sense inventory for the parallel language 
we present the status of an on going work aiming at introducingsymbolic simulation and theorem proving in a design flow that usesconventional description and simulation language this paper focuseson the formalization of the cycle simulation semantics of a synchronoussubset of vhdl in the acl logic the model is executable and theresults of it symbolic simulation can be proven equal to a specified expression 
we introduce a probabilistic noisy channel model for question answering and we show how it can be exploited in the context of an end to end qa system our noisy channel system outperforms a state of the art rule based qa system that us similar resource we also show that the model we propose is flexible enough to accommodate within one mathematical framework many qa specific resource and technique which range from the exploitation of wordnet structured and semi structured database to reasoning and paraphrasing 
question answering qa evaluation effort have largely been tailored to open domain system the trec qa test collection contain newswire article and the accompanying query cover a wide variety of topic while some apprehension about the limitation of restricteddomain system is no doubt justified the strict promotion of unlimited domain qa evaluation may have some unintended consequence simply applying the open domain qa evaluation paradigm to a restricted domain system pose problem in the area of test question development answer key creation and test collection construction this paper examines the evaluation requirement of restricted domain system it incorporates evaluation criterion identified by user of an operational qa system in the aerospace engineering domain while the paper demonstrates that user centered task based evaluation are required for restricted domain system these evaluation are found to be equally applicable to open domain system 
we discus existing approach to train lr parser which have been used for statistical resolution of structural ambiguity these approach are nonoptimal in the sense that a collection of probability distribution cannot be obtained in particular some probability distribution expressible in term of a context free grammar cannot be expressed in term of the lr parser constructed from that grammar under the restriction of the existing approach to training of lr parser we present an alternative way of training that is provably optimal and that allows all probability distribution expressible in the context free grammar to be carried over to the lr parser we also demonstrate empirically that this kind of training can be effectively applied on a large treebank 
we present a system for identifying the semantic relationship or semantic role filled by constituent of a sentence within a semantic frame given an input sentence and a target word and frame the system label constituent with either abstract semantic role such a agent or patient or more domain specific semantic role such a speaker message and topic the system is based on statistical classifier trained on roughly sentence that were hand annotated with semantic role by the framenet semantic labeling project we then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic feature including the phrase type of each constituent it grammatical function and it position in the sentence these feature were combined with knowledge of the predicate verb noun or adjective a well a information such a the prior probability of various combination of semantic role we used various lexical clustering algorithm to generalize across possible filler of role test sentence were parsed were annotated with these feature and were then passed through the classifier our system achieves accuracy in identifying the semantic role of presegmented constituent at the more difficult task of simultaneously segmenting constituent and identifying their semantic role the system achieved precision and recall our study also allowed u to compare the usefulness of different feature and feature combination method in the semantic role labeling task we also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicate unseen in the training data 
aligning word from sentence which are mutual translation is an important problem in different setting such a bilingual terminology extraction machine translation or projection of linguistic feature here we view word alignment a matrix factorisation in order to produce proper alignment we show that factor must satisfy a number of constraint such a orthogonality we then propose an algorithm for orthogonal non negative matrix factorisation based on a probabilistic model of the alignment data and apply it to word alignment this is illustrated on a french english alignment task from the hansard 
we present a technique that improves the efficiency of word lattice parsing a used in speech recognition language modeling our technique applies a probabilistic parser iteratively where on each iteration it focus on a different subset of the wordlattice the parser s attention is shifted towards word lattice subset for which there are few or no syntactic analysis posited this attention shifting technique provides a six time increase in speed measured a the number of parser analysis evaluated while performing equivalently when used a the first stage of a multi stage parsing based language model 
one of the main goal of the rialist group at ames is to support human exploration and development of space heds by applying spoken dialogue technology to training and task performance on the ground and in space we are moving to address this goal by developing a system aimed at taking technical documentation from the international space station and producing spoken dialogue geared towards astronaut training and task support in this demo abstract we will introduce the scope and nature of task performed on board the international space station describe our system architecture and discus the range of functionality afforded by the current system in the demonstration itself conference participant will be provided a hand on experience with our system which convert a technical procedure written a a set of 
abstract we present a syntax based statistical translation model our model transforms a source language parse tree into a target language string by applying stochastic operation at each node these operation capture linguistic difference such a word order and case marking model parameter are estimated in polynomial time using an em algorithm the model produce word alignment that are better than those produced by ibm model 
we use machine learner trained on a combination of acoustic confidence and pragmatic plausibility feature computed from dialogue context to predict the accuracy of incoming n best recognition hypothesis to a spoken dialogue system our best result show a weighted f score improvement over a baseline system that implement a grammar switching approach to context sensitive speech recognition 
this paper describes a noisy channel model of speech repair which can identify and correct repair in speech transcript a syntactic parser is used a the source model and a novel type of tag based transducer is the channel model the use of tag is motivated by the intuition that the reparandum is a rough copy of the repair the model is trained and tested on the switchboard disfluency annotated corpus 
we describe a single document text summarizer using the text engineering framework gate the summariser extract sentence using a combination of simple bayes classifier resolve anaphora using gate s annie module simplifies word using the mrc psycho linguistic database and wordnet and supply background information to named person and place using internet resource 
in this paper we present a methodology for extracting subcategorisation frame based on an automatic lfg f structure annotation algorithm for the penn ii treebank we extract abstract syntactic function based subcategorisation frame lfg semantic form traditional cfg category based subcategorisation frame a well a mixed function category based frame with or without preposition information for oblique and particle information for particle verb our approach doe not predefine frame associate probability with frame conditional on the lemma distinguishes between active and passive frame and fully reflects the effect of long distance dependency in the source data structure we extract verb lemma semantic form type an average of per lemma with frame type we present a large scale evaluation of the complete set of form extracted against the full comlex resource 
we investigate the verbal and nonverbal mean for grounding and propose a design for embodied conversational agent that relies on both kind of signal to establish common ground in human computer interaction we analyzed eye gaze head nod and attentional focus in the context of a direction giving task the distribution of nonverbal behavior differed depending on the type of dialogue move being grounded and the overall pattern reflected a monitoring of lack of negative feedback based on these result we present an eca that us verbal and nonverbal grounding act to update dialogue state 
coreference resolution system usually attempt to find a suitable antecedent for almost every noun phrase recent study however show that many definite np are not anaphoric the same claim obviously hold for the indefinites a well in this study we try to learn automatically two classification relevant for this problem we use a small training corpus muc but also acquire some data from the internet combining our classifier sequentially we achieve precision and recall for discourse new entity we expect our classifier to provide a good prefiltering for coreference resolution system improving both their speed and performance 
the parameter of statistical translation model are typically estimated from sentence aligned parallel corpus we show that significant improvement in the alignment and translation quality of such model can be achieved by additionally including word aligned data during training incorporating word level alignment into the parameter estimation of the ibm model reduces alignment error rate and increase the bleu score when compared to training the same model only on sentence aligned data on the verbmobil data set we attain a reduction in the alignment error rate and a higher bleu score with half a many training example we discus how varying the ratio of word aligned to sentence aligned data affect the expected performance gain 
this paper present a new bottom up chart parsing algorithm for prolog along with a compilation procedure that reduces the amount of copying at run time to a constant number per edge it ha application to unification based grammar with very large partially ordered category in which copying is expensive and can facilitate the use of more sophisticated indexing strategy for retrieving such category that may otherwise be overwhelmed by the cost of such copying it also provides a new perspective on quick checking and related heuristic which seems to confirm that forcing an early failure a opposed to seeking an early guarantee of success is in fact the best approach to use a preliminary empirical evaluation of it performance is also provided 
in this paper we explore the use of random forest rf amit and geman breiman in language modeling the problem of predicting the next word based on word already seen before the goal in this work is to develop a new language modeling approach based on randomly grown decision tree dts and apply it to automatic speech recognition we study our rf approach in the context of gram type language modeling unlike regular gram language model rf language model have the potential to generalize well to unseen data even when a complicated history is used we show that our rf language model are superior to regular gram language model in reducing both the perplexity ppl and word error rate wer in a large vocabulary speech recognition system 
the purpose of this paper is to re examine the balance between clarity and efficiency in hpsg design with particular reference to the design decision made in the english resource grammar lingo erg it is argued that a simple generalization of the conventional delay statement used in logic programming is sufficient to restore much of the functionality and concomitant benefit that the erg elected to forego with an acceptable although still perceptible computational cost 
spoken dialogue system promise ef cient and natural access to information service from any phone recently spo ken dialogue system for widely used ap plication such a email travel informa tion and customer care have moved from research lab into commercial use these application can receive million of call a month this huge amount of spoken dialogue data ha led to a need for fully automatic method for selecting a subset of caller dialogue that are most likely to be useful for further system improve ment to be stored transcribed and further analyzed this paper report result on automatically training a problematic di alogue identi er to classify problematic human computer dialogue using a corpus of darpa communicator dialogue in the travel planning domain we show that using fully automatic feature we can identify class of problematic dialogue with accuracy from to 
abstract this paper introduces an approach to sentiment analysis which us support vector machine svms to bring together diverse source of potentially pertinent information including several favorability measure for phrase and adjective and where available knowledge of the topic of the text model using the feature introduced are further combined with unigram model which have been shown to be effective in the past pang et al and lemmatized version of the unigram model experiment on movie review data from epinions com demonstrate that hybrid svms which combine unigram style feature based svms with those based on real valued favorability measure obtain superior performance producing the best result yet published using this data further experiment using a feature set enriched with topic information on a smaller dataset of music review handannotated for topic are also reported the result of which suggest that incorporating topic information into such model may also yield improvement 
abstract we prove the soundness of a compositional model checking algorithm using acl the algorithm usesconjunctive and cone of inuence reduction to reduce a large model checking problem into a collectionof smaller problem and we prove the soundness of the composition of these reduction the algorithmchecks property specied in linear temporal logic ltl but the acl logic doe not allow u toexpress either the classical semantics of ltl or the classical soundness proof for these 
minimal recursion semantics mr is the standard formalism used in large scale hpsg grammar to model underspecified semantics we present the first provably efficient algorithm to enumerate the reading of mr structure by translating them into normal dominance constraint 
we propose a meta grammatical framework for dependency grammar accommodating any number of dimension and restricting their licensed model through the application of parametric principle we rst instantiate this framework to obtain the version of topological dependency grammar tdg proposed by duchier and debusmann we then describe an extended instantiation which add support for semantic dependency thus also providing an account of control and raising construction and meaning assembly 
this paper provides evidence for genzel and charniak s entropy rate principle which predicts that the entropy of a sentence increase with it position in the text we show that this principle hold for individual sentence not just for average but we also find that the entropy rate effect is partly an artifact of sentence length which also correlate with sentence position secondly we evaluate a set of prediction that the entropy rate principle make for human language processing using a corpus of eye tracking data we show that entropy and processing effort are correlated and that processing effort is constant throughout a text 
it is fairly common that different people are associated with the same name in tracking person entity in a large document pool it is important to determine whether multiple mention of the same name across document refer to the same entity or not previous approach to this problem involves measuring context similarity only based on co occurring word this paper present a new algorithm using information extraction support in addition to co occurring word a learning scheme with minimal supervision is developed within the bayesian framework maximum entropy modeling is then used to represent the probability distribution of context similarity based on heterogeneous feature statistical annealing is applied to derive the final entity coreference chain by globally fitting the pairwise context similarity benchmarking show that our new approach significantly outperforms the existing algorithm by percentage point in overall f measure 
we present a novel representation of parse tree a list of path leaf projection path from leaf to the top level of the tree this representation allows u to achieve significantly higher accuracy in the task of hpsg parse selection than standard model and make the application of string kernel natural we define tree kernel via string kernel on projection path and explore their performance in the context of parse disambiguation we apply svm ranking model and achieve an exact sentence accuracy of on the redwood corpus 
multi document person name resolution focus on the problem of determining if two instance with the same name and from different document refer to the same individual we present a two step approach in which a maximum entropy model is trained to give the probability that two name refer to the same individual we then apply a modified agglomerative clustering technique to partition the instance according to their referent 
this paper describes a multi site project to annotate six sizable bilingual parallel corpus for interlingual content after presenting the background and objective of the effort we will go on to describe the data set that is being annotated the interlingua representation language used an interface environment that support the annotation task and the annotation process itself we will then present a preliminary version of our evaluation methodology and conclude with a summary of the current status of the project along with a number of issue which have arisen 
we use machine learning technique to find the best combination of local focus and lexical distance feature for identifying the anchor of mereological bridging reference we find that using first mention utterance distance and lexical distance computed using either google or wordnet result in an accuracy significantly higher than obtained in previous experiment 
this paper present a new bootstrapping approach to named entity ne classification this approach only requires a few common noun pronoun seed that correspond to the concept for the target ne type e g he she man woman for person ne the entire bootstrapping procedure is implemented a training two successive learner i a decision list is used to learn the parsing based high precision ne rule ii a hidden markov model is then trained to learn string sequence based ne pattern the second learner us the training corpus automatically tagged by the first learner the resulting ne system approach supervised ne performance for some ne type the system also demonstrates intuitive support for tagging user defined ne type the difference of this approach from the co training based ne bootstrapping are also discussed 
the paper describes a particular approach to multiengine machine translation memt where we make use of voted language model to selectively combine translation output from multiple off the shelf mt system experiment are done using large corpus from three distinct domain the study found that the use of voted language model lead to an improved performance of memt system 
there is a long history of research in automatic text summarization system by both the text retrieval and the natural language processing community but evaluation of such system output ha always presented problem one critical problem remains how to handle the unavoidable variability in human judgment at the core of all the evaluation sponsored by the darpa tide project nist launched a new text summarization evaluation effort called duc in with follow on workshop in and human judgment provided the foundation for all three evaluation and this paper examines how the variation in those judgment doe and doe not affect the result and their interpretation 
traditional word alignment approach cannot come up with satisfactory result for named entity in this paper we propose a novel approach using a maximum entropy model for named entity alignment to ease the training of the maximum entropy model bootstrapping is used to help supervised learning unlike previous work reported in the literature our work conduct bilingual named entity alignment without word segmentation for chinese and it performance is much better than that with word segmentation when compared with ibm and hmm alignment model experimental result show that our approach outperforms ibm model and hmm significantly 
in this paper we describe a new methodology to develop mixed initiative spoken dialog system which is based on the extensive use of simulation to accelerate the development process with the help of simulation a system providing information about a database of nearly restaurant in the boston area ha been developed the simulator can produce thousand of unique dialog which benefit not only dialog development but also provide data to train the speech recognizer and understanding component in preparation for real user interaction also described is a strategy for creating cooperative response to user query incorporating an intelligent language generation capability that produce content dependent verbal description of listed item 
this paper introduces an indexing method based on static analysis of grammar rule and type signature for typed feature structure grammar tfsgs the static analysis try to predict at compile time which feature path will cause unification failure during parsing at run time to support the static analysis we introduce a new classification of the instance of variable used in tfsgs based on what type of structure sharing they create the indexing action that can be performed during parsing are also enumerated non statistical indexing ha the advantage of not requiring training and a the evaluation using large scale hpsgs demonstrates the improvement are comparable with those of statistical optimization such statistical optimization rely on data collected during training and their performance doe not always compensate for the training cost 
we have aligned japanese and english news article and sentence to make a large parallel corpus we first used a method based on cross language information retrieval clir to align the japanese and english article and then used a method based on dynamic programming dp matching to align the japanese and english sentence in these article however the result included many incorrect alignment to remove these we propose two measure score that evaluate the validity of alignment the measure for article alignment us similarity in sentence aligned by dp matching and that for sentence alignment us similarity in article aligned by clir they enhance each other to improve the accuracy of alignment using these measure we have successfully constructed a large scale article and sentence alignment corpus available to the public 
a method for the definition of verb predicate is proposed the definition of the predicate is essentially tied to a semantic interpretation algorithm that determines the predicate for the verb it semantic role and adjunct a predicate definition are complete they can be tested by running the algorithm on some sentence and verifying the resolution of the predicate semantic role and adjunct in those sentence the predicate are defined semiautomatically with the help of a software environment that us several section of a corpus to provide feedback for the definition of the predicate and then for the subsequent testing and refining of the definition the method is very flexible in adding a new predicate to a list of already defined predicate for a given verb the method build on an existing approach that defines predicate for wordnet verb class and that plan to define predicate for every english verb the definition of the predicate and the semantic interpretation algorithm are being used to automatically create a corpus of annotated verb predicate semantic role and adjunct 
state of the art machine translation technique are still far from producing high quality translation this drawback lead u to introduce an alternative approach to the translation problem that brings human expertise into the machine translation scenario in this framework namely computer assisted translation cat human translator interact with a translation system a an assistance tool that dinamically offer a list of translation that best completes the part of the sentence already translated in this paper finite state transducer are presented a a candidate technology in the cat paradigm the appropriateness of this technique is evaluated on a printer manual corpus and result from preliminary experiment confirm that human translator would reduce to le than the amount of work to be done for the same task 
temporal relation resolution involves extraction of temporal information explicitly or implicitly embedded in a language this information is often inferred from a variety of interactive grammatical and lexical cue especially in chinese for this purpose inter clause relation temporal or otherwise in a multiple clause sentence play an important role in this paper a computational model based on machine learning and heterogeneous collaborative bootstrapping is proposed for analyzing temporal relation in a chinese multiple clause sentence the model make use of the fact that event are represented in different temporal structure it take into account the effect of linguistic feature such a tense aspect temporal connective and discourse structure a set of experiment ha been conducted to investigate how linguistic feature could affect temporal relation resolution 
this paper present an unsupervised word sense learning algorithm which induces sens of target word by grouping it occurrence into a natural number of cluster based on the similarity of their context for removing noisy word in feature set feature selection is conducted by optimizing a cluster validation criterion subject to some constraint in an unsupervised manner gaussian mixture model and minimum description length criterion are used to estimate cluster structure and cluster number experimental result show that our algorithm can find important feature subset estimate model order cluster number and achieve better performance than another algorithm which requires cluster number to be provided 
parsing system which rely on hand coded linguistic description can only perform adequately in a far a these description are correct and complete the paper describes an error mining technique to discover problem in hand coded linguistic description for parsing such a grammar and lexicon by analysing parse result for very large unannotated corpus the technique discovers missing incorrect or incomplete linguistic description the technique us the frequency of n gram of word for arbitrary value of n it is shown how a new combination of suffix array and perfect hash finite automaton allows an efficient implementation 
anticipating the availability of large questionanswer datasets we propose a principled datadriven instance based approach to question answering most question answering system incorporate three major step classify question according to answer type formulate query for document retrieval and extract actual answer under our approach strategy for answering new question are directly learned from training data we learn model of answer type query content and answer extraction from cluster of similar question we view the answer type a a distribution rather than a class in an ontology in addition to query expansion we learn general content feature from training data and use them to enhance the query finally we treat answer extraction a a binary classification problem in which text snippet are labeled a correct or incorrect answer we present a basic implementation of these concept that achieves a good performance on trec test data 
we apply a decision tree based approach to pronoun resolution in spoken dialogue our system deal with pronoun with np and non np antecedent we present a set of feature designed for pronoun resolution in spoken dialogue and determine the most promising feature we evaluate the system on twenty switchboard dialogue and show that it compare well to byron s manually tuned system 
arabic morphology represents a special type of morphological system it is generally considered to be of the nonconcatenative type which depends on manipulating root letter in a nonconcatenative manner in addition to prefixation and suffixation inflectional and derivational process may cause stem to undergo infixational modification in the presence of different syntactic feature a well a certain stem consonant the basic problem then is the large number of variant that must be analyzed or generated in this paper we seek to reduce the complexity of arabic morphology using the lexeme based morphology theory to represent the linguistic resource and morphe a a computational tool to implement them we show that the space of rule can be kept small if we consider the stem a the phonological domain of realisation rule the reduction in the number of rule keep the system small and also increase it understandability and maintainability we primarily focus on generation of verb and broken plural 
in p manolios and j moore show that a tail recursive dening equation for a new function can always be consistently added to acl this is done by constructing a function that satises the proposed tail recursive dening equation their construction is extended to many primitive recursive dening equation this extends the known recursive scheme that can be consistently introduced into acl s logic exactly what is meant by primitive recursive and the exact restriction placed on the denitions are explained below 
knowledge of the anaphoricity of a noun phrase might be profitably exploited by a coreference system to bypass the resolution of non anaphoric noun phrase perhaps surprisingly recent attempt to incorporate automatically acquired anaphoricity information into coreference system however have led to the degradation in resolution performance this paper examines several key issue in computing and using anaphoricity information to improve learning based coreference system in particular we present a new corpus based approach to anaphoricity determination experiment on three standard coreference data set demonstrate the effectiveness of our approach 
abstract knowledge of the anaphoricity of a noun phrase might be profitably exploited by a coreference system to bypass the resolution of non anaphoric noun phrase perhaps surprisingly recent attempt to incorporate automatically acquired anaphoricity information into coreference system however have led to the degradation in resolution performance this paper examines several key issue in computing and using anaphoricity information to improve learning based coreference system in particular we present a new corpus based approach to anaphoricity determination experiment on three standard coreference data set demonstrate the effectiveness of our approach 
broad coverage repository of semantic relation between verb could benefit many nlp task we present a semi automatic method for extracting fine grained semantic relation between verb we detect similarity strength antonymy enablement and temporal happens before relation between pair of strongly associated verb using lexicosyntactic pattern over the web on a set of strongly associated verb pair our extraction algorithm yielded accuracy analysis of error type show that on the relation strength we achieved accuracy we provide the resource called verbocean for download at http semantics isi edu ocean 
we present a novel data driven method for integrated shallow and deep parsing mediated by an xml based multi layer annotation architecture we interleave a robust but accurate stochastic topological field parser of german with a constraint based hpsg parser our annotation based method for dovetailing shallow and deep phrasal constraint is highly flexible allowing targeted and fine grained guidance of constraint based parsing we conduct systematic experiment that demonstrate substantial performance gain 
we introduce a new method for disambiguating word sens that exploit a nonlinear kernel principal component analysis kpca technique to achieve accuracy superior to the best published individual model we present empirical result demonstrating significantly better accuracy compared to the state of the art achieved by either na ve bayes or maximum entropy model on senseval data we also contrast against another type of kernel method the support vector machine svm model and show that our kpca based model outperforms the svm based model it is hoped that these highly encouraging first result on kpca for natural language processing task will inspire further development of these direction 
irregular so called broken plural identification in modern standard arabic is a problematic issue for information retrieval ir and language engineering application but their effect on the performance of ir ha never been examined broken plural bps are formed by altering the singular a in english toothteeth through an application of interdigitating pattern on stem and singular word cannot be recovered by standard affix stripping stemming technique we developed several method for bp detection and evaluated them using an unseen test set we incorporated the bp detection component into a new light stemming algorithm that conflates both regular and broken plural with their singular form we also evaluated the new light stemming algorithm within the context of information retrieval comparing it performance with other stemming algorithm 
we have developed willex a tool that help grammar developer to work efficiently by using annotated corpus and recording parsing error willex ha two major new function first it decrease ambiguity of the parsing result by comparing them to an annotated corpus and removing wrong partial result both automatically and manually second willex accumulates parsing error a data for the developer to clarify the defect of the grammar statistically we applied willex to a large scale hpsg style grammar a an example 
we discus feature latent semantic analysis flsa an extension to latent semantic analysis lsa lsa is a statistical method that is ordinarily trained on word only flsa add to lsa the richness of the many other linguistic feature that a corpus may be labeled with we applied flsa to dialogue act classification with excellent result we report result on three corpus callhome spanish maptask and our own corpus of tutoring dialogue 
we address appropriate user modeling in order to generate cooperative response to each user in spoken dialogue system unlike previous study that focus on user s knowledge or typical kind of user the user model we propose is more comprehensive specifically we set up three dimension of user model skill level to the system knowledge level on the target domain and the degree of hastiness moreover the model are automatically derived by decision tree learning using real dialogue data collected by the system we obtained reasonable classification accuracy for all dimension dialogue strategy based on the user modeling are implemented in kyoto city bus information system that ha been developed at our laboratory experimental evaluation show that the cooperative response adaptive to individual user serve a good guidance for novice user without increasing the dialogue duration for skilled user 
this paper describes an extension of the semantic grammar used in conventional statistical spoken language interface to allow the probability of derived analysis to be conditioned on the meaning or denotation of input utterance in the context of an interface s underlying application environment or world model since these denotation will be used to guide disambiguation in interactive application they must be efficiently shared among the many possible analysis that may be assigned to an input utterance this paper therefore present a formal restriction on the scope of variable in a semantic grammar which guarantee that the denotation of all possible analysis of an input utterance can be calculated in polynomial time without undue constraint on the expressivity of the derived semantics empirical test show that this model theoretic interpretation yield a statistically significant improvement on standard measure of parsing accuracy over a baseline grammar not conditioned on denotation 
we have been investigating an interactive approach for open domain qa odqa and have constructed a spoken interactive odqa system the system derives disambiguating query dqs that draw out additional information to test the efficiency of additional information requested by the dqs the system reconstructs the user s initial question by combining the addition information with question the combination is then used for answer extraction experimental result revealed the potential of the generated dqs 
in this paper we propose a multi criterion based active learning approach and effectively apply it to named entity recognition active learning target to minimize the human annotation effort by selecting example for labeling to maximize the contribution of the selected example we consider the multiple criterion informativeness representativeness and diversity and propose measure to quantify them more comprehensively we incorporate all the criterion using two selection strategy both of which result in le labeling cost than single criterion based method the result of the named entity recognition in both muc and genia show that the labeling cost can be reduced by at least without degrading the performance 
accurate dependency recovery ha recently been reported for a number of wide coverage statistical parser using combinatory categorial grammar ccg however overall figure give no indication of a parser s performance on specific construction nor how suitable a parser is for specific application in this paper we give a detailed evaluation of a ccg parser on object extraction dependency found in wsj text we also show how the parser can be used to parse question for question answering the accuracy of the original parser on question is very poor and we propose a novel technique for porting the parser to a new domain by creating new labelled data at the lexical category level only using a supertagger to assign category to word trained on the new data lead to a dramatic increase in question parsing accuracy 
we perform noun phrase bracketing by using a local maximum entropy based tagging model which produce bracketing hypothesis these hypothesis are subsequently fed into a reranking framework based on support vector machine we solve the problem of hierarchical structure in our tagging model by modeling underspecified tag which are fully determined only at decoding time the tagging model performs comparably to competing approach and the subsequent reranking increase our system s performance from an f score of to surpassing the best reported result to date of 
noun extraction is very important for many nlp application such a information retrieval automatic text classification and information extraction most of the previous korean noun extraction system use a morphological analyzer or a part of speech po tagger therefore they require much of the linguistic knowledge such a morpheme dictionary and rule e g morphosyntactic rule and morphological rule this paper proposes a new noun extraction method that us the syllable based word recognition model it find the most probable syllable tag sequence of the input sentence by using automatically acquired statistical information from the po tagged corpus and extract noun by detecting word boundary furthermore it doe not require any labor for constructing and maintaining linguistic knowledge we have performed various experiment with a wide range of variable influencing the performance the experimental result show that without morphological analysis or po tagging the proposed method achieves comparable performance with the previous method 
in this paper we introduce textrank a graph based ranking model for text processing and show how this model can be successfully used in natural language application in particular we propose two innovative unsupervised method for keyword and sentence extraction and show that the result obtained compare favorably with previously published result on established benchmark 
we compare two approach for describing and generating body of rule used for natural language parsing in today s parser rule body do not exist a priori but are generated on the fly usually with method based on n gram which are one particular way of inducing probabilistic regular language we compare two approach for inducing such language one is based on n gram the other on minimization of the kullback leibler divergence the inferred regular language are used for generating body of rule inside a parsing procedure we compare the two approach along two dimension the quality of the probabilistic regular language they produce and the performance of the parser they were used to build the second approach outperforms the first one along both dimension 
we compare two approach for describing and generating body of rule used for natural language parsing in today s parser rule body do not exist a priori but are generated on the y usually with method based on n gram which are one particular way of inducing probabilistic regular language we compare two approach for inducing such language one is based on n gram the other on minimization of the kullback leibler divergence the inferred regular language are used for generating body of rule inside a parsing procedure we compare the two approach along two dimension the quality of the probabilistic regular language they produce and the performance of the parser they were used to build the second approach outperforms the rst one along both dimension 
the framenet project ha developed a lexical knowledge base providing a unique level of detail a to the the possible syntactic realization of the specific semantic role evoked by each predicator for roughly lexical unit on the basis of annotating more than example sentence extracted from corpus an interim version of the framenet data wa released in october and is being widely used a new more portable version of the framenet software is also being made available to researcher elsewhere including the spanish framenet project this demo and poster will briefly explain the principle of frame semantics and demonstrate the new unified tool for lexicon building and annotation and also framesql a search tool for finding pattern in annotated sentence we will discus the content and format of the data release and how the software and data can be used by other nlp researcher 
in this paper we describe a resource light system for the automatic morphological analysis and tagging of russian we eschew the use of extensive resource particularly large annotated corpus and lexicon exploiting instead i pre existing annotated corpus of czech ii an unannotated corpus of russian we show that our approach ha benefit and present what we believe to be one of the first full evaluation of a russian tagger in the openly available literature krasiv a beautiful short adjective feminine muz a husband noun masc sing genitive husband noun masc sing accusative okn a window noun neuter sing genitive window noun neuter pl nominative window noun neuter pl accusative knig a book noun fem sing nominative dom a house noun masc sing genitive house noun masc pl nominative house noun masc pl accusative skazal a say verb fem sing past tense dv a two numeral masc nominative 
recent work in question answering ha focused on web based system that extract answer using simple lexico syntactic pattern we present an alternative strategy in which pattern are used to extract highly precise relational information offline creating a data repository that is used to efficiently answer question we evaluate our strategy on a challenging subset of question i e who is question against a state of the art web based question answering system result indicate that the extracted relation answer more question correctly and do so three order of magnitude faster than the state of the art system 
we investigate the change in performance of automatic subcategorization acquisition when a word sense disambiguation wsd system is employed to guide the acquisition process a a subgoal this involves creating a probabilistic wsd system which we evaluate on the senseval english all word task data we carry out an evaluation of the enriched subcategorization acquisition system using difficult english verb which show that wsd help to improve the acquisition performance 
we present a probabilistic parsing model for german trained on the negra treebank we observe that existing lexicalized parsing model using head head dependency while successful for english fail to outperform an unlexicalized baseline model for german learning curve show that this effect is not due to lack of training data we propose an alternative model that us sister head dependency instead of head head dependency this model out performs the baseline achieving a labeled precision and recall of up to this indicates that sister head dependency are more appropriate for treebanks with very flat structure such a negra 
abstract we present a syntax based language model for use in noisy channel machine translation in particular a languagemodel based upon that described in cha is combined with the syntax based translation model described in yk the resulting system wa used to translate sentence from chinese to english and compared with theresults of an ibm model based system a well a that of yk all trained on the same data the translationswere sorted into four group good bad syntax 
in this paper we present a novel customizable ie paradigm that take advantage of predicate argument structure we also introduce a new way of automatically identifying predicate argument structure which is central to our ie paradigm it is based on an extended set of feature and inductive decision tree learning the experimental result prove our claim that accurate predicate argument structure enable high quality ie result 
this paper present an in depth analysis of a state of the art question answering system several scenario are examined the performance of each module in a serial baseline system the impact of feedback and the insertion of a logic prover and the impact of various retrieval strategy and lexical resource the main conclusion is that the overall performance depends on the depth of natural language processing resource and the tool used for answer finding 
kernel based learning e g support vector machine ha been successfully applied to many hard problem in natural language processing nlp in nlp although feature combination are crucial to improving performance they are heuristically selected kernel method change this situation the merit of the kernel method is that effective feature combination is implicitly expanded without loss of generality and increasing the computational cost kernel based text analysis show an excellent performance in term in accuracy however these method are usually too slow to apply to large scale text analysis in this paper we extend a basket mining algorithm to convert a kernel based classifier into a simple and fast linear classifier experimental result on english basenp chunking japanese word segmentation and japanese dependency parsing show that our new classifier are about to time faster than the standard kernel based classifier 
this paper describes a data source and methodology for producing customized test suite for molecular biology entity identification system the data consists of a a set of gene name and symbol classified by a taxonomy of feature that are relevant to the performance of entity identification system and b a set of sentential environment into which name and symbol are inserted to create test data and the associated gold standard we illustrate the utility of test set producible by this methodology by applying it to five entity identification system and describing the error pattern uncovered by it and investigate relationship between performance on a customized test suite generated from this data and the performance of a system on two corpus 
we present a novel approach for finding discontinuity that outperforms previously published result on this task rather than using a deeper grammar formalism our system combine a simple unlexicalized pcfg parser with a shallow pre processor this pre processor which we call a trace tagger doe surprisingly well on detecting where discontinuity can occur without using phase structure information 
text normalization is an important aspect of successful information retrieval from medical document such a clinical note radiology report and discharge summary in the medical domain a significant part of the general problem of text normalization is abbreviation and acronym disambiguation numerous abbreviation are used routinely throughout such text and knowing their meaning is critical to data retrieval from the document in this paper i will demonstrate a method of automatically generating training data for maximum entropy me modeling of abbreviation and acronym and will show that using me modeling is a promising technique for abbreviation and acronym normalization i report on the result of an experiment involving training a number of me model abbreviation and acronym on a sample of rheumatology note with accuracy 
the paper describes two parsing scheme a shallow approach based on machine learning and a cascaded finite state parser with a hand crafted grammar it discus several way to combine them and present evaluation result for the two individual approach and their combination an underspecification scheme for the output of the finite state parser is introduced and shown to improve performance 
this work applies boosted wrapper induction bwi a machine learning algorithm for information extraction from semi structured document to the problem of named entity recognition the default feature set of bwi is augmented with feature based on distributional term cluster induced from a large unlabeled text corpus using no traditional linguistic resource such a syntactic tag or specialpurpose gazetteer this approach yield result near the state of the art in the muc named entity domain supervised learning using feature derived through unsupervised corpus analysis may be regarded a an alternative to bootstrapping method 
this paper present domain relevance estimation dre a fully unsupervised text categorization technique based on the statistical estimation of the relevance of a text with respect to a certain category we use a pre dened set of category we call them domain which have been previously associated to wordnet word sens given a certain domain dre distinguishes between relevant and non relevant text by mean of a gaussian mixture model that describes the frequency distribution of domain word inside a large scale corpus then an expectation maximization algorithm computes the parameter that maximize the likelihood of the model on the empirical data the correct identication of the domain of the text is a crucial point for domain driven disambiguation an unsupervised word sense disambiguation wsd methodology that make use of only domain information therefore dre ha been exploited and evaluated in the context of a wsd task result are comparable to those of state ofthe art unsupervised wsd system and show that dre provides an important contribution 
the purpose of this paper is to automatically create multilingual translation lexicon with regional variation we propose a transitive translation approach to determine translation variation across language that have insufficient corpus for translation via the mining of bilingual search result page and clue of geographic information obtained from web search engine the experimental result have shown the feasibility of the proposed approach in efficiently generating translation equivalent of various term not covered by general translation dictionary it also revealed that the created translation lexicon can reflect different cultural aspect across region such a taiwan hong kong and mainland china 
we address the issue of judging the significance of rare event a it typically arises in statistical naturallanguage processing we first define a general approach to the problem and we empirically compare result obtained using log likelihood ratio and fisher s exact test applied to measuring strength of bilingual word association 
we extend previous work on tree kernel to estimate the similarity between the dependency tree of sentence using this kernel within a support vector machine we detect and classify relation between entity in the automatic content extraction ace corpus of news article we examine the utility of different feature such a wordnet hypernym part of speech and entity type and find that the dependency tree kernel achieves a f improvement over a bag of word kernel 
a new technique is introduced linguistic profiling in which large number of count of linguistic feature are used a a text profile which can then be compared to average profile for group of text the technique prof to be quite effective for authorship verification and recognition the best parameter setting yield a false accept rate of at a false reject rate equal to zero for the verification task on a test corpus of student essay and a way recognition accuracy on the same corpus 
we present a supervised machine learning algorithm for metonymy resolution which exploit the similarity between example of conventional metonymy we show that syntactic head modifier relation are a high precision feature for metonymy recognition but suffer from data sparseness we partially overcome this problem by integrating a thesaurus and introducing simpler grammatical feature thereby preserving precision and increasing recall our algorithm generalises over two level of contextual similarity resulting inference exceed the complexity of inference undertaken in word sense disambiguation we also compare automatic and manual method for syntactic feature extraction 
discovering the significant relation embedded in document would be very useful not only for information retrieval but also for question answering and summarization prior method for relation discovery however needed large annotated corpus which cost a great deal of time and effort we propose an unsupervised method for relation discovery from large corpus the key idea is clustering pair of named entity according to the similarity of context word intervening between the named entity our experiment using one year of newspaper reveals not only that the relation among named entity could be detected with high recall and precision but also that appropriate label could be automatically provided for the relation 
discriminative method have shown significant improvement over traditional generative method in many machine learning application but there ha been difficulty in extending them to natural language parsing one problem is that much of the work on discriminative method conflates change to the learning method with change to the parameterization of the problem we show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model we present three method for training a neural network to estimate the probability for a statistical parser one generative one discriminative and one where the probability model is generative but the training criterion is discriminative the latter model outperforms the previous two achieving state of the art level of performance f measure on constituent 
we present a domain independent topic segmentation algorithm for multi party speech our feature based algorithm combine knowledge about content using a text based algorithm a a feature and about form using linguistic and acoustic cue about topic shift extracted from speech this segmentation algorithm us automatically induced decision rule to combine the different feature the embedded text based algorithm build on lexical cohesion and ha performance comparable to state of the art algorithm based on lexical information a significant error reduction is obtained by combining the two knowledge source 
we examine the utility of speech and lexical feature for predicting student emotion in computer human spoken tutoring dialogue we first annotate student turn for negative neutral positive and mixed emotion we then extract acoustic prosodic feature from the speech signal and lexical item from the transcribed or recognized speech we compare the result of machine learning experiment using these feature alone or in combination to predict various categorization of the annotated student emotion our best result yield a relative improvement in error reduction over a baseline finally we compare our result with emotion prediction in human human tutoring dialogue 
traditional concatenative speech synthesis system use a number of heuristic to define the target and concatenation cost essential for the design of the unit selection component in contrast to these approach we introduce a general statistical modeling framework for unit selection inspired by automatic speech recognition given appropriate data technique based on that framework can result in a more accurate unit selection thereby improving the general quality of a speech synthesizer they can also lead to a more modular and a substantially more efficient system we present a new unit selection system based on statistical modeling to overcome the original absence of data we use an existing high quality unit selection system to generate a corpus of unit sequence we show that the concatenation cost can be accurately estimated from this corpus using a statistical n gram language model over unit we used weighted automaton and transducer for the representation of the component of the system and designed a new and more efficient composition algorithm making use of string potential for their combination the resulting statistical unit selection is shown to be about time faster than the last release of the at t natural voice product while preserving the same quality and offer much flexibility for the use and integration of new and more complex component 
the focus of research in text classification ha expanded from simple topic identification to more challenging task such a opinion modality identification unfortunately the latter goal exceed the ability of the traditional bag of word representation approach and a richer more structural representation is required accordingly learning algorithm must be created that can handle the structure observed in text in this paper we propose a boosting algorithm that capture sub structure embedded in text the proposal consists of i decision stump that use subtrees a feature and ii the boosting algorithm which employ the subtree based decision stump a weak learner we also discus the relation between our algorithm and svms with tree kernel two experiment on opinion modality classification confirm that subtree feature are important 
this paper describes an incremental parsing approach where parameter are estimated using a variant of the perceptron algorithm a beam search algorithm is used during both training and decoding phase of the method the perceptron approach wa implemented with the same feature set a that of an existing generative model roark a and experimental result show that it give competitive performance to the generative model on parsing the penn treebank we demonstrate that training a perceptron model to combine with the generative model during search provides a percent f measure improvement over the generative model alone to percent 
topic segmentation can be used a a preprocessing step in numerous natural language processing application in this short paper we will discus how we adapted our segmentation algorithm for automatic summarization 
multilingual application frequently involve dealing with proper name but name are often missing in bilingual lexicon this problem is exacerbated for application involving translation between latin scripted language and asian language such a chinese japanese and korean cjk where simple string copying is not a solution we present a novel approach for generating the ideographic representation of a cjk name written in a latin script the proposed approach involves first identifying the origin of the name and then back transliterating the name to all possible chinese character using language specific mapping to reduce the massive number of possibility for computation we apply a three tier filtering process by filtering first through a set of attested bigram then through a set of attested term and lastly through the www for a final validation we illustrate the approach with english to japanese back transliteration against test set of japanese given name and surname we have achieved average precision of and respectively 
multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentence in a document centrality is typically defined in term of the presence of particular important word or in term of similarity to a centroid pseudo sentence we are now considering an approach for computing sentence importance based on the concept of eigenvector centrality prestige that we call lexpagerank in this model a sentence connectivity matrix is constructed based on cosine similarity if the cosine similarity between two sentence exceeds a particular predefined threshold a corresponding edge is added to the connectivity matrix we provide an evaluation of our method on duc data the result show that our approach outperforms centroid based summarization and is quite successful compared to other summarization system 
most information extraction ie system treat separate potential extraction a independent however in many case considering influence between different potential extraction could improve overall accuracy statistical method based on undirected graphical model such a conditional random field crfs have been shown to be an effective approach to learning accurate ie system we present a new ie method that employ relational markov network a generalization of crfs which can represent arbitrary dependency between extraction this allows for collective information extraction that exploit the mutual influence between possible extraction experiment on learning to extract protein name from biomedical text demonstrate the advantage of this approach 
the detection of prosodic characteristic is an important aspect of both speech synthesis and speech recognition correct placement of pitch accent aid in more natural sounding speech while automatic detection of accent can contribute to better word level recognition and better textual understanding in this paper we investigate probabilistic contextual and phonological factor that influence pitch accent placement in natural conversational speech in a sequence labeling setting we introduce conditional random field crfs to pitch accent prediction task in order to incorporate these factor efficiently in a sequence model we demonstrate the usefulness and the incremental effect of these factor in a sequence model by performing experiment on hand labeled data from the switchboard corpus our model outperforms the baseline and previous model of pitch accent prediction on the switch board corpus 
this paper show how finite approximation of long distance dependency ldd resolution can be obtained automatically for wide coverage robust probabilistic lexical functional grammar lfg resource acquired from treebanks we extract lfg subcategorisation frame and path linking ldd reentrancies from f structure generated automatically for the penn ii treebank tree and use them in an ldd resolution algorithm to parse new text unlike collins johnson in our approach resolution of ldds is done at f structure attribute value structure representation of basic predicate argument or dependency structure without empty production trace and coindexation in cfg parse tree currently our best automatically induced grammar achieve f score for f structure parsing section of the wsj part of the penn ii treebank and evaluating against the dcu and against the parc dependency bank king et al performing at the same or a slightly better level than state of the art hand crafted grammar kaplan et al 
this paper applies machine learning technique to acquiring aspect of the meaning of discourse marker three subtasks of acquiring the meaning of a discourse marker are considered learning it polarity veridicality and type i e causal temporal or additive accuracy of over is achieved for all three task well above the baseline 
supervised learning method for wsd yield better performance than unsupervised method yet the availability of clean training data for the former is still a severe challenge in this paper we present an unsupervised bootstrapping approach for wsd which exploit huge amount of automatically generated noisy data for training within a supervised learning framework the method is evaluated using the noun in the english lexical sample task of senseval our algorithm doe a well a supervised algorithm on of this test set which is an improvement of absolute over state of the art bootstrapping wsd algorithm we identify seven different factor that impact the performance of our system 
in this paper we present a method for the semantic tagging of word chunk extracted from a written transcription of conversation this work is part of an ongoing project for an information extraction system in the field of maritime search and rescue sar our purpose is to automatically annotate part of text with concept from a sar ontology our approach combine two knowledge source a sar ontology and the wordsmyth dictionary thesaurus and it us a similarity measure for the classification evaluation is carried out by comparing the output of the system with key answer of predefined extraction template 
phrase level translation model are effective in improving translation quality by addressing the problem of local re ordering across language boundary method that attempt to fundamentally modify the traditional ibm translation model to incorporate phrase typically do so at a prohibitive computational cost we present a technique that begin with improved ibm model to create phrase level knowledge source that effectively represent local a well a global phrasal context our method is robust to noisy alignment at both the sentence and corpus level delivering high quality phrase level translation pair that contribute to significant improvement in translation quality a measured by the bleu metric over word based lexica a well a a competing alignment based method 
chinese part of speech po tagging assigns one po tag to each word in a chinese sentence however since word are not demarcated in a chinese sentence chinese po tagging requires word segmentation a a prerequisite we could perform chinese po tagging strictly after word segmentation one at a time approach or perform both word segmentation and po tagging in a combined single step simultaneously all atonce approach also we could choose to assign po tag on a word by word basis making use of word feature in the surrounding context word based or on a character by character basis with character feature character based this paper present an in depth study on such issue of processing architecture and feature representation for chinese po tagging within a maximum entropy framework we found that while the all at once characterbased approach is the best the one at a time character based approach is a worthwhile compromise performing only slightly worse in term of accuracy but taking shorter time to train and run a part of our investigation we also built a state of the art chinese word segmenter which outperforms the best sighan word segmenters in the closed track on out of test corpus 
given a parallel parsed corpus statistical treeto tree alignment attempt to match node in the syntactic tree for a given sentence in two language we train a probabilistic tree transduction model on a large automatically parsed chinese english corpus and evaluate result against human annotated word level alignment we find that a constituent based model performs better than a similar probability model trained on the same tree converted to a dependency representation 
we present a prototype natural language problem solving application for a financial service call center developed a part of the amiti s multilingual human computer dialogue project our automated dialogue system based on empirical evidence from real call center conversation feature a data driven approach that allows for mixed system customer initiative and spontaneous conversation preliminary evaluation result indicate efficient dialogue and high user satisfaction with performance comparable to or better than that of current conversational travel information system 
this paper a security property that we believe ha boththese desired property we are currently demonstrating that the securitycan be proved byshowing that it hold of a system under development thegoal of this paper is to show that the security policy can be used whichwe demonstrate two way first we introduce some theorem similar towhat others have used to describe a separation kernel and prove that ourspecication implies theirs second we formalize an example applicationthat 
this paper proposes a hybrid of hand crafted rule and a machine learning method for chunking korean in the partially free word order language such a korean and japanese a small number of rule dominate the performance due to their well developed postposition and ending thus the proposed method is primarily based on the rule and then the residual error are corrected by adopting a memory based machine learning method since the memory based learning is an efficient method to handle exception in natural language processing it is good at checking whether the estimate are exceptional case of the rule and revising them an evaluation of the method yield the improvement in f score over the rule or various machine learning method alone 
we introduce a metagrammar which allows u to automatically generate from a single and compact metagrammar hierarchy parallel lexical functional grammar lfg and tree adjoining grammar tag for french and for english the grammar writer specifies in compact manner syntactic property that are potentially framework and to some extent language independent such a subcategorization valency alternation and realization of syntactic function from which grammar for several framework and language are automatically generated offline 
in this paper we propose a unified framework for automatic evaluation of nlp application using n gram co occurrence statistic the automatic evaluation metric proposed to date for machine translation and automatic summarization are particular instance from the family of metric we propose we show that different member of the same family of metric explain best the variation obtained with human evaluation according to the application being evaluated machine translation automatic summarization and automatic question answering and the evaluation guideline used by human for evaluating such application 
many application of natural language processing technology involve analyzing text that concern the psychological state and process of people including their belief goal prediction explanation and plan in this paper we describe our effort to create a robust large scale lexical semantic resource for the recognition and classification of expression of commonsense psychology in english text we achieve high level of precision and recall by hand authoring set of local grammar for commonsense psychology concept and show that this approach can achieve classification performance greater than that obtained by using machine learning technique we demonstrate the utility of this resource for large scale corpus analysis by identifying reference to adversarial and competitive goal in political speech throughout u s history 
starting from first principle we re visit the statistical approach and study two form of the bayes decision rule the common rule for minimizing the number of string error and a novel rule for minimizing the number of symbol error the bayes decision rule for minimizing the number of string error is widely used e g in speech recognition po tagging and machine translation but it justification is rarely questioned to minimize the number of symbol error a is more suitable for a task like po tagging we show that another form of the bayes decision rule can be derived the major purpose of this paper is to show that the form of the bayes decision rule should not be taken for granted a it is done in virtually all statistical nlp work but should be adapted to the error measure being used we present first experimental result for po tagging task 
this paper describes discriminative language modeling for a large vocabulary speech recognition task we contrast two parameter estimation method the perceptron algorithm and a method based on conditional random field crfs the model are encoded a deterministic weighted finite state automaton and are applied by intersecting the automaton with word lattice that are the output from a baseline recognizer the perceptron algorithm ha the benefit of automatically selecting a relatively small feature set in just a couple of pass over the training data however using the feature set output from the perceptron algorithm initialized with their weight crf training provides an additional reduction in word error rate for a total absolute reduction from the baseline of 
in this paper we present a learning approach to the scenario template task of information extraction where information filling one template could come from multiple sentence when tested on the muc task our learning approach achieves accuracy competitive to the best of the muc system which were all built with manually engineered rule our analysis reveals that our use of full parsing and state of the art learning algorithm have contributed to the good performance to our knowledge this is the first research to have demonstrated that a learning approach to the full scale information extraction task could achieve performance rivaling that of the knowledge engineering approach 
this paper present how the development of a polynomial ordering andthe verification of it property can be fit in the framework of acl 
we describe a model for creating word to word and phrase to phrase alignment between document and their human written abstract such alignment are critical for the development of statistical summarization system that can be trained on large corpus of document abstract pair our model which is based on a novel phrase based hmm outperforms both the cut paste alignment model jing and model developed in the context of machine translation brown et al 
we investigate a number of simple method for improving the word alignment accuracy of ibm model we demonstrate reduction in alignment error rate of approximately resulting from giving extra weight to the probability of alignment to the null word smoothing probability estimate for rare word and using a simple heuristic estimation method to initialize or replace em training of model parameter 
if two translation system differ differ in performance on a test set can we trust that this indicates a difference in true system quality to answer this question we describe bootstrap resampling method to compute statistical significance of test result and validate them on the concrete example of the bleu score even for small test size of only sentence our method may give u assurance that test result difference are real 
this paper describes a method for learning the countability preference of english noun from raw text corpus the method map the corpus attested lexico syntactic property of each noun onto a feature vector and us a suite of memory based classifier to predict membership in countability class we were able to assign countability to english noun with a precision of 
ordering information is a critical task for natural language generation application in this paper we propose an approach to information ordering that is particularly suited for text to text generation we describe a model that learns constraint on sentence order from a corpus of domain specific text and an algorithm that yield the most likely order among several alternative we evaluate the automatically generated ordering against authored text from our corpus and against human subject that are asked to mimic the model s task we also ass the appropriateness of such a model for multidocument summarization 
we present a linguistically motivated algorithm for reconstructing nonlocal dependency in broad coverage context free parse tree derived from treebanks we use an algorithm based on loglinear classifier to augment and reshape context free tree so a to reintroduce underlying nonlocal dependency lost in the context free approximation we find that our algorithm compare favorably with prior work on english using an existing evaluation metric and also introduce and argue for a new dependency based evaluation metric by this new evaluation metric our algorithm achieves error reduction on gold standard input tree and error reduction on state of the art machine parsed input tree when compared with the best previous work we also present the first result on non local dependency reconstruction for a language other than english comparing performance on english and german our new evaluation metric quantitatively corroborates the intuition that in a language with freer word order the surface dependency in context free parse tree are a poorer approximation to underlying dependency structure 
exploiting unannotated natural language data is hard largely because unsupervised parameter estimation is hard we describe deterministic annealing rose et al a an appealing alternative to the expectation maximization algorithm dempster et al seeking to avoid search error da begin by globally maximizing an easy concave function and maintains a local maximum a it gradually morphs the function into the desired non concave likelihood function applying da to parsing and tagging model is shown to be straightforward significant improvement over em are shown on a part of speech tagging task we describe a variant skewed da which can incorporate a good initializer when it is available and show significant improvement over em on a grammar induction task 
a challenging problem for spoken dialog system is the design of utterance generation module that are fast flexible and general yet produce high quality output in particular domain a promising approach is trainable generation which us general purpose linguistic knowledge automatically adapted to the application domain this paper present a trainable sentence planner for the match dialog system we show that trainable sentence planning can produce output comparable to that of match s template based generator even for quite complex information presentation 
this paper present semframe a system that induces frame semantic verb class from wordnet and ldoce semantic frame are thought to have significant potential in resolving the paraphrase problem challenging many language based application when compared to the handcrafted framenet semframe achieves it best recall precision balance with recall based on semframe s coverage of framenet frame and precision based on semframe verb semantic relatedness to frame evoking verb the next best performing semantic verb class achieve recall and precision 
we present a language independent and unsupervised algorithm for the segmentation of word into morphs the algorithm is based on a new generative probabilistic model which make use of relevant prior information on the length and frequency distribution of morphs in a language our algorithm is shown to outperform two competing algorithm when evaluated on data from a language with agglutinative morphology finnish and to perform well also on english data 
this paper present a dependency language model dlm that capture linguistic constraint via a dependency structure i e a set of probabilistic dependency that express the relation between headword of each phrase in a sentence by an acyclic planar undirected graph our contribution are three fold first we incorporate the dependency structure into an n gram language model to capture long distance word dependency second we present an unsupervised learning method that discovers the dependency structure of a sentence using a bootstrapping procedure finally we evaluate the proposed model on a realistic application japanese kana kanji conversion experiment show that the best dlm achieves an error rate reduction over the word trigram model 
this paper present a method to develop a class of variable memory markov model that have higher memory capacity than traditional uniform memory markov model the structure of the variable memory model is induced from a manually annotated corpus through a decision tree learning algorithm a series of comparative experiment show the resulting model outperform uniform memory markov model in a part of speech tagging task 
an investment of effort over the last two year ha begun to produce a wealth of data concerning computational psycholinguistic model of syntax acquisition the data is generated by running simulation on a recently completed database of word order pattern from over abstract language this article present the design of the database which contains sentence pattern grammar and derivation that can be used to test acquisition model from widely divergent paradigm the domain is generated from grammar that are linguistically motivated by current syntactic theory and the sentence pattern have been validated a psychologically developmentally plausible by checking their frequency of occurrence in corpus of child directed speech a small case study simulation is also presented 
we present a framework for statistical machine translation of natural language based on direct maximum entropy model which contains the widely used source channel approach a a special case all knowledge source are treated a feature function which depend on the source language sentence the target language sentence and possible hidden variable this approach allows a baseline machine translation system to be extended easily by adding new feature function we show that a baseline statistical machine translation system is significantly improved using this approach 
