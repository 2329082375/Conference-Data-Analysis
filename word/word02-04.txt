we present a general purpose method for dynamically factoring a planning domain whose structure is then exploited by our generic planning method to find sound and complete plan the planning algorithm s time complexity scale linearly with the size of the domain and at worst exponentially with the size of the largest subdomain and interaction between subdomains the factorization procedure divide a planning domain into subdomains that are organized in a tree structure such that interaction between neighboring subdomains in the tree is minimized the combined planning algorithm is sound and complete and we demonstrate it on a representative planning domain the algorithm appears to scale to very large problem regardless of the black box planner used our planning procedure find plan for multiple subgoals in each of the subdomains separately using a generic black box planner it search over possible plan using complex action descriptor from each of the subdomains to form a plan for the overall goal the planning procedure us dynamic programming principle and backtracking occurs only within a subdomain a part of the black box planner we prove that our planning procedure run in time linear in the number of subdomains and take time that is at most exponential in the size of the largest subdomain and the number of dependency between subdomains the type of factoring that we select is justified by this complexity result we also prove that the combined algorithm is sound and complete and that it can be applied to solve any planning problem using any generic black box planner for planning within subdomains the complexity is upper bounded by the complexity of the black box planner on the unpartitioned domain we implemented and tested our planning algorithm on a simple domain to guide further development we created two implementation one with the ipp planning system koehler and hoffmann and one with the ff planner hoffmann and nebel we compared the result of our algorithm with those of ipp and ff and have shown that for a single domain our result scale much better than these planner alone the example validates our analytical result and show that our planner s performance scale linearly with the size of the domain for this problem motivating further development 
frequent itemset mining ha been the subject of a lot of work in data mining research ever since association rule were introduced in this paper we address a problem with frequent itemsets that they only count row where all their attribute are present and do not allow for any noise we show that generalizing the concept of frequency while preserving the performance of mining algorithm is nontrivial and introduce a generalization of frequent itemsets dense itemsets dense itemsets do not require all attribute to be present at the same time instead the itemset need to define a sufficiently large submatrix that exceeds a given density threshold of attribute present we consider the problem of computing all dense itemsets in a database we give a levelwise algorithm for this problem and also study the top k variation i e finding the k densest set with a given support or the k best supported set with a given density these algorithm select the other parameter automatically which simplifies mining dense itemsets in an explorative way we show that the concept capture natural facet of data set and give extensive empirical result on the performance of the algorithm combining the concept of dense itemsets with set cover idea we also show that dense itemsets can be used to obtain succinct description of large datasets we also discus some variation of dense itemsets 
a sensible use of classifier must be based on the estimated reliability of their prediction a cautious classifier would delegate the difficult or uncertain prediction to other possibly more specialised classifier in this paper we analyse and develop this idea of delegating classifier in a systematic way first we design a two step scenario where a first classifier chooses which example to classify and delegate the difficult example to train a second classifier secondly we present an iterated scenario involving an arbitrary number of chained classifier we compare these scenario to classical ensemble method such a bagging and boosting we show experimentally that our approach is not far behind these method in term of accuracy but with several advantage i improved efficiency since each classifier learns from fewer example than the previous one ii improved comprehensibility since each classification derives from a single classifier and iii the possibility to simplify the overall multi classifier by removing the part that lead to delegation 
provides a listing of current committee member 
essentially all data mining algorithm assume that the datagenerating process is independent of the data miner s activity however in many domain including spam detection intrusion detection fraud detection surveillance and counter terrorism this is far from the case the data is actively manipulated by an adversary seeking to make the classier produce false negative in these domain the performance of a classier can degrade rapidly after it is deployed a the adversary learns to defeat it currently the only solution to this is repeated manual ad hoc reconstruction of the classier in this paper we develop a formal framework and algorithm for this problem we view classication a a game between the classier and the adversary and produce a classier that is optimal given the adversary s optimal strategy experiment in a spam detection domain show that this approach can greatly outperform a classier learned in the standard way and within the parameter of the problem automatically adapt the classier to the adversary s evolving manipulation 
in this paper we study how we can design an effective parallel crawler a the size of the web grows it becomes imperative to parallelize a crawling process in order to finish downloading page in a reasonable amount of time we first propose multiple architecture for a parallel crawler and identify fundamental issue related to parallel crawling based on this understanding we then propose metric to evaluate a parallel crawler and compare the proposed architecture using million page collected from the web our result clarify the relative merit of each architecture and provide a good guideline on when to adopt which architecture 
in field such a medicine geography and me chanics spatial reasoning involves reasoning about entity for example cavity and invad ing particle that may coincide without over lapping the purpose of this paper is to develop a mereotopology for domain that include coin cident entity such a material object hole geopolitical entity and spatial region in addi tion i construct mathematical model of this mereotopology in which nontrivial coincidence relation are defined 
we present a novel method for approximate inference in bayesian model and regularized risk functionals it is based on the propagation of mean and variance derived from the laplace approximation of conditional probability in factorizing distribution much akin to minka s expectation propagation in the jointly normal case it coincides with the latter and belief propagation whereas in the general case it provides an optimization strategy containing support vector chunking the bayes committee machine and gaussian process chunking a special case 
spectral method for nonlinear dimensionality reduction nldr impose a neighborhood graph on point data and compute eigenfunctions of a quadratic form generated from the graph we introduce a more general and more robust formulation of nldr based on the singular value decomposition svd in this framework most spectral nldr principle can be recovered by taking a subset of the constraint in a quadratic form built from local nullspaces on the manifold the minimax formulation also open up an interesting class of method in which the graph is decorated with information at the vertex offering discrete or continuous map reduced computational complexity and immunity to some solution instability of eigenfunction approach apropos we show almost all nldr method based on eigenvalue decomposition evd have a solution instability that increase faster than problem size this pathology can be observed and corrected via the minimax formulation in problem a small a n point 
an evaluation methodology that target ineffective topic is needed to support research on obtaining more consistent retrieval across topic using average value of traditional evaluation measure is not an appropriate methodology because it emphasizes effective topic poorly performing topic score are by definition small and they are therefore difficult to distinguish from the noise inherent in retrieval evaluation we examine two new measure that emphasize a system s worst topic while these measure focus on different aspect of retrieval behavior than traditional measure the measure are le stable than traditional measure and the margin of error associated with the new measure is large relative to the observed difference in score 
we propose a gossip based distributed algorithm for gaussian mixture learning newscast em the algorithm operates on network topology where each node observes a local quantity and can communicate with other node in an arbitrary point to point fashion the mai n difference between newscast em and the standard em algorithm is that the m step in our case is implemented in a decentralized manner random pair of node repeatedly exchange their local parameter estimat e and combine them by weighted averaging we provide theoretical evidence and demonstrate experimentally that under this protocol nod e converge exponentially fast to the correct estimate in each m step of t he em algorithm 
filtering denotes any method whereby an agent up date it belief state it knowledge of the state of the world from a sequence of action and obser vations in logical filtering the belief state is a log ical formula describing possible world state and the agent ha a possibly nondeterministic logi cal model of it environment and sensor this paper present efficient logical filtering algorithm that maintain a compact belief state representa tion indefinitely for a broad range of environment class including nondeterministic partially ob servable strip environment and environment in which action permute the state space efficient filtering is also possible when the belief state is rep resented using prime implicates or when it is ap proximated by a logically weaker formula 
inverse or identification problem involve decid ing whether or not an explicitly given set of data point have an implicit description for instance in the form of a constraint network such prob lem provide insight into the relationship among various representation of knowledge which may have differing computational property this pa per formalizes and study the inverse circumscrip tion problem which roughly speaking is to de cide given a set of model if there exists a formula whose circumscription describes the input set 
low rank approximation technique are widespread in pattern recognition research they include latent semantic analysis lsa probabilistic lsa principal component analysus pca the generative aspect model and many form of bibliometric analysis all make use of a low dimensional manifold onto which data are projected such technique are generally unsupervised which allows them to model data in the absence of label or category with many practical problem however some prior knowledge is available in the form of context in this paper i describe a principled approach to incorporating such information and demonstrate it application to pca based approximation of several data set 
in this paper we describe a flexible approach for determining the relative orientation of the camera with respect to the scene the main premise of the approach is the fact that in man made environment the majority of line is aligned with the principal orthogonal direction of the world coordinate frame we exploit this observation towards ecient detection and estimation of vanishing point which provide strong constraint on camera parameter and relative orientation of the camera with respect to the scene by combining ecient image processing technique in the line detection and initialization stage we demonstrate that simultaneous grouping and estimation of vanishing direction can be achieved in the absence of internal parameter of the camera constraint between vanishing point are then used for partial calibration and relative rotation estimation the algorithm ha been tested in a variety of indoors and outdoors scene and it eciency and automation make it amenable for implementation on robotic platform 
we cast the problem of multiframe stereo reconstruction of a smooth surface a the global region segmentation of a collection of image of the scene dually the problem of segmenting multiple calibrated image of an object becomes that of estimating the solid shape that give rise to such image we assume that the radiance of the scene result in piecewise homogeneous image statistic this simplifying assumption cover lambertian scene with constant albedo a well a fine homogeneous texture which are known challenge to stereo algorithm based on local correspondence we pose the segmentation problem within a variational framework and use fast level set method to find the optimal solution numerically our algorithm doe not work in the presence of strong photometric feature where traditional reconstruction algorithm do it enjoys significant robustness to noise under the assumptiong it is designed for 
activity such a web service and the semantic web are working to create a web of distributed machine understandable data in this paper we present an application called semantic search which is built on these supporting technology and is designed to improve traditional web searching we provide an overview of tap the application framework upon which the semantic search is built we describe two implemented semantic search system which based on the denotation of the search query augment traditional search result with relevant data aggregated from distributed source we also discus some general issue related to searching and the semantic web and outline how an understanding of the semantics of the search term can be used to provide better result 
the interested reader model we present a simple easily implemented spectral learning algorithm which applies equally whether we have no supervisory information pairwise link constraint or labeled example in the unsuper vised case it performs consistently with other spec tral clustering algorithm in the supervised case our approach achieves high accuracy on the cate gorization of thousand of document given only a few dozen labeled training document for the newsgroups data set furthermore it classifica tion accuracy increase with the addition of unla beled document demonstrating effective use of unlabeled data by using normalized affinity ma trice which are both symmetric and stochastic we also obtain both a probabilistic interpretation of our method and certain guarantee of performance 
rao blackwellization is an approximation technique for probabilistic inference that flexibly combine exact inference with sampling it is useful in model where conditioning on some of the variable leaf a simpler inference problem that can be solved tractably this paper present sample propagation an efficient implementation of rao blackwellized approximate inference for a large class of model sample propagation tightly integrates sampling with message passing in a junction tree and is named for it simple appealing structure it walk the cluster of a junction tree sampling some of the current cluster s variable and then passing a message to one of it neighbor we discus the application of sample propagation to conditional gaussian inference problem such a switching linear dynamical system 
we discus a retrieval model in which the task is to complete a sentence given an initial fragment and given an application specific document collection this model is motivated by administrative and call center environment in which user have to write document with a certain repetitiveness we formulate the problem setting and discus appropriate performance metric we present an index based retrieval algorithm and a cluster based approach and evaluate our algorithm using collection of email that have been written by two distinct service center 
a cocluster of a m x n matrix x is a submatrix determined by a subset of the row and a subset of the column the problem of finding coclusters with specific property is of interest in particular in the analysis of microarray experiment in that case the entry of the matrix x are the expression level of m gene in each of n tissue sample one goal of the analysis is to extract a subset of the sample and a subset of the gene such that the expression level of the chosen gene behave similarly across the subset of the sample presumably reflecting an underlying regulatory mechanism governing the expression level of the gene we propose to base the similarity of the gene in a cocluster on a simple biological model in which the strength of the regulatory mechanism in sample j is hj and the response strength of gene i to the regulatory mechanism is gi in other word every two gene participating in a good cocluster should have expression value in each of the participating sample whose ratio is a constant depending only on the two gene noise in the expression level of gene is taken into account by allowing a deviation from the model measured by a relative error criterion the sleeve width of the cocluster reflects the extent to which entry i j in the cocluster is allowed to deviate relatively from being expressed a the product gihj we present a polynomial time monte carlo algorithm which output a list of coclusters whose sleeve width do not exceed a prespecified value moreover we prove that the list includes with fixed probability a cocluster which is near optimal in it dimension extensive experimentation with synthetic data show that the algorithm performs well 
this paper investigates how the vision of the semantic web can be carried overto the realm of email we introduce a general notion of semantice mail in which an email message consists of an rdf query or update coupled with corresponding explanatory text semantic email open the door to a wide range of automated email mediated application with formally guaranteed property in particular this paper introduces a broad class of semantic email process for example consider the process of sending an email to a program committee asking who will attend the pc dinner automatically collecting the response and tallying them up we define bothlogical and decision theoretic model where an email process ismodeled a a set of update to a data set on which we specify goal via certain constraint or utility we then describe a set ofinference problem that arise while trying to satisfy these goal and analyze their computational tractability in particular weshow that for the logical model it is possible to automatically infer which email response are acceptable w r t a set ofconstraints in polynomial time and for the decision theoreticmodel it is possible to compute the optimal message handling policy in polynomial time finally we discus our publicly available implementation of semantic email and outline research challenge inthis realm 
the problem considered in this paper is the fully asutomaticconstruction of panorama fundamentally thisproblem requires recognition a we need to know whichparts of the panorama join up previous approach haveused human input or restriction on the image sequencefor the matching step in this work we use object recognitiontechniques based on invariant local feature to selectmatchings image and a probabilistic model for verification because of this our method is insensitive to the ordering orientation scale and illumination of the image it is also insensitive to noise image which are not partof the panorama at all that is it recognises panorama this suggests a useful application for photographer thesystem take a input the image on an entire flash card orfilm recognises image that form part of a panorama andstitches them with no user input whatsoever 
we introduce a general family of kernel based on weighted transducer or rational relation rational kernel that can be used for analysis of variable length sequence or more generally weighted automaton in application such a computational biology or speech recognition we show that rational kernel can be computed efficiently using a general algorithm of composition of weighted transducer and a general single source shortest distance algorithm we also describe several general family of positive definite symmetric rational kernel these general kernel can be combined with support vector machine to form efficient and powerful technique for spoken dialog classification highly complex kernel become easy to design and implement and lead to substantial improvement in the classification accuracy we also show that the string kernel considered in application to computational biology are all specific instance of rational kernel 
we present a trainable sequential inference technique for process with large state and observation space and relational structure our method assumes reliable observation i e that each process state persists long enough to be reliably inferred from the observation it generates we introduce the idea of a state inference function from observation sequence to underlying hidden state for representing knowledge about a process and develop an efficient sequential inference algorithm utilizing this function that is correct for process that generate reliable observation consistent with the state inference function we describe a representation for state inference function in relational domain and give a corresponding supervised learning algorithm experiment in relational video interpretation show that our technique provides significantly improved accuracy and speed relative to a variety of recent hand coded non trainable system 
the paper describes on going work on generating dialogue response on two level first generation of factual response from rdf xml information second generation of meta level explanation of an ontology s structure from daml oil domain ontology we also discus way to use an ontology to improve a current dialogue system s response for example by identifying misconception 
finding page on the web that are similar to a query page related page is an important component of modern search engine a variety of strategy have been proposed for answering related page query but comparative evaluation by user study is expensive especially when large strategy space must be searched e g when tuning parameter we present a technique for automatically evaluating strategy using web hierarchy such a open directory in place of user feedback we apply this evaluation methodology to a mix of document representation strategy including the use of text anchor text and link we discus the relative advantage and disadvantage of the various approach examined finally we describe how to efficiently construct a similarity index out of our chosen strategy and provide sample result from our index 
we investigate bayesian alternative to classical monte carlo method for evaluating integral bayesian monte carlo bmc allows the incorporation of prior knowledge such a smoothness of the integrand into the estimation in a simple problem we show that this outperforms any classical importance sampling method we also attempt more challenging multidimensional integral involved in computing marginal likelihood of statistical model a k a partition function and model evidence we find that bayesian monte carlo outperformed anne aled importance sampling although for very high dimensional problem or problem with massive multimodality bmc may be le adequate one advantage of the bayesian approach to monte carlo is that sample can be drawn from any distribution this allows for the possibil ity of active design of sample point so a to maximise information gain 
several author have suggested viewing boosting a a gradient descent search for a good fit in function space we apply gradient based boostin g methodology to the unsupervised learning problem of density estimation we show convergence property of the algorithm and prove that a strength of weak learnability property applies to this problem a well we illustrate the poten tial of this approach through experiment with boosting bayesian network to learn density model 
we propose a family of kernel based on the binet cauchy theorem and it extension to fredholm operator this includes a special case all currently known kernel derived from the behavioral framework diffusion process marginalized kernel kernel on graph and the kernel on set arising from the subspace angle approach many of these kernel can be seen a the extremum of a new continuum of kernel function which lead to numerous new special case a an application we apply the new class of kernel to the problem of clustering of video sequence with encouraging result 
helmholtz stereopsis ha been introduced recently a a surfacereconstruction technique that doe not assume a modelof surface reflectance in the reported formulation correspondencewas established using a rank constraint necessitatingat least three viewpoint and three pair of image here it is revealed that the fundamental helmholtz stereopsisconstraint defines a nonlinear partial differential equation which can be solved using only two image it is shownthat unlike conventional stereo binocular helmholtz stereopsisis able to establish correspondence and thereby recoversurface depth for object having an arbitrary andunknown brdf and in textureless region i e region ofconstant or slowly varying brdf an implementation andexperimental result validate the method for specular surfaceswith and without texture 
motivated by the interest in relational reinforcement learning we introduce a novel relational bellman update operator called rebel it employ a constraint logic programming language to compactly represent markov decision process over relational domain using rebel a novel value iteration algorithm is developed in which abstraction over state and action play a major role this framework provides new insight into relational reinforcement learning convergence result a well a experiment are presented 
we construct a nonlinear mapping from a high dimensional sample space to a low dimensional vector space effectively recovering a cartesian coordinate system for the manifold from which the data is sampled the mapping preserve local geometric relation in the manifold and is pseudo invertible we show how to estimate the intrinsic dimensionality of the manifold from sample decompose the sample data into locally linear low dimensional patch merge these patch into a single lowdimensional coordinate system and compute forward and reverse mapping between the sample and coordinate space the objective function are convex and their solution are given in closed form nonlinear dimensionality reduction nldr by charting charting is the problem of assigning a low dimensional coordinate system to data point in a high dimensional sample space it is presumed that the data lie on or near a lowdimensional manifold embedded in the sample space and that there exists a to smooth nonlinear transform between the manifold and a low dimensional vector space the datamodeler s goal is to estimate smooth continuous mapping between the sample and coordinate space often this analysis will shed light on the intrinsic variable of the datagenerating phenomenon for example revealing perceptual or configuration space our goal is to find a mapping expressed a a kernel based mixture of linear projection that minimizes information loss about the density and relative location of sample point this constraint is expressed in a posterior that combine a standard gaussian mixture model gmm likelihood function with a prior that penalizes uncertainty due to inconsistent projection in the mixture section develops a special case where this posterior is unimodal and maximizable in closed form yielding a gmm whose covariance reveal a patchwork of overlapping locally linear subspace that cover the manifold section show that for this or any gmm and a choice of reduced dimension d there is a unique closed form solution for a minimally distorting merger of the subspace into a d dimensional coordinate space a well a an reverse mapping defining the surface of the manifold in the sample space the intrinsic dimensionality d of the data manifold can be estimated from the growth process of point to point distance in analogy to differential geometry we call the subspace chart and their merger the connection section considers example problem where these method are used to untie knot unroll and untwist sheet and visualize video data background topology neutral nldr algorithm can be divided into those that compute mapping and 
we investigate improvement of adaboost that can exploit the fact that the weak hypothesis are one sided i e either all it positive or negative prediction are correct in particular for any set of m labeled example consistent with a disjunction of k literal which are one sided in this case adaboost construct a consistent hypothesis by using o k log m iteration on the other hand a greedy set covering algorithm find a consistent hypothesis of size o k log m our primary question is whether there is a simple boosting algorithm that performs a well a the greedy set covering we first show that infoboost a modification of adaboost proposed by aslam for a different purpose doe perform a well a the greedy set covering algorithm we then show that adaboost requires k log m iteration for learning k literal disjunction we achieve this with an adversary construction and a well a in simple experiment based on artificial data further we give a variant called semiboost that can handle the degenerate case when the given example all have the same label we conclude by showing that semiboost can be used to produce small conjunction a well 
abstract recently there have been several advance in the machine learning and pattern recognition community for developing manifold learning algo rithms to construct nonlinear low dimensional manifold from sample data point embedded in high dimensional space in this paper we de velop algorithm that address two key issue in manifold learning the adaptive selection of the neighborhood size and better fitting the local geometric structure to account for the variation in the curvature of the manifold and it interplay with the sampling density of the data set we also illustrate the effectiveness of our method on some synthetic data set 
we investigate the combination of answer set pro gramming and qualitative optimization technique answer set optimization program aso pro gram have two part the generating program produce answer set representing possible solution the preference program express user preference it induces a preference relation on the answer set of based on the degree to which rule are satisfied we discus possible application of aso program ming give complexity result and propose imple mentation technique we also analyze the relation ship between a so program and cp network 
we present a probabilistic method for path planning that considers trajectory constrained by both the environment and an ensemble of restriction or preference on preferred motion for a moving robot our system learns constraint and preference bias on a robot s motion from example and then synthesizes behavior that satisfy these constraint this behavior can encompass motion that satisfy diverse requirement such a a sweep pattern for floor coverage or in particular in our experiment satisfy restriction on the robot s physical capability such a restriction on it turning radius given an approximate path that may not satisfy the required condition our system computes a refined path that satisfies the constraint and also avoids obstacle our approach is based on a bayesian framework for combining a prior probability distribution on the trajectory with environmental constraint the prior distribution is generated by decoding a hidden markov model which is itself is trained over a particular set of preferred motion environmental constraint are modeled using a potential field over the configuration space this paper pose the requisite theoretical framework and demonstrates it effectiveness with several example 
we demonstrate that an unlexicalized pcfg can parse much more accurately than previously shown by making use of simple linguistically motivated state split which break down false independence assumption latent in a vanilla treebank grammar indeed it performance of lp lr f pcfg model and surprisingly close to the current state of the art this result ha potential us beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized model an unlexicalized pcfg is much more compact easier to replicate and easier to interpret than more complex lexical model and the parsing algorithm are simpler more widely understood of lower asymptotic complexity and easier to optimize 
web link analysis ha been proved to provide significant enhancement to the precision of web search in practice the pagerank algorithm which is used in google search engine play an important role on improving the quality of it resuls by employing the explicit hyperlink structure among the web page the prestige of web page defined by pagerank is purely derived from surfer random walk on the web graph without textual content content consideration however in the practical sense user surfing behavior is far from random jumping in this paper we present a unified model for a more accurate page rank user s surfing is guided by a probabilistic model that is based on literal matching between connected page the result show that our proposed ranking algorithm do perform better than the original pagerank 
we report here on the problem of estimating a smooth planar curve t and it derivative from an ordered sample of interpolation point t t t i t i t m t m where t t t i t i t m t m t and the t i are not known precisely for i m such situtation may appear while searching for the boundary of planar object or tracking the mass center of a rigid body with no time available in this paper we assume that the distribution of t i coincides with more or le uniform sampling a fast algorithm yielding quartic convergence rate based on point piecewise quadratic interpolation is analysed and tested our algorithm form a substantial improvement with respect to the speed of convergence of piecewise point quadratic lagrange intepolation and some related work can be found in our result may be of interest in computer vision and digital image processing or computer graphic or approximation and complexity theory or and digital and computational geometry and 
we discus the definition of key for xml document paying particular attention to the concept of a relative key which is commonly used in hierarchically structured document and scientific database 
abstract we consider the learning problem ofnding a dependency betweena general class of object and another possibly dierent generalclass of object the object can be for example vector image string tree or graph such a task is made possible by employingsimilarity measure in both input and output space using kernelfunctions thus embedding the object into vector space weexperimentally validate our approach on several task mappingstrings to string pattern 
sequential random sampling markov chain monte carlo is a popular strategy for many vision problem involving multimodal distribution over high dimensional parameter space it applies both to importance sampling where one want to sample point according to their importance for some calculation but otherwise fairly and toglobal optimization where one want to find good minimum or at least good starting point for local minimization regardless of fairness unfortunately most sequential sampler are very prone to becoming trapped for long period in unrepresentative local minimum which lead to biased or highly variable estimate we present a general strategy for reducing mcmc trapping that generalizes voter s hyperdynamic sampling from computational chemistry the local gradient and curvature of the input distribution are used to construct an adaptive importance sampler that focus sample on low cost negative curvature region likely to contain transition state codimension saddle point representing mountain pass connecting adjacent cost basin this substantially accelerates inter basin transition rate while still preserving correct relative transition probability experimental test on the difficult problem of d articulated human pose estimation from monocular image show significantly enhanced minimum exploration 
linear model allow the description of a continuous symmetric response in term of a linear combination of predictor variable generalized linear model extend this framework to a wider range of response type including categorical binary and skewed continuous response this allows a common approach to statistical inference in the fitting and testing of such model a well a diagnostic checking 
generalized multitext grammar gmtg is a synchronous grammar formalism that is weakly equivalent to linear context free rewriting system lcfrs but retains much of the notational and intuitive simplicity of context free grammar cfg gmtg allows both synchronous and independent rewriting such flexibility facilitates more perspicuous modeling of parallel text than what is possible with other synchronous formalism this paper investigates the generative capacity of gmtg prof that each component grammar of a gmtg retains it generative power and proposes a generalization of chomsky normal form which is necessary for synchronous cky style parsing 
the multiple view geometry of static scene is now well understood recently attention wa turned to dynamic scene where scene point may move while the camera move the triangulation of linear trajectory is now well handled the case of quadratic trajectory also received some attention we present a complete generalization and address the problem of general trajectory triangulation of moving point from non synchronized camera our method is based on a particular representation of curve trajectory where a curve is represented by a family of hypersurfaces in the projective space this representation is linear even for highly non linear trajectory we show how this representation allows the recovery of the trajectory of a moving point from non synchronized sequence we show how this representation can be converted into a more standard representation we also show how one can extract directly from this representation the position of the moving point at each time instant an image wa made experiment on synthetic data and on real image demonstrate the feasibility of our approach 
a new family of kernel for statistical learning is introduc ed that exploit the geometric structure of statistical model base d on the heat equation on the riemannian manifold defined by the fisher inf ormation metric information diffusion kernel generalize the gaussian kernel of euclidean space and provide a natural way of combining generative statistical modeling with non parametric discriminative learning a a special case the kernel give a new approach to applying kernel based learning algorithm to discrete data bound on covering number for the new kernel are proved using spectral theory in differen tial geometry and experimental result are presented for text classificat ion 
we introduce a probabilistic model that generalizesclassical linear discriminant analysisand give an interpretation for the componentsas informative or relevant componentsof data the component maximize thepredictability of class distribution which isasymptotically equivalent to i maximizingmutual information with the class and ii nding principal component in the so calledlearning or fisher metric the fisher metricmeasures only distance that are relevantto the 
in this paper we consider tipping s relevance vector machi ne rvm and formalize an incremental training strategy a a vari ant of the expectation maximization em algorithm that we call subspace em working with a subset of active basis function the sparsit y of the rvm solution will ensure that the number of basis function and t hereby the computational complexity is kept low we also introduce a mean field approach to the intractable classification model that is exp ected to give a very good approximation to exact bayesian inference and contains the laplace approximation a a special case we test the algorithm on two large data set with example the result indicate that bayesian learning of large data set e g the mnist database is realistic 
in this paper we propose a novel approach for facialexpression decomposition higher order singular valuedecomposition hosvd a natural generalization ofmatrix svd we learn the expression subspace and personsubspace from a corpus of image showing seven basicfacial expression rather than resort to expert coded facialexpression parameter a in we propose a simultaneousface and facial expression recognition algorithm which can classify the given image into one of the sevenbasic facial expression category and then other facialexpressions of the new person can be synthesized using thelearned expression subspace model the contribution ofthis work lie mainly in two aspect first we propose a newhosvd based approach to model the mapping betweenpersons and expression used for facial expression synthesisfor a new person second we realize simultaneous faceand facial expression recognition a a result of facialexpression decomposition experimental result are presentedthat illustrate the capability of the person subspaceand expression subspace in both synthesis and recognitiontasks a a quantitative measure of the quality of synthesis we propose using gradient minimum square error gmse which measure the gradient difference between the originaland synthesized image 
we consider loopy belief propagation for approximate inference in probabilistic graphical model a limitation of the standard algorithm is that clique marginals are computed a if there were no loop in the graph to overcome this limitation we introduce fractional belief propagation fractional belief propagation is formulated in term of a family of approximate free energy which includes the bethe free energy and the naive mean field free a special case using the linear response correction of the clique marginals the scale parameter can be tuned simulation result illustrate the potential merit of the approach 
ontology a a discipline of computer science ha made many claim about it usefulness however to date there ha been very little evaluation of those claim we present the result of an experiment using a hybrid search system with a significant knowledge based component to measure using precision and recall the impact of improving the quality of an ontology on overall performance we demonstrate that improving the ontology using ontoclean guarino and welty doe positively impact performance and that having knowledge of the search domain is more effective than domain knowledge free search technique such a link analysis 
existing autocalibration technique use numerical optimizationalgorithms that are prone to the problem of localminima to address this problem we have developeda method where an interval branch and bound method isemployed for numerical minimization thanks to the propertiesof interval analysis this method is guaranteed to convergeto the global solution with mathematical certainty andarbitrary accuracy and the only input information it requiresfrom the user is a set of point correspondence anda search box the cost function is based on the huang faugerasconstraint of the fundamental matrix a recentlyproposed interval extension based on bernstein polynomialforms ha been investigated to speed up the search for thesolution finally some experimental result on syntheticimages are presented 
we introduce grid based sensordcsp a geometri cally structured benchmark problem for the study of distributed csp algorithm this domain pro vides realistic structure of the communication and tracking constraint we formally define this prob lem and perform it worst case complexity analy si likewise we provide an average case empirical analysis of the awc algorithm studying it behav ior on tractable and intractable sub class of our problem 
high dimensional collection of data occur in many application the attribute in such data set are typically considered to be unordered however in many case there is a natural total or partial order pr underlying the variable of the data set example of variable for which such order exist include term in document course in enrollment data and paleontological site in fossil data collection the observation in such application are flat unordered set however the data set respect the underlying ordering of the variable by this we mean that if a pr b pr c are three variable respecting the underlying ordering pr and both of variable a and c appear in an observation then up to noise level variable b also appears in this observation similarly if a pr a pr pr al pr ai is a longer sequence of variable we do not expect to see many observation for which there are index i j k such that ai and ak occur in the observation but aj doe not in this paper we study the problem of discovering fragment of order of variable implicit in collection of unordered observation we define measure that capture how well a given order agrees with the observed data we describe a simple and efficient algorithm for finding all the fragment that satisfy certain condition we also discus the sometimes necessary postprocessing for selecting only the best fragment of order also we relate our method with a sequencing approach that us a spectral algorithm and with the consecutive one problem we present experimental result on some real data set author list of database paper exam result data and paleontological data 
neural symbolic system are hybrid system that integrate symbolic logic and neural network the goal of neural symbolic integration is to benefit from the combination of feature of the symbolic and connectionist paradigm of artificial intelligence this paper introduces a new neural network architecture based on the idea of fibring logical system fibring allows one to combine different logical system in a principled way fibred neural network may be composed not only of interconnected neuron but also of other network forming a recursive architecture a fibring function then defines how this recursive architecture must behave by defining how the network in the ensemble relate to each other typically by allowing the activation of neuron in one network a to influence the change of weight in another network b intuitively this can be seen a training network b at the same time that one run network a we show that in addition to being universal approximators like standard feedforward network fibred neural network can approximate any polynomial function to any desired degree of accuracy thus being more expressive than standard feedforward network 
data cleaning method are used for finding duplicate within a file or across set of file this overview provides background on the fellegi sunter model of record linkage the fellegi sunter model provides an optimal theoretical classification rule fellegi and sunter introduced method for automatically estimating optimal parameter without training data that we extend to many real world situation 
current psychological theory of human causal learning and judgment focus primarily on long run prediction two by estimating parameter of a causal bayes net though for different parameterizations and a third through structural learning this paper focus on people s short run behavior by examining dynamical version of these three theory and comparing their prediction to a real world dataset 
texture can often more easily be described a a composition of subtextures than a a single texture the paper proposes a way to model and synthesize such composite texture where the layout of the different subtextures is itself modeled a a texture which can be generated automatically example are shown for building material with an intricate structure and for the automatic creation of landscape texture first a model of the composite texture is generated this procedure comprises manual or unsupervised texture segmentation to learn the spatial layout of the composite texture and the extraction of model for each of the subtextures synthesis of a composite texture includes the generation of a layout texture which is subsequently filled in with the appropriate subtextures this scheme is refined further by also including interaction between neighboring subtextures 
web search engine struggle to satisfy the need of web user user are notoriously poor at representing their need in the form of a query and search engine are poor at responding to vague query however progress ha been made by introducing context into the search process in this paper we describe and evaluate a novel approach to using context in web search that adapts a generic search engine for the need of a specialist community of user this collaborative search method enjoys significant performance benefit and avoids the privacy and security concern that are commonly associated with related personalization research 
due to the advance in positioning technology the real time information of moving object becomes increasingly available which ha posed new challenge to the database research a a long standing technique to identify overall distribution pattern in data clustering ha achieved brilliant success in analyzing static datasets in this paper we study the problem of clustering moving object which could catch interesting pattern change during the motion process and provide better insight into the essence of the mobile data point in order to catch the spatial temporal regularity of moving object and handle large amount of data micro clustering is employed efficient technique are proposed to keep the moving micro cluster geographically small important event such a the collision among moving micro cluster are also identified in this way high quality moving micro cluster are dynamically maintained which lead to fast and competitive clustering result at any given time instance we validate our approach with a through experimental evaluation where order of magnitude improvement on running time is observed over normal k mean clustering method 
the core of scientific theory are law these law often make use of theoretical term linguistic entity which do not directly refer to observables there is therefore no direct way of determining which theoretical assertion are true this suggests that multiple theory may exist which are incompatible with each other but compatible with all possible observation since such theory make the same empirical claim empirical test cannot be used to differentiate or rank such theory one property that ha been suggested for evaluating rival theory is coherence this wa only understood qualitatively until we kwok et al introduced a coherence measure based on the average use of formula in support set for observation the idea wa to identify highly coherent theory with those whose formula that are tightly coupled to account for observation while low coherence theory contain many disjointed and isolated statement our current approach generalizes that insight to accommodate fundamental idea from the philosophy of science and better mirror scientific practice moreover this new approach is neutral with respect to the philosophy and practice of science and is able to explain notion like modularization using coherence 
we present a generalization of similarity based retrieval in recommender system which ensures that for any case that is acceptable to the user the retrieval set contains a case that is at least a good in an objective sense and so also likely to be acceptable our approach recognizes that similarity to the target query is only one of several possible criterion according to which a given case might be considered at least a good a another 
a pattern database pdb is a heuristic function implemented a a lookup table that store the length of optimal solution for subproblem instance standard pdbs have a distinct entry in the table for each subproblem instance in this paper we investigate compressing pdbs by merging several entry into one thereby allowing the use of pdbs that exceed available memory in their uncompressed form we introduce a number of method for determining which entry to merge and discus their relative merit these vary from domainindependent approach that allow any set of entry in the pdb to be merged to more intelligent method that take into account the structure of the problem the choice of the best compression method is based on domain dependent attribute we present experimental result on a number of combinatorial problem including the four peg tower of hanoi problem the sliding tile puzzle and the top spin puzzle for the tower of hanoi we show that the search time can be reduced by up to three order of magnitude by using compressed pdbs compared to uncompressed pdbs of the same size more modest improvement were observed for the other domain 
we investigate the problem of non covariant behavior of policy gradient reinforcement learning algorithm the policy gradient approach is amenable to analysis by information geometric method this lead u to propose a natural metric on controller parameterization that result from considering the manifold of probability distribution over path induced by a stochastic controller investigation of this approach lead to a covariant gradient ascent rule interesting property of this rule are discussed including it relation with actor critic style reinforcement learning algorithm the algorithm discussed here are computationally quite efficient and on some interesting problem lead to dramatic performance improvement over noncovariant rule 
this paper introduces correlated q ce q learning a multiagent q learning algorithm based on the correlated equilibrium ce solution concept ce q generalizes both nashq and friend and foe q in general sum game the set of correlated equilibrium contains the set of nash equilibrium in constantsum game the set of correlated equilibrium contains the set of minimax equilibrium this paper describes experiment with four variant of ce q demonstrating empirical convergence to equilibrium policy on a testbed of general sum markov game 
we present several new algorithm for multiagent reinforcementlearning a commonfeatureof these algorithm is a parameterized structured representation of a policy or value function this structure is leveraged in an approach we call coordinated reinforcement learning by which agent coordinate both their action selection activity and their parameter update within the limit of our parametric representation the agent will determine a jointly optimal action without explicitly considering every possible action in their exponentially large joint action space our method differ from many previous reinforcement learning approach to multiagent coordination in that structured communication and coordination between agent appears at the core of both the learning algorithm and the execution architecture our experimental result comparing our approach to other rl method illustrate both the quality of the policy obtained and the additional benefit of coordination 
providing knowledge worker with access to expert and community of practice is central to sharing expertise and crucial to organizational performance adaptation and even survival this paper cover ongoing research to develop an expert locator prototype a model based system for detecting expert and broader community of practice the underlying expertise model is extensible and support aggregation of evidence across diverse source the prototype is being used to locate critical expertise in key project area and current evaluation indicates it potential effectiveness 
principal component analysis pca is one of the most widely used technique in machine learning and data mining minor component analysis mca is le well known but can also play an important role in the presence of constraint on the data distribution in this paper we present a probabilistic model for extreme component analysis xca which at the maximum likelihood solution extract an optimal combination of principal and minor component for a given number of component the log likelihood of the xca model is guaranteed to be larger or equal than that of the probabilistic model for pca and mca we describe an efficient algorithm to solve for the globally optimal solution for log convex spectrum we prove that the solution consists of principal component only while for log concave spectrum the solution consists of minor component in general the solution admits a combination of both in experiment we explore the property of xca on some synthetic and real world datasets 
cosine pivoted document length normalization ha reached a point of stability where many researcher indiscriminately apply a specific value of regardless of the collection our effort however demonstrate that applying this specific value without tuning for the document collection degrades average precision by a much a 
we introduce the community to a new construction principle whose practical implication are very broad central to this research is the idea of improving the presentation of algorithm in the literature and making them more appealing we dene a new notion of capacity for data set and derive a methodology for selecting from them our experiment demonstrate that even not so good algorithm can be shown signicantly better than competitor we present some experimental result which are very promising 
we address the problem of segmenting a sequence of imagesof natural scene into disjoint region that are characterizedby constant spatio temporal statistic we model thespatio temporal dynamic in each region by gauss markovmodels and infer the model parameter a well a theboundary of the region in a variational optimization framework numerical result demonstrate that in contrast topurely texture based segmentation scheme our method iseffective in segmenting region that differ in their dynamicseven when spatial statistic are identical 
gaussian process are usually parameterised in term of their covariance function however this make it difficult to deal with multiple output because ensuring that the covariance matrix is positive definite is problematic an alternative formulation is to treat gaussian process a white noise source convolved with smoothing kernel and to parameterise the kernel instead using this we extend gaussian process to handle multiple coupled output 
the paper present a novel expressive logic based formalism intended for reasoning about numerical distance we investigate it computational prop erties in particular show that it is exptimecomplete and devise a tableau based satisfiabilitychecking algorithm to be able to express knowl edge about implicit or unknown distance we then extend the language with variable ranging over distance and prove that the resulting logic is decidable a well 
repetition is an important phenomenon in a variety of domain such a music computer program and architectural drawing a generative model for these domain should account for the possibility of repetition we present repeated observation model rom a framework for modeling sequence that explicitly allows for repetition in a rom an element is either generated by copying a previous element or by using a base model we show how to build rom using gram and hidden markov model a the base model we also describe an extension of rom in which entire subsequence are repeated together result from a music modeling domain show that rom can lead to dramatic improvement in predictive ability 
abstract variable selection the process of identifying input variable that are relevant to a particular learning problem ha received much attention in the learning community method that employ a learning algorithm a a part of the selection process wrapper have been shown to outperform method that select variable independently from the learning algorithm filter but only at great compu tational expense we present a randomized wrapper algorithm whose computational requirement are within a constant factor of simply learning in the presence of all input variable provided that the number of relevant variable is small and known in advance we then show how to remove the latter assumption and demonstrate performance on several problem 
we formulate the problem of graph inference where part of the graph is known a a supervised learning problem and propose an algorithm to solve it the method involves the learning of a mapping of the vertex to a euclidean space where the graph is easy to infer and can be formulated a an optimization problem in a reproducing kernel hilbert space we report encouraging result on the problem of metabolic network reconstruction from genomic data 
we describe a probabilistic approach to the task of placing object described by high dimensional vector or by pairwise dissimilarity in a low dimensional space in a way that preserve neighbor identity a gaussian is centered on each object in the high dimensional space and the density under this gaussian or the given dissimilarity are used to define a probability distribution over all the potential neighbor of the object the aim of the embedding is to approximate this distribution a well a possible when the same operation is performed on the low dimensional image of the object a natural cost function is a sum of kullback leibler divergence one per object which lead to a simple gradient for adjusting the position of the low dimensional image unlike other dimensionality reduction method this probabilistic framework make it easy to represent each object by a mixture of widely separated low dimensional image this allows ambiguous object like the document count vector for the word bank to have version close to the image of both river and finance without forcing the image of outdoor concept to be located close to those of corporate concept 
why are sensory modality segregated the way they are in this paper we show that sensory modality are well designed for self supervised cross modal learning using the minimizing disagreement algorithm on an unsupervised speech categorization task with visual moving lip and auditory sound signal input we show that very informative auditory dimension actually harm performance when moved to the visual side of the network it is better to throw them away than to consider them part of the visual input we explain this finding in term of the statistical structure in sensory input 
boosting algorithm and successful application thereof abound for classification and regression learning problem but not for unsupervised learning we propose a sequential approach to adding feature to a random field model by training them to improve classification performance between the data and an equal sized sample of negative example generated from the model s current estimate of the data density training in each boosting round proceeds in three stage first we sample negative example from the model s current boltzmann distribution next a feature is trained to improve classification performance between data and negative example finally a coefficient is learned which determines the importance of this feature relative to one already in the pool negative example only need to be generated once to learn each new feature the validity of the approach is demonstrated on binary digit and continuous synthetic data 
considerable progress wa recently achieved on semi supervised learning which diers from the traditional supervised learning by additionally exploring the information of the unlabelled example however a disadvantage of many existing method is that it doe not generalize to unseen input this paper investigates learning method that eectively make use of both labelled and unlabelled data to build predictive function which are defined on not just the seen input but the whole space a a nice property the proposed method allows ecient training and can easily handle new test point we validate the method based on both toy data and real world data set 
we consider the problem of recovering an underwater image distorted by surface wave a large amount of video data of the distorted image is acquired the problem is posed in term of finding an undistor ted image patch at each spatial location this challenging reconstruction task can be formulated a a manifold learning problem such that the center of the manifold is the image of the undistorted patch to compute the center we present a new technique to estimate global distance on the manifold our technique achieves robustness through convex flow computation and solves the leakage problem inherent in recent manifold embedding technique 
we propose a new tracking technique that is able to capture non rigid motion by exploiting a space time rank constraint most tracking method use a prior model in order to deal with challenging local feature the model usually ha to be trained on carefully handlabeled example data before the tracking algorithm can be used our new model free tracking technique can overcome such limitation this can be achieved in redefining the problem instead of first training a model and then tracking the model parameter we are able to derive trajectory constraint first and then estimate the model this reduces the search space significantly and allows for a better feature disambiguation that would not be possible withtraditional tracker we demonstrate th at sampling in the trajectory space instead of in the space of shape configuration allows u to track challenging footage without use of prior model 
we show two related thing given a classier which consists of a weighted sum of featureswith a large margin we can construct a stochastic classier withnegligibly larger training error rate the stochastic classier hasa future error rate bound that depends on the margin distributionand is independent of the size of the base hypothesis class 
we present new result on the relation between purely symbolic context free parsing strategy and their probabilistic counterpart such parsing strategy are seen a construction of push down device from grammar we show that preservation of probability distribution is possible under two condition viz the correct prefix property and the property of strong predictiveness these result generalize existing result in the literature that were obtained by considering parsing strategy in isolation from our general result we also derive negative result on so called generalized lr parsing 
abstract we present new result on the relation between context free parsing strategy and their probabilistic counter part we provide a necessary condition and a sufficient condition for the probabilistic extension of parsing strategy these result generalize existing result in the literature that were obtained by considering parsing strategy in isolation 
we present a framework in which probabilistic model for textual and visual information retrieval can be integrated seamlessly the framework facilitates searching for imagery using textual description and visual example simultaneously the underlying language model for text and gaussian mixture model for image have proven successful in various retrieval task 
muhiagent system ma can go down for a large number of reason ranging from system mal function and power failure to malicious attack the placement of agent on node is called a de ployment of the ma we develop a probabilis tic model of survivability of a deployed ma and provide two algorithm to compute the probability of survival of a deployed ma our probabilistic model doc not make independence assumption though such assumption can be added if so de sired an optimal deployment of a ma is one that maximizes it survival probability we provide a mathematical answerto this question an algorithm that computes an exact solution to this problem a well a several algorithm that quickly compute approximate solution to the problem we have implemented our algorithm our implementation demonstrates that computing deployment can be done scalably 
we develop a method for predicting query performance by computing the relative entropy between a query language model and the corresponding collection language model the resulting clarity score measure the coherence of the language usage in document whose model are likely to generate the query we suggest that clarity score measure the ambiguity of a query with respect to a collection of document and show that they correlate positively with average precision in a variety of trec test set thus the clarity score may be used to identify ineffective query on average without relevance information we develop an algorithm for automatically setting the clarity score threshold between predicted poorly performing query and acceptable query and validate it using trec data in particular we compare the automatic threshold to optimum threshold and also check how frequently result a good are achieved in sampling experiment that randomly assign query to the two class 
in this paper we propose a novel method for learning a mahalanobis distance measure to be used in the knn classification algorit hm the algorithm directly maximizes a stochastic variant of the le ave one out knn score on the training set it can also learn a low dimensional linear embedding of labeled data that can be used for data visualization and fast classification unlike other method our classific ation model is non parametric making no assumption about the shape of the class distribution or the boundary between them the performance of the method is demonstrated on several data set both for metric learning and linear dimensionality reduction 
we show that a large and realistic face dataset can be built from news photograph and their associated caption our automatically constructed face dataset consists of face image obtained by applying a face nder to approximately half a million captioned news image and labeled using image information from the photograph and word information extracted from the corresponding caption this dataset is more realistic than usual face recognition datasets because it contains face captured in the wild in a variety of congurations with respect to the camera taking a variety of expression and under illumination of widely varying color face are extracted from the image and name with context are extracted from the associated caption our system us a clustering procedure to nd the correspondence between face and associated name in news picture caption pair the context in which a name appears in a caption provides powerful cue a to whether it is depicted in the associated image by incorporating simple natural language technique we are able to improve our name assignment signicantly we use two model of word context a naive bayes model and a maximum entropy model once our procedure is complete we have an accurately labeled set of face an appearance model for each individual depicted and a natural language model that can produce accurate result on caption in isolation 
in first order logic a theory t is considered stronger than another theory t if every formula derived from t is also derived from t such an order relation is useful to know relative value between different theory in the context of de fault logic a theory contains default information a well a definite information to order default theory it is necessary to ass the information content of a default theory to this end we intro duce a multi valued interpretation of default the ories based on a nine valued bilattice it distin guishes definite and credulous skeptical default in formation derived from a theory and is used for ordering default theory based on their informa tion content the technique is also applied to or der nonmonotonic logic program the result of this paper provide a method for comparing differ ent default theory and have important application to learning nonmonotonic theory 
the success of the semantic web crucially depends on the easy creation integration and use of semantic data for this purpose we consider an integration scenario that defies core assumption of current metadata construction method we describe a framework of metadata creation when web page are generated from a database and the database owner is cooperatively participating in the semantic web this lead u to the definition of ontology mapping rule by manual semantic annotation and the usage of the mapping rule and of web service for semantic query in order to create metadata the framework combine the presentation layer with the data description layer in contrast to conventional annotation which remains at the presentation layer therefore we refer to the framework a deep annotation we consider deep annotation a particularly valid because i web page generated from database outnumber static web page ii annotation of web page may be a very intuitive way to create semantic data from a database and iii data from database should not be materialized a rdf file it should remain where it can be handled most efficiently in it database 
this paper present a new boosting arcing algorithm called poca parallel online continuous arcing unlike traditional boosting algorithm such a arc x and adaboost that construct ensemble by adding and training weak learner sequentially on a round by round basis training in poca is performed over an entire ensemble continuously and in parallel since member of the ensemble are not frozen after an initial learning period a in traditional boosting poca is able to adapt rapidly to nonstationary environment and because poca doe not require the explicit scoring of a fixed exemplar set it can perform online learning of non repeating data we present result from experiment conducted using neural network expert that show poca is typically faster and more adaptive than existing boosting algorithm result presented for the uci letter dataset are to our knowledge the best published score to date 
many problem in information processing involve some form of dimensionality reduction in this paper we introduce locality preserving projection lpp these are linear projective map that arise by solving a variational problem that optimally preserve the neighborhood structure of the data set lpp should be seen a an alternative to principal component analysis pca a classical linear technique that project the data along the direction of maximal variance when the high dimensional data lie on a low dimensional manifold embedded in the ambient space the locality preserving projection are obtained by finding the optimal linear approximation to the eigenfunctions of the laplace beltrami operator on the manifold a a result lpp share many of the data representation property of nonlinear technique such a laplacian eigenmaps or locally linear embedding yet lpp is linear and more crucially is defined everywhere in ambient space rather than just on the training data point this is borne out by illustrative example on some high dimensional data set 
spectral clustering refers to a class of technique which re ly on the eigenstructure of a similarity matrix to partition point into di sjoint cluster with point in the same cluster having high similarity and point in different cluster having low similarity in this paper we der ive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem minimizing this cost function with respect to the partition lead to a new spectral clustering algorithm minimizing with respect to the similarity matrix lead to an algorithm f or learning the similarity matrix we develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors 
we present a new class of game local effect game leg which exploit structure in a different way from other compact game representation studied in ai we show both theoretically and empirically that these game often but not always have pure strategy nash equilibrium finding a potential function is a good technique for finding such equilibrium we give a complete characterization of which leg have potential function and provide the function in each case we also show a general case where pure strategy equilibrium exist in the absence of potential function in experiment we show that myopic best response dynamic converge quickly to pure strategy equilibrium in game not covered by our positive theoretical result 
we consider the problem of modeling annotated data data with multiple type where the instance of one type such a a caption serf a a description of the other type such a an image we describe three hierarchical probabilistic mixture model which aim to describe such data culminating in correspondence latent dirichlet allocation a latent variable model that is effective at modeling the joint distribution of both type and the conditional distribution of the annotation given the primary type we conduct experiment on the corel database of image and caption assessing performance in term of held out likelihood automatic annotation and text based image retrieval 
we address in this paper the question of how the knowledge of the marginal distribution p x can be incorporated in a learning algorithm we suggest three theoretical method for taking into account this distribution for regularization and provide link to existing graph based semi supervised learning algorithm we also propose practical implementation 
many of the computer vision algorithm have been posed invariousforms of differential equation derived from minimization ofspecific energy functionals and the finite element representationand computation have become the de facto numerical strategy forsolving these problem however for case where domain mappingsbetween numerical iteration or image frame involve largegeometrical shape change such a deformable model for objectsegmentation and non rigid motion tracking these strategy mayexhibit considerable loss of accuracy when themesh element becomeextremely skewed or compressed we present a new computationalparadigm the meshfree particle method where the objectrepresentation and the numerical calculation are purely based onthe nodal point and do not require the meshing of the analysisdomain this meshfree strategy can naturally handle largedeformation and domain discontinuity issue and achieve desirednumerical accuracy through adaptive node and polynomial shapefunction refinement we discus in detail the element free galerkinmethod including the shape function construction using the movingleast square approximation and the galerkin weak form formulation and we demonstrate it application to deformable model basedsegmentation and mechanically motivated left ventricular motionanalysis 
the similarity between object is a fundamental element of many learning algorithm most non parametric method take this similarity to be fixed but much recent work ha shown the advantage of learni ng it in particular to exploit the local invariance in the data or to capture the possibly non linear manifold on which most of the data lie we propose a new non parametric kernel density estimation method which capture the local structure of an underlying manifold through the le ading eigenvectors of regularized local covariance matrix experim ents in density estimation show significant improvement with respect to pa rzen density estimator the density estimator can also be used within bayes classifier yielding classification rate similar to svms and much superior to the parzen classifier 
we identify a new and important global or non binary constraint this constraint ensures that the value taken by two vector of varia bles when viewed a multisets are ordered this constraint is useful for a numb er of different application including breaking symmetry and fuzzy constraint satisfaction we propose and implement an efficient linear time algorithm for enf orcing generalised arc consistency on such a multiset ordering constraint ex perimental result on several problem domain show considerable promise 
we describe a way of using multiple different type of similarity relationship to learn a low dimensional embedding of a dataset our method chooses different possibly overlapping representation of similarity by individually reweighting the dimension of a common underlying latent space when applied to a single similarity relation that is based on euclidean distance between the input data point the method reduces to simple dimensionality reduction if additional information is available about the dataset or about subset of it we can use this information to clean up or otherwise improve the embedding we demonstrate the potential usefulness of this form of semi supervised dimensionality reduction on some simple example 
we propose a principled account on multiclass spectral clustering given a discrete clustering formulation we first solve a relaxedcontinuous optimization problem by eigen decomposition we clarifythe role of eigenvectors a a generator of all optimal solutionsthrough orthonormal transforms we then solve an optimaldiscretization problem which seek a discrete solution closest tothe continuous optimum the discretization is efficiently computedin an iterative fashion using singular value decomposition andnon maximum suppression the resulting discrete solution arenearly global optimal our method is robust to randominitialization and converges faster than other clustering method experiment on real image segmentation are reported 
monaural speech separation ha been studied in previous system that incorporate auditory scene analysis principle a major problem for these system is their inability to deal with speech in the highfrequency range psychoacoustic evidence suggests that different perceptual mechanism are involved in handling resolved and unresolved harmonic motivated by this we propose a model for monaural separation that deal with low frequency and highfrequency signal differently for resolved harmonic our model generates segment based on temporal continuity and cross channel correlation and group them according to periodicity for unresolved harmonic the model generates segment based on amplitude modulation am in addition to temporal continuity and group them according to am repetition rate derived from sinusoidal modeling underlying the separation process is a pitch contour obtained according to psychoacoustic constraint our model is systematically evaluated and it yield substantially better performance than previous system especially in the high frequency range 
learning from structured data is becoming increasingly important however most prior work on kernel method ha focused on learning from attribute value data only recently research started investigating kernel for structured data this paper considers kernel for multi instance problem a class of concept on individual represented by set the main result of this paper is a kernel on multi instance data that can be shown to separate positive and negative set under natural assumption this kernel compare favorably with state of the art multi instance learning algorithm in an empirical study finally we give some concluding remark and propose future work that might further improve the result 
we present a novel discriminative approach to parsing inspired by the large margin criterion underlying support vector machine our formulation us a factorization analogous to the standard dynamic program for parsing in particular it allows one to efficiently learn a model which discriminates among the entire space of parse tree a opposed to reranking the top few candidate our model can condition on arbitrary feature of input sentence thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness we provide an efficient algorithm for learning such model and show experimental evidence of the model s improved performance over a natural baseline model and a lexicalized probabilistic context free grammar 
we propose a new method for clustering based on finding maximum mar gin hyperplanes through data by reformulating the problem in term of the implied equivalence relation matrix we can pose the problem a a convex integer program although this still yield a difficult com putational problem the hard clustering constraint can be relaxed to a soft clustering formulation which can be feasibly solved with a semidef inite program since our clustering technique only depends on the data through the kernel matrix we can easily achieve nonlinear clustering in the same manner a spectral clustering experimental result show that our maximum margin clustering technique often obtains more accurate result than conventional clustering method the real benefit of our ap proach however is that it lead naturally to a semi supervised training method for support vector machine by maximizing the margin simul taneously on labeled and unlabeled training data we achieve state of the art performance by using a single integrated learning principle 
we present a novel framework for motion segmentation that combine the concept of layer based method and featurebased motion estimation we estimate the initial correspondence by comparing vector of filter output at interest point from which we compute candidate scene relation via random sampling of minimal subset of correspondence we achieve a dense piecewise smooth assignment of pixel to motion layer using a fast approximate graphcut algorithm based on a markov random field formulation we demonstrate our approach on image pair containing large inter frame motion and partial occlusion the approach is efficient and it successfully segment scene with inter frame disparity previously beyond the scope of layerbased motion segmentation method 
abstract we explore the descriptive power in term of syntactic phenomenon of a formalism that extends treeadjoining grammar tag by adding a fourth level of hierarchical decomposition to the three level tag already employ while extending the descriptive power minimally the additional level of decomposition allows u to obtain a uniform account of a range of phenomenon that ha heretofore been difficult to encompass an account that employ unitary elementary structure and eschews synchronized derivation operation and which is in many respect closer to the spirit of the intuition underlying tag based linguistic theory than previously considered extension to tag 
we generalise the gaussian process gp framework for regression by learning a nonlinear transformation of the gp output this allows for non gaussian process and non gaussian noise the learning algorithm chooses a nonlinear transformation such that transformed data is well modelled by a gp this can be seen a including a preprocessing transformation a an integral part of the probabilistic modelling problem rather than a an ad hoc step we demonstrate on several real regression problem that learning the transformation can lead to significantly better performance than using a regular gp or a gp with a fix ed transformation 
textons refer to fundamental micro structure in generic natural image and thus constitute the basic element in early preattentive visual perception however the word texton remains a vague concept in the literature of computer vision and visual perception and a precise mathematical definition ha yet to be found in this article we argue that the definition of texton should be governed by a sound mathematical model of image and the set of textons must be learned from or best tuned to an image ensemble we adopt a generative image model that an image is a superposition of base from an over complete dictionary then a texton is defined a a mini template that consists of a varying number of image base with some geometric and photometric configuration by analogy to physic if image base are like proton neutron and electron then textons are like atom then a small number of textons can be learned from training image a repeating micro structure we report four experiment for comparison the first experiment computes cluster in feature space of filter response the second use transformed component analysis in both feature space and image patch the third adopts a two layer generative model where an image is generated by image base and image base are generated by textons the fourth experiment show textons from motion image sequence which we call movetons 
a general linear response method for deriving improved estimate of correlation in the variational bayes framework is presented three application are given and it is discussed how to use linear response a a general principle for improving mean field approximation 
we describe a visualization technique that us brushed parallel histogram to aid in understanding concept drift in multidimensional problem space this technique illustrates the relationship between change in distribution of multiple antecedent feature value and the outcome distribution we can also observe effect on the relative utilization of predictive rule our parallel histogram technique solves the over plotting difficulty of parallel coordinate graph and the difficulty of comparing distribution of brushed and original data we demonstrate our technique s usefulness in understanding concept drift in power demand and stock investment return 
it is difficult to serialize an rdf graph a a humanly readable rdf xml document this paper describes the approach taken in jena in which a design pattern of guarded procedure invoked using top down recursive descent is used each procedure corresponds to a grammar rule the guard make the choice about the applicability of the production this approach is seen to correspond closely to the design of an ll k parser and a theoretical justification of this correspondence is found in universal algebra 
an interesting and potentially useful vision graphic task is to render an input image in an enhanced form or also in an unusual style for example with increased sharpness or with some artistic quality in previous work researcher showed that by estimating the mapping from an input image to a registered aligned image of the same scene in a different style or resolution the mapping could be used to render a new input image in that style or resolution frequently a registered pair is not available but instead the user may have only a source image of an unrelated scene that contains the desired style in this case the task of inferring the output image is much more difficult since the algorithm must both infer correspondence between feature in the input image and the source image and infer the unknown mapping between the image we describe a bayesian technique for inferring the most likely output image the prior on the output image p x is a patch based markov random field obtained from the source image the likelihood of the input p y x is a bayesian network that can represent different rendering style we describe a computationally efficient probabilistic inference and learning algorithm for inferring the most likely output image and learning the rendering style we also show that current technique for image restoration or reconstruction proposed in the vision literature e g image super resolution or de noising and image based nonphotorealistic rendering could be seen a special case of our model we demonstrate our technique using several task including rendering a photograph in the artistic style of an unrelated scene de noising and texture transfer 
in we introduced a linear statistical model of joint color change in image due to variation in lighting and certain non geometric camera parameter we did this by measuring the mapping of color in one image of a scene to color in another image of the same scene under different lighting condition here we increase the flexibility of this color flow model by allowing flow coefficient to vary according to a low order polynomial over the image this allows u to better fit smoothly varying lighting condition a well a curved surface without endowing our model with too much capacity we show result on image matching and shadow removal and detection addressing the variability of image due to these photic parameter ha been an important problem in machine vision we distinguish photic parameter from geometric parameter such a camera orientation or blurring that affect which part of the scene a particular pixel represents we also note that photic parameter are more general than lighting parameter and include anything which affect the final rgb value in an image given that the geometric parameter and the object in the scene have been fixed we present a statistical linear model of color change space that is learned by observing how the color in static image change jointly under common naturally occurring lighting change such a model can be used for a number of task including synthesis of image of new object under different lighting condition image matching and shadow detection result for each of these task will be reported several aspect of our model merit discussion first it is obtained from video data in a completely unsupervised fashion the model us no prior knowledge of lighting condition surface reflectance or other parameter during data collection and modeling it also ha no built in knowledge of the physic of image acquisition or typical image color 
the problem we study is given n view and a subset of the interview fundamental matrix which of the other fundamental matrix can we compute using only the precomputed fundamental matrix this ha application in d reconstruction and when we want to reproject an area of one view on another or to compute epipolar line when the correspondence problem is too difficult to compute between every two view a complete solution using linear algorithm to compute the missing fundamental matrix is given for up to six view in many case problem with more than six view can also be handled 
transitive retrieval and triangulation have been proposed a way to improve cross language retrieval quality when translation resource have poor lexical coverage we demonstrate that cross language retrieval is viable for european language with no translation resource at all that transitive retrieval without translation doe not suffer the drop off in retrieval quality sometimes reported for transitive retrieval with translation and that triangulation that combine multiple transitive run with no translation can boost performance over direct translation free retrieval 
in this paper we present a method based on document probe to quantify and diagnose topic structure distinguishing topic a monolithic structured or diffuse the method also yield a structure analysis that can be used directly to optimize filter classifier creation preliminary result illustrate the predictive value of the approach on trec reuters topic 
in the original pagerank algorithm for improving the ranking of search query result a single pagerank vector is computed using the link structure of the web to capture the relative importance of web page independent of any particular search query to yield more accurate search result we propose computing a set of pagerank vector biased using a set of representative topic to capture more accurately the notion of importance with respect to a particular topic by using these precomputed biased pagerank vector to generate query specific importance score for page at query time we show that we can generate more accurate ranking than with a single generic pagerank vector for ordinary keyword search query we compute the topic sensitive pagerank score for page satisfying the query using the topic of the query keywords for search done in context e g when the search query is performed by highlighting word in a web page we compute the topic sensitive pagerank score using the topic of the context in which the query appeared 
pervasive robotics will require in a near future small light and cheap robot that exhibit complex behavior these demand led to the development of the m m macaco project a robotic active vision head macaco is a portable system capable of emulating the head of different creature both aesthetically and functionally it integrates mechanism for social interaction autonomous navigation and object analysis 
temperature discovery search tds is a new minimaxbased game tree search method designed to compute or approximate the temperature of a combinatorial game tds is based on the concept of an enriched environment where a combinatorial game g is embedded in an environment consisting of a large set of simple game of decreasing temperature optimal play start in the environment but eventually must switch to g tds nd the temperature of g by determining when this switch must happen both exact and heuristic version of tds are described and evaluated experimentally in experiment with sum game in amazon tds outperforms an searcher 
a public virtual laboratory is presented where animats are controlled by mechanism from different cognitive paradigm a brief description of the characteristic of the laboratory and the us it ha had is given mainly it ha been used to contrast philosophical idea related with the notion of cognition and to elucidate debate on proper paradigm in ai and cognitive science 
we present a detailed investigation of the challenge posed when applying parsing model developed against english corpus to chinese we develop a factored model statistical parser for the penn chinese treebank showing the implication of gross statistical difference between wsj and chinese tree bank for the most general method of parser adaptation we then provide a detailed analysis of the major source of statistical parse error for this corpus showing their cause and relative frequency and show that while some type of error are due to difficult ambiguity inherent in chinese grammar others arise due to treebank annotation practice we show how each type of error can be addressed with simple targeted change to the independence assumption of the maximum likelihood estimated pcfg factor of the parsing model which raise our f from to on our development set and achieves parse accuracy close to the best published figure for chinese parsing 
although discriminatively trained classifier are usually more accurate when labeled training data is abundant previous work ha shown that when training data is limited generative classifier can ou t perform them this paper describes a hybrid model in which a high dimensional subset of the parameter are trained to maximize generative likelihood and another small subset of parameter are discriminatively trained to maximize conditional likelihood we give a sample complexity bound showing that in order to fit the discriminative parameter we ll the number of training example required depends only on the logarithm of the number of feature occurrence and feature set size experimental result show that hybrid model can provide lower test error and can produce better accuracy coverage curve than either their purely g enerative or purely discriminative counterpart we also discus several advantage of hybrid model and advocate further work in this area 
we address the issue of regularizing osher and rudin s shock filter used for image deblurring in order to allow process that are more robust against noise previous solution to the problem suggested adding some sort of diffusion term to the shock equation we analyze and prove some property of coupled shock and diffusion process finally we propose an original solution of adding a complex diffusion term to the shock equation this new term is used to smooth out noise and indicate inflection point simultaneously the imaginary value which is an approximated smoothed second derivative scaled by time is used to control the process this result in a robust deblurring process that performs well also on noisy signal 
this work present an architecture based on perceptrons to recognize phrase structure and an online learning algorithm to train the perceptrons together and dependently the recognition strategy applies learning in two layer a filtering layer which reduces the search space by identifying plausible phrase candidate and a ranking layer which recursively build the optimal phrase structure we provide a recognition based feedback rule which reflects to each local function it committed error from a global point of view and allows to train them together online a perceptrons experimentation on a syntactic parsing problem the recognition of clause hierarchy improves state of the art result and evinces the advantage of our global training method over optimizing each function locally and independently 
abstract we consider a learning setting in which thereare well dened relation that exist amonginstances of certain class in particular we consider the domain of predicting varioustypes of gene regulation element in bacterialgenomes given instance of one class wecan often acquire weakly labeled quot trainingdata for another class by taking advantage ofknown relationship that exist between thetwo class the example are weakly labeledin that either the class label is 
we present an approach for learning part of speech distinction by induction over the lexicon of the cyc knowledge base this produce good result using a decision tree that incorporates both semantic feature and syntactic feature accurate result are achieved for the special case of deciding whether lexical mapping should use count noun or mass noun headword comparable result are also obtained using opencyc the publicly available version of cyc 
we present an algorithm for unsupervised learning and semantic classification of name and term given a small number of seed example and an unlabeled training corpus the algorithm learns pattern that identify more example in a bootstrapping cycle multiple class are learned simultaneously including negative class that serve to provide negative example for the target class we apply the algorithm to text from several domain in english and chinese 
in jegou a decomposition method ha been introduced for improving search efficiency in the area of constraint satisfaction problem this method is based on property of micro structure of csps related to property of triangulated graph this decomposition allows to transform an instance of csp in a collection of sub problem easier to solve and then give a natural and efficient way for a parallel implementation habbas et al in this paper we present a generalization of this approach which is based on a generalization of triangulated graph this generalization allows to define the level of decomposition which can be fixed by a graph parameter the larger this parameter is the more level of decomposition that is the number of sub problem is a a consequence we can then define the level of decomposition with respect to the nature of the parallel configuration used the number of processor first experiment reported here show that this extension increase significantly the advantage of the basic decomposition already shown in habbas et al 
an empirical study ha been conducted investigating the relationship between the performance of an aspect based language model in term of perplexity and the corresponding information retrieval performance obtained it is observed on the corpus considered that the perplexity of the language model ha a systematic relationship with the achievable precision recall performance though it is not statistically significant 
detection of near duplicate document is an important problem in many data mining and information filtering application when faced with massive quantity of data traditional duplicate detection technique relying on direct inter document similarity computation e g using the cosine measure are often not feasible given the time and memory performance constraint on the other hand fingerprint based method such a i match are very attractive computationally but may be brittle with respect to small change to document content we focus on approach to near replica detection that are based upon large collection statistic and present a general technique of increasing their robustness via multiple lexicon randomization in experiment with large web page and spam email datasets the proposed method is shown to consistently outperform traditional i match with the relative improvement in duplicate document recall reaching a high a the large gain in detection accuracy are offset by only small increase in computational requirement 
we consider an mdp setting in which the reward function is allowed to change during each time step of play possibly in an adversarial manner yet the dynamic remain fixed similar to the expert setting we address the question of how well can an agent do when compared to the reward achieved under the best stationary policy over time we provide efficient algorithm which have regret bound with no dependenceon the size of state space instead these bound depend only on a certain horizon time of the process and logarithmically on the number of action we also show that in the case that the dynamic change over time the problem becomes computationally hard 
collaborative and content based filtering are two paradigm that have been applied in the context of recommender system and user preference prediction this paper proposes a novel unified approach that systematically integrates all available training information such a past user item rating a well a attribute of item or user to learn a prediction function the key ingredient of our method is the design of a suitable kernel or similarity function between user item pair that allows simultaneous generalization across the user and item dimension we propose an on line algorithm jrank that generalizes perceptron learning experimental result on the eachmovie data set show significant improvement over standard approach 
cross entropy and mean squared error are typical cost function used to optimize classifier performance the goal of the optimization is usually to achieve the best correct classification rate however for many two class real world problem the roc curve is a more meaningful performance measure we demonstrate that minimizing cross entropy or mean squared error doe not necessarily maximize the area under the roc curve auc we then consider alternative objective function for training a 
this paper present a task allocation scheme via self organizing swarm coalition for distributed mobile sensor network coverage our approach us the concept of ant behavior to self regulate the regional distribution of sensor in proportion to that of the moving target to be tracked in a non stationary environment a a result the adverse effect of task interference between robot are minimized and sensor network coverage is improved quantitative comparison with other tracking strategy such a static sensor placement potential field and auction based negotiation show that our approach can provide better coverage and greater flexibility to respond to environmental change 
we e xplore dynamic shaping to integrate our prior belief of the final policy into a conventional reinforcement learning system shaping provides a positive or negative artificial increment t o the native task reward in order to encourage or discourage behavior previously shaping function have been static the additional reward do not vary with experience but some prior knowledge ca nnot be e xpressed a s tatic shaping we take an explanation based approach in which the specific shaping function emerges from initial experience with the world we compare no shaping static shaping and dynamic shaping in the task of learning bipedal walking on a simulator we e mpirically evaluate the convergence rate a nd final performance a mong these c onditions while varying the acc uracy of the prior knowledge we c onclude that i n the appropriate context dynamic shaping can greatly improve the learning of action policy 
a logic of conditional preference is defined with a language which allows she compact representation of certain kind of conditional preference statement a semantics and a proof theory cp net can be expressed in this language and the semantics and proof theory generalise those of cp net despite being substantially more expressive the formalism maintains important property of cp net there are simple sufficient condition for consistency and under these condition optimal outcome can be efficiently generated it is also then easy to find a total order on outcome which extends the conditional preference order and an approach to constrained optimisation can be used which generalises a natural approach for cp net some result regarding the expressive power of cp net are also given 
hidden variable evolving over time appear in multiple setting where it is valuable to recover them typically from observed sum our driving application is network tomography where we need to estimate the origin destination od traffic flow to determine e g who is communicating with whom in a local area network this information allows network engineer and manager to solve problem in design routing configuration debugging monitoring and pricing unfortunately the direct measurement of the od traffic is usually difficult or even impossible instead we can easily measure the load on every link that is sum of desirable od flow in this paper we propose i filter a method to solve this problem which improves the state of the art by a introducing explicit time dependence and by b using realistic non gaussian marginals in the statistical model for the traffic flow a never attempted before we give experiment on real data where i filter scale linearly with new observation and out performs the best existing solution in a wide variety of setting specifically on real network traffic measured at cmu and at at t i filter reduced the estimation error between and in all case 
this paper describes a question answering system that is designed to capitalize on the tremendous amount of data that is now available online most question answering system use a wide variety of linguistic resource we focus instead on the redundancy available in large corpus a an important resource we use this redundancy to simplify the query rewrite that we need to use and to support answer mining from returned snippet our system performs quite well given the simplicity of the technique being utilized experimental result show that question answering accuracy can be greatly improved by analyzing more and more matching passage simple passage ranking and n gram extraction technique work well in our system making it efficient to use with many backend retrieval engine 
if an e service approach to electronic commerce is to become widespread standardisation of ontology message content and message protocol will be necessary in this paper we present a lifecycle of a business to business e commerce interaction and show how the semantic web can support a service description language that can be used throughout this lifecycle by using daml we develop a service description language sufficiently expressive and flexible to be used not only in advertisement but also in matchmaking query negotiation proposal and agreement we also identify which operation must be carried out on this description language if the b b lifecycle is to be fully supported we do not propose specific standard protocol but instead argue that our operator are able to support a wide variety of interaction protocol and so will be fundamental irrespective of which protocol are finally adopted 
the banking industry regularly mount campaign to improve customer value by offering new product to existing customer in recent year this approach ha gained significant momentum because of the increasing availability of customer data and the improved analysis capability in data mining typically response model based on historical data are used to estimate the probability of a customer purchasing an additional product and the expected return from that additional purchase even with these computational improvement and accurate model of customer behavior the problem of efficiently using marketing resource to maximize the return on marketing investment is a challenge this problem is compounded because of the capability to launch multiple campaign through several distribution channel over multiple time period the combination of alternative creates a complicated array of possible action this paper present a solution that answer the question of what product if any to offer to each customer in a way that maximizes the marketing return on investment the solution is an improvement over the usual approach of picking the customer that have the largest expected value for a particular product because it is a global maximization from the viewpoint of the bank and allows for the effective implementation of business constraint across customer and business unit the approach account for limited resource multiple sequential campaign and other business constraint furthermore the solution provides insight into the cost of these constraint in term of decreased profit and thus is an effective tool for both tactical campaign execution and strategic planning 
much attention ha been accorded to location based service and location tracking a necessary component in active trigger based lb application tracking the location of a large population of moving object requires very high update and query performance of the underlying spatial index in this paper we investigate the performance and scalability of three main memory based spatial indexing method under dynamic update and query load an r tree a zb tree and an array hashtable method by leveraging the locus performance evaluation testbed and the city simulator dynamic spatial data generator we are able to demonstrate the scalability of these method and determine the maximum population size supported by each method a useful parameter for capacity planning by wireless carrier 
we consider from a computational perspective the problem of how to aggregate the ranking preference of a number of alternative by a number of different voter into a single consensus ranking following the majority voting rule social welfare function for aggregating preference in this way have been widely studied since the time of condorcet one drawback of majority voting procedure when three or more alternative are being ranked is the presence of cycle in the majority preference relation the kemeny order is a social welfare function which ha been designed to tackle the presence of such cycle however computing a kemeny order is known to be np hard we develop a greedy heuristic and an exact branch and bound procedure for computing kemeny order we present result of a computational study on these procedure 
pipelined natural language generation nlg system have grown increasingly complex a architectural module were added to support language functionality such a referring expression lexical choice and revision this ha given rise to discussion about the relative placement of these new module in the overall architecture recent work on another aspect of multi paragraph text discourse marker indicates it is time to consider where a discourse marker insertion algorithm fit in we present example which suggest that in a pipelined nlg architecture the best approach is to strongly tie it to a revision component finally we evaluate the approach in a working multi page system 
we consider the problem of segmentation of image that can bemodelled a piecewise continuous signal having unknown non stationary statistic we propose a solution to this problemwhich first us a regression framework to estimate the image pdf and then mean shift to find the mode of this pdf the segmentationfollows from mode identification wherein pixel cluster or imagesegments are identified with unique mode of the multi modal pdf each pixel is mapped to a mode using a convergent iterativeprocess the effectiveness of the approach depends upon theaccuracy of the implicit estimate of the underlying multi modaldensity function and thus on the bandwidth parameter used for itsestimate using parzen window automatic selection of bandwidthparameters is a desired feature of the algorithm we show that theproposed regression based model admits a realistic framework toautomatically choose bandwidth parameter which minimizes a globalerror criterion we validate the theory presented with result onreal image 
significant progress in image segmentation ha beenmade by viewing the problem in the framework of graphpartitioning in particular spectral clustering method suchas normalized cut ncuts can efficiently calculate goodsegmentations using eigenvector calculation however spectral method when applied to image with local connectivityoften oversegment homogenous region more importantly they lack a straightforward probabilistic interpretationwhich make it difficult to automatically set parametersusing training data in this paper we revisit the typical cut criterion proposedin we show that computing the typical cut isequivalent to performing inference in an undirected graphicalmodel this equivalence allows u to use the powerfulmachinery of graphical model for learning and inferringimage segmentation for inferring segmentation weshow that the generalized belief propagation gbp algorithmcan give excellent result with a runtime that is usuallyfaster than the ncut eigensolver for learning segmentationswe derive a maximum likelihood learning algorithmto learn affinity matrix from labelled datasets we illustrateboth learning and inference on challenging real andsynthetic image 
in this paper we address the problem of statistical learnin g for multitopic text categorization mtc whose goal is to choose all relevant topic a label from a given set of topic the proposed algorithm maximal margin labeling mml treat all possible label a independent class and learns a multi class classifier on the induced mu lti class categorization problem to cope with the data sparseness caused by the huge number of possible label mml combine some prior knowledge about label prototype and a maximal margin criterion in a novel way experiment with multi topic web page show that mml outperforms existing learning algorithm including support vector machine 
is it feasible to train classifier to decode the cognitive state of a human subject based on single episode fmri data if so these trained classifier could be used a virtual sensor to detect hidden cognitive state of a subject providing a key tool for experimental research in cognitive science and in diagnosis of mental process in patient with brain injury whereas much work ha been done on fmri data analysis method that average together data collected from repeated stimulus over multiple episode little is known about the feasibility of training classifier to decode cognitive state from single episode this paper present several case study in which we have successfully trained such classifier we explore the technical issue involved in training such single episode classifier and discus area for future research these case study include training a classifier to determine which of twelve semantic category of word is being read by a human subject e g a word describing animal or one describing building whether or not a subject find a sentence ambiguous and whether the subject is looking at a picture or at a sentence describing a picture 
de novo sequencing of peptide is a challenging task in proteome research while there exist reliable dna sequencing method the highthroughput de novo sequencing of protein by mass spectrometry is still an open problem current approach suffer from a lack in precision to detect mass peak in the spectrogram in this paper we present a novel method for de novo peptide sequencing based on a hidden markov model experiment effectively demonstrate that this new method significantly outperforms standard approach in matching quality 
many automated learning procedure lack interpretability operating effectively a a black box providing a prediction tool but no explanation of the underlying dynamic that drive it a common approach to interpretation is to plot the dependence of a learned function on one or two predictor we present a method that seek not to display the behavior of a function but to evaluate the importance of non additive interaction within any set of variable should the function be close to a sum of low dimensional component these component can be viewed and even modeled parametrically alternatively the work here provides an indication of where intrinsically high dimensional behavior take place the calculation used in this paper correspond closely with the functional anova decomposition a well developed construction in statistic in particular the proposed score of interaction importance measure the loss associated with the projection of the prediction function onto a space of additive model the algorithm run in linear time and we present display of the output a a graphical model of the function for interpretation purpose 
abstract the hierarchical hidden markov model hhmm is an ex tension of the hidden markov model to include a hierarchy of the hidden state this form of hierarchical modeling ha been found useful in application such a handwritten char acter recognition behavior recognition video indexing and text retrieval nevertheless the state hierarchy in the original hhmm is restricted to a tree structure this prohibits two different state from having the same child and thus doe not allow for sharing of common substructure in the model in this paper we present a general hhmm in which the state hierarchy can be a lattice allowing arbitrary sharing of sub structure furthermore we provide a method for numerical scaling to avoid underflow an important issue in dealing with long observation sequence we demonstrate the working of our method in a simulated environment where a hierarchical behavioral model is automatically learned and later used for recognition 
segmentation and recognition have long been treated a two separate process we propose a mechanism based on spectral graph partitioning that readily combine the two process into one a part based recognition system detects object patch supply their partial segmentation and knowledge about the spatial configuration of the object the goal of patch grouping is to find a set of patch that conform best to the object configuration this process is integrated with the pixel grouping based on low level feature similarity through pixel patch interaction and patch competition that is encoded a constraint in the solution space the globally optimal partition is obtained by solving a constrained eigenvalue problem we demonstrate that the resulting object segmentation eliminates local false positive at the high level of part detection while overcoming occlusion and weak contour at the low level of edge detection 
a new approach to the text categorization problem is here presented it is called gaussian weighting and it is a supervised learning algorithm that during the training phase estimate two very simple and easily computable statistic which are the presence p how much a term is present in a category c in the expressiveness e how much is present outside c in the rest of the domain once the system ha learned this information a gaussian function is shaped for each term of a category in order to assign the term a weight that estimate the level of it importance for that particular category we tested our learning method on the task of single label classification using the reuters benchmark the outcome of the result wa quite impressive in different experimental setup we reached a micro averaged fl measure of with a peak of moreover a macro averaged recall and precision wa calculated the former reported a the latter a these result reach most of the state of the art technique of machine learning applied to text categorization demonstrating that this new weighting scheme doe perform well on this particular task 
many inference and optimization task in machine learning can be solved by sampling approach such a markov chain monte carlo mcmc and simulated annealing these method can be slow if a single target density query requires many run of a simulation or a complete sweep of a training data set we introduce a hierarchy of mcmc sampler that allow most step to be taken in the solution space using only a small sample of simulation run or training example this is shown to accelerate learning in a policy search optimization task 
we present an approach to bounded constraintrelaxation for entropy maximization that corresponds to using a double exponential prior or regularizer in likelihood maximization for log linear model we show that a combined incremental feature selection and regularization method can be established for maximum entropy modeling by a natural incorporation of the regularizer into gradientbased feature selection following perkins et al this provides an efficient alternative to standard regularization on the full feature set and a mathematical justification for thresholding technique used in likelihood based feature selection also we motivate an extension to n best feature selection for linguistic feature set with moderate redundancy and present experimental result showing it advantage over best regularization and over standard incremental feature selection for the task of maximum entropy parsing 
the basic tool of machine learning appear in the inner loop of most reinforcement learning algorithm typically in the form of monte carlo method or function approximation technique to a large extent however current reinforcement learning algorithm draw upon machine learning technique that are at least ten year old and with a few exception very little ha been done to exploit recent advance in classification learning for the purpose of reinforcement learning we use a variant of approximate policy iteration based on rollouts that allows u to use a pure classification learner such a a support vector machine svm in the inner loop of the algorithm we argue that the use of svms particularly in combination with the kernel trick can make it easier to apply reinforcement learning a an outof the box technique without extensive feature engineering our approach open the door to modern classification method but doe not preclude the use of classical method we present experimental result in the pendulum balancing and bicycle riding domain using both svms and neural network for classifier 
is there a way for an algorithm linked to an unknown body to infer by itself information about this body and the world it is in taking the case of space for example is there a way for this algorithm to realize that it body is in a three dimensional world is it possible for this algorithm to discover how to move in a straight line and more basically do these question make any sense at all given that the algorithm only ha access to the very high dimensional data consisting of it sensory input and motor output we demonstrate in this article how these question can be given a positive answer we show that it is possible to make an algorithm that by analyzing the law that link it motor output to it sensory input discovers information about the structure of the world regardless of the device constituting the body it is linked to we present result from simulation demonstrating a way to issue motor order resulting in fundamental movement of the body a regard the structure of the physical world 
to operate effectively in complex environment learning agent require the ability to selectively ignore irrelevant detail and form useful abstraction in this article we consider the question of what constitutes a useful abstraction in a stochastic sequential decision problem modeled a a semi markov decision process smdps we introduce the notion of smdp homomorphism and argue that it provides a useful tool for a rigorous study of abstraction for smdps we present an smdp minimization framework and an abstraction framework for factored mdps based on smdp homomorphism we also model different class of abstraction that arise in hierarchical system although we use the option framework for purpose of illustration the idea are more generally applicable we also show that the condition for abstraction we employ are a generalization of earlier work by dietterich a applied to the option framework 
this paper investigates the pre condition for successful combination of document representation formed from structural markup for the task of known item search a this task is very similar to work in meta search and data fusion we adapt several hypothesis from those research area and investigate them in this context to investigate these hypothesis we present a mixture based language model and also examine many of the current meta search algorithm we find that compatible output from system is important for successful combination of document representation we also demonstrate that combining low performing document representation can improve performance but not consistently we find that the technique best suited for this task are robust to the inclusion of poorly performing document representation we also explore the role of variance of result across system and it impact on the performance of fusion with the surprising result that the correct document have higher variance across document representation than highly ranking incorrect document 
we present an approach using syntacto semantic rule for the extraction of relational information from biomedical abstract the result show that by overcoming the hurdle of technical terminology high precision result can be achieved from abstract related to baker s yeast we manage to extract a regulatory network comprised of pairwise relation from abstract with an accuracy of to achieve this we made use of a resource of gene protein name considerably larger than those used in most other biology related information extraction approach this list of name wa included in the lexicon of our retrained part of speech tagger for use on molecular biology abstract for the domain in question an accuracy of wa attained on po tag the method is easily adapted to other organism than yeast allowing u to extract many more biologically relevant relation 
in this paper we elucidate how korean temporal marker contribute to specifying the event time and formalize it in term of typed lambda calculus we also present a computational method for constructing temporal representation of korean sentence on the basis of g grammar proposed by renaud 
many web information service utilize technique of information extraction ie to collect important fact from the web to create more advanced service one possible method is to discover thematic information from the collected fact through text classification however most conventional text classification technique rely on manual labelled corpus and are thus ill suited to cooperate with web information service with open domain in this work we present a system named liveclassifier that can automatically train classifiersthrough web corpus based on user defined topic hierarchy due to it flexibility and convenience liveclassifier can be easily adapted for various purpose new web information service can be created to fully exploit it human user can use it to create classifier for their personal application the effectiveness of classifier created by liveclassifier is well supportedby empirical evidence 
thanks to it increasing availability electronic literature can now be a major source of information when developing complex statistical model where data is scarce or contains much noise this raise the question of how to integrate information from domain literature with statistical data because quantifying similarity or dependency between variable is a basic building block in knowledge discovery we consider here the following question which vector representation of text and which statistical score of similarity or dependency support best the use of literature in statistical model for the text source we assume to have annotation for the domain variable a short free text description and optionally to have a large literature repository from which we can further expand the annotation for evaluation we contrast the variable similarity or dependency obtained from text using different annotation source and vector representation with those obtained from measurement data or expert assessment specifically we consider two learning problem clustering and bayesian network learning firstly we report performance against an expert reference for clustering yeast gene from textual annotation secondly we ass the agreement between text based and data based score of variable dependency when learning bayesian network substructure for the task of modeling the joint distribution of clinical measurement of ovarian tumor 
in applying hidden markov model to the analysis of massive data stream it is often necessary to use an artiflcially reduced set of state this is due in large part to the fact that the basic hmm estimation algorithm have a quadratic dependence on the size of the state set we present algorithm that reduce this computational bottleneck to linear or near linear time when the state can be embedded in an underlying grid of parameter this type of state representation arises in many domain in particular we show an application to tra c analysis at a high volume web site 
a new method for visual tracking of articulated objectsis presented analyzing articulated motion is challengingbecause the dimensionality increase potentially demandstremendous increase of computation to ease this problem we propose an approach that analyzes subpart locallywhile reinforcing the structural constraint at the meantime the computational model of the proposed approachis based on a dynamic markov network a generative modelwhich characterizes the dynamic and the image observationsof each individual subpart a well a the motion constraintsamong different subpart probabilistic variationalanalysis of the model reveals a mean field approximationto the posterior density of each subpart given visual evidence and provides a computationally efficient way forsuch a difficult bayesian inference problem in addition we design mean field monte carlo mfmc algorithm inwhich a set of low dimensional particle filter interact witheach other and solve the high dimensional problem collaboratively extensive experiment on tracking human bodyparts demonstrate the effectiveness significance and computationalefficiency of the proposed method 
a growing number of application seek to incorporate automatically generated narrative structure into interactive virtual environment in this paper we evaluate a representation for narrative structure generated by an automatic planning system by mapping the plan that control plot into conceptual graph used by quest an existing framework for question answering analysis that includes structure for modeling a reader s narrative comprehension and using method originally employed by quest s developer to determine if the plan structure can serve a effective model of the understanding that human user form after viewing corresponding story played out within a virtual world result from our analysis are encouraging though additional work is required to expand the plan language to cover a broader class of narrative structure 
this paper introduces the concept of resource temporal network rtn a constraint network that subsumes both classical attribute used in a i planning and capacity resource traditionally handled in scheduling after giving a formal definition of rtns we analyze their expressive power and study complexity of several fragment of the rtn framework we show that solving an rtn is in general np complete which is not surprising given the expressivity of the framework whereas computing a necessary truth criterion is polynomial this last result open the door for promising algorithm to solve rtns 
retrieval mechanism are frequently compared by computing the respective average score for some effectiveness metric across a common set of information need or topic with researcher concluding one method is superior based on those average since comparative retrieval system behavior is known to be highly variable across topic good experimental design requires that a sufficient number of topic be used in the test this paper us trec result to empirically derive error rate based on the number of topic used in a test and the observed difference in the average score the error rate quantify the likelihood that a different set of topic of the same size would lead to a different conclusion we directly compute error rate for topic set up to size and extrapolate those rate for larger topic set size the error rate found are larger than anticipated indicating researcher need to take care when concluding one method is better than another especially if few topic are used 
this paper investigates the effect of kernel principal component analysis kpca within the classification framework essentially the regularization property of this dimensionality reduction method kpca ha been previously used a a pre processing step before applying an svm but we point out that this method is somewhat redundant from a regularization point of view and we propose a new algorithm called kernel projection machine to avoid this redundancy based on an analogy with the statistical framework of regression for a gaussian white noise model preliminary experimental result show that this algorithm reach the same performance a an svm 
motor control depends on sensory feedback in multiple modality with different latency in this paper we consider within the framework of reinforcement learning how different sensory modality can be combined and selected for real time optimal movement control we propose an actor critic architecture with multiple module whose output are combined using a softmax function we tested our architecture in a simulation of a sequential reaching task reaching wa initially guided by visual feedback with a long latency our learning scheme allowed the agent to utilize the somatosensory feedback with shorter latency when the hand is near the experienced trajectory in simulation with different latency for visual and somatosensory feedback we found that the agent depended more on feedback with shorter latency 
this paper deal with the autocalibration of a systemthat consists of a planar screen multiple projector and acamera in the system either multiple projector or a singlemoving projector project pattern on a screen while a stationarycamera placed in front of the screen take image ofthe pattern we treat the case in which the pattern that theprojectors project toward space are assumed to be known i e the projector are calibrated whereas pose of theprojectors are unknown under these condition we considerthe problem of estimating screen to camera homographyfrom the image alone this is intended for case wherethere is no clue on the screen surface that enables directestimation of the screen to camera homography one applicationis a dof input device pose of a multi beamprojector freely moving in space are computed from the imagesof beam spot on the screen the primary contributionof the paper is theoretical result on the uniqueness of solutionsand a noniterative algorithm for the problem theeffectiveness of the method is shown by experimental resultson synthetic a well a on real image 
while being successful in providing keyword based access to web page commercial search portal still lack the ability to answer question expressed in a natural language we present a probabilistic approach to automated question answering on the web based on trainable pattern answer triangulation and semantic filtering in contrast to the other shallow approach our approach is entirely self learning it doe not require any manually created scoring and filtering rule while still performing comparably it also performs better than other fully trainable approach 
in many real world classification problem the input contains a large number of potentially irrelevant feature this paper proposes a new bayesian framework for determining the relevance of input feature this approach extends one of the most successful bayesian method for feature selection and sparse learning known a automatic relevance determination ard ard find the relevance of feature by optimizing the model marginal likelihood also known a the evidence we show that this can lead to overfitting to address this problem we propose predictive ard based on estimating the predictive performance of the classifier while the actual leave one out predictive performance is generally very costly to compute the expectation propagation ep algorithm proposed by minka provides an estimate of this predictive performance a a side effect of it iteration we exploit this in our algorithm to do feature selection and to select data point in a sparse bayesian kernel classifier moreover we provide two other improvement to previous algorithm by replacing laplace s approximation with the generally more accurate ep and by incorporating the fast optimization algorithm proposed by faul and tipping our experiment show that our method based on the ep estimate of predictive performance is more accurate on test data than relevance determination by optimizing the evidence 
procedure for collective inference make simultaneous statistical judgment about the same variable for a set of related data instance for example collective inference could be used to simultaneously classify a set of hyperlinked document or infer the legitimacy of a set of related financial transaction several recent study indicate that collective inference can significantly reduce classification error when compared with traditional inference technique we investigate the underlying mechanism for this error reduction by reviewing past work on collective inference and characterizing different type of statistical model used for making inference in relational data we show important difference among these model and we characterize the necessary and sufficient condition for reduced classification error based on experiment with real and simulated data 
the detection and pose estimation of people in image and video is made challenging by the variability of human appearance the complexity of natural scene and the high dimensionality of articulated body model to cope with these problem we represent the d human body a a graphical model in which the relationship between the body part are represented by conditional probability distribution we formulate the pose estimation problem a one of probabilistic inference over a graphical model where the random variable correspond to the individual limb parameter position and orientation because the limb are described by dimensional vector encoding pose in space discretization is impractical and the random variable in our model must be continuousvalued to approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle filter this framework facilitates the automatic initialization of the body model from low level cue and is robust to occlusion of body part and scene clutter 
this paper present a novel hidden markov model architecture to model the joint probability of pair of asynchronous sequence describing the same event it is based on two other markovian model namely asynchronous input output hidden markov model and pair hidden markov model an em algorithm to train the model is presented a well a a viterbi decoder that can be used to obtain the optimal state sequence a well a the alignment between the two sequence the model ha been tested on an audio visual speech recognition task using the m vt database and yielded robust performance under various noise condition 
an important issue in the dissemination of time varying web data such a sport score and stock price is the maintenance of temporal coherency in the case of server adhering to the http protocol client need to frequently pull the data based on the dynamic of the data and a user s coherency requirement in contrast server that posse push capability maintain state information pertaining to client and push only those change that are of interest to a user these two canonical technique have complementary property with respect to the level of temporal coherency maintained communication overhead state space overhead and loss of coherency due to server failure in this paper we show how to combine push and pull based technique to achieve the best feature of both approach our combined technique tailor the dissemination of data from server to client based on the capability and load at server and proxy and client coherency requirement our experimental result demonstrate that such adaptive data dissemination is essential to meet diverse temporal coherency requirement to be resilient to failure and for the efficient and scalable utilization of server and network resource 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
shape from shading sfs is a fundamental problem incomputer vision the vast majority of research in this fieldhave assumed orthography a it projection model thispaper re examines the basis of sfs the image irradianceequation under an assumption of perspective projection the paper also show that the perspective image irradianceequation depends merely on the natural logarithm of thedepth function and not on the depth function itself and assuch it is invariant to scale change of the depth function we then suggest a simple reconstruction algorithm basedon the perspective formula and compare it to existing orthographicsfs algorithm this simple algorithm obtainedlower error rate than legacy sfs algorithm and equatedwith and sometimes surpassed state of the art algorithm these finding lend support to the assumption that transitionto a more realistic set of assumption improves reconstructionsignificantly 
certain simple image are known to trigger a percept of transparency the input image i is perceived a the sum of two image i x y i x y i x y this percept is puzzling first why dowechoosethe morecomplicated descriptionwithtwoimages ratherthanthe simpler explanation i x y i x y second giventheinflnitenumberofwaystoexpress i asasumoftwo image how do we compute the best decomposition herewesuggestthattransparencyistherationalperceptofasystemthatisadaptedtothestatisticsofnaturalscenes wepresent a probabilistic model of image based on the qualitative statistic of derivative fllters and corner detector in natural scene and usethismodeltoflndthemostprobabledecompositionofanovel image the optimization is performed using loopy belief propagation we show that our model computes perceptually correct decomposition on synthetic image and discus it application to real image 
classifier learning method commonly assume that the training data consist of randomly drawn example from the same distribution a the test example about which the learned model is expected to make prediction in many practical situation however this assumption is violated in a problem known in econometrics a sample selection bias in this paper we formalize the sample selection bias problem in machine learning term and study analytically and experimentally how a number of well known classifier learning method are affected by it we also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias 
a problem facing many textbook author including one of the author of this paper is the inevitable delay between new advance in the subject area and their incorporation in a new paper edition of the textbook this mean that some textbook are quickly considered out of date particularly in active technological area such a the web even though the idea presented in the textbook are still valid and important to the community this paper describes our approach to building a companion website for the textbook hypermedia and the web an engineering approach we use bloom s taxonomy of educational objective to critically evaluate a number of authoring and presentation technique used in existing companion website and adapt these technique to create our own companion website using semantic web technology in order to overcome the identified weakness finally we discus a potential model of future companion website in the context of an e publishing e commerce semantic web service scenario 
this research summary describes some work in progress on using graphical model to represent relational data in computational science portal such a mygrid the objective is to provide a integrative collaborative filtering cf capability to user of data metadata source code and experimental documentation in some domain of interest recent system such a researchindex citeseer provide collaborative recommendation through citation indexing and system such a sourceforge and the open bioinformatics project provide similar tool such a content based indexing of software our current research aim at learning probabilistic relational model prms from data in order to support intellignet retrieval of data source code and experimental record we present a system design and a pr ci of a test bed under development that applies prm structure learning and inference to cf in repository of bioinformatic s data and software 
we demonstrate a novel approach to modelling arbitrary temporally deforming object using spatio temporal fourier descriptor this is a continuous boundary descriptor which can handle shape that vary in a periodic manner such a a walking subject a such we can handle non rigid moving shape that self occlude we show how this approach ha led to successful shape extraction and description with both laboratory sourced and real world data a consequence of exploiting temporal shape correlation in this approach ha led to very good tolerance of noise and other positive performance factor further to this our new approach hold sufficient descriptive power not only for extraction but also for description purpose and we have been pleased to note high recognition rate in human gait recognition on a large database 
statistical language model estimate the probability of a word occurring in a given context the most common language model rely on a discrete enumeration of predictive context e g n gram and consequently fail to capture and exploit statistical regularity across these context in this paper we show how to learn hierarchical distributed representation of word context that maximize the predictive value of a statistical language model the representation are initialized by unsupervised algorithm for linear and nonlinear dimensionality reduction then fed a input into a hierarchical mixture of expert where each expert is a multinomial distribution over predicted word while the distributed representation in our model are inspired by the neural probabilistic language model of bengio et al our particular architecture enables u to work with significantly larger vocabulary and training corpus for example on a large scale bigram modeling task involving a sixty thousand word vocabulary and a training corpus of three million sentence we demonstrate consistent improvement over class based bigram model we also discus extension of our approach to longer multiword context 
the standard model of supervised learning assumes that training and test data are drawn from the same underlying distribution this paper explores an application in which a second auxiliary source of data is available drawn from a different distribution this auxiliary data is more plentiful but of significantly lower quality than the training and test data in the svm framework a training example ha two role a a a data point to constrain the learning process and b a a candidate support vector that can form part of the definition of the classifier the paper considers using the auxiliary data in either or both of these role this auxiliary data framework is applied to a problem of classifying image of leaf of maple and oak tree using a kernel derived from the shape of the leaf experiment show that when the training data set is very small training with auxiliary data can produce large improvement in accuracy even when the auxiliary data is significantly different from the training and test data the paper also introduces technique for adjusting the kernel score of the auxiliary data point to make them more comparable to the training data point 
previous work have demonstrated that the face recognitionperformance can be improved significantly in low dimensional linearsubspaces conventionally principal component analysis pca andlinear discriminant analysis lda are considered effective inderiving such a face subspace however both of them effectivelysee only the euclidean structure of face space in this paper wepropose a new approach to mapping face image into a sub spaceobtained by locality preserving projection lpp for faceanalysis we call this laplacian face approach different from pcaand lda lpp find an embedding that preserve local information and obtains a face space that best detects the essential manifoldstructure in this way the unwanted variation resulting fromchanges in lighting facial expression and pose may be eliminatedor reduced we compare the proposed laplacian face approach witheigenface and fisherface method on three test datasets experimental result show that the proposed laplacianface approachprovides a better representation and achieves lower error rate inface recognition 
most statistical parser have used the grammar induction approach in which a stochastic grammar is induced from a treebank an alternative approach is to induce a controller for a given parsing automaton such controller may be stochastic here we focus on greedy controller which result in deterministic parser we use decision tree to learn the controller the resulting parser are surprisingly accurate and robust considering their speed and simplicity they are almost a fast a current part ofspeech tagger and considerably more accurate than a basic unlexicalized pcfg parser we also describe markov parsing model a general framework for parser modeling and control of which the parser reported here are a special case 
we present two extension to the space carving framework the first is a progressive scheme to better reconstruct surface lacking sufficient texture the second is a novel photo consistency measure that is valid for both specular and diffuse surface under unknown lighting condition 
content extraction signature ce enable selective disclosure of verifiable content provide privacy for blinded content and enable the signer to specify the content the document owner is allowed to extract or blind combined these property give what we call ce functionality in this paper we describe our work in developing custom transform algorithm to expand the functionality of an xml signature to include ce functionality in xml signature core validation we also describe a custom revocation mechanism and our implementation for non xml content where the custom transforms are dynamically loaded demonstrating that custom signing and verification is not constrained to a closed system through the use of dynamic loading we show that a verifier can still verify an xml signature compliant signature even though a custom signature wa produced 
the semantic web envisions a world wide web in which data is described with rich semantics and application can pose complex query to this point researcher have defined new language for specifying meaning for concept and developed technique for reasoning about them using rdf a the data model to flourish the semantic web need to be able to accommodate the huge amount of existing data and the application operating on them to achieve this we are faced with two problem first most of the world s data is available not in rdf but in xml xml and the application consuming it rely not only on the domain structure of the data but also on it document structure hence to provide interoperability between such source we must map between both their domain structure and their document structure second data management practitioner often prefer to exchange data through local point to point data translation rather than mapping to common mediated schema or ontology this paper describes the piazza system which address these challenge piazza offer a language for mediating between data source on the semantic web which map both the domain structure and document structure piazza also enables interoperation of xml data with rdf data that is accompanied by rich owl ontology mapping in piazza are provided at a local scale between small set of node and our query answering algorithm is able to chain set mapping together to obtain relevant data from across the piazza network we also describe an implemented scenario in piazza and the lesson we learned from it 
support vector machine and other kernel method have proven to be very effective for nonlinear inference practical issue are how to select the type of kernel including any parameter and how to deal with the computational issue caused by the fact that the kernel matrix grows quadratically with the data inspired by ensemble and boosting method like mart we propose the multiple additive regression kernel mark algorithm to address these issue mark considers a large potentially infinite library of kernel matrix formed by different kernel function and parameter using gradient boosting column generation mark construct column of the heterogeneous kernel matrix the base hypothesis on the fly and then add them into the kernel ensemble regularization method such a used in svm kernel ridge regression and mart are used to prevent overfitting we investigate how mark is applied to heterogeneous kernel ridge regression the resulting algorithm is simple to implement and efficient kernel parameter selection is handled within mark sampling and weak kernel are used to further enhance the computational efficiency of the resulting additive algorithm the user can incorporate and potentially extract domain knowledge by restricting the kernel library to interpretable kernel mark compare very favorably with svm and kernel ridge regression on several benchmark datasets 
this paper proposes a novel method to compile statistical model for machine translation to achieve efficient decoding in our method each statistical submodel is represented by a weighted finite s tate transducer wfst and all of the submodels are expanded into a composition model beforehand furthermore the ambiguity of the composition model is reduced by the statistic of hypothesis while decoding the experimental result show that the proposed model representation drastically improves the efficienc y of decoding compared to the dynamic composition of the submodels which corresponds to conventional approach 
information retrieval using word sens is emerging a a good research challenge on semantic information retrieval in this paper we propose a new method using word sens in information retrieval root sense tagging method this method assigns coarse grained word sens defined in wordnet to query term and document term by unsupervised way using co occurrence information constructed automatically our sense tagger is crude but performs consistent disambiguation by considering only the single most informative word a evidence to disambiguate the target word we also allow multiple sense assignment to alleviate the problem caused by incorrect disambiguation experimental result on a large scale trec collection show that our approach to improve retrieval effectiveness is successful while most of the previous work failed to improve performance even on small text collection our method also show promising result when is combined with pseudo relevance feedback and state of the art retrieval function such a bm 
we address the problem of integrating object from a source taxonomy into a master taxonomy this problem is not only currently pervasive on the web but also important to the emerging semantic web a straightforward approach to automating this process would be to train a classifier for each category in the master taxonomy and then classify object from the source taxonomy into these category in this paper we attempt to use a powerful classification method support vector machine svm to attack this problem our key insight is that the availability of the source taxonomy data could be helpful to build better classifier in this scenario therefore it would be beneficial to do transductive learning rather than inductive learning i e learning to optimize classification performance on a particular set of test example noticing that the categorization of the master and source taxonomy often have some semantic overlap we propose a method cluster shrinkage c to further enhance the classification by exploiting such implicit knowledge our experiment with real world web data show substantial improvement in the performance of taxonomy integration 
real scene are full of specularities highlight and reflection and yet most vision algorithm ignore them in order to capture the appearance of realistic scene we need to model specularities a separate layer in this paper we study the behavior of specularities in static scene a the camera move and describe their dependence on varying surface geometry orientation and scene point and camera location for a rectilinear camera motion with constant velocity we study how the specular motion deviate from a straight trajectory disparity deviation and how much it violates the epipolar constraint epipolar deviation surprisingly for surface that are convex or not highly undulating these deviation are usually quite small we also study the appearance of specularities i e how they interact with the body reflection and with the usual occlusion ordering constraint applicable to diffuse opaque layer we present a taxonomy of specularities based on their photometric property a a guide for designing separation technique finally we propose a technique to extract specularities a a separate layer and demonstrate it using an image sequence of a complex scene 
a representational gap exists between low level measurement segmentation object classification tracking and high level understanding of video sequence in this paper we propose a novel representation of event in video to bridge this gap based on the case representation of natural language the proposed representation ha three significant contribution over existing framework first we recognize the importance of causal and temporal relationship between subevents and extend case to allow the representation of temporal structure and causality between sub event second in order to capture both multi agent and multithreaded event we introduce a hierarchical case representation of event in term of sub event and case list last for purpose of implementation we present the concept of a temporal event tree and pose the problem of event detection a subtree pattern matching by extending case a natural language representation for the representation of event the proposed work allows a plausible mean of interface between user and the computer we show two important application of the proposed event representation for the automated annotation of standard meeting video sequence and for event detection in extended video of railroad crossing 
we present result for automated text categorization of the reuters collection of news story our experiment use the entire one year collection of story and the entire subject index we divide the data into monthly group and provide an initial benchmark of text categorization performance on the complete collection experimental result show that efficient sparse feature implementation of linear method and decision tree using a global unstemmed dictionary can readily handle application of this size predictive performance is approximately a strong a the best result for the much smaller older reuters collection detailed result are provided over time period it is shown that a smaller time horizon doe not diminish predictive quality implying reduced demand for retraining when sample size is large 
we describe method for taking into account unlabeled data in the training of a kernel based classifier such a a support vector machine svm we propose two approach utilizing unlabeled point in the vicinity of labeled one both of the approach effectively modify the metric of the pattern space either by using nonspherical gaussian density estimate which are determined using em or by modifying the kernel function using displacement vector computed from pair of unlabeled and labeled point the latter is linked to technique for training invariant svms we present experimental result indicating that the proposed technique can lead to substantial improvement of classification accuracy 
while clustering is usually an unsupervised operation there are circumstance in which we believe with varying degree of certainty that item a and b should be assigned to the same cluster while item a and c should not we would like such pairwise relation to influence cluster assignment of out of sample data in a manner consistent with the prior knowledge expressed in the training set our starting point is probabilistic clustering based on gaussian mixture model gmm of the data distribution we express clustering preference in the prior distribution over assignment of data point to cluster this prior penalizes cluster assignment according to the degree with which they violate the preference we fit the model parameter with em experiment on a variety of data set show that ppc can consistently improve clustering result 
this paper investigates a new approach for training discriminant classifier when only a small set of labeled data is available together with a large set of unlabeled data this algorithm optimizes the classification maximum likelihood of a set of labeled unlabeled data using a variant form of the classification expectation maximization cem algorithm it originality is that it make use of both unlabeled data and of a probabilistic misclassification model for these data the parameter of the label error model are learned together with the classifier parameter we demonstrate the effectiveness of the approach on four data set and show the advantage of this method over a previously developed semi supervised algorithm which doe not consider imperfection in the labeling process 
vision task such a segmentation grouping recognition can beformulated a graph partition problem the recent literaturewitnessed two popular graph cut algorithm the ncut using spectralgraph analysis and the minimum cut using the maximum flowalgorithm this paper present a third major approach bygeneralizing the swendsen wang methoda well celebrated algorithmin statistical mechanic our algorithm simulates ergodic reversible markov chain jump in the space of graph partition tosample a posterior probability at each step the algorithm split merges or re group a sizable subgraph and achieves fast mixingat low temperature enabling a fast annealing procedure experimentsshow it converges in second in a pc for image segmentation this is time faster than the single site update gibbs sampler and time faster than the ddmcmc algorithm the algorithm canoptimize over the number of model and work for general form ofposterior probability so it is more general than the existinggraph cut approach 
this research explores the relationship between information seeking strategy i and information retrieval ir system design when people seek information they engage in a variety of i in order to search for specific item learn about the content of the database evaluate retrieved information and so on the theoretical foundation of the work are based on the information seeking episode model developed by belkin and the multi facet classification scheme of information behavior proposed by cool belkin the goal of this research is to construct and evaluate an interactive retrieval system which us different combination of ir technique to support different i example ir technique include comparison using exact and probabilistic matching algorithm summarization of information object using title snippet or abstract visualization technique such a list or classified result and navigation technique such a scrolling or following link by designing a retrieval system with diverse strategy in mind we can adaptively support multiple i permitting a user to move seamlessly from one strategy to another choosing instantiation of each support technique tailored to the specific i the research will be conducted in a series of four step develop an object oriented framework for representing basic ir technique design implement and evaluate system which support individual i such a browsing and searching specify an interaction structure for guiding and controlling sequence of different supporting technique design implement and evaluate a dynamically adaptive system supporting multiple i in comparison to a non adaptive baseline system 
a the artificial intelligence ai system in military simulation and computer game become more complex their action become increasingly difficult for user to understand expert system for medical diagnosis have addressed this challenge though the addition of explanation generation system that explain a system s internal process this paper describes the ai architecture and associated explanation capability used by full spectrum command a training system developed for the u s army by commercial game developer and academic researcher 
this paper introduces the concept of eigen dynamic andproposes an eigen dynamic analysis eda method to learnthe dynamic of natural hand motion from labelled set ofmotion captured with a data glove the result is parameterizedwith a high order stochastic linear dynamic system lds consisting of five lower order lds each correspondingto one eigen dynamic based on the eda model weconstruct a dynamic bayesian network dbn to analyzethe generative process of a image sequence of natural handmotion using the dbn a hand tracking system is implemented experiment on both synthesized and real worlddata demonstrate the robustness and effectiveness of thesetechniques 
the aim of this paper is to compare bayesian network classifier to the k nn classifier based on a subset of feature this subset is established by mean of sequential feature selection method experimental result show that bayesian network classifier more often achieve a better classification rate on different data set than selective k nn classifier the k nn classifier performs well in the case where the number of sample for learning the parameter of the bayesian network is small bayesian network classifier outperform selective knn method in term of memory requirement and computational demand this paper demonstrates the strength of bayesian network for classification 
log of user query to an internet search engine p rovide a large amount of implicit and explicit inform ation about language in this paper we investigate their use in spelling correction of search query a task which pose many additional challenge beyond the traditional spelling correction problem we pre sent an approach that us an iterative transformat ion of the input query string into other string that correspond to more and more likely query according to statistic extracted from internet search query log s 
uncertainty handling play an important role during shape tracking we have recently shown that the fusion of measurement information with system dynamic and shape prior greatly improves the tracking performance for very noisy image such a ultrasound sequence nevertheless this approach required user initialization of the tracking process this paper solves the automatic initialization problem by performing boosted shape detection a a generic measurement process and integrating it in our tracking framework we show how to propagate the local detection uncertainty of multiple shape candidate during shape alignment fusion with the predicted shape prior and fusion with subspace constraint a a result we treat all source of information in a unified way and derive the posterior shape model a the shape with the maximum likelihood our framework is applied for the automatic tracking of endocardium in ultrasound sequence of the human heart reliable detection and robust tracking result are achieved when compared to existing approach and interexpert variation 
this paper address the problem of computing visual hull from image contour we propose a new hybrid approach which overcomes the precision complexity trade off inherent to voxel based approach by taking advantage of surface based approach to this aim we introduce a space discretization which doe not rely on a regular grid where most cell are ineffective but rather on an irregular grid where sample point lie on the surface of the visual hull such a grid is composed of tetrahedral cell obtained by applying a delaunay triangulation on the sample point these cell are carved afterward according to image silhouette information the proposed approach keep the robustness of volumetric approach while drastically improving their precision and reducing their time and space complexity it thus allows modeling of object with complex geometry and it also make real time feasible for precise model preliminary result with synthetic and real data are presented 
decision tree construction is a well studied problem in data mining recently there ha been much interest in mining streaming data domingo and hulten have presented a one pas algorithm for decision tree construction their work us hoeffding inequality to achieve a probabilistic bound on the accuracy of the tree constructed in this paper we revisit this problem we make the following two contribution we present a numerical interval pruning nip approach for efficiently processing numerical attribute our result show an average of reduction in execution time we exploit the property of the gain function entropy and gini to reduce the sample size required for obtaining a given bound on the accuracy our experimental result show a reduction in the number of data instance required 
data mining with bayesian network learning ha two important characteristic under condition learned edge between variable correspond to casual influence and second for every variable t in the network a special subset markov blanket identifiable by the network is the minimal variable set required to predict t however all known algorithm learning a complete bn do not scale up beyond a few hundred variable on the other hand all known sound algorithm learning a local region of the network require an exponential number of training instance to the size of the learned region the contribution of this paper is two fold we introduce a novel local algorithm that return all variable with direct edge to and from a target variable t a well a a local algorithm that return the markov blanket of t both algorithm i are sound ii can be run efficiently in datasets with thousand of variable and iii significantly outperform in term of approximating the true neighborhood previous state of the art algorithm using only a fraction of the training size required by the existing method a fundamental difference between our approach and existing one is that the required sample depends on the generating graph connectivity and not the size of the local region this yield up to exponential saving in sample relative to previously known algorithm the result presented here are promising not only for discovery of local causal structure and variable selection for classification but also for the induction of complete bns 
a computationally enhanced message contains some embedded programmatic component that are interpreted and executed automatically upon receipt unlike ordinary text email or instant message they make possible a number of useful application in this paper we describe a general and flexible messaging system called shock that extends the functionality of prior computational email system by allowing xml encoded shock message to interact with an automatically created profile of a user these profile consist of information about the most common task user perform such a their web browsing behavior their conventional email usage etc since user are sensitive about such data the system is designed with privacy a a central design goal and employ a distributed peer to peer architecture to achieve it the system is largely implemented with commodity web technology and provides both a web interface a well a one that is tightly integrated with user ordinary email client with shock user can send highly targeted message without violating others privacy and engage in structured conversation appropriate to the context without disrupting their existing work practice we describe our implementation in detail the most useful novel application of the system and our experience with the system in a pilot field test 
this paper present a new linear method for reconstructing simultaneously d feature point line and plane and camera from many perspective view by solving a single linear system it assumes that a real or virtual reference plane is visible in all view we call it the direct reference plane drp method it is well knownthatthe projectionrelationship between uncalibrated camera and d feature is non linear in the absence of a reference plane with a known reference plane point and camera have a linear relationship a shown in the main contribution of this paper is that line and camera a well a plane and camera also have a linear relationship consequently all d feature and all camera can be reconstructed simultaneously from a single linear system which handle missing image measurement naturally a further contribution is an extensive experimental comparison using real data of different reference plane and non reference plane reconstruction method for difficult reference plane scenario with point or line feature the drp method is superior to all compared method finally an extensive list of reference plane scenario is presented which show the wide applicability of the drp method 
in this paper we propose a new information theoretic divisive algorithm for word clustering applied to text classification in previous work such distributional clustering of feature ha been found to achieve improvement over feature selection in term of classification accuracy especially at lower number of feature however the existing clustering technique are agglomerative in nature and result in i sub optimal word cluster and ii high computational cost in order to explicitly capture the optimality of word cluster in an information theoretic framework we first derive a global criterion for feature clustering we then present a fast divisive algorithm that monotonically decrease this objective function value thus converging to a local minimum we show that our algorithm minimizes the within cluster jensen shannon divergence while simultaneously maximizing the between cluster jensen shannon divergence in comparison to the previously proposed agglomerative strategy our divisive algorithm achieves higher classification accuracy especially at lower number of feature we further show that feature clustering is an effective technique for building smaller class model in hierarchical classification we present detailed experimental result using naive bayes and support vector machine on the newsgroups data set and a level hierarchy of html document collected from dmoz open directory 
for cross language information retrieval clir based on bilingual translation dictionary good performance depends upon lexical coverage in the dictionary this is especially true for language possessing few inter language cognate such a between japanese and english in this paper we describe a method for automatically creating and validating candidate japanese transliterated term of english word a phonetic english dictionary and a set of probabilistic mapping rule are used for automatically generating transliteration candidate a monolingual japanese corpus is then used for automatically validating the transliterated term we evaluate the usage of the extracted english japanese transliteration pair with japanese to english retrieval experiment over the clef bilingual test collection the use of our automatically derived extension to a bilingual translation dictionary improves average precision both before and after pseudo relevance feedback with gain ranging from to 
this paper explores the use of bayesian online classifier to classify text document empirical result indicate that these classifier are comparable with the best text classification system furthermore the online approach offer the advantage of continuous learning in the batch adaptive text filtering task 
we study the problem of modeling specie geographic distribution a critical problem in conservation biology we propose the use of maximum entropy technique for this problem specifically sequential update algorithm that can handle a very large number of feature we describe experiment comparing maxent with a standard distribution modeling tool called garp on a dataset containing observation data for north american breeding bird we also study how well maxent performs a a function of the number of training example and training time analyze the use of regularization to avoid overfitting when the number of example is small and explore the interpretability of model constructed using maxent 
neural network are successful in acquiring hidden knowledge in datasets their biggest weakness is that the knowledge they acquire is represented in a form not understandable to human researcher tried to address this problem by extracting rule from trained neural network most of the proposed rule extraction method required specialized type of neural network some required binary input and some were computationally expensive craven proposed extracting mofn type decision tree from neural network we believe mofn type decision tree are only good for mofn type problem and tree created for regular high dimensional real world problem may be very complex in this paper we introduced a new method for extracting regular c like decision tree from trained neural network we showed that the new method dectext is effective in extracting high fidelity tree from trained network we also introduced a new discretization technique to make dectext be able to handle continuous feature and a new pruning technique for finding simplest tree with the highest fidelity 
this poster show an artificial neural network capable of learning a temporal sequence directly inspired from a hippocampus model banquet et al this architecture allows an autonomous robot to learn how to imitate a sequence of movement with the correct timing 
this paper address the problem of finding a small and coherent subset of point in a given data this problem sometimes referred to a one class or set covering requires to find a small radius ball that cover a many data point a possible it rise naturally in a wide range of application from finding gene module to extracting document topic where many data point are irrelevant to the task at hand or in application where only positive example are available most previous approach to this problem focus on identifying and discarding a possible set of outlier in this paper we adopt an opposite approach which directly aim to find a small set of coherently structured region by using a loss function that focus on local property of the data we formalize the learning task a an optimization problem using the information bottleneck principle an algorithm to solve this optimization problem is then derived and analyzed experiment on gene expression data and a text document corpus demonstrate the merit of our approach 
we describe a sparse bayesian regression method for recovering d human body motion directly from silhouette extracted from monocular video sequence no detailed body shape model is needed and realism is ensured by training on real human motion capture data the tracker estimate d body pose by using relevance vector machine regression to combine a learned autoregressive dynamical model with robust shape descriptor extracted automatically from image silhouette we studied several different combination method the most effective being to learn a nonlinear observation update correction based on joint regression with respect to the predicted state and the observation we demonstrate the method on a parameter full body pose model both quantitatively using motion capture based test sequence and qualitatively on a test video sequence 
surrogate maximization or minimization sm algorithm are a family of algorithm that can be regarded a a generalization of expectation maximization em algorithm there are three major approach to the construction of surrogate function all relying on the convexity of some function in this paper we solve the boosting problem by proposing sm algorithm for the corresponding optimization problem specifically for adaboost we derive an sm algorithm that can be shown to be identical to the algorithm proposed by collins et al based on bregman distance more importantly for logitboost or logistic boosting we use several method to construct different surrogate function which result in different sm algorithm by combining multiple method we are able to derive an sm algorithm that is also the same a an algorithm derived by collins et al our approach based on sm algorithm is much simpler and convergence result follow naturally 
in this paper we propose the framework of monte carlo algorithm a a useful one to analyze ensemble learning in particular this framework allows one to guess when bagging will be useful explains why increasing the margin improves performance and suggests a new way of performing ensemble learning and error estimation 
texture segmentation is a difficult problem a is apparentfrom camouflage picture a textured region can containtexture element of various size each of which can itselfbe textured we approach this problem using a bottom upaggregation framework that combine structural characteristicsof texture element with filter response our processadaptively identifies the shape of texture element and characterizethem by their size aspect ratio orientation brightness etc and then us various statistic of these propertiesto distinguish between different texture at the sametime our process us the statistic of filter response tocharacterize texture in our process the shape measuresand the filter response crosstalk extensively in addition a top down cleaning process is applied to avoid mixing thestatistics of neighboring segment we tested our algorithmon real image and demonstrate that it can accurately segmentregions that contain challenging texture 
we develop a fast and accurate variable window approach the two main idea for achieving accuracy are choosing a useful range of window size shape for evaluation and developing a new window cost which is particularly suitable for comparing window of different size the speed of our approach is due to the integral image technique which allows computation of our window cost over any rectangular window in constant time regardless of window size our method rank in the top four on the middlebury stereo database with ground truth and performs best out of method which have comparable efficiency 
a novel hardware assisted top doc hat component is disclosed hat is an optimized content indexing device based on a modified inverted index structure hat accommodates pattern of different length and support a varied posting list versus term count feature sustaining high reusability and efficiency the developed component can be used either a an internal slave component or a an external co processor and is efficient in resource demand a the component controller take only a minimal percentage of the target device space leaving the majority of the space to term and posting entry a very high speed integrated circuit vhsic hardware description language vhdl is used to model the hat system 
utility elicitation is a critical function of any automated decision aid allowing decision to be tailored to the preference of a specific user however the size and complexity of utility function often precludes full elicitation requiring that decision be made without full utility information adopting the minimax regret criterion for decision making with incomplete utility information we describe and empirically compare several new procedure for incremental elicitation of utility function that attempt to reduce minimax regret with a few question a possible specifically using the continuous space of standard gamble query we show that myopically optimal query can be computed effectively in polynomial time for several different improvement criterion one such criterion in particular empirically outperforms the others we examine considerably and ha provable improvement guarantee 
in this paper we propose a novel method to recover thesurface shape of transparent object the degree of polarizationof the light reflected from the object surface dependson the reflection angle which in turn depends on the object ssurface normal thus by measuring the degree of polarization we are able to calculate the surface normal of theobject however degree of polarization and surface normaldoes not correspond one to one making u to analyze twopolarization image taken from two different view in orderto solve the ambiguity a parabolic curve will be a strongclue to correspond a point in one image to a point in theother image where both point represent the same point onobject surface by comparing the degree of polarization atsuch corresponding point the true surface normal can bedetermined 
this paper present a framework to reconstruct a scenecaptured in multiple camera view based on a prior modelof the scene geometry the framework is applied to thecapture of animated model of people a multiple camerastudio is used to simultaneously capture a moving personfrom multiple viewpoint a humanoid computer graphicsmodel is animated to match the pose at each time frame constrained optimisation is then used to recover the multipleview correspondence from silhouette stereo and featurecues updating the geometry and appearance of the model the key contribution of this paper is a model based computervision framework for the reconstruction of shape andappearance from multiple view this is compared to currentmodel free approach for multiple view scene capture the technique demonstrates improved scene reconstructionin the presence of visual ambiguity and providesthe mean to capture a dynamic scene with a consistentmodel that is instrumented with an animation structure toedit the scene dynamic or to synthesise new content 
for sinequa s second participation to the senseval evaluation two system using contextual semantic have been proposed based on different approach they both share the same data preprocessing and enrichment the first system is a combined approach using semantic classification tree and information retrieval technique for the second system the word from the context are considered a clue the final sense is determined by summing the weight assigned to each clue for a given example 
we propose a formal model of cross language information retrieval that doe not rely on either query translation or document translation our approach leverage recent advance in language modeling to directly estimate an accurate topic model in the target language starting with a query in the source language the model integrates popular technique of disambiguation and query expansion in a unified formal framework we describe how the topic model can be estimated with either a parallel corpus or a dictionary we test the framework by constructing chinese topic model from english query and using them in the clir task of trec the model achieves performance around of the strong mono lingual baseline in term of average precision in initial precision our model outperforms the mono lingual baseline by the main contribution of this work is the unified formal model which integrates technique that are essential for effective cross language retrieval 
the class transduction problem a formulated by vapnik involves finding a separating hyperplane for a labelled data set that is also maximally distant from a given set of unlabelled test point in this form the problem ha exponential computational complexity in the size of the working set so far it ha been attacked by mean of integer programming technique that do not scale to reasonable problem size or by local search procedure in this paper we present a relaxation of this task based on semidefinite programming sdp resulting in a convex optimization problem that ha polynomial complexity in the size of the data set the result are very encouraging for mid sized data set however the cost is still too high for large scale problem due to the high dimensional search space to this end we restrict the feasible region by introducing an approximation based on solving an eigenproblem with this approximation the computational cost of the algorithm is such that problem with more than point can be treated 
we present the first algorithm that computes optimal ordering of sentence into a locally coherent discourse the algorithm run very efficiently on a variety of coherence measure from the literature we also show that the discourse ordering problem is np complete and cannot be approximated 
this paper delineates the computational complexity of propositional multi context system we establish np membership by translating multi context system into bounded modal kn and obtain more refined complexity result by achieving the so called bounded model property the number of local model needed to satisfy a set of formula in a multi context system m is bounded by the number of context addressed by plus the number of bridge rule in m exploiting this property of multi context system we are able to encode contextual satisfiability into purely propositional satisfiability providing for the implementation of contextual reasoner based on already existing specialized s at solver finally we apply our result to improve complexity bound for mccarthy s propositional logic of context we show that satisfiability in this framework can be settled in nondeterministic polynomial time o 
our paper address the problem of enforcing constraint in human body tracking a projection technique is derived to impose kinematic constraint on independent multibody motion we show that for small motion the multibody articulated motion space can be approximated by a linear manifold estimated directly from the previous body pose we propose a learning approach to model nonlinear constraint we train a support vector classifier from motion capture data to model the boundary of the space of valid pose linear and nonlinear body pose constraint are enforced by first projecting unconstrained motion onto the articulated motion space and then optimizing to find point on this linear manifold that lie within the non linear constraint surface modeled by the svm classifier 
we describe ongoing work on i i a system aimed at fostering opportunistic communication among user viewing or manipulating content on the web and in productivity application unlike previous work in which the url of web resource are used to group user visiting the same resource we present a more general framework for clustering work context to group user together that account for dynamic content and distributional property of web access which can limit the utility url based system in addition we describe a method for scaffolding asynchronous communication in the context of an ongoing task that take into account the ephemeral nature of the location of content on the web the technique we describe also nicely cover local file in progress in addition to publicly available web content we present the result of several evaluation that indicate system that use the technique we employ may be more useful than system that are strictly url based 
recently interest is growing to develop an effective communication interface connecting the human brain to a computer the brain computer interface bci one motivation of bci research is to provide a new communication channel substituting normal motor output in patient with severe neuromuscular disability in the last decade various neurophysiological cortical process such a slow potential shift movement related potential mrps or event related desynchronization erd of spontaneous eeg rhythm were shown to be suitable for bci and consequently different independent approach of extracting bci relevant eeg feature for single trial analysis are under investigation here we present and systematically compare several concept for combining such eeg feature to improve the single trial classification feature combination are evaluated on movement imagination experiment with subject where eeg feature are based on either mrps or erd or both those combination method that incorporate the assumption that the single eeg feature are physiologically mutually independent outperform the plain method of adding evidence where the single feature vector are simply concatenated these result strengthen the hypothesis that mrp and erd reflect at least partially independent aspect of cortical process and open a new perspective to boost bci effectiveness 
recently active behavior ha received attention in the xml field to automatically react to occurred event aside from proprietary approach for enriching xml with active behavior the w c standardized the document object model dom event module for the detection of event in xml document when using any of these approach however it is often impossible to decide which event to react upon because not a single event but a combination of multiple event i e a composite event determines a situation to react upon the paper present the first approach for detecting composite event in xml document by addressing the peculiarity of xml event which are caused by their hierarchical order in addition to their temporal order it also provides for the detection of satisfied multiplicity constraint defined by xml schema thereby the approach enables application operating on xml document to react to composite event which have richer semantics 
time difference of arrival tdoa is commonly used to estimate the azimuth of a source in a microphone array the most common method to estimate tdoa are based on finding extremum in generalized crosscorrelation waveform in this paper we apply microphone array technique to a manikin head by considering the entire cross correlation waveform we achieve azimuth prediction accuracy that exceeds extremum locating method we do so by quantizing the azimuthal angle and treating the prediction problem a a multiclass categorization task we demonstrate the merit of our approach by evaluating the various approach on sony s aibo robot 
the volume of mass unsolicited electronic mail often known a spam ha recently increased enormously and ha become a serious threat to not only the internet but also to society this paper proposes a new spam detection method which us document space density information although it requires extensive e mail traffic to acquire the necessary information an unsupervised learning engine with a short white list can achieve a recall rate and precision a direct mapped cache method contributes handling of over e mail per second experimental result which were conducted using over million actual e mail of traffic are also reported in this paper 
document clustering is useful in many information retrieval task document browsing organization and viewing of retrieval result generation of yahoo like hierarchy of document etc the general goal of clustering is to group data element such that the intra group similarity are high and the inter group similarity are low we present a clustering algorithm called cbc clustering by committee that is shown to produce higher quality cluster in document clustering task a compared to several well known clustering algorithm it initially discovers a set of tight cluster high intra group similarity called committee that are well scattered in the similarity space low inter group similarity the union of the committee is but a subset of all element the algorithm proceeds by assigning element to their most similar committee evaluating cluster quality ha always been a difficult task we present a new evaluation methodology that is based on the editing distance between output cluster and manually constructed class the answer key this evaluation measure is more intuitive and easier to interpret than previous evaluation measure 
we present and empirically test a novel approach for categorizing d free form object shape represented by range data in contrast to traditional surface signature based system that use alignment to match specific object we adapted the newly introduced symbolicsignature representation to classify deformable shape our approach construct an abstract description of shape class using an ensemble of classifier that learn object class part and their corresponding geometrical relationship from a set of numeric and symbolic descriptor we used our classification engine in a series of discrimination experiment on two well defined class that share many common distinctive feature the experimental result suggest that our method is to the best of our knowledge the first capable of classifying shape class that are difficult to discriminate by human standard 
this paper explores the problem of how to construct lazy decision tree ensemble we present and empirically evaluate a relevancebased boosting style algorithm that build a lazy decision tree ensemble customized for each test instance from the experimental result we conclude that our boosting style algorithm signicantly improves the performance of the base learner an empirical comparison to boosted regular decision tree show that ensemble of lazy decision tree achieve comparable accuracy and better comprehensibility we also introduce a novel distance based pruning strategy for the lazy decision tree algorithm to address the problem of over tting our experiment show that the pruning strategy improves the accuracy and comprehensibility of both single lazy decision tree and boosted ensemble 
what determines the caliber of axonal branch we pursue the hypothesis that the axonal caliber ha evolved to m inimize signal propagation delay while keeping arbor volume to a minimum we show that for a general cost function the optimal d iameters of mother d and daughter d d branch at a bifurcation obey 
link analysis ha shown great potential in improving the performance of web search pagerank and hit are two of the most popular algorithm most of the existing link analysis algorithm treat a web page a a single node in the web graph however in most case a web page contains multiple semantics and hence the web page might not be considered a the atomic node in this paper the web page is partitioned into block using the vision based page segmentation algorithm by extracting the page to block block to page relationship from link structure and page layout analysis we can construct a semantic graph over the www such that each node exactly represents a single semantic topic this graph can better describe the semantic structure of the web based on block level link analysis we proposed two new algorithm block level pagerank and block level hit whose performance we study extensively using web data 
successful application of reinforcement learning algorithm often involves considerable hand crafting of the necessary non linear feature to reduce the complexity of the value function and hence to promote convergence of the algorithm in contrast the human brain readily and autonomously find the complex feature when provided with sufficient training recent work in machine learning and neurophysiology ha demonstrated the role of the basal ganglion and the frontal cortex in mammalian reinforcement learning this paper develops and explores new reinforcement learning algorithm inspired by neurological evidence that provides potential new approach to the feature construction problem the algorithm are compared and evaluated on the acrobot task 
the extraction of a single high quality image from a set of low resolution image is an important problem which arises in flelds such a remote sensing surveillance medical imaging and the extraction of still image from video typical approach are based on the use of cross correlation to register the image followed by the inversion of the transformation from the unknown high resolution image to the observed low resolution image using regularization to resolve the ill posed nature of the inversion process in this paper we develop a bayesian treatment of the super resolution problem in which the likelihood function for the image registration parameter is based on a marginalization over the unknown highresolution image this approach allows u to estimate the unknown point spread function and is rendered tractable through the introduction of a gaussian process prior over image result indicate a signiflcant improvement over technique based on map maximum a posteriori point optimization of the high resolution image and associated registration parameter 
we describe a new approximation algorithm for solving partially observable mdps our bounded policy iterationapproach search through the space of bounded size stochastic finite state controller combining several advantage of gradient ascent efficiency search through restricted controller space and policy iteration le vulnerability to local optimum 
we introduce a novel learning algorithm for binary classication with hyperplane discriminants based on pair of training point from opposite class dyadic hypercuts this algorithm is further extended to nonlinear discriminants using kernel function satisfying mercer s condition an ensemble of simple dyadic hypercuts is learned incrementally by mean of a condence rated version of adaboost which provides a sound strategy for searching through the nite set of hypercut hypothesis in experiment with real world datasets from the uci repository the generalization performance of the hypercut classiers wa found to be comparable to that of svms and k nn classiers furthermore the computational cost of classication at run time wa found to be similar to or better than that of svm similarly to svms boosted dyadic kernel discriminants tend to maximize the margin via adaboost in contrast to svms however we oer an on line and incremental learning machine for building kernel discriminants whose complexity number of kernel evaluation can be directly controlled traded o for accuracy 
multiple topic and varying length of web page are two negative factor significantly affecting the performance of web search in this paper we explore the use of page segmentation algorithm to partition web page into block and investigate how to take advantage of block level evidence to improve retrieval performance in the web context because of the special characteristic of web page different page segmentation method will have different impact on web search performance we compare four type of method including fixed length page segmentation dom based page segmentation vision based page segmentation and a combined method which integrates both semantic and fixed length property experiment on block level query expansion and retrieval are performed among the four approach the combined method achieves the best performance for web search our experimental result also show that such a semantic partitioning of web page effectively deal with the problem of multiple drifting topic and mixed length and thus ha great potential to boost up the performance of current web search engine 
this paper reconsiders the notion of actual cause and explanation in functional causal model we demonstrate that isomorphic causal model can generate intuitively different causal pronounce ments this occurs because psychological factor not represented in the model determine what cri teria we use to determine causation this par tially explains the difficulty encountered in previ ous attempt to define actual cause freed from trying fit all example to match intuition directly which is not possible using only the information in causal model we provide definition for cau sation matching the different causal criterion we in tuitively apply this formulation avoids difficulty associated with previous definition and allows a more refined discussion of what constitutes a cause in a given situation the definition of actual cause also allow for more refined formulation of expla nation 
abstract we propose a new framework to study propertiesof consistency in a constraint network from theperspective of property of set intersection ourframework come with a proof schema which givesa generic way of lifting a set intersection propertyto one on consistency various well known resultscan be derived with this framework more importantly we use the framework to obtain a numberof new result we identify a new class of treeconvex constraint where local consistency ensures 
this paper describes an alternative translation model based on a text chunk under the framework of statistical machine translation the translation model suggested here first performs chunking then each word in a chunk is translated finally translated chunk are reordered under this scenario of translation modeling we have experimented on a broad coverage japanese english traveling corpus and achieved improved performance 
color based tracker recently proposed in have been proved robust and versatile for a modest computational cost they are especially appealing for tracking task where the spatial structure of the tracked object exhibit such a dramatic variability that tracker based on a space dependent appearance reference would break down very fast tracker in rely on the deterministic search of a window whose color content match a reference histogram color model relying on the same principle of color histogram distance but within a probabilistic framework we introduce a new monte carlo tracking technique the use of a particle filter allows u to better handle color clutter in the background a well a complete occlusion of the tracked entity over a few frame this probabilistic approach is very flexible and can be extended in a number of useful way in particular we introduce the following ingredient multi part color modeling to capture a rough spatial layout ignored by global histogram incorporation of a background color model when relevant and extension to multiple object 
a corpus based knowledge representation system consists of a large collection of disparate knowledge fragment or schema and a rich set of statistic computed over the corpus we argue that by collecting such a corpus and computing the appropriate statistic corpus based representation offer an alternative to traditional knowledge representation for a broad class of application the key advantage of corpus based representation is that we avoid the laborious process of building a often brittle knowledge base we describe the basic building block of a corpus based representation system and a set of application for which such a paradigm is appropriate including one application where the approach is already showing promising result 
we consider image texture due to the illumination of d surface corrugation on globally smooth curved surface the same surface corrugation give rise to different image texture depending on illumination and viewing geometry we study surface that are on the average approximately lambertian the surface roughness give rise to luminance modulation of the global shading pattern the extreme value of the luminance depend on simple geometrical factor such a whether surface micro facet exist that squarely face the light source or are in shadow we find that a simple microfacet based model suffices to describe texture in natural scene robustly in a semi quantitative manner robust statistical measure of the texture yield the parameter for simple model that allow prediction of the brdf thus texture analysis allows the input parameter for inverse renderingand material recognition to be estimated 
inconsistency frequently occur in knowledge about the real world some of these inconsistency may be more significant than others and some knowledgebases set of formula may contain more inconsistency than others this creates problem of deciding whether to act on these inconsistency and if so how to address this we provide a general characterization of inconsistency based on quasi classical logic a form of paraconsistent logic with a more expressive semantics than belnap s four valued logic and unlike other paraconsistent logic allows the connective to appear to behave a classical connective we analyse inconsistent knowledge by considering the conflict arising in the minimal quasi classical model for that knowledge this is used for a measure of coherence for each knowledgebase and for a measure of significance of inconsistency in each knowledgebase in this paper we formalize this framework and consider application in managing heterogeneous source of knowledge 
collocation translation is important for machine translation and many other nlp task unlike previous method using bilingual parallel corpus this paper present a new method for acquiring collocation translation by making use of monolingual corpus and linguistic knowledge first dependency triple are extracted from chinese and english corpus with dependency parser then a dependency triple translation model is estimated using the em algorithm based on a dependency correspondence assumption the generated triple translation model is used to extract collocation translation from two monolingual corpus experiment show that our approach outperforms the existing monolingual corpus based method in dependency triple translation and achieves promising result in collocation translation extraction 
we present a fast robust and automatic method for computing central path through tubular structure for application to virtual endoscopy the key idea is to utilize a medial surface algorithm which exploit property of the average outward flux of the gradient vector field of a euclidean distance function the boundary of the structure of interest the algorithm is modified to yield a collection of d curve each of which is locally centered the approach requires no user interaction is virtually parameter free and ha low computational complexity we illustrate the approach on segmented colon and vessel data 
large sparse binary matrix arise in numerous data mining application such a the analysis of market basket web graph social network co citation a well a information retrieval collaborative filtering sparse matrix reordering etc virtually all popular method for the analysis of such matrix e g k mean clustering metis graph partitioning svd pca and frequent itemset mining require the user to specify various parameter such a the number of cluster number of principal component number of partition and support choosing suitable value for such parameter is a challenging problem cross association is a joint decomposition of a binary matrix into disjoint row and column group such that the rectangular intersection of group are homogeneous starting from first principle we furnish a clear information theoretic criterion to choose a good cross association a well a it parameter namely the number of row and column group we provide scalable algorithm to approach the optimal our algorithm is parameter free and requires no user intervention in practice it scale linearly with the problem size and is thus applicable to very large matrix finally we present experiment on multiple synthetic and real life datasets where our method give high quality intuitive result 
the problem of super resolution involves generating feasible higher resolution image which are pleasing to the eye and realistic from a given low resolution image this might be attempted by u ing simple lters for smoothing out the high resolution block or through application where substantial prior information is used to imply the texture and shape which will occur in the image in this paper we describe an approach which lie between the two extreme it is a generic unsupervised method which is usable in all domain but go beyond simple smoothing method in what it achieves we use a dynamic tree like architecture to model the high resolution data approximate conditioning on the low resolution image is achieved through a mean eld approach 
we present propagation network p net a novel approach for representing and recognizing sequential activity that include parallel stream of action we represent each activity using partially ordered interval each interval is restricted by both temporal and logical constraint including information about it duration and it temporal relationship with other interval p net associate one node with each temporal interval each node is triggered according to a probability density function that depends on the state of it parent node each node also ha an associated observation function that characterizes supporting perceptual evidence to facilitate realtime analysis we introduce a particle filter framework to explore the conditional state space we modify the original condensation algorithm to more efficiently sample a discrete state space d condensation experiment in the domain of blood glucose monitor calibration demonstrate both the representational power of p net and the effectiveness of the d condensation algorithm 
based on the probabilistic reformulation of principal component analysis pca we consider the problem of determining the number of principal component a a model selection problem we present a hierarchical model for probabilistic pca and construct a bayesian inference method for this model using reversible jump markov chain monte carlo mcmc by regarding each principal component a a point in a one dimensional space and employing only birth death move in our reversible jump methodology our proposed method is simple and capable of automatically determining the number of principal component and estimating the parameter simultaneously under the same disciplined framework simulation experiment are performed to demonstrate the effectiveness of our mcmc method 
we present a novel non parametric unsupervised segmentationalgorithm based on region competition but implemented within a level set framework thekey novelty of the algorithm is that it can solve n classsegmentation problem using just one embedded surface this is achieved by controlling the merging and splitting behaviourof the level set according to a minimum descriptionlength mdl cost function this is in contrastto n class region based level set segmentation method todate which operate by evolving multiple coupled embeddedsurfaces in parallel furthermore it operates inan unsupervised manner it is necessary neither to specifythe value of n nor the class model a priori we argue that the level set methodology provides amore convenient framework for the implementation of theregion competition algorithm which is conventionally implementedusing region membership array due to the lackof a intrinsic curve representation finally we generalisethe gaussian region model used in standard region competitionto the non parametric case the region boundary motionand merge equation become simple expression containingcross entropy and entropy term 
adaptation is a ubiquitous neural and psychological phenomenon with a wealth of instantiation and implication although a basic form of plasticity it ha bar some notable exception attracted computational theory of only one main variety in this paper we study adaptation from the perspective of factor analysis a paradigmatic technique of unsupervised learning we use factor analysis to re interpret a standard view of adaptation and apply our new model to some recent data on adaptation in the domain of face discrimination 
we are constructing caching policy that have lower miss rate than the best of twelve baseline policy over a large variety of request stream this represents an improvement of over least recently used the most commonly implemented policy we achieve this not by designing a specific new policy but by using on line machine learning algorithm to dynamically shift between the standard policy based on their observed miss rate a thorough experimental evaluation of our technique is given a well a a discussion of what make caching an interesting on line learning problem 
this paper present a chinese word segmentation system which can adapt to different domain and standard we first present a statistical framework where domain specific word are identified in a unified approach to word segmentation based on linear model we explore several feature and describe how to create training data by sampling we then describe a transformation based learning method used to adapt our system to different word segmentation standard evaluation of the proposed system on five test set with different standard show that the system achieves stateof the art performance on all of them 
alias i threattrackers are an advanced information access application designed around the need of analyst working through a large daily data feed threattrackers help analyst decompose an information gathering topic like the unfolding political situation in iraq into specification including people place organization and relationship these specification are then used to collect and browse information on a daily basis the nearest related technology are information retrieval search engine document categorization information extraction and named entity detection threattrackers are currently being used in the total information awareness program 
the poster report on a project in which we are investigating method for breaking the human metadata generation bottleneck that plague digital library the research question is whether metadata element and value can be automatically generated from the content of educational resource and correctly assigned to mathematics and science educational material natural language processing and machine learning technique were implemented to automatically assign value of the gemgenerate metadata element set tofor learning resource provided by the gateway for education gem a service that offer web access to a wide range of educational material in a user study education professional evaluated the metadata assigned to learning resource by either automatic tagging or manual assignment result show minimal difference in the eye of the evaluator between automatically generated metadata and manually assigned metadata 
we consider the question of how well a given distribution can be approximated with probabilistic graphical model we introduce a new parameter effective treewidth that capture the degree of approximability a a tradeoff between the accuracy and the complexity of approximation we present a simple approach to analyzing achievable tradeoff that exploit the threshold behavior of monotone graph property and provide experimental result that support the approach 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this paper describes a system for the unsupervised learning of morphological sufx e and stem from word list the system is composed of a generative probability model and hill climbing and directed search algorithm by extracting and examining morphologically rich subset of an input lexicon the directed search identies highly productive paradigm the hill climbing algorithm then further maximizes the probability of the hypothesis quantitative result are shown by measuring the accuracy of the morphological relation identied experiment in english and polish a well a comparison with another recent unsupervised morphology learning algorithm demonstrate the effectiveness of this technique 
computing reflective symmetry of d and d shape is a classical problem in computer vision and computational geometry most prior work ha focused on finding the main ax of symmetry or determining that none exists in this paper we introduce a new reflective symmetry descriptor that represents a measure of reflective symmetry for an arbitrary d voxel model for all plane through the model s center of mass even if they are not plane of symmetry the main benefit of this new shape descriptor are that it is defined over a canonical parameterization the sphere and describes global property of a d shape using fourier method our algorithm computes the symmetry descriptor in time for an voxel grid and computes a multiresolution approximation in time in our initial experiment we have found the symmetry descriptor to be useful for registration matching and classification of shape 
determining shape from stereo ha often been posed a a global minimization problem once formulated the minimization problem are then solved with a variety of algorithmic approach these approach include technique such a dynamic programming min cut and alpha expansion in this paper we show how an algorithmic technique that construct a discrete spatial minimal cost surface can be brought to bear on stereo global minimization problem this problem can then be reduced to a single min cut problem we use this approach to solve a new global minimization problem that naturally arises when solving for three camera trinocular stereo our formulation treat the three camera symmetrically while imposing a natural occlusion cost and uniqueness constraint 
determining shape from stereo ha often been posed a a global minimization problem once formulated the minimization problem are then solved with a variety of algorithmic approach these approach include technique such a dynamic programming min cut and alpha expansion in this paper we show how an algorithmic technique that construct a discrete spatial minimal cost surface can be brought to bear on stereo global minimization problem this problem can then be reduced to a single min cut problem we use this approach to solve a new global minimization problem that naturally arises when solving for three camera trinocular stereo our formulation treat the three camera symmetrically while imposing a natural occlusion cost and uniqueness constraint 
there are a number of framework for modelling argumentation in logic they incorporate a formal representation of individual argument and technique for comparing conflicting argument a problem with these proposal is that they do not consider the believability of the argument from the perspective of the intended audience in this paper we start by reviewing a logic based framework for argumentation based on argument tree which provide a way of exhaustively collating argument and counter argument we then extend this framework to it model theoretic evaluation of the believability of argument this extension assumes that the belief of a typical member of the audience for argumentation can be represented by a set of classical formula a beliefbase we compare a beliefbase with each argument to evaluate the empathy or similarly the antipathy that an agent ha for the argument we show how we can use empathy and antipathy to define a pre ordering relation over argument tree that capture how nne argument tree is more believable than another we also use these to define criterion for deciding whether an argument at the root of an argument tree is defeated or undeleated given the other argument in the tree 
we propose a model that can learn part based representation of highdimensional data our key assumption is that the dimension of the data can be separated into several disjoint subset or factor which take on value independently of each other we assume each factor ha a small number of discrete state and model it using a vector quantizer the selected state of each factor represent the multiple cause of the input given a set of training example our model learns the association of data dimension with factor a well a the state of each vq inference and learning are carried out efficiently via variational algorithm we present application of this model to problem in image decomposition collaborative filtering and text classification 
in typical classification task we seek a function which assigns a label to a single object kernel based approach such a support vector machine svms which maximize the margin of confidence of the classifier are the method of choice for many such task their popularity stem both from the ability to use high dimensional feature space and from their strong theoretical guarantee however many real world task involve sequential spatial or structured data where multiple label must be assigned existing kernel based method ignore structure in the problem assigning label independently to each object losing much useful information conversely probabilistic graphical model such a markov network can represent correlation between label by exploiting problem structure but cannot handle high dimensional feature space and lack strong theoretical generalization guarantee in this paper we present a new framework that combine the advantage of both approach maximum margin markov m network incorporate both kernel which efficiently deal with high dimensional feature and the ability to capture correlation in structured data we present an efficient algorithm for learning m network based on a compact quadratic program formulation we provide a new theoretical bound for generalization in structured domain experiment on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gain over previous approach 
margin maximizing property play an important role in the analysis of classification model such a boosting and support vector machine margin maximization is theoretically interesting because it facilitates generalization error analysis and practically interesting because it present a clear geometric interpretation of the model being built we formulate and prove a sufficient condition for the solution of regularized loss function to converge to margin maximizing separator a the regularization vanishes this condition cover the hinge loss of svm the exponential loss of adaboost and logistic regression loss we also generalize it to multi class classification problem and present margin maximizing multiclass version of logistic regression and support vector machine 
we present a novel approach to collaborative prediction using low norm instead of low rank factorization the approach is inspired by and ha strong connection to large margin linear discrimination we show how to learn low norm factorization by solving a semi definite program and discus generalization error bound for them 
we expand on the problem of learning a kernel via a rkhs on the space of kernel itself the resulting optimization problem is shown to have a semidefinite programming solution we demonstrate that it is possible to learn the kernel for various formulation of machine learning problem specifically we provide mathematical programming formulation and experimental result for the csvm svm and lagrangian svm for classification on uci data and novelty detection 
in video sequence processing shadow remains a major source of error for object segmentation traditional method of shadow removal are mainly based on colour difference thresholding between the background and current image the application of colour filter on mpeg or mjpeg image however is often erroneous a the chrominance information is significantly reduced due to compression in addition a the colour attribute of shadow and object are often very similar discrete thresholding cannot always provide reliable result this paper present a novel approach for adaptive shadow removal by incorporating four different filter in a neuro fuzzy framework the neurofuzzy classifier ha the ability of real time self adaptation and training and it performance ha been quantitatively assessed with both indoor and outdoor video sequence 
we propose and test an objective criterion for evaluation of clustering performance how well doe a clustering algorithm run on unlabeled data aid a classification algorithm the accuracy is quantified using the pac mdl bound in a semisupervised setting clustering algorithm which naturally separate the data according to hidden label with a small number of cluster perform well a simple extension of the argument lead to an objective model selection method experimental result on text analysis datasets demonstrate that this approach empirically result in very competitive bound on test set performance on natural datasets 
we describe technique for combining two type of knowledge system expert and machine learning both the expert system and the learning system represent information by logical decision rule or tree unlike the classical view of knowledge base evaluation or refinement our view accepts the content of the knowledge base a completely correct the knowledge base and the result of it stored case will provide direction for the discovery of new relationship in the form of newly induced decision rule an expert system called sea wa built to discover sale lead for computer product and solution the system interview executive by asking question and based on the response recommends product that may improve a business operation leveraging this expert system we record the result of the interview and the program s recommendation the very same data stored by the expert system is used to find new predictive rule among the potential advantage of this approach are a the capability to spot new sale trend and b the substitution of le expensive probabilistic rule that use database data instead of interview 
the focus of the paper is the problem of learning kernel operator from empirical data we cast the kernel design problem a the construction of an accurate kernel from simple and le accurate base kernel we use the boosting paradigm to perform the kernel construction process to do so we modify the booster so a to accommodate kernel operator we also devise an efficient weak learner for simple kernel that is based on generalized eigen vector decomposition we demonstrate the effectiveness of our approach on synthetic data and on the usps dataset on the usps dataset the performance of the perceptron algorithm with learned kernel is systematically better than a fixed rbf kernel 
we introduce the first algorithm for learning predictive state representation predictive state representation or psrs are a way of representing the state of a controlled dynamical system in term of prediction of test where test are sequence of action and observation said to be true if and only if all the observation occur given that all the action are taken the problem of finding a good psr one that is a sufficient statistic for the dynamical system can be divided into two part discovery of a good set of test and learning to make accurate prediction for those test in this paper we address the learning part of the problem for linear psrs and show that our algorithm make correct prediction in several sample system 
an important aspect of clustering algorithm is whether the partition constructed on finite sample converge to a useful clustering of the whole data space a the sample size increase this paper investigates this question for normalized and unnormalized version of the popular spectral clustering algorithm surprisingly the convergence of unnormalized spectral clustering is more difficult to handle than the normalized case even though recently some first result on the convergence of normalized spectral clustering have been obtained for the unnormalized case we have to develop a completely new approach combining tool from numerical integration spectral and perturbation theory and probability it turn out that while in the normalized case spectral clustering usually converges to a nice partition of the data space in the unnormalized case the same only hold under strong additional assumption which are not always satisfied we conclude that our analysis give strong evidence for the superiority of normalized spectral clustering it also provides a basis for future exploration of other laplacian based method 
classification tree are widely used in the machine learning and data mining community for modeling propositional data recent work ha extended this basic paradigm to probability estimation tree traditional tree learning algorithm assume that instance in the training data are homogenous and independently distributed relational probability tree rpts extend standard probability estimation tree to a relational setting in which data instance are heterogeneous and interdependent our algorithm for learning the structure and parameter of an rpt search over a space of relational feature that use aggregation function e g average mode count to dynamically propositionalize relational data and create binary split within the rpt previous work ha identified a number of statistical bias due to characteristic of relational data such a autocorrelation and degree disparity the rpt algorithm us a novel form of randomization test to adjust for these bias on a variety of relational learning task rpts built using randomization test are significantly smaller than other model and achieve equivalent or better performance 
we consider situation where training data is abundant and computing resource are comparatively scarce we argue that suitably designed online learning algorithm asymptotically outperform any batch learning algorithm both theoretical and experimental evidence are presented 
current mapping algorithm using consistent pose estimation cpe algorithm can successfully map area of square meter using thousand of pose however the computation to construct the map grows a o n log n so larger map get increasingly difficult to build we present an abstraction method for postponing the growth in computation this method solves a much smaller problem in the space of the connection graph of the map 
we present a novel method for inferring three dimensional shape froma collection of defocused image it is based on the observation that defocusedimages are the null space of certain linear operator that depend on the threedimensionalshape of the scene a well a on the optic of the camera unlike mostcurrent work based on inverting the imaging model to recover the quot deblurred quot imageand the shape of the scene we approach the problem from a new angle bycollecting a number of 
the kernel function play a central role in kernel method existing method typically fix the functional form of the kernel in advance and then only adapt the associated kernel parameter based on empirical data in this paper we consider the problem of adapting the kernel so that it becomes more similar to the so called ideal kernel we formulate this a a distance metric learning problem that search for a suitable linear transform fcature weighting in the kernel induced feature space this formulation is applicable even when the training set can only provide example of similar and dissimilar pair but not explicit class label information computationally this lead to a local optimum free quadratic programming problem with the number of variable independent of the number of feature performance of this method is evaluated on classification and clustering task on both toy and real world data set 
in this paper we study a special kind of learning problem in which each training instance is given a set of or distribution over candidate class label and only one of the candidate label is the correct one such a problem can occur e g in an information retrieval setting where a set of word is associated with an image or if class label are organized hierarchically we propose a novel discriminative approach for handling the ambiguity of class label in the training example the experiment with the proposed approach over five different uci datasets show that our approach is able to find the correct label among the set of candidate label and actually achieve performance close to the case when each training instance is given a single correct label in contrast na ve method degrade rapidly a more ambiguity is introduced into the label 
we formulate linear dimensionality reduction a a semi parametric estimation problem enabling u to study it asymptotic behavior we generalize the problem beyond additive gaussian noise to unknown nongaussian additive noise and to unbiased non additive model 
markov network are extensively used to model complex sequential spatial and relational interaction in field a diverse a image processing natural language analysis and bioinformatics however inference and learning in general markov network is intractable in this paper we focus on learning a large subclass of such model called associative markov network that are tractable or closely approximable this subclass contains network of discrete variable with k label each and clique potential that favor the same label for all variable in the clique such network capture the guilt by association pattern of reasoning present in many domain in which connected associated variable tend to have the same label our approach exploit a linear programming relaxation for the task of finding the best joint assignment in such network which provides an approximate quadratic program qp for the problem of learning a margin maximizing markov network we show that for associative markov network over binary valued variable this approximate qp is guaranteed to return an optimal parameterization for markov network of arbitrary topology for the nonbinary case optimality is not guaranteed but the relaxation produce good solution in practice experimental result with hypertext and newswire classification show significant advantage over standard approach 
clark s completion is a simple nonmonotonic formalism and a special case of several nonmonotonic logic recently there ha been work on extending completion with loop formula so that general case of nonmonotonic logic such a logic program under the answer set semantics and mccain turner causal logic can be characterized by propositional logic in the form of completion loop formula in this paper we show that the idea is applicable to mccarthy s circumscription in the propositional case with lifschitz s pointwise circumscription playing the role of completion we also show how to embed propositional circumscription in logic program and in causal logic inspired by the uniform characterization of completion loop formula 
we present an algorithmic framework for supervised classification learning where the set of label is organized in a predefined hierarchical structure this structure is encoded by a rooted tree which induces a metric over the label set our approach combine idea from large margin kernel method and bayesian analysis following the large margin principle we associate a prototype with each label in the tree and formulate the learning task a an optimization problem with varying margin constraint in the spirit of bayesian method we impose similarity requirement between the prototype corresponding to adjacent label in the hierarchy we describe new online and batch algorithm for solving the constrained optimization problem we derive a worst case loss bound for the online algorithm and provide generalization analysis for it batch counterpart we demonstrate the merit of our approach with a series of experiment on synthetic text and speech data 
this paper address the question of allocating computational resource among a set of algorithm in order to achieve the best performance on a scheduling problem instance our primary motivation in addressing this problem is to reduce the expertise needed to apply constraint technology therefore we investigate algorithm control technique that make decision based only on observation of the improvement in solution quality achieved by each algorithm we call our approach low knowledge since it doe not rely on complex prediction model we show that such an approach result in a system that achieves significantly better performance than all of the pure algorithm without requiring additional human expertise furthermore the low knowledge approach achieves performance equivalent to a perfect high knowledge classification approach 
we propose and study extension of logic programming with constraint represented a generalized atom of the form c x where x is a finite set of atom and c is an abstract constraint formally a collection of set of atom atom c x are satisfied by an interpretation set of atom m if m x c we focus here on monotone constraint that is those collection c that are closed under the superset they include in particular weight or pseudo boolean constraint studied both by the logic programming and sat community we show that key concept of the theory of normal logic program such a the one step provability operator the semantics of supported and stable model a well a several of their property including complexity result can be lifted to such case 
two dimensional contingency or co occurrence table arise frequently in important application such a text web log and market basket data analysis a basic problem in contingency table analysis is co clustering simultaneous clustering of the row and column a novel theoretical formulation view the contingency table a an empirical joint probability distribution of two discrete random variable and pose the co clustering problem a an optimization problem in information theory the optimal co clustering maximizes the mutual information between the clustered random variable subject to constraint on the number of row and column cluster we present an innovative co clustering algorithm that monotonically increase the preserved mutual information by intertwining both the row and column clustering at all stage using the practical example of simultaneous word document clustering we demonstrate that our algorithm work well in practice especially in the presence of sparsity and high dimensionality 
statistical approach to language learning typically foc u on either short range syntactic dependency or long range semantic dependency between word we present a generative model that us both kind of dependency and can be used to simultaneously find syntact ic class and semantic topic despite having no representation of syntax or semantics beyond statistical dependency this model is competitive on task like part of speech tagging and document classification wi th model that exclusively use shortand long range dependency respectively 
this poster describes method to enable intelligent access to multimodal information stream we illustrate these method in two integrated system the broadcast news editor bne which incorporates image speech and language processing and the broadcast news navigator bnn which provides search visualization and personalized access to broadcast news video bnn enables user to perform keyword and named entity search temporally and geospatially visualize entity and story cluster story discover entity relation and obtain personalized multimedia summary by transforming access from sequential to direct search and providing hierarchical hyperlinked summary bne and bnn enable user to access topic and entity news cluster nearly three time a fast a direct search of video 
psychologist call behavior intrinsically motivated when it is engaged in for it own sake rather than a a step toward solving a specific problem of clear practical value but what we learn during intrinsically motivated behavior is essential for our development a competent autonomous entity able to efficiently solve a wide range of practical problem a they arise in this paper we present initial result from a computational study of intrinsically motivated reinforcement learningaimed at allowing artificial agent to construct and extend hierarchy of reusable skill that are needed for competent autonomy 
a large part of image processing involves the computation of significant point curve and area feature these can be defined a locus where absolute differential invariant of the image assume fiducial value taking spatial scale and intensity in a generic sense scale into account differential invariance implies a group of similarity or congruence these motion define the geometrical structure of image space classical euclidian invariant don t apply to image because image space is non euclidian we analyze image structure from first principle and construct the fundamental group of image space motion image space is a cayley klein geometry with one isotropic dimension the analysis lead to a principled definition of feature and the operator that define them 
this paper concern the assessment of linear cause effect relationship from a combination of observational data and qualitative causal structure the paper show how technique developed for identifying causal effect in causal bayesian network can be used to identify linear causal effect and thus provides a new approach for assessing linear causal effect in structural equation model using this approach the paper develops a systematic procedure for recognizing identifiable direct causal effect 
knowledge based question answering system have become quite competent and robust at answering a wide range of question in different domain however in order to ask question correctly one need to have intimate knowledge of the structure of the knowledge base and typical user lack this knowledge we address this problem by developing a system that us the content of the knowledge base to automatically align a user s encoding of a query to the structure of the knowledge base our preliminary evaluation show the system detects and corrects most misalignment and user are able to pose most question quickly 
we consider a variant of q learning in continuous state space under the total expected discounted cost criterion combined with local function approximation method provided that the function approximator satisfies certain interpolation property the resulting algorithm is shown to converge with probability one the limit function is shown to satisfy a fixed point equation of the bellman type where the fixed point operator depends on the stationary distribution of the exploration policy and the function approximation method the basic algorithm is extended in several way in particular a variant of the algorithm is obtained that is shown to converge in probability to the optimal q function preliminary computer simulation are presented that confirm the validity of the approach 
committee of classifier with learning capability have good performance in a variety of domain we focus on committee of agent with learning capability where no agent is omniscient but ha a local limited individual view of data in this framework a major issue is how to integrate the individual result in an overall result usually a voting mechanism is used we propose a setting where agent can express a symbolic justification of their individual result justification can then be examined by other agent and accepted or found wanting we propose a specific interaction protocol that support revision of justification created by dierent agent finally the opinion of individual agent are aggregated into a global outcome using a weighted voting scheme 
anomaly detection is an area that ha received much attention in recent year it ha a wide variety of application including fraud detection and network intrusion detection a good deal of research ha been performed in this area often using string or attribute value data a the medium from which anomaly are to be extracted little work however ha focused on anomaly detection in graph based data in this paper we introduce two technique for graph based anomaly detection in addition we introduce a new method for calculating the regularity of a graph with application to anomaly detection we hypothesize that these method will prove useful both for finding anomaly and for determining the likelihood of successful anomaly detection within graph based data we provide experimental result using both real world network intrusion data and artificially created data 
recent progress in stereo algorithm performance is quickly outpacing the ability of existing stereo data set to discriminate among the best performing algorithm motivating the need for more challenging scene with accurate ground truth information this paper describes a method for acquiring high complexity stereo image pair with pixel accurate correspondence information using structured light unlike traditional range sensing approach our method doe not require the calibration of the light source and yield registered disparity map between all pair of camera and illumination projector we present new stereo data set acquired with our method and demonstrate their suitability for stereo algorithm evaluation our result are available at http www middlebury edu stereo 
natural language nl ha evolved to facilitate human communication it enables the speaker to make the listener s mind wander among her experience and mental association roughly according to the intention of the speaker the speaker and the listener usually share experience and expectation and they use mostly the same unit and rule of a shared nl written language function similarly but in a le interactive way with fewer possibility for feedback both the symbol of nl i e word or morpheme and their arrangement are meaningful not with universal and precise meaning but similar enough among different speaker and accurate enough for the communication mostly to succeed nls are mostly very large system hundred of thousand of word and infinitely many possible utterance even inflection alone might produce huge number of form e g more than ten thousand distinct form out of every finnish verb entry nl processing for ir or any other purpose must cope with phenomenon like inflection and compounding synonymy polysemy ambiguity anaphora and head modifier relation among word and phrase language technology can neutralize much of the effect of these inconvenience inherent with nl but what kind of advantage could nl have redundant use of synonymous expression can effectively identify new concept multilingual parallel document may help in identifying their exact content nls typically carry connotation i e what is implied but not explicitly said e g attitude politeness vague association are easy to express in nl but not always in formal system e g a few year ago there wa an article about the rival of yeltsin i don t remember his name but he then went over to some region in siberia but what did the guy promise joke and humor belong to nls not to formal system are there any alternative for nl not really because any artificial and more precise formalism fail to adapt to new concept and they do not easily allow restructuring of previous idea one challenge for language technology is to find better solution for the above inconvenience in order to provide various ir document classification indexing and summarizing method with more accurate and adequate input data with more accurate input some of the more demanding task of ir can perhaps be solved 
this paper describes a set of representation of gait appearance feature for the purpose of person identification our gait representation ha two stage the first stage computes a set of image feature that are based on moment extracted from orthogonal view video silhouette of human walking motion the second stage applies three method of aggregating these image feature over time to create the gait sequence feature despite their simplicity the resulting gait sequence feature vector contain enough information to perform well on human identification we demonstrate the accuracy of recognition using gait video sequence collected over different day and time under varying lighting environment and explore the difference in the three time aggregation method for the purpose of recognition 
this paper present a novel solution to the illuminant estimationproblem the problem of how given an image of ascene taken under an unknown illuminant we can recoveran estimate of that light the work is founded on previousgamut mapping solution to the problem which solvefor a scene illuminant by determining the set of diagonalmappings which take image data captured under an unknownlight to a gamut of reference colour taken undera known light unfortunately a diagonal model is not alwaysa valid model of illumination change and so previousapproaches sometimes return a null solution in addition previous method are difficult to implement we addressthese problem by recasting the problem a one ofilluminant classification we define a prioria set of plausiblelights thus ensuring that a scene illuminant estimatewill always be found a plausible light is represented bythe gamut of colour observable under it and the illuminantin an image is classified by determining the plausible lightwhose gamut is most consistent with the image data weshow that this step the main computational burden of thealgorithm can be performed simply quickly and efficientlyby mean of a non negative least square optimisation wereport result on a large set of real image which show thatit provides excellent illuminant estimation outperformingprevious algorithm 
adaboost minimizes an upper error bound which is an exponential function of the margin on the training set however the ultimate goal in application of pattern classification is always minimum error rate on the other hand adaboost need an effective procedure for learning weak classifier which by itself is difficult especially for high dimensional data in this paper we present a novel procedure called floatboost for learning a better boosted classifier floatboost us a backtrack mechanism after each iteration of adaboost to remove weak classifier which cause higher error rate the resulting float boosted classifier consists of fewer weak classifier yet achieves lower error rate than adaboost in both training and test we also propose a statistical model for learning weak classifier based on a stagewise approximation of the posterior using an overcomplete set of scalar feature experimental comparison of floatboost and adaboost are provided through a difficult classification problem face detection where the goal is to learn from training example a highly nonlinear classifier to differentiate between face and nonface pattern in a high dimensional space the result clearly demonstrate the promise made by floatboost over adaboost 
this paper introduces the field programmable learning array a new paradigm for rapid prototyping of learning primitive and machinelearning algorithm in silicon the fpla is a mixed signal counterpart to the all digital field programmable gate array in that it enables rapid prototyping of algorithm in hardware unlike the fpga the fpla is targeted directly for machine learning by providing local parallel online analog learning using floating gate mo synapse transistor we present a prototype fpla chip comprising an array of reconfigurable computational block and local interconnect we demonstrate the viability of this architecture by mapping several learning circuit onto the prototype chip 
there have been many proposal for first order belief network i e where we quantify over individual but these typically only let u reason about the individual that we know about there are many instance where we have to quantify over all of the individual in a population when we do this the population size often matter and we need to reason about all of the member of the population but not necessarily individually this paper present an algorithm to reason about multiple individual where we may know particular fact about some of them but want to treat the others a a group combining unification with variable elimination let u reason about class of individual without needing to ground out the theory 
hierarchical reinforcement learning is a general framework which attempt to accelerate policy learning in large domain on the other hand policy gradient reinforcement learning pgrl method have received recent attention a a mean to solve problem with continuous state space however they suer from slow convergence in this paper we combine these two approach and propose a family of hierarchical policy gradient algorithm for problem with continuous state and or action space we also introduce a class of hierarchical hybrid algorithm in which a group of subtasks usually at the higher level of the hierarchy are formulated a value function based rl vfrl problem and the others a pgrl problem we demonstrate the performance of our proposed algorithm using a simple taxi fuel problem and a complex continuous state and action ship steering domain 
the evolution of description logic dl and propositional dynamic logic produced a hierarchy of decidable logic with multiple maximal element it would be desirable to combine different maximal logic into one super logic but then inference may turn out to be undecidable then it is important to characterize the decidability threshold for these logic in this perspective an interesting open question pointed out by sattler and vardi sattler and vardi is whether inference in a hybrid calculus with restricted form of graded modality is decidable and which complexity class it belongs to in this paper we prove that this calculus and the corresponding dl alciof are undecidable second we prove undecidability result for logic that support both a transitive closure operator over role and number restriction 
object recognition and detection represent a relevant component in cognitive computer vision system such a in robot vision intelligent video surveillance system or multimodal interface object identification from local information ha recently been investigated with respect to it potential for robust recognition e g in case of partial object occlusion scale variation noise and background clutter in detection task this work contributes to this research by a thorough analysis of the discriminative power of local appearance pattern and by proposing to exploit local information content to model object representation and recognition we identify discriminative region in the object view from a posterior entropy measure and then derive object model from selected discriminative local pattern for recognition we determine rapid attentive search for location of high information content from learned decision tree the recognition system is evaluated by various degree of partial occlusion and gaussian image noise resulting in highly robust recognition even in the presence of severe occlusion effect 
this paper present a set of tool and technique for analyzing interaction of composite web service which are specified in bpel and communicate through asynchronous xml message we model the interaction of composite web service a conversation the global sequence of message exchanged by the web service a opposed to earlier work our tool set handle rich data manipulation via xpath expression this allows u to verify design at a more detailed level and check property about message content we present a framework where bpel specification of web service are translated to an intermediate representation followed by the translation of the intermediate representation to a verification language a an intermediate representation we use guarded automaton augmented with unbounded queue for incoming message where the guard are expressed a xpath expression a the target verification language we use promela input language of the model checker spin since spin model checker is a finite state verification tool we can only achieve partial verification by fixing the size of the input queue in the translation we propose the concept of synchronizability to address this problem we show that if a composite web service is synchronizable then it conversation set remains same when asynchronous communication is replaced with synchronous communication we give a set of sufficient condition that guarantee synchronizability and that can be checked statically based on our synchronizability result we show that a large class of composite web service with unbounded input queue can be completely verified using a finite state model checker such a spin 
despite traditional web caching technique redundant data is often transferred over http link these redundant transfer result from both resource modification and aliasing resource modification cause the data represented by a single uri to change often in transferring the new data some old data is retransmitted aliasing in contrast occurs when the same data is named by multiple uris often in the context of dynamic or advertising content traditional web caching technique index data by it name and thus often fail to recognize and take advantage of aliasing despite traditional web caching technique redundant data is often transferred over http link these redundant transfer result from both resource modification and aliasing resource modification cause the data represented by a single uri to change often in transferring the new data some old data is retransmitted aliasing in contrast occurs when the same data is named by multiple uris often in the context of dynamic or advertising content traditional web caching technique index data by it name and thus often fail to recognize and take advantage of aliasing 
particle filter are used extensively for tracking the state of non linear dynamic system this paper present a new particle filter that maintains sample in the state space at dynamically varying resolution for computational efficiency resolution within siatespace varies by region depending on the belief that the true state lie within each region where belief is strong resolution is fine where belief is low resolution is coarse abstracting multiple similar state together the resolution of the statespace is dynamically updated a the belief change the proposed algorithm make an explicit bias variance tradeoff to select between maintaining sample in a biased generalization of a region of state space versus in a high variance specialization at fine resolution sample are maintained at a coarser resolution when the bias introduced by the generalization to a coarse resolution is outweighed by the gain in term of reduction in variance and at a finer resolution when it is not maintaining sample in abstraction prevents potential hypothesis from being eliminated prematurely for lack of a sufficient number of particle empirical result show that our variable resolution particle filter requires significantly lower computation for performance comparable to a classical particle filter 
we propose a bayesian extension to the ad hoc language model many smoothed estimator used for the multinomial query model in ad hoc language model including laplace and bayes smoothing are approximation to the bayesian predictive distribution in this paper we derive the full predictive distribution in a form amenable to implementation by classical ir model and then compare it to other currently used estimator in our experiment the proposed model outperforms bayes smoothing and it combination with linear interpolation smoothing outperforms all other estimator 
we present a model based method for accurate extraction of pedestrian silhouette from video sequence our approach is based on two assumption there is a common appearance to all pedestrian and each individual look like him herself over a short amount of time these assumption allow u to learn pedestrian model that encompass both a pedestrian population appearance and the individual appearance variation using our model we are able to produce pedestrian silhouette that have fewer noise pixel and missing part we apply our silhouette extraction approach to the nist gait data set and show that under the gait recognition task our model based silhouette result in much higher recognition rate than silhouette directly extracted from background subtraction or any nonmodel based smoothing scheme 
the web is increasingly becoming an important channel for conducting business disseminating information and communicating with people on a global scale more and more company organization and individual are publishing their information on the web with all this information publicly available naturally company and individual want to find useful information from these web page a an example company always want to know what their competitor are doing and what product and service they are offering knowing such information the company can learn from their competitor and or design countermeasure to improve their own competitiveness the ability to effectively find such business intelligence information is increasingly becoming crucial to the survival and growth of any company despite it importance little work ha been done in this area in this paper we propose a novel visualization technique to help the user find useful information from his her competitor web site easily and quickly it involves visualizing with the help of a clustering system the comparison of the user s web site and the competitor s web site to find similarity and difference between the site the visualization is such that with a single glance the user is able to see the key similarity and difference of the two site he she can then quickly focus on those interesting cluster and page to browse the detail experiment result and practical application show that the technique is effective 
word alignment play a crucial role in statistical machine translation word aligned corpus have been found to be an excellent source of translation related knowledge we present a statistical model for computing the probability of an alignment given a sentence pair this model allows easy integration of context specific feature our experiment show that this model can be an effective tool for improving an existing word alignment 
this paper introduces fast a novel two phase sampling based algorithm for discovering association rule in large database in phase i a large initial sample of transaction is collected and used to quickly and accurately estimate the support of each individual item in the database in phase ii these estimated support are used to either trim outlier transaction or select representative transaction from the initial sample thereby forming a small final sample that more accurately reflects the statistical characteristic i e itemset support of the entire database the expensive operation of discovering association rule is then performed on the final sample in an empirical study fast wa able to achieve accuracy using a final sample having a size of only of that of a comparable random sample this efficiency gain resulted in a speedup by roughly a factor of over previous algorithm that require expensive processing of the entire database even efficient algorithm that exploit sampling our new sampling technique can be used in conjunction with almost any standard association rule algorithm and can potentially render scalable other algorithm that mine count data 
the context in which a name appears in a caption provides powerful cue a to who is depicted in the associated image we obtain face image using a face detector from approximately half a million captioned news image and automatically link name obtained using a named entity recognizer with these face a simple clustering method can produce fair result we improve these result significantly by combining the clustering process with a model of the probability that an individual is depicted given it context once the labeling procedure is over we have an accurately labeled set of face an appearance model for each individual depicted and a natural language model that can produce accurate result on caption in isolation 
we study the common problem of approximating a target matrix with a matrix of lower rank we provide a simple and ecient em algorithm for solving weighted low rank approximation problem which unlike their unweighted version do not admit a closedform solution in general we analyze in addition the nature of locally optimal solution that arise in this context demonstrate the utility of accommodating the weight in reconstructing the underlying low rank representation and extend the formulation to nongaussian noise model such a logistic model finally we apply the method developed to a collaborative filtering task 
in model that define probability via energy maximum likelihood learning typically involves using markov chain monte carlo to sample from the model s distribution if the markov chain is started at the data distribution learning often work well even if the chain is only run for a few time step but if the data distribution contains mode separated by region of very low density brief mcmc will not ensure that different mode have the correct relative energy because it cannot move particle from one mode to another we show how to improve brief mcmc by allowing long range move that are suggested by the data distribution if the model is approximately correct these long range move have a reasonable acceptance rate 
the effectiveness of query in information retrieval can be improved through query expansion this technique automatically introduces additional query term that are statistically likely to match document on the intended topic however query expansion technique rely on fixed parameter our investigation of the effect of varying these parameter show that the strategy of using fixed value is questionable 
the social impact from the world wide web cannot be underestimated but technology used to build the web are also revolutionizing the sharing of business and government information within intranet in many way the lesson learned from the internet carry over directly to intranet but others do not apply in particular the social force that guide the development of intranet are quite different and the determination of a good answer for intranet search is quite different than on the internet in this paper we study the problem of intranet search our approach focus on the use of rank aggregation and allows u to examine the effect of different heuristic on ranking of search result 
we study a number of open issue in spectral clustering i selecting the appropriate scale of analysis ii handling multi scale data iii clustering with irregular background clutter and iv finding automatically the number of group we first propose that a local scale should be used to compute the affinity between each pair of point this local scaling lead to better clustering especially when the data includes multiple scale and when the cluster are placed within a cluttered background we further suggest exploiting the structure of the eigenvectors to infer automatically the number of group this lead to a new algorithm in which the final randomly initialized k mean stage is eliminated 
local image feature or interest point provide compact andabstract representation of pattern in an image in this paper wepropose to extend the notion of spatial interest point into thespatio temporal domain and show how the resulting feature oftenreflect interesting event that can be used for a compactrepresentation of video data a well a for it interpretation todetect spatio temporal event we build on the idea of the harrisand f rstner interest point operator and detect localstructures in space time where the image value have significantlocal variation in both space and time we then estimate thespatio temporal extent of the detected event and compute theirscale invariant spatio temporal descriptor using suchdescriptors we classify event and construct video representationin term of labeled space time point for the problem of humanmotion analysis we illustrate how the proposed method allows fordetection of walking people in scene with occlusion and dynamicback ground 
automated detection of the first document reporting each new event in temporally sequenced stream of document is an open challenge in this paper we propose a new approach which address this problem in two stage using a supervised learning algorithm to classify the on line document stream into pre defined broad topic category and performing topic conditioned novelty detection for document in each topic we also focus on exploiting named entity for event level novelty detection and using feature based heuristic derived from the topic history evaluating these method using a set of broadcast news story our result show substantial performance gain over the traditional one level approach to the novelty detection problem 
we introduce a new learning algorithm for decision list to allow feature that are constructed from the data and to allow a tradeo between accuracy and complexity we bound it generalization error in term of the number of error and the size of the classifier it find on the training data we also compare it performance on some natural data set with the set covering machine and the support vector machine 
recent work on incremental crawling ha enabled the indexed document collection of a search engine to be more synchronized with the changing world wide web however this synchronized collection is not immediately searchable because the keyword index is rebuilt from scratch le frequently than the collection can be refreshed an inverted index is usually used to index document crawled from the web complete index rebuild at high frequency is expensive previous work on incremental inverted index update have been restricted to adding and removing document updating the inverted index for previously indexed document that have changed ha not been addressed in this paper we propose an efficient method to update the inverted index for previously indexed document whose content have changed our method us the idea of landmark together with the diff algorithm to significantly reduce the number of posting in the inverted index that need to be updated our experiment verify that our landmark diff method result in significant saving in the number of update operation on the inverted index 
flux belongs to the high level programming language for cognitive agent that have been developed in recent year based on the established general action representation formalism of the fluent calculus flux allows to implement complex strategy in a concise and modular fashion in this paper we extend the flux language to reason about domain involving continuous change and where action occur concurrently using constraint logic programming we show that this reasoning is performed in an efficient way 
we study the problem of object in particular face recognition under varying imaging condition object are represented using local characteristic feature called textons appearance variation due to changing condition are encoded by the correlation between the textons we propose two solution to model these correlation the first one assumes locational independence we call it the conditional texton distribution model the second capture the second order variation across location using fisher linear discriminant analysis we call it the fisher texton model our two model are effective in the problem of face recognition from a single image across a wide range of illumination pose and time 
people routinely make sophisticated causal inference unconsciously effortlessly and from very little data often from just one or a few observation we argue that these inference can be explained a bayesian computation over a hypothesis space of causal graphical model shaped by strong top down prior knowledge in the form of intuitive theory we present two case study of our approach including quantitative model of human causal judgment and brief comparison with traditional bottom up model of inference 
there are a number of framework for modelling argumentation in logic they incorporate a formal representation of individual argument and technique for comparing conflicting argument an example is the framework by besnard and hunter that is based on classical logic and in which an argument obtained from a knowledgebase is a pair where the first item is a minimal consistent set of formula that prof the second item which is a formula in the framework the only counter argument defeaters that need to be taken into account are canonical argument a form of minimal undercut argument tree then provide a way of exhaustively collating argument and counter argument a problem with this set up is that some argument tree may be too big to have sufficient impact in this paper we address the need to increase the impact of argumentation by using pruned argument tree we formalize this in term of how argument resonate with the intended audience of the argument for example if a politician want to make a case for raising tax the argument used would depend on what is important to the audience argument based on increased tax are needed to pay for improved healthcare would resonate better with an audience of pensioner whereas argument based on increased tax are needed to pay for improved transport infrastructure would resonate better with an audience of business executive by analysing the resonance of argument we can prune argument tree to raise their impact 
headline summarization is a difficult task because it requires maximizing text content in short summary length while maintaining grammaticality this paper describes our first attempt toward solving this problem with a sy stem that generates key headline cluster and fine tune them using template 
abstract the majority of theoretical work in machine learning is done under the assumption of exchangeability essentially it is assumed that the example are generated from the same probability distribution independently this paper is concerned with the problem of testing the exchangeability assumption in the on line mode example are observed one by one and the goal is to monitor on line the strength of evidence against the hypothesis of exchangeability we introduce the notion of exchangeability martingale which are on line procedure for detecting deviation from exchangeability in essence they are betting scheme that never risk bankruptcy and are fair under the hypothesis of exchangeability some specific exchangeability martingale are constructed using transductive confi dence machine we report experimental re sults showing their performance on the usps benchmark data set of hand written digit known to be somewhat heterogeneous one of them multiplies the initial capital by more than this mean that the hypothesis of exchangeability is rejected at the significance level 
in several research domain concerned with classification task curve like roc are often used to ass the quality of a particular model or to compare two or more model with respect to various operating point researcher also often publish some statistic coming from the roc such a the socalled break even point or equal error rate the purpose of this paper is to first argue that these measure can be misleading in a machine learning context and should be used with care instead we propose to use the expected performance curve epc which provide unbiased estimate of performance at various operating point furthermore we show how to use adequately a non parametric statistical test in order to produce epcs with confidence interval or ass the statistical significant dierence between two model under various setting 
user feedback is vital in many recommender system to help guide the search for good recommendation preference based feedback e g show me more like item a is an inherently ambiguous form of feedback with a limited ability to guide the recommendation process and for this reason it is usually avoided nevertheless we believe that certain domain demand the use of preference based feedback a such we describe and evaluate a flexible recommendation strategy that ha the potential to improve the performance of case based recommenders that rely on preference based feedback 
theory of access consciousness address how it is that some mental state but not others are available for evaluation choice behavior and verbal report farah o reilly and vecera argue that quality of representation is critical dehaene sergent and changeux argue that the ability to communicate representation is critical we present a probabilistic information transmissionor pit model that suggests both of these condition are essential for access consciousness having successfully modeled data from the repetition priming literature in the past we use the pit model to account for data from two experiment on subliminal priming showing that the model produce priming even in the absence of accessibility and reportability of internal state the model provides a mechanistic basis for understanding the dissociation of priming and awareness philosophy ha made many attempt to identify distinct aspect of consciousness perhaps the most famous effort is block s delineation of phenomenal and access consciousness phenomenal consciousness ha to do with what it is like to experience chocolate or a pin prick access consciousness refers to internal state whose content is inferentially promiscuous i e poised to be used a a premise in reasoning poised for control of action and poised for rational control of speech p the scientific study of consciousness ha exploded in the past six year and an important catalyst for this explosion ha been the decision to focus on the problem of access consciousness how is it that some mental state but not others become available for evaluation choice behavior verbal report and storage in working memory another reason for the recent explosion of consciousness research is the availability of functional imaging technique to explore difference in brain activation between conscious and unconscious state a well a the development of clever psychological experiment that show that a stimulus that is not consciously perceived can nonetheless influence cognition which we describe shortly 
the information age ha brought with it the promise of unprecedented economic growth based on the efficiency made possible by new technology this same greater efficiency ha left society with le and le time to adapt to technological progress perhaps the greatest cost of this progress is the threat to privacy we all face from unconstrained exchange of our personal information in response to this threat the world wide web consortium ha introduced the platform for privacy preference p p to allow site to express policy in machine readable form and to expose these policy to site visitor however today p p doe not protect the privacy of individual nor doe it implementation empower community or group to negotiate and establish standard of behavior we propose a privacy architecture we call the social contract core scc designed to speed the establishment of new social contract needed to protect private data the goal of scc is to empower community speed the socialization of new technology and encourage the rapid access to and exchange of information addressing these issue is essential we feel to both liberty and economic prosperity in the information age 
in many vision problem the observed data lie in a nonlinear manifold in a high dimensional space this paper present a generic modelling scheme to characterize the nonlinear structure of the manifold and to learn it multimodal distribution our approach represents the data a a linear combination of parameterized local component where the statistic of the component parameterization describe the nonlinear structure of the manifold the component are adaptively selected from the training data through a progressive density approximation procedure which lead to the maximum likelihood estimate of the underlying density we show result on both synthetic and real training set and demonstrate that the proposed scheme ha the ability to reveal important structure of the data 
when searching for multi attribute service or product understanding and representing user s preference is a crucial task however many computer tool do not afford user to adequately focus on fundamental decision objective reveal hidden preference revise conflicting preference or explicitly reason about tradeoff with competing decision goal a a result user often fail to find the best solution from building decision support system for various application domain we have observed some common area of design pitfall which could lead to undesirable user behavior and ineffective use of decision system by incorporating finding from behavior decision theory we have identified and accumulated a set of principle for avoiding these design pitfall provide a flexible order and choice in preference elicitation so that user can focus on fundamental objective include appropriate information in a decision context to guide user in revealing hidden preference and make tradeoff attribute explicit to facilitate preference revision and flexible decision making we describe these principle and the corresponding interface affordances and discus concrete scenario where they have been applied and tested 
the interactive query performance analyser qpa for information retrieval system is a web based tool for analysing and comparing the performance of individual query on top of a standard test collection it give an instant visualisation of the performance achieved in a given search topic by any user generated query in addition to experimental ir research qpa can be used in user training to demonstrate the characteristic of and compare difference between ir system and searching strategy the first prototype version and of the query performance analyser wa developed at the department of information study university of tampere to serve a a tool for rapid query performance analysis comparison and visualisation later it ha been applied to interactive optimisation of query the analyser ha served also in learning environment for ir the demonstration is based on the newest version of the query performance analyser v it is interfaced to a traditional boolean ir system trip and a probabilistic ir system inquery providing access to the trec collection and two finnish test collection version support multigraded relevance scale new type of performance visualisation and query conversion based on monoand multi lingual dictionary the motivation in developing the analyser is to emphasise the necessity of analysing the behaviour of individual query information retrieval experiment usually measure the average effectiveness of ir method developed the analysis of individual query is neglected although test result may contain individual test topic where general finding do not hold for the real user of an ir system the study of variation in result is even more important than average 
recognition system have generally treated specular highlightsas noise we show how to use these highlight asa positive source of information that improves recognitionof shiny object this also enables u to recognize verychallenging shiny transparent object such a wine glass specifically we show how to find highlight that are consistentwith an hypothesized pose of an object of known dshape we do this using only a qualitative description ofhighlight formation that is consistent with most model ofspecular reflection so no specific knowledge of an object sreflectance property is needed we first present a methodthat find highlight produced by a dominant compact lightsource whose position is roughly known we then show howto estimate the lighting automatically for object whose reflectionis part specular and part lambertian we demonstratethis method for two class of object first we showthat specular information alone can suffice to identify objectswith no lambertian reflectance such a transparentwine glass second we use our complete system to recognizeshiny object such a pottery 
we present an unsupervised method for labelling the argument of verb with their semantic role our bootstrapping algorithm make initial unambiguous role assignment and then iteratively update the probability model on which future assignment are based a novel aspect of our approach is the use of verb slot and noun class information a the basis for backing off in our probability model we achieve reduction in the error rate over an informed baseline indicating the potential of our approach for a task that ha heretofore relied on large amount of manually generated training data 
the computation of page importance in a huge dynamic graph ha recently attracted a lot of attention because of the web page importance or page rank is defined a the fixpoint of a matrix equation previous algorithm compute it off line and require the use of a lot of extra cpu a well a disk resource e g to store maintain and read the link matrix we introduce a new algorithm opic that work on line and us much le resource in particular it doe not require storing the link matrix it is on line in that it continuously refines it estimate of page importance while the web graph is visited thus it can be used to focus crawling to the most interesting page we prove the correctness of opic we present adaptive opic that also work on line but adapts dynamically to change of the web a variant of this algorithm is now used by xyleme we report on experiment with synthetic data in particular we study the convergence and adaptiveness of the algorithm for various scheduling strategy for the page to visit we also report on experiment based on crawl of significant portion of the web 
web search query log contain trace of user search modification one strategy user employ is deleting term presumably to obtain greater coverage it is useful to model and automate term deletion when arbitrary search are conjunctively matched against a small hand constructed collection such a a hand built hierarchy or collection of high quality page matched with key phrase query with no match can have word deleted till a match is obtained we provide algorithm which perform substantially better than the baseline in predicting which word should be deleted from a reformulated query for increasing query coverage in the context of web search on small high quality collection 
two mode and co occurrence data have been frequently seen in the real world we address the issue of predicting unknown cooccurrence event from known event for this issue we propose a new method that naturally combine observable co occurrence event with their existing latent knowledge by using tlierarchical latent variable this method build a space efficient hierarchical latent variable model and estimate the probability parameter of the model with an em algorithm in this paper we focus on a data set of protein protein interaction a typical co occurrence data set and apply our method to the problem of predicting unknown protein protein interaction an important research issue in current computational biology using a real data set of protein protein interaction and latent knowledge we tested the performance of our method comparing it with other recent machine learning approach including support vector machine our experimental result show that our method clearly outperforms the predictive performance obtained by other approach the result indicate that both our idea of using existing knowledge a a latent variable and our inethodology impleinenting it are effective for drastically improving the predictive performance of existing unsupervised learning approach for cooccurrence data 
the google search engine ha enjoyed huge success with it web page ranking algorithm which exploit global rather than local hyperlink structure of the web using random walk here we propose a simple universal ranking algorithm for data lying in the euclidean space such a text or image data the core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data encouraging experimental result from synthetic image and text data illustrate the validity of our method 
past empirical work ha shown that learning multiple related task from data simultaneously can be advantageous in term of predictive performance relative to learning these task independently in this paper we present an approach to multi task learning based on the minimization of regularization functionals similar to existing one such a the one for support vector machine svms that have been successfully used in the past for single task learning our approach allows to model the relation between task in term of a novel kernel function that us a task coupling parameter we implement an instance of the proposed approach similar to svms and test it empirically using simulated a well a real data the experimental result show that the proposed method performs better than existing multi task learning method and largely outperforms single task learning using svms 
greedy importance sampling is an unbiased estimation technique that reduces the variance of standard importance sampling by explicitly searching for mode in the estimation objective previous work ha demonstrated the feasibility of implementing this method and proved that the technique is unbiased in both discrete and continuous domain in this paper we present a reformulation of greedy importance sampling that eliminates the free parameter from the original estimator and introduces a new regularization strategy that further reduces variance without compromising unbiasedness the resulting estimator is shown to be effective for difficult estimation problem arising in markov random field inference in particular improvement are achieved over standard mcmc estimator when the distribution ha multiple peaked mode 
receiver operating characteristic roc curve provide a powerful tool for visualizing and comparing classification result regression error characteristic rec curve generalize roc curve to regression rec curve plot the error tolerance on the xaxis versus the percentage of point predicted within the tolerance on the y axis the resulting curve estimate the cumulative distribution function of the error the rec curve visually present commonly used statistic the area over the curve aoc is a biased estimate of the expected error the r value can be estimated using the ratio of the aoc for a given model to the aoc for the null model user can quickly ass the relative merit of many regression function by examining the relative position of their rec curve the shape of the curve reveals additional information that can be used to guide modeling 
to improve performance in text categorization it is important to extract distinctive feature for each class this paper proposes topic difference factor analysis tdfa a a method to extract projection ax that reflect topic difference between two document set suppose all sentence vector that compose each document are projected onto projection ax tdfa obtains the ax that maximize the ratio between the document set a to the sum of squared projection by solving a generalized eigenvalue problem the ax are called topic difference factor tdf s by applying tdfa to the document set that belongs to a given class and a set of document that is misclassified a belonging to that class by an existent classifier we can obtain feature that take large value in the given class but small one in other class a well a feature that take large value in other class but small one in the given class a classifier wa constructed applying the above feature to complement the knn classifier a the result the micro averaged f measure for reuters improved from to 
the celebrated pagerank algorithm ha proved to be a very effective paradigm for ranking result of web search algorithm in this paper we refine this basic paradigm to take into account several evolving prominent feature of the web and propose several algorithmic innovation first we analyze feature of the rapidly growing frontier of the web namely the part of the web that crawler are unable to cover for one reason or another we analyze the effect of these page and find it to be significant we suggest way to improve the quality of ranking by modeling the growing presence of link rot on the web a more site and page fall out of maintenance finally we suggest new method of ranking that are motivated by the hierarchical structure of the web are more efficient than pagerank and may be more resistant to direct manipulation 
abstract principal component analysis pca ha been successfully applied to construct linear model of shape graylevel and motion in particular pca ha been widely used to model the variation in the appearance of people s face we extend previous work on facial modeling for tracking face in video sequence a they undergo significant change due to facial expression here we develop person specific facial appearance model psfam which use modular pca to model complex intra person appearance change such model require aligned vi sual training data in previous work this ha involved a time consuming and error prone hand alignment and cropping process instead we introduce parameterized component analysis to learn a subspace that is invariant to affine or higher order geometric transformation the automatic learning of a psfam given a training image sequence is posed a a continuous optimization problem and is solved with a mixture of stochastic and deterministic technique achieving sub pixel accuracy we illustrate the use of the d psfam model with several application including video conferencing realistic avatar animation and eye tracking 
the ability to recognize when an agent abandon a plan is an open problem in the plan recognition literature and is a significant problem if these method are to be applied in real system this paper present an explicit formal and implemented solution to the problem of recognizing when an agent ha abandoned one of it goal based on a theory of probabilistic model revision 
a standard view of memory consolidation is that episode are stored temporarily in the hippocampus and are transferred to the neocortex through replay various recent experimental challenge to the idea of transfer particularly for human memory are forcing it re evaluation however although there is independent neurophysiological evidence for replay short of transfer there are few theoretical idea for what it might be doing we suggest and demonstrate two important computational role associated with neocortical index 
we propose a new particle filter that incorporates a model of cost when generating particle the approach is motivated by the observation that the cost of accidentally not tracking hypothesis might be significant in some area of state space and next to irrelevant in others by incorporating a cost model into particle filtering state that are more critical to the system performance are more likely to be tracked automatic calculation of the cost model is implemented using an mdp value function calculation that estimate the value of tracking a particular state experiment in two mobile robot domain illustrate the appropriateness of the approach this paper address a primary deficiency of particle filter particle filter are insensitive to cost that might arise from the approximate nature of the particle representation their only criterion for generating a particle is the posterior likelihood of a state to illustrate this point consider the example of a space shuttle failure of the engine system are extremely unlikely even in the presence of evidence to the contrary should we therefore not track the possibility of such failure just because they are unlikely if failure to track such lowlikelihood event may incur high cost such a a mission failure these variable should be tracked even when their posterior probability is low this observation suggests that cost should be taken into consideration when generating particle in the filtering process this paper proposes a particle filter that generates particle according to a distribution that combine the posterior probability with a risk function the risk function measure the importance of a state location on future cumulative cost we obtain this risk function via an mdp that calculates the approximate future risk of decision made in a particular state experimental result in two robotic domain illustrate that our approach yield significantly better result than a particle filter insensitive to cost 
this paper deal with automatically learning the spatial distribution of a set of image that is given a sequence of image acquired from well separated location how can they be arranged to best explain their genesis the solution to this problem can be viewed a an instance of robot mapping although it can also be used in other context we examine the problem where only limited prior odometric information is available employing a feature based method derived from a probabilistic pose estimation framework initially a set of visual feature is selected from the image and correspondence are found across the ensemble the image are then localized by first assembling the small subset of image for which odometric confidence is high and sequentially inserting the remaining image localizing each against the previous estimate and taking advantage of any prior that are available we present experimental result validating the approach and demonstrating metrically and topologically accurate result over two large image ensemble finally we discus the result their relationship to the autonomous exploration of an unknown environment and their utility for robot localization and navigation 
stemming can improve retrieval accuracy but stemmer are language specific character n gram tokenization achieves many of the benefit of stemming in a language independent way but it use incurs a performance penalty we demonstrate that selection of a single n gram a a pseudo stem for a word can be an effective and efficient language neutral approach for some language 
recent web search technique augment traditional text matching with a global notion of importance based on the linkage structure of the web such a in google s pagerank algorithm for more refined search this global notion of importance can be specialized to create personalized view of importance for example importance score can be biased according to a user specified set of initially interesting page computing and storing all possible personalized view in advance is impractical a is computing personalized view at query time since the computation of each view requires an iterative computation over the web graph we present new graph theoretical result and a new technique based on these result that encode personalized view a partial vector partial vector are shared across multiple personalized view and their computation and storage cost scale well with the number of view our approach enables incremental computation so that the construction of personalized view from partial vector is practical at query time we present efficient dynamic programming algorithm for computing partial vector an algorithm for constructing personalized view from partial vector and experimental result demonstrating the effectiveness and scalability of our technique 
in this paper we use the rollout method for policy improvement to analyze a version of klondike solitaire this version sometimes called thoughtful solitaire ha all card revealed to the player but then follows the usual klondike rule a strategy that we establish using iterated rollouts win about twice a many game on average a an expert human player doe 
we describe a framework for reducing the space complexity of graph search algorithm such a a that use open and closed list to keep track of the frontier and interior node of the search space we propose a sparse representation of the closed list in which only a fraction of already expanded node need to be stored to perform the two function of the closed list preventing duplicate search effort and allowing solution extraction our proposal is related to earlier work on search algorithm that do not use a closed list at all korf and zhang however the approach we describe ha several advantage that make it effective for a wider variety of problem 
we describe a method to define and use subwebs user defined neighborhood of the internet subwebs help improve search performance by inducing a topic specific page relevance bias over a collection of document subwebs may be automatically identified using a simple algorithm we describe and used to provide highly relevant topic specific information retrieval using subwebs in a help and support topic we see marked improvement in precision compared to generic search engine result 
model selection is linked to model assessment which is the problem of comparing different model or model parameter for a specific learning task for supervised learning the standard practical technique is crossvalidation which is not applicable for semi supervised and unsupervised setting in this paper a new model assessment scheme is introduced which is based on a notion of stability the stability measure yield an upper bound to cross validation in the supervised case but extends to semi supervised and unsupervised problem in the experimental part the performance of the stability measure is studied for model order selection in comparison to standard technique in this area 
we propose a method for sequential bayesian kernel regression a is the case for the popular relevance vector machine rvm the method automatically identifies the number and location of the kernel our algorithm overcomes some of the computational difficulty related to batch method for kernel regression it is non iterative and requires only a single pas over the data it is thus applicable to truly sequential data set and batch data set alike the algorithm is based on a generalisation of importance sampling which allows the design of intuitively simple and efficient proposal distribution for the model parameter comparative result on two standard data set show our algorithm to compare favourably with existing batch estimation strategy standard batch method for kernel regression suffer from a computational drawback in that they are iterative in nature with a computational complexity that is normally cubic in the number of data point at each iteration a large proportion of the research effort in this area is devoted to the development of estimation algorithm with reduced computational complexity for the rvm for example a strategy is proposed in that exploit the structure of the marginal likelihood function to significantly reduce the number of computation in this paper we propose a full bayesian formulation for kernel regression on sequential data our algorithm is non iterative and requires only a single pas over the data it is equally applicable to batch data set by presenting the data point one at a time with the order of presentation being unimportant the algorithm is especially effective for large data set a opposed to batch strategy that attempt to find the optimal solution conditional on all the data the sequential strategy includes the data one at a time so that the poste 
we addressed two issue concerning the practical aspect of optimally scheduling web advertising proposed by langheinrich et al which scheduling maximizes the total number of click throughs for all banner advertisement one is the problem of multi impression in which two or more banner ad are impressed at the same time the other is inventory management which is important in order to prevent over selling and maximize revenue we propose efficient method which deal with these two issue 
learning in multiagent system suffers from the fact that both the state and the action space scale exponentially with the number of agent in this paper we are interested in using q learning to learn the coordinated action of a group of cooperative agent using a sparse representation of the joint state action space of the agent we first examine a compact representation in which the agent need to explicitly coordinate their action only in a predefined set of state next we use a coordination graph approach in which we represent the q value by value rule that specify the coordination dependency of the agent at particular state we show how q learning can be efficiently applied to learn a coordinated policy for the agent in the above framework we demonstrate the proposed method on the predator prey domain and we compare it with other related multiagent q learning method 
in the problem of probability forecasting the learner s goal is to output given a training set and a new object a suitable probability measure on the possible value of the new object s label an on line algorithm for probability forecasting is said to be well calibrated if the probability it output agree with the observed frequency we give a natural nonasymptotic formalization of the notion of well calibratedness which we then study under the assumption of randomness the object label pair are independent and identically distributed it turn out that although no probability forecasting algorithm is automatically well calibrated in our sense there exists a wide class of algorithm for multiprobability forecasting such algorithm are allowed to output a set ideally very narrow of probability measure which satisfy this property we call the algorithm in this class venn probability machine our experimental result demonstrate that a nearest neighbor venn probability machine performs reasonably well on a standard benchmark data set and one of our theoretical result asserts that a simple venn probability machine asymptotically approach the true conditional probability regardless and without knowledge of the true probability measure generating the example 
in this paper we introduce an approach to exploit knowledge represented in an ontology in answer to query to an information base we assume that the ontology is embedded in a knowledge base covering the domain of the information base the ontology is first of all to influence ranking of object in answer to query a measured by similarity to the query we consider a generative framework where an ontology in combination with a concept language defines a set of well formed concept wellformed concept is assumed to be the basis for an indexing of the information base in the sense that these concept appear a descriptor attached to object in the base concept are thus applied to obtain a mean for description that generalizes simple word based information base indexing in effect query evaluation is generalized to be a matter of comparison at the level of concept rather than word 
we present a unified view for online classification regression and uniclass problem this view lead to a single algorithmic framework for the three problem we prove worst case loss bound for various algorithm for both the realizable case and the non realizable case the end result is new algorithm and accompanying loss bound for hinge loss regression and uniclass we also get refined loss bound for previously studied classification algorithm 
abstract projection matrix from projective space p and method for extracting the non rigid structure and motion for each application keywords dynamic structure from motion multiple view geometry multi linear constraint 
we investigate the problem of reducing the complexity of a graphical model g pg by finding a subgraph h of g chosen from a class of subgraphsh such that h is optimal with respect to kl divergence we do this by first defining a decomposition tree representation for g which is closely related to the junction tree representation for g we then give an algorithm which us this representation to compute the optimal h h gavril and tarjan have used graph separation property to solve several combinatorial optimization problem when the size of the minimal separator in the graph is bounded we present an extension of this technique which applies to some important choice ofh even when the size of the minimal separator of g are arbitrarily large in particular this applies to problem such a finding an optimal subgraphical model over a k tree of a graphical model over a k tree for arbitrary k and selecting an optimal subgraphical model with a constant d fewer edge with respect to kl divergence can be solved in time polynomial in jv g j using this formulation 
a graph based prior is proposed for parametric semi supervised classification the prior utilizes both labelled and unlabelled data it also integrates feature from multiple view of a given sample e g multiple sensor thus implementing a bayesian form of co training an em algorithm for training the classifier automatically adjusts the tradeoff between the contribution of a the labelled data b the unlabelled data and c the co training information active label query selection is performed using a mutual information based criterion that explicitly us the unlabelled data and the co training information encouraging result are presented on public benchmark and on measured data from single and multiple sensor 
in a multimodal conversation the way user communicate with a system depends on the available interaction channel and the situated context e g conversation focus visual feedback these dependency form a rich set of constraint from various perspective such a temporal alignment between different modality coherence of conversation and the domain semantics there is strong evidence that competition and ranking of these constraint is important to achieve an optimal interpretation thus we have developed an optimization approach for multimodal interpretation particularly for interpreting multimodal reference a preliminary evaluation indicates the effectiveness of this approach especially for complex user input that involve multiple referring expression in a speech utterance and multiple gesture 
we introduce a new type of local feature based on the phase and amplitude response of complex valued steerable filter the design of this local feature is motivated by a desire to obtain feature vector which are semi invariant under common image deformation yet distinctive enough to provide useful identity information a recent proposal for such local feature involves combining differential invariant to particular image deformation such a rotation our approach differs in that we consider a wider class of image deformation including the addition of noise along with both global and local brightness variation we use steerable filter to make the feature robust to rotation and we exploit the fact that phase data is often locally stable with respect to scale change noise and common brightness change we provide empirical result comparing our local feature with one based on differential invariant the result show that our phase based local feature lead to better performance when dealing with common illumination change and d rotation while giving comparable effect in term of scale change 
there exist many different generalization error bound for classification each of these bound contains an improvement over the others for certain situation our goal is to combine these different improvement into a single bound in particular we combine the pac bayes approach introduced by mcallester which is interesting for averaging classifier with the optimal union bound provided by the generic chaining technique developed by fernique and talagrand this combination is quite natural since the generic chaining is based on the notion of majorizing measure which can be considered a prior on the set of classifier and such prior also arise in the pac bayesian setting 
abstract we explore the consequence of viewing semantic association asthe result of attempting to predict the concept likely to arise in aparticular context we argue that the success of existing accountsof semantic representation come a a result of indirectly addressingthis problem and show that a closer correspondence to human datacan be obtained by taking a probabilistic approach that explicitlymodels the generative structure of language 
we show that state of a dynamical system can be usefully represented by multi step action conditional prediction of future observation state representation that are grounded in data in this way may be easier to learn generalize better and be le dependent on accurate prior model than for example pomdp state representation building on prior work by jaeger and by rivest and schapire in this paper we compare and contrast a linear specialization of the predictive approach with the state representation used in pomdps and in k order markov model ours is the flrst speciflc formulation of the predictive idea that includes both stochasticity and action control we show that any system ha a linear predictive state representation with number of prediction le than or equal to the number of state in it minimal pomdp model in predicting or controlling a sequence of observation the concept of state and state estimation inevitably arise there have been two dominant approach the generative model approach typifled by research on partially observable markov decision process hypothesizes a structure for generating observation and estimate it state and state dynamic the history based approach typifled by k order markov method us simple function of past observation a state that is a the immediate basis for prediction or control the data ow in these two approach are diagrammed in figure of the two the generative model approach is more general the model s internal state give it temporally unlimited memory the ability to remember an event that happened arbitrarily long ago whereas a history based approach can only remember a far back a it history extends the bane of generative model approach is that they are often strongly dependent on a good model of the system s dynamic most us of partially observable markov decision process pomdps for example assume a perfect dynamic model and attempt only to estimate state there are algorithm for simultaneously estimating state and dynamic e g chrisman analogous to the baum welch algorithm for the uncontrolled case baum et al but these are only efiective at 
structured method for query term replacement rely on separate estimate of term te of replacement probability statistically significantfrequency and document frequency to compute a weight for each query term this paper review prior work on structured query technique and introduces three new variant that leverage estima improvement in retrieval effectiveness are demonstrated for cross language retrieval and for retrieval based on optical character recognition when replacement probability are used to estimate both term frequency and document frequency 
roc analysis is increasingly being recognised a an important tool for evaluation and comparison of classifier when the operating characteristic i e class distribution and cost parameter are not known at training time usually each classi fier is characterised by it estimated true and false positive rate and is represented by a single point in the roc diagram in this paper we show how a single decision tree can represent a set of classifier by choosing different labellings of it leaf or equivalently an ordering on the leaf in this setting rather than estimating the accuracy of a single tree it make more sense to use the area under the roc curve auc a a quality metric we also propose a novel splitting criterion which chooses the split with the highest local auc to the best of our knowledge this is the first probabilistic splitting criterion that is not based on weighted average impurity we present experiment suggesting that the auc splitting criterion lead to tree with equal or better auc value without sacrificing accuracy if a single labelling is chosen 
the availability of web search ha revolutionised the way people discover information yet a search service maintain larger and larger index they are in danger of becoming a victim of their own success many common search can return a vast number of web page many of which will be irrelevant to the searcher and of which only about ten or twenty of the top ranked result will be browsed the problem is that while page returned by a search may be relevant to the keywords entered the keywords generally give only a partial expression of the searcher s information need personalised web search take keywords from the user a an expression of their information need but also us additional information about the user such a their preference community location or history to assist in determining the relevance of page there are many approach to providing personalised web search each with the aim of returning the result most relevant to the user ranked highest the feature that distinguish the approach are the kind of information about the user that is used the level of interaction with the user explicit or implicit collection of data how the information is stored client side or server side the algorithm used to incorporate the information about the user into the search and how information is presented to the user mobile device present some unique challenge in this respect some of these personalisation method stem from technique previously used in traditional information retrieval whilst others are unique to the web environment this chapter describes the many technique that have been applied to adapt the web search process to the individual user we also present a novel system that we are developing which us a client side user proflle to provide a personalised ranking of result from multiple search portal we conclude with a brief consideration of the future of personalised search and how it may afiect the development of the web 
many daily activity present information in the form of a stream of text and often people can benefit from additional information on the topic discussed tv broadcast news can be treated a one such stream of text in this paper we discus finding news article on the web that are relevant to news currently being broadcast we evaluated a variety of algorithm for this problem looking at the impact of inverse document frequency stemming compound history and query length on the relevance and coverage of news article returned in real time during a broadcast we also evaluated several postprocessing technique for improving the precision including reranking using additional term reranking by document similarity and filtering on document similarity for the best algorithm of the article found were relevant with at least of the article being on the exact topic of the broadcast in addition a relevant article wa found for at least of the topic 
abstract we describe an approach to machine learning from numerical data that combine both qualitative and numerical learning this approach is carried out in two stage induction of a qualitative model from numerical example of the behaviour of a physical system and induction of a numerical regression function that both respect the qualitative constraint and fit the training data numerically we call this approach q learning to the identification of a car wheel suspension system a complex industrially relevant mechanical system elsevier b v all right reserved keywords automated model building system identification machine learning qualitative reasoning learning 
this paper introduces a novel method for constructingand selecting scale invariant object part scale invariantlocal descriptor are first grouped into basic part a classifieris then learned for each of these part and featureselection is used to determine the most discriminative one this approach allows robust part detection and it is invariantunder scale change that is neither the training imagesnor the test image have to be normalized the proposed method is evaluated in car detectiontasks with significant variation in viewing condition andpromising result are demonstrated different local region classifier and feature selection method are quantitativelycompared our evaluation show that local invariantdescriptors are an appropriate representation for objectclasses such a car and it underline the importance offeature selection 
we present a generative model for the unsupervised learning of dependency structure we also describe the multiplicative combination of this dependency model with a model of linear constituency the product model outperforms both component on their respective evaluation metric giving the best published figure for unsupervised dependency parsing and unsupervised constituency parsing we also demonstrate that the combined model work and is robust cross linguistically being able to exploit either attachment or distributional regularity that are salient in the data 
the area under an roc curve auc is a criterion used in many application to measure the quality of a classification algorithm however the objective function optimized in most of these algorithm is the error rate and not the auc value we give a detailed statistical analysis of the relationship between the auc and the error rate including the first exact expression of the expected value and the variance of the auc for a fixed error rate our result show that the average auc is monotonically increasing a a function of the classification accuracy but th at the standard deviation for uneven distribution and higher error rate i s noticeable thus algorithm designed to minimize the error rate may not lead to the best possible auc value we show that under certain condition the global function optimized by the rankboost algorithm is exactly the auc we report the result of our experiment with rankboost in several datasets demonstrating the benefit of an algorithm specific ally designed to globally optimize the auc over other existing algorithm optimizing an approximation of the auc or only locally optimizing the auc 
admissible and consistent heuristic function are usually preferred in single agent heuristic search a they guarantee optimal solution with complete search method such a a and ida larger problem however frequently make a complete search intractable due to space and or time limitation in particular a path planning agent in a real time strategy game may need to take an action before it complete search ha the time to finish in such case incomplete search technique such a rta srta rtdp dta can be used such algorithm conduct a limited ply lookahead and then evaluate the state envisioned using a heuristic function the action selected on the basis of such evaluation can be suboptimal due to the incompleteness of search and inaccuracy in the heuristic it is usually believed that deeper lookahead increase the chance of taking the optimal action in this paper we demonstrate that this is not necessarily the case even when admissible and consistent heuristic function are used 
many criterion can be used to evaluate the performance of supervised learning different criterion are appropriate in different setting and it is not always clear which criterion to use a further complication is that learning method that perform well on one criterion may not perform well on other criterion for example svms and boosting are designed to optimize accuracy whereas neural net typically optimize squared error or cross entropy we conducted an empirical study using a variety of learning method svms neural net k nearest neighbor bagged and boosted tree and boosted stump to compare nine boolean classification performance metric accuracy lift f score area under the roc curve average precision precision recall break even point squared error cross entropy and probability calibration multidimensional scaling md show that these metric span a low dimensional manifold the three metric that are appropriate when prediction are interpreted a probability squared error cross entropy and calibration lay in one part of metric space far away from metric that depend on the relative order of the predicted value roc area average precision break even point and lift in between them fall two metric that depend on comparing prediction to a threshold accuracy and f score a expected maximum margin method such a svms and boosted tree have excellent performance on metric like accuracy but perform poorly on probability metric such a squared error what wa not expected wa that the margin method have excellent performance on ordering metric such a roc area and average precision we introduce a new metric sar that combine squared error accuracy and roc area into one metric md and correlation analysis show that sar is centrally located and correlate well with other metric suggesting that it is a good general purpose metric to use when more specific criterion are not known 
we describe web a where a system for associating geography with web page web a where locates mention of place and determines the place each name refers to in addition it assigns to each page a geographic focus a locality that the page discus a a whole the tagging process is simple and fast aimed to be applied to large collection of web page and to facilitate a variety of location based application and data analysis geotagging involves arbitrating two type of ambiguity geo non geo and geo geo a geo non geo ambiguity occurs when a place name also ha a non geographic meaning such a a person name e g berlin or a common word turkey geo geo ambiguity arises when distinct place have the same name a in london england v london ontario an implementation of the tagger within the framework of the webfountain data mining system is described and evaluated on several corpus of real web page precision of up to on individual geotags is achieved we also evaluate the relative contribution of various heuristic the tagger employ and evaluate the focus finding algorithm using a corpus pretagged with locality showing that a many a of the focus reported are correct up to the country level 
in the eye gaze tracking problem the goal is to determine where on a monitor screen a computer user is looking the gaze point existing system generally have one of two limitation either the head must remain fixed in front of a stationary camera or to allow for head motion the user must wear an obtrusive device we introduce a d eye tracking system where head motion is allowed without the need for marker or worn device we use a pair of stereo system a wide angle stereo system detects the face and steer an active narrow fov stereo system to track the eye at high resolution for high resolution tracking the eye is modeled in d including the corneal ball pupil and fovea in this paper we discus the calibration of the stereo system the eye model eye detection and tracking and we close with an evaluation of the accuracy of the estimated gaze point on the monitor 
document representation and indexing is a key problem for document analysis and processing such a clustering classification and retrieval conventionally latent semantic indexing lsi is considered effective in deriving such an indexing lsi essentially detects the most representative feature for document representation rather than the most discriminative feature therefore lsi might not be optimal in discriminating document with different semantics in this paper a novel algorithm called locality preserving indexing lpi is proposed for document indexing each document is represented by a vector with low dimensionality in contrast to lsi which discovers the global structure of the document space lpi discovers the local structure and obtains a compact document representation subspace that best detects the essential semantic structure we compare the proposed lpi approach with lsi on two standard database experimental result show that lpi provides better representation in the sense of semantic structure 
the growth of bioinformatics ha resulted in datasets with new characteristic these datasets typically contain a large number of column and a small number of row for example many gene expression datasets may contain column but only row such datasets pose a great challenge for existing closed frequent pattern discovery algorithm since they have an exponential dependence on the average row length in this paper we describe a new algorithm called carpenter that is specially designed to handle datasets having a large number of attribute and relatively small number of row several experiment on real bioinformatics datasets show that carpenter is order of magnitude better than previous closed pattern mining algorithm like closet and charm 
we study the caching of query result page in web search engine popular search engine receive million of query per day and efficient policy for caching query result may enable them to lower their response time and reduce their hardware requirement we present pdc probability driven cache a novel scheme tailored for caching search result that is based on a probabilistic model of search engine user we then use a trace of over seven million query submitted to the search engine altavista to evaluate pdc a well a traditional lru and slru based caching scheme the trace driven simulation show that pdc outperforms the other policy we also examine the prefetching of search result and demonstrate that prefetching can increase cache hit ratio by for large cache and can double the hit ratio of small cache when integrating prefetching into pdc we attain hit ratio of over 
this paper present some of the first data visualization and analysis of distribution for a lexicalized statistical parsing model in order to better understand their nature in the course of this analysis we have paid particular attention to parameter that include bilexical dependency the prevailing view ha been that such statistic are very informative but su er greatly from sparse data problem by using a parser to constrain parse it own output and by hypothesizing and testing for distributional similarity with back o distribution we have evidence that finally explains that a bilexical statistic are actually getting used quite often but that b the distribution are so similar to those that do not include head word a to be nearly indistinguishable insofar a making parse decision finally our analysis ha provided for the first time an e ective way to do parameter selection for a generative lexicalized statistical parsing model 
we present an examination of the state of the art for using value iteration to solve large scale discrete markov decision process we introduce an architecture which combine three independent performance enhancement the intelligent prioritization of computation state partitioning and massively parallel processing into a single algorithm we show that each idea improves performance in a different way meaning that algorithm designer do not have to trade one improvement for another we give special attention to parallelization issue discussing how to efficiently partition state distribute partition to processor minimize message passing and ensure high scalability we present experimental result which demonstrate that this approach solves large problem in reasonable time 
we attempt to understand visual classification in human using both psychophysical and machine learning technique frontal view of human face were used for a gender classification task human subject classified the face and their gender judgment reaction time and confidence rating were recorded several hyperplane learning algorithm were used on the same classification task using the principal component of the texture and shape representation of the face the classification performance of the learning algorithm wa estimated using the face database with the true gender of the face a label and also with the gender estimated by the subject we then correlated the human response to the distance of the stimulus to the separating hyperplane of the learning algorithm our result suggest that human classification can be modeled by some hyperplane algorithm in the feature space we used for classification the brain need more processing for stimulus close to that hyperplane than for those further away 
multibody factorization algorithm give an elegant and simple solution to the problem of structure from motion even for scene containing multiple independent motion despite this elegance it is still quite difficult to apply these algorithm to arbitrary scene first their performance deteriorates rapidly with increasing noise second they cannot be applied unless all the point can be tracked in all the frame a will rarely happen in real scene third they cannot incorporate prior knowledge on the structure or the motion of the object in this paper we present a multibody factorization algorithm that can handle arbitrary noise covariance for each feature a well a missing data we show how to formulate the problem a one of factor analysis and derive an expectation maximization based maximum likelihood algorithm one of the advantage of our formulation is that we can easily incorporate prior knowledge including the assumption of temporal coherence we show that this assumption greatly enhances the robustness of our algorithm and present result on challenging sequence 
we present an algorithm for generating referring expression in open domain existing algorithm work at the semantic level and assume the availability of a classification for attribute which is only feasible for restricted domain our alternative work at the realisation level relies on word net synonym and antonym set and give equivalent result on the example cited in the literature and improved result for example that prior approach cannot handle we believe that ours is also the first algorithm that allows for the incremental incorporation of relation we present a novel corpus evaluation using referring expression from the penn wall street journal treebank 
the online incremental gradient or backpropagation algorithm is widely considered to be the fastest method for solving large scale neural network nn learning problem in contrast we show that an appropriately implemented iterative batch mode or block mode learning method can be much faster for example it is three time faster in the uci letter classification problem output data item parameter with a two hidden layer multilayer perceptron and time faster in a nonlinear regression problem arising in color recipe prediction output data item parameter with a neuro fuzzy modular network the three principal innovative ingredient in our algorithm are the following first we use scaled trust region regularization with inner outer iteration to solve the associated overdetermined nonlinear least square problem where the inner iteration performs a truncated or inexact newton method second we employ pearlmutter s implicit sparse hessian matrix vector multiply algorithm to construct the krylov subspace used to solve for the truncated newton update third we exploit sparsity for preconditioning in the matrix resulting from the nns having many output 
we categorize the set of client communicating with a server on the web based on information that can be determined by the server the web server us the information to direct tailored action user with poor connectivity may choose not to stay at a web site if it take a long time to receive a page even if the web server at the site is not the bottleneck retaining such client may be of interest to a web site better connected client can receive enhanced representation of web page such a with higher quality image we explore a variety of consideration that could be used by a web server in characterizing a client once a client is characterized a poor or rich the server can deliver altered content alter how content is delivered alter policy and caching decision or decide when to redirect the client to a mirror site we also use network aware client clustering technique to provide a coarser level of client categorization and use it to categorize subsequent client from that cluster for which a client specific categorization is not available our result for client characterization and applicable server action are derived from real recent and diverse set of web server log our experiment demonstrate that a relatively simple characterization policy can classify poor client such that these client subsequently make the majority of badly performing request to a web server this policy is also stable in term of client staying in the same class for a large portion of the analysis period client clustering can significantly help in initially classifying client for which no previous information about the client is known we also show that different server action can be applied to a significant number of request sequence with poor performance 
abstract extends mcallester s pac bayesian theorem allowing for other tail bound than hoeding s and simplies the proof somewhat mcallester s pac bayesian theorem 
multinomial naive bayes classifier have been widely used for the probabilistic text classification however their parameter estimation method sometimes generates inappropriate probability in this paper we propose a topic document model approach for naive bayes text classification where their parameter are estimated with an expectation from the training document experiment are conducted on reuters and newsgroup collection and our proposed approach obtained a significant improvement in performace over the conventional approach 
the paper extends the notion of linear programming boosting to handle uneven datasets extensive experiment with text classification problem compare the performance of a number of dierent boosting strategy concentrating on the problem posed by uneven datasets 
in real world planning problem time for deliberation is often limited anytime planner are well suited for these problem they find a feasible solution quickly and then continually work on improving it until time run out in this paper we propose an anytime heuristic search ara which tune it performance bound based on available search time it start by finding a suboptimal solution quickly using a loose bound then tightens the bound progressively a time allows given enough time it find a provably optimal solution while improving it bound ara reuses previous search effort and a a result is significantly more efficient than other anytime search method in addition to our theoretical analysis we demonstrate the practical utility of ara with experiment on a simulated robot kinematic arm and a dynamic path planning problem for an outdoor rover 
previous work show that a web page can be partitioned into multiple segment or block and often the importance of those block in a page is not equivalent also it ha been proven that differentiating noisy or unimportant block from page can facilitate web mining search and accessibility however no uniform approach and model ha been presented to measure the importance of different segment in web page through a user study we found that people do have a consistent view about the importance of block in web page in this paper we investigate how to find a model to automatically assign importance value to block in a web page we define the block importance estimation a a learning problem first we use a vision based page segmentation algorithm to partition a web page into semantic block with a hierarchical structure then spatial feature such a position and size and content feature such a the number of image and link are extracted to construct a feature vector for each block based on these feature learning algorithm are used to train a model to assign importance to different segment in the web page in our experiment the best model can achieve the performance with micro f and micro accuracy which is quite close to a person s view 
to provide a compact generative representation of the sequential activity of a number of individual within a group there is a tradeoff between the defini tion of individual specific and global model this paper proposes a linear time distributed model for finite state symbolic sequence representing trace of individual user activity by making the assumption that heterogeneous user be havior may be explained by a relatively small number of structurally simple common behavioral pattern which may interleave randomly in a user specific proportion the result of an empirical study on three different source of user trace indicates that this modelling approach provides an efficient representation scheme reflected by improved prediction performance a well a providing low complexity and intuitively interpretable representation 
in this paper we show that efficient object recognition canbeobtained by combining informative feature with linearclassification the result demonstrate the superiority ofinformative class specific feature a compared with generic typefeatures such a wavelet for the task of object recognition weshow that information rich feature can reach optimal performancewith simple linear separation rule while generic feature basedclassifiers require more complex classification scheme this issignificant because efficient and optimal method have beendeveloped for space that allow linear separation to comparedifferent strategy for feature extraction we trained andcompared classifier working in feature space of the same lowdimensionality using two feature type image fragment v wavelet and two classification rule linear hyperplane and abayesian network the result show that by maximizing theindividual information of the feature it is possible to obtainefficient classification by a simple linear separating rule aswellas more efficient learning 
feature selection is the task of choosing a small set out of a given set of feature that capture the relevant property of the data in the context of supervised classification problem the relevance is determined by the given label on the training data a good choice of feature is a key for building compact and accurate classifier in this paper we introduce a margin based feature selection criterion and apply it to measure the quality of set of feature using margin we devise novel selection algorithm for multi class classification problem and provide theoretical generalization bound we also study the well known relief algorithm and show that it resembles a gradient ascent over our margin criterion we apply our new algorithm to various datasets and show that our new simba algorithm which directly optimizes the margin outperforms relief 
centralized resource description framework rdf repository have limitation both in their failure tolerance and in their scalability existing peer to peer p p rdf repository either cannot guarantee to find query result even if these result exist in the network or require up front definition of rdf schema and designation of super peer we present a scalable distributed rdf repository rdfpeers that store each triple at three place in a multi attribute addressable network by applying globally known hash function to it subject predicate and object thus all node know which node is responsible for storing triple value they are looking for and both exact match and range query can be efficiently routed to those node rdfpeers ha no single point of failure nor elevated peer and doe not require the prior definition of rdf schema query are guaranteed to find matched triple in the network if the triple exist in rdfpeers both the number of neighbor per node and the number of routing hop for inserting rdf triple and for resolving most query are logarithmic to the number of node in the network we further performed experiment that show that the triple storing load in rdfpeers differs by le than an order of magnitude between the most and the least loaded node for real world rdf data 
independent representation have recently attracted significant attention from the biological vision and cognitive science community it ha been argued that property such a sparseness and independence play a major role in visual perception and shown that imposing such property on visual representation originates receptive field similar to those found in human vision we present a study of the impact of feature independence in the performance of visual recognition architecture the contribution of this study are of both theoretical and empirical nature and support two main conclusion the first is that the intrinsic complexity of the recognition problem bayes error is higher for independent representation the increase can be significant close to in the database we considered the second is that criterion commonly used in independent component analysis are not sufficient to eliminate all the dependency that impact recognition in fact independent component can be le independent than previous representation such a principal component or wavelet base 
we devise a boosting approach to classification and regression based on column generation using a mixture of kernel traditional kernel method construct model based on a single positive semi definite kernel with the type of kernel predefined and kernel parameter chosen according to cross validation performance our approach creates model that are mixture of a library of kernel model and our algorithm automatically determines kernel to be used in the final model the norm and norm regularization method are employed to restrict the ensemble of kernel model the proposed method produce sparser solution and thus significantly reduces the testing time by extending the column generation cg optimization which existed for linear program with norm regularization to quadratic program with norm regularization we are able to solve many learning formulation by leveraging various algorithm for constructing single kernel model by giving different priority to column to be generated we are able to scale cg boosting to large datasets experimental result on benchmark data are included to demonstrate it effectiveness 
many real life optimization problem contain both hard and soft constraint a well a qualitative conditional preference however there is no single formalism to specify all three kind of information we therefore propose a framework based on both cp net and soft constraint that handle both hard and soft constraint a well a conditional preference efficiently and uniformly we study the complexity of testing the consistency of preference statement and show how soft constraint can faithfully approximate the semantics of conditional preference statement whilst improving the computational complexity 
niche search engine offer an efficient alternative to traditional search engine when the result returned by general purpose search engine do not provide a sufficient degree of relevance by taking advantage of their domain of concentration they achieve higher relevance and offer enhanced feature we discus a new niche search engine ebizsearch based on the technology of citeseer and dedicated to e business and e business document we present the integration of citeseer in the framework of ebizsearch and the process necessary to tune the whole system towards the specific area of e business we also discus how using machine learning algorithm we generate metadata to make ebizsearch open archive compliant ebizsearch is a publicly available service and can be reached at 
this paper develops a new representational model of similarity data that combine continuous dimension with discrete feature an algorithm capable of learning these representation is described and a bayesian model selection approach for choosing the appropriate number of dimension and feature is developed the approach is demonstrated on a classic data set that considers the similarity between the number through 
organizing web search result into cluster facilitates user quick browsing through search result traditional clustering technique are inadequate since they don t generate cluster with highly readable name in this paper we reformalize the clustering problem a a salient phrase ranking problem given a query and the ranked list of document typically a list of title and snippet returned by a certain web search engine our method first extract and rank salient phrase a candidate cluster name based on a regression model learned from human labeled training data the document are assigned to relevant salient phrase to form candidate cluster and the final cluster are generated by merging these candidate cluster experimental result verify our method s feasibility and effectiveness 
during the last ten year there ha been growing interest in the development of brain computer interface bcis the eld ha mainly been driven by the need of completely paralyzed patient to communicate with a few exception most human bcis are based on extracranial electroencephalography eeg however reported bit rate are still low one reason for this is the low signal to noise ratio of the eeg we are currently investigating if bcis based on electrocorticography ecog are a viable alternative in this paper we present the method and example of intracranial eeg recording of three epilepsy patient with electrode grid placed on the motor cortex the patient were asked to repeatedly imagine movement of two kind e g tongue or nger movement we analyze the classiability of the data using support vector machine svms and recursive channel elimination rce 
we consider learning in a markov decision process where we are not explicitly given a reward function but where instead we can observe an expert demonstrating the task that we want to learn to perform this setting is useful in application such a the task of driving where it may be difficult to write down an explicit reward function specifying exactly how different desideratum should be traded off we think of the expert a trying to maximize a reward function that is expressible a a linear combination of known feature and give an algorithm for learning the task demonstrated by the expert our algorithm is based on using inverse reinforcement learning to try to recover the unknown reward function we show that our algorithm terminates in a small number of iteration and that even though we may never recover the expert s reward function the policy output by the algorithm will attain performance close to that of the expert where here performance is measured with respect to the expert s unknown reward function 
sweetdeal is a rule based approach to representation of business contract that enables software agent to create evaluate negotiate and execute contract with substantial automation and modularity it build upon the situated courteous logic program knowledge representation in ruleml the emerging standard for semantic web xml rule here we newly extend the sweetdeal approach by also incorporating process knowledge description whose ontology are represented in daml oil emerging standard for semantic web ontology thereby enabling more complex contract with behavioral provision especially for handling exception condition e g late delivery or non payment that might arise during the execution of the contract this provides a foundation for representing and automating deal about service in particular about web service so a to help search select and compose them our system is also the first to combine emerging semantic web standard for knowledge representation of rule ruleml with ontology daml oil for a practical e business application domain and further to do so with process knowledge this also newly flesh out the evolving concept of semantic web service a prototype soon public is running 
in this paper we study the problem of constructing accurate decision tree model from data stream data stream are incremental task that require incremental online and any time learning algorithm one of the most successful algorithm for mining data stream is vfdt in this paper we extend the vfdt system in two direction the ability to deal with continuous data and the use of more powerful classification technique at tree leaf the proposed system vfdtc can incorporate and classify new information online with a single scan of the data in time constant per example the most relevant property of our system is the ability to obtain a performance similar to a standard decision tree algorithm even for medium size datasets this is relevant due to the any time property we study the behaviour of vfdtc in different problem and demonstrate it utility in large and medium data set under a bias variance analysis we observe that vfdtc in comparison to c is able to reduce the variance component 
pattern ordering is an important task in data mining because the number of pattern extracted by standard data mining algorithm often exceeds our capacity to manually analyze them in this paper we present an effective approach to address the pattern ordering problem by combining the rank information gathered from disparate source although rank aggregation technique have been developed for application such a meta search engine they are not directly applicable to pattern ordering for two reason first the technique are mostly supervised i e they require a sufficient amount of labeled data second the object to be ranked are assumed to be independent and identically distributed i i d an assumption that seldom hold in pattern ordering the method proposed in this paper is an adaptation of the original hedge algorithm modified to work in an unsupervised learning setting technique for addressing the i i d violation in pattern ordering are also presented experimental result demonstrate that our unsupervised hedge algorithm outperforms many alternative technique such a those based on weighted average ranking and singular value decomposition 
generating vehicle trajectory from video data is an important application of it intelligent transportation system we introduce a new tracking approach which us model based d vehicle detection and description algorithm our vehicle detection and description algorithm is based on a probabilistic line feature grouping and it is faster by up to an order of magnitude and more flexible than previous image based algorithm we present the system implementation and the vehicle detection and tracking result 
breaking news often contains timely definition and description of current term organization and personality we utilize such web source to construct definition for such term previous work ha identified definition using hand crafted rule or supervised learning that construct rigid hard text pattern in contrast we demonstrate a new approach that us flexible soft matching pattern to characterize definition sentence our soft pattern are able to effectively accommodate the diversity of definition sentence structure exhibited in news we use pseudo relevance feedback to automatically label sentence for use in soft pattern generation the application of our unsupervised method significantly improves baseline system on both the standardized trec corpus a well a crawled online news article by and respectively in term of f measure when applied to a state of art definition generation system recently fielded in the trec definitional question answering task it improves the performance by 
in this paper we will present three browsing system that should save user s time the first us named entity and give a way to reduce search space by using a information visualization system the user can comprehend more easily the content of a corpus or a document named entity are highlighted for quick reading temporal and geographic representation give a global view of the result of a query all these browse and search help seem to be very useful nevertheless an evaluation would give more practical result 
the resource constrained project scheduling problem with time window rcpsp max is an important generalization of a number of well studied scheduling problem in this paper we present a new heuristic algorithm that combine the benefit of squeaky wheel optimization with an effective conflict resolution mechanism called bulldozing to address rcpsp max problem on a range of benchmark problem the algorithm is competitive with state of the art systematic and non systematic method and scale well 
we present an unsupervised approach to recognizing discourse relation of contrast explanation evidence condition and elaboration that hold between arbitrary span of text we show that discourse relation classifier trained on example that are automatically extracted from massive amount of text can be used to distinguish between some of these relation with accuracy a high a even when the relation are not explicitly marked by cue phrase 
edge in man made environment grouped according to vanishing point direction provide single view constraint that have been exploited before a a precursor to both scene understanding and camera calibration a bayesian approach to edge grouping wa proposed in the manhattan world paper by coughlan and yuille where they assume the existence of three mutually orthogonal vanishing direction in the scene we extend the thread of work spawned by coughlan and yuille in several signicant way we propose to use the expectation maximization em algorithm to perform the search over all continuous parameter that inuence the location of the vanishing point in a scene because em behaves well in high dimensional space our method can optimize over many more parameter than the exhaustive and stochastic algorithm used previously for this task among other thing this let u optimize over multiple group of orthogonal vanishing direction each of which induces one additional degree of freedom em is also well suited to recursive estimation of the kind needed for image sequence and or in mobile robotics we present experimental result on image of atlanta world complex urban scene with multiple orthogonal edge group that validate our approach we also show result for continuous relative orientation estimation on a mobile robot 
magnetic resonance imaging mri is used in clinical routine to map the brain s morphology structural change due to brain growth aging surgical intervention or pathological process may be detected by image registration of time series imaging data to monitor structural change a three step approach is pursued here rigid registration and intensity matching of an initial reference and follow up mri scan a non rigid registration of the scan and the segmentation of the resulting displacement field cro correlation is used a a similarity measure for rigid registration non rigid registration is based on a fluid dynamical model the resulting displacement field are usually large and therefore hard to interpret for a simplified but sufficient description of such vector field contraction mapping is proposed to detect vector field singularity this enables the detection and analysis of singularity of any order a critical point which reflect the topology of the vector field an application demonstrates how this method help to increase the understanding of pathological process in the brain 
in this paper we propose to combine two powerful idea boosting and manifold learning on the one hand we improve adaboost by incorporating knowledge on the structure of the data into base classifier design and selection on the other hand we use adaboost s efficient learning mechanism to significantly improve supervised and semi supervised algorithm proposed in the context of manifold learning beside the specific manifold based penalization the resulting algorithm also accommodates the boosting of a large family of regularized learning algorithm 
the effort necessary to construct labeled set of example in a supervised learning scenario is often disregarded though in many application it is a time consuming and expensive procedure while this already constitutes a major issue in classification learning it becomes an even more serious problem when dealing with the more complex target domain of total order over a set of alternative considering both the pairwise decomposition and the constraint classification technique to represent label ranking function we introduce a novel generalization of pool based active learning to address this problem 
this paper demonstrates the applicability of auto mated reasoning to text processing specifically to question answering it is shown that the approach is feasible effective and scalable a logic prover ha been implemented and integrated into a stateof the art question answering system 
this paper address the problem of extending an adaptive information filtering system to make decision about the novelty and redundancy of relevant document it argues that relevance and redundance should each be modelled explicitly and separately a set of five redundancy measure are proposed and evaluated in experiment with and without redundancy threshold the experimental result demonstrate that the cosine similarity metric and a redundancy measure based on a mixture of language model are both effective for identifying redundant document 
clustering algorithm typically operate on a feature vector representation of the data and find cluster that are compact with respect to an assumed dis similarity measure between the data point in feature space this make the type of cluster identified highly dependent on the assumed similarity measure building on recent work in this area we formally define a class of spatially varying dissimilarity measure and propose algorithm to learn the dissimilarity measure automatically from the data the idea is to identify cluster that are compact with respect to the unknown spatially varying dissimilarity measure our experiment show that the proposed algorithm are more stable and achieve better accuracy on various textual data set when compared with similar algorithm proposed in the literature 
traditional vector based model use word co occurrence count from large corpus to represent lexical meaning in this paper we present a novel approach for constructing semantic space that take syntactic relation into account we introduce a formalisation for this class of model and evaluate their adequacy on two modelling task semantic priming and automatic discrimination of lexical relation 
event management is a focal point in building and maintaining high quality information infrastructure we have witnessed the shift of the paradigm of event management in practice from root cause analysis rca to action oriented analysis aoa ibm ha developed a pioneer event management methodology emd based on the aoa paradigm and applied it to more than two hundred production site with success foreseeably more and more event management professional will apply aoa in different incarnation in building proactive management facility by that building correct and effective event relationship network ern becomes the dominating activity in aoa service design process currently the quality of ern and the cost of building them largely depend on the knowledge of domain expert we believe that we can utilize historical event log in shortening the ern design process and perfecting the quality of ern in this paper we describe in detail how to apply this data driven approach in ern validation completion and construction 
we present a hierarchical bayesian model for learning efficient code of higher order structure in natural image the model a non linear generalization of independent component analysis replaces the standard assumption of independence for the joint distribution of coefficient with a distribution that is adapted to the variance structure of the coefficient of an efficient image basis this offer a novel description of higherorder image structure and provides a way to learn coarse coded sparsedistributed representation of abstract image property such a object location scale and texture 
information extraction ie aim at extracting specific information from a collection of document a lot of previous work on from semi structured document in xml or html us learning technique based on string some recent work convert the document to a ranked tree and us tree automaton induction this paper introduces an algorithm that us unranked tree to induce an automaton experiment show that this give the best result obtained so far for ie from semi structured document based on learning 
we describe a novel approach to the visualization of hierarchical clustering that superimposes the classical dendrogram over a fully synchronized low dimensional embedding thereby gaining the benefit of both approach in a single image one can view all the cluster examine the relation between them and study many of their property the method is based on an algorithm for low dimensional embedding of clustered data with the property that separation between all cluster is guaranteed regardless of their nature in particular the algorithm wa designed to produce embeddings that strictly adhere to a given hierarchical clustering of the data so that every two disjoint cluster in the hierarchy are drawn separately 
this paper present a new approach for identifying and eliminating mislabeled instance in large or distributed datasets we first partition a dataset into subset each of which is small enough to be processed by an induction algorithm at one time we construct good rule from each subset and use the good rule to evaluate the whole dataset for a given instance ik two error count variable are used to count the number of time it ha been identified a noise by all subset the instance with higher error value will have a higher probability of being a mislabeled example two threshold scheme majority and non objection are used to identify the noise experimental result and comparative study from real world datasets are reported to evaluate the effectiveness and efficiency of the proposed approach 
this paper describes a pde based method for densedepth extraction from multiple wide baseline image emphasislies on the usage of only a small amount of image the integration of these multiple wide baseline view isguided by the relative confidence that the system ha in thematching to different view this weighting is fine grainedin that it is determined for every pixel at every iteration reliable information spread fast at the expense of le reliabledata both in term of spatial communication withina view and in term of information exchange between theviews change in intensity between image can be handledin a similar fine grained fashion 
alternative splicing a is an important and frequent step in mammalian gene expression that allows a single gene to specify multiple product and is crucial for the regulation of fundamental biological process the extent of a regulation and the mechanism involved are not well understood we have developed a custom dna microarray platform for surveying a level on a large scale we present here a generative model for the a array platform genasap and demonstrate it utility for quantifying a level in different mouse tissue learning is performed using a variational expectation maximization algorithm and the parameter are shown to correctly capture expected a trend a comparison of the result obtained with a well established but low through put experimental method demonstrate that a level obtained from genasap are highly predictive of a level in mammalian tissue biological diversity through alternative splicing current estimate place the number of gene in the human genome at approximately which is a surprisingly small number when one considers that the genome of yeast a singlecelled organism ha gene the number of gene alone cannot account for the complexity and cell specialization exhibited by higher eukaryote i e mammal plant etc some of that added complexity can be achieved through the use of alternative splicing whereby a single gene can be used to code for a multitude of product gene are segment of the double stranded dna that contain the information required by the cell for protein synthesis that information is coded using an alphabet of a c g and t corresponding to the four nucleotide that make up the dna in what is known a the central dogma of molecular biology dna is transcribed to rna which in turn is translated into protein messenger rna mrna is synthesized in the nucleus of the cell and carry the genomic information to the ribosome in eukaryote gene are generally comprised of both exon which contain the information needed by the cell to synthesize protein and intron sometimes referred to a spacer dna which are spliced out of the pre mrna to create mature mrna an estimated of human gene can be 
personal webservers have proven to be a popular mean of sharing file and peer collaboration unfortunately the transient availability and rapidly evolving content on such host render centralized crawl based search index stale and incomplete to address this problem we propose yousearch a distributed search application for personal webservers operating within a shared context e g a corporate intranet with yousearch search result are always fast fresh and complete property we show arise from an architecture that exploit both the extensive distributed resource available at the peer webservers in addition to a centralized repository of summarized network state yousearch extends the concept of a shared context within web community by enabling peer to aggregate into group and user to search over specific group in this paper we describe the challenge design implementation and experience with a successful intranet deployment of yousearch 
dopamine exerts two class of effect on the sustained neural activity in prefrontal cortex that underlies working memory direct release in the cortex increase the contrast of prefrontal neuron enhancing the robustness of storage release of dopamine in the striatum is associated with salient stimulus and make medium spiny neuron bistable this modulation of the output of spiny neuron affect prefrontal cortex so a to indirectly gate access to working memory and additionally damp sensitivity to noise existing model have treated dopamine in one or other structure or have addressed basal ganglion gating of working memory exclusive of dopamine effect in this paper we combine these mechanism and explore their joint effect we model a memory guided saccade task to illustrate how dopamine s action lead to working memory that is selective for salient input and ha increased robustness to distraction 
this paper discus the application of the expectation maximization em clustering algorithm to the task of chinese verb sense discrimination the model utilized rich linguistic feature that capture predicate argument structure information of the target verb a semantic taxonomy for chinese noun which wa built semi automatically based on two electronic chinese semantic dictionary wa used to provide semantic feature for the model purity and normalized mutual information were used to evaluate the clustering performance on chinese verb the experimental result show that the em clustering model can learn sense or sense group distinction for most of the verb successfully we further enhanced the model with certain fine grained semantic category called lexical set our result indicate that these lexical set improve the model s performance for the three most challenging verb chosen from the first set of experiment 
we investigate the problem of learning a classiflcation task for datasets which are described by matrix row and column of these matrix correspond to object where row and column object may belong to difierent set and the entry in the matrix express the relationship between them we interpret the matrix element a being produced by an unknown kernel which operates on object pair and we show that under mild assumption these kernel correspond to dot product in some unknown feature space minimizing a bound for the generalization error of a linear classifler which ha been obtained using covering number we derive an objective function for model selection according to the principle of structural risk minimization the new objective function ha the advantage that it allows the analysis of matrix which are not positive deflnite and not even symmetric or square we then consider the case that row object are interpreted a feature we suggest an additional constraint which imposes sparseness on the row object and show that the method can then be used for feature selection finally we apply this method to data obtained from dna microarrays where column object correspond to sample row object correspond to gene and matrix element correspond to expression level benchmark are conducted using standard one gene classiflcation and support vector machine and k nearest neighbor after standard feature selection our new method extract a sparse set of gene and provides superior classiflcation result 
this paper study the inference of d shape from a set of n noisy photo we derive a probabilistic framework to specify what one can infer about d shape for arbitrarily shaped lambertian scene and arbitrary viewpoint configuration based on formal definition of visibility occupancy emptiness and photo consistency the theoretical development yield a formulation of the photo hull distribution the tightest probabilistic bound on the scene s true shape that can be inferred from the photo we show how to express this distribution in term of image measurement represent it compactly by assigning an occupancy probability to each point in space and design a stochastic reconstruction algorithm that draw fair sample i e d photo hull from it we also present experimental result for complex d scene 
the paper address the problem of automatic abstraction of component variable in the context of model based diagnosis in order to produce model capable of deriving fewer and more general diagnosis when the current observability of the system is reduced the notion of indiscriminability among fault of a set of component is introduced and constitutes the basis for a formal definition of admissible abstraction which preserve all the distinction that are relevant for diagnosis given the current observability of the system the automatic synthesis of abstract model further restricts abstraction such that the behavior of abstract component is expressed in term of a simple and intuitive combination of the behavior of their subcomponents a a validation of our proposal we present experimental result which show the reduction in the number of diagnosis returned by a diagnostic agent for a space robotic arm 
the denoising of color image is an increasingly studied problem whose state of the art solution employ a variety of diffusion scheme specifying the correct diffusion is difficult however in part because of the subtlety of color interaction we address this difficulty by proposing a perceptual organization approach to color denoising based on the principle of good continuation we exploit the periodic chromatic hue component of the color in it representation a a frame field we derive two hue curvature and use them to construct a local model for the behavior of the color which in turn specifies consistency constraint between nearby color measurement these constraint are then used to replace noisy pixel by examining their spatial context such a contextual analysis combined with standard method to handle the scalar channel saturation and lightness result in a robust noise removal process that preserve discontinuity singularity and fine chromatic structure including those that diffusion process are prone to distort we demonstrate our approach on a variety of synthetic and natural image 
a recent area of significant progress in speaker recognition is the use of high level feature idiolect phonetic relation prosody discourse structure etc a speaker not only ha a distinctive acoustic sound but us language in a characteristic manner large corpus of speech data available in recent year allow experimentation with long term statistic of phone pattern word pattern etc of an individual we propose the use of support vector machine and term frequency analysis of phone sequence to model a given speaker to this end we explore technique for text categorization applied to the problem we derive a new kernel based upon a linearization of likelihood ratio scoring we introduce a new phone based svm speaker recognition approach that half the error rate of conventional phone based approach 
given an image or video clip or audio song how do we automatically assign keywords to it the general problem is to find correlation across the medium in a collection of multimedia object like video clip with color and or motion and or audio and or text script we propose a novel graph based approach mmg to discover such cross modal correlation our mmg method requires no tuning no clustering no user determined constant it can be applied to any multimedia collection a long a we have a similarity function for each medium and it scale linearly with the database size we report auto captioning experiment on the standard corel image database of mb where it outperforms domain specific fine tuned method by up to percentage point in captioning accuracy relative improvement 
a number of medically important disease causing bacteria collectively called gram negative bacteria are noted for the extra outer membrane that surround their cell protein resident in this membrane outer membrane protein or omps are of primary research interest for antibiotic and vaccine drug design a they are on the surface of the bacteria and so are the most accessible target to develop new drug against with the development of genome sequencing technology and bioinformatics biologist can now deduce all the protein that are likely produced in a given bacteria and have attempted to classify where protein are located in a bacterial cell however such protein localization program are currently least accurate when predicting omps and so there is a current need for the development of a better omp classifier data mining research suggests that the use of frequent pattern ha good performance in aiding the development of accurate and efficient classification algorithm in this paper we present two method to identify omps based on frequent subsequence and test them on all gram negative bacterial protein whose localization have been determined by biological experiment one classifier follows an association rule approach while the other is based on support vector machine svms we compare the proposed method with the state of the art method in the biological domain the result demonstrate that our method are better both in term of accurately identifying omps and providing biological insight that increase our understanding of the structure and function of these important protein 
an adaptive information filtering system monitor a document stream to identify the document that match information need specified by user profile a the system filter it also refines it knowledge about the user s information need based on long term observation of the document stream and periodic feedback training data from the user low variance profile learning algorithm such a rocchio work well at the early stage of filtering when the system ha very few training data low bias profile learning algorithm such a logistic regression work well at the later stage of filtering when the system ha accumulated enough training data however an empirical system need to work well consistently at all stage of filtering process this paper address this problem by proposing a new technique to combine different text classification algorithm via a constrained maximum likelihood bayesian prior this technique provides a trade off between bias and variance and the combined classifier may achieve a consistent good performance at different stage of filtering we implemented the proposed technique to combine two complementary classification algorithm rocchio and logistic regression the new algorithm is shown to compare favorably with rocchio logistic regression and the best method in the trec and trec adaptive filtering track 
we model the dynamic geometry of a time varying scene a a d isosurface in space time the intersection of the isosurface with plane of constant time yield the geometry at a single time instant an optimal fit of our model to multiple video sequence is defined a the minimum of an energy functional this functional is given by an integral over the entire hypersurface which is designed to optimize photo consistency a pde based evolution derived from the euler lagrange equation maximizes consistency with all of the given video data simultaneously the result is a d model of the scene which varies smoothly over time the geometry reconstructed by this scheme is significantly better than result obtained by space carving approach that do not enforce temporal coherence 
security of computer system is essential to their acceptance and utility computer security analyst use intrusion detection system to assist them in maintaining computer system security this paper deal with the problem of differentiating between masquerader and the true user of a computer terminal prior efficient solution are le suited to real time application often requiring all training data to be labeled and do not inherently provide an intuitive idea of what the data model mean our system called admit relaxes these constraint by creating user profile using semi incremental technique it is a real time intrusion detection system with host based data collection and processing our method also suggests idea for dealing with concept drift and affords a detection rate a high a and a false positive rate a low a 
this paper report the result of our experimental study on a new method of applying an association rule miner to discover useful information from customer inquiry database in a call center of a company it ha been claimed that association rule mining is not suited for text mining to overcome this problem we propose to generate sequential data set of word with dependency structure from the japanese text database and to employ a new method for extracting meaningful association rule by applying a new rule selection criterion each inquiry in the sequential data wa represented a a list of word pair each of which consists of a verb and it dependent noun the association rule were induced regarding each pair of word a an item the rule selection criterion come from our principle that we put heavier weight to co occurrence of multiple item more than single item occurrence we regarded a rule important if the existence of the item in the rule body significantly affect the occurrence of the item in the rule head the selected rule were then categorized to form meaningful information class with this method we succeeded in extracting useful information class from the text database which were not acquired by only simple keyword retrieval also inquiry with multiple aspect were properly classified into corresponding multiple category 
many supervised and unsupervised learning algorithm are very sensitive to the choice of an appropriate distance metric while classification task can make use of class label information for metric learning such information is generally unavailable in conventional clustering task some recent research sought to address a variant of the conventional clustering problem called semi supervised clustering which performs clustering in the presence of some background knowledge or supervisory information expressed a pairwise similarity or dissimilarity constraint however existing metric learning method for semi supervised clustering mostly perform global metric learning through a linear transformation in this paper we propose a new metric learning method which performs nonlinear transformation globally but linear transformation locally in particular we formulate the learning problem a an optimization problem and present two method for solving it through some toy data set we show empirically that our locally linear metric adaptation llma method can handle some difficult case that cannot be handled satisfactorily by previous method we also demonstrate the effectiveness of our method on some real data set 
we present a geometric view on bilingual lexicon extraction from comparable corpus which allows to re interpret the method proposed so far and identify unresolved problem this motivates three new method that aim at solving these problem empirical evaluation show the strength and weakness of these method a well a a significant gain in the accuracy of extracted lexicon 
nearest neighbor nn classification relies on the assumption that class conditional probability are locally constant this assumption becomes false in high dimension with finite sample due to the curse of dimensionality the nn rule introduces severe bias under these condition we propose a locally adaptive neighborhood morphing classification method to try to minimize bias we use local support vector machine learning to estimate an effective metric for producing neighborhood that are elongated along le discriminant feature dimension and constricted along most discriminant one a a result the class conditional probability can be expected to be approximately constant in the modified neighborhood whereby better classification performance can be achieved the efficacy of our method is validated and compared against other competing technique using a number of datasets 
in this paper we propose a document clustering method that strives to achieve a high accuracy of document clustering and the capability of estimating the number of cluster in the document corpus i e the model selection capability to accurately cluster the given document corpus we employ a richer feature set to represent each document and use the gaussian mixture model gmm together with the expectation maximization em algorithm to conduct an initial document clustering from this initial result we identify a set of discriminative featuresfor each cluster and refine the initially obtained document cluster by voting on the cluster label of each document using this discriminative feature set this self refinement process of discriminative feature identification and cluster label voting is iteratively applied until the convergence of document cluster on the other hand the model selection capability is achieved by introducing randomness in the cluster initialization stage and then discovering a value c for the number of cluster n by which running the document clustering process for a fixed number of time yield sufficiently similar result performance evaluation exhibit clear superiority of the proposed method with it improved document clustering and model selection accuracy the evaluation also demonstrate how each feature a well a the cluster refinement process contribute to the document clustering accuracy 
we introduce a novel approach to modeling the dynamic of human facial motion induced by the action of speech for the purpose of synthesis we represent the trajectory of a number of salient feature on the human face a the output of a dynamical system made up of two subsystem one driven by the deterministic speech input and a second driven by an unknown stochastic input inference of the model learning is performed automatically and involves an extension of independent component analysis to time depentend data using a shapetexture decompositional representation for the face we generate facial image sequence reconstructed from synthesized feature point position 
in this paper we propose a new approach to discover informative content from a set of tabular document or web page of a web site our system infodiscoverer first partition a page into several content block according to html tag in a web page based on the occurrence of the feature term in the set of page it calculates entropy value of each feature according to the entropy value of each feature in a content block the entropy value of the block is defined by analyzing the information measure we propose a method to dynamically select the entropy threshold that partition block into either informative or redundant informative content block are distinguished part of the page whereas redundant content block are common part based on the answer set generated from manually tagged news web site with a total of web page experiment show that both recall and precision rate are greater than that is using the approach informative block news article of these site can be automatically separated from semantically redundant content such a advertisement banner navigation panel news category etc by adopting infodiscoverer a the preprocessor of information retrieval and extraction application the retrieval and extracting precision will be increased and the indexing size and extracting complexity will also be reduced 
this paper present a simple yet profound idea by thinking about the relationship between and within term and document we can generate a richer representation that encompasses aspect of web link analysis a well a text analysis technique from information retrieval this paper show one path to this unified representation and demonstrates the use of eigenvector calculation from web link analysis by stepping through a simple example 
classification tree are one of the most popular type of classifier with ease of implementation and interpretation being among their attractive feature despite the widespread use of classification tree theoretical analysis of their performance is scarce in this paper we show that a new family of classification tree called dyadic classification tree dcts are near optimal in a minimax sense for a very broad range of classification problem this demonstrates that other scheme e g neural network support vector machine cannot perform significantly better than dcts in many case we also show that this near optimal performance is attained with linear in the number of training data complexity growing and pruning algorithm moreover the performance of dcts on benchmark datasets compare favorably to that of standard cart which is generally more computationally intensive and which doe not posse similar near optimality property our analysis stem from theoretical result on structural risk minimization on which the pruning rule for dcts is based 
this paper proposes a new approach for coreference resolution which us the bell tree to represent the search space and cast the coreference resolution problem a finding the best path from the root of the bell tree to the leaf node a maximum entropy model is used to rank these path the coreference performance on the and automatic content extraction ace data will be reported we also train a coreference system using the muc data and competitive result are obtained 
query expansion ha long been suggested a an effective way to resolve the short query and word mismatching problem a number of query expansion method have been proposed in traditional information retrieval however these previous method do not take into account the specific characteristic of web searching in particular of the availability of large amount of user interaction information recorded in the web query log in this study we propose a new method for query expansion based on query log the central idea is to extract probabilistic correlation between query term and document term by analyzing query log these correlation are then used to select high quality expansion term for new query the experimental result show that our log based probabilistic query expansion method can greatly improve the search performance and ha several advantage over other existing method 
in gene expression microarray data analysis selecting a small number of discriminative gene from thousand of gene is an important problem for accurate classification of disease or phenotype the problem becomes particularly challenging due to the large number of feature gene and small sample size traditional gene selection method often select the top ranked gene according to their individual discriminative power without handling the high degree of redundancy among the gene latest research show that removing redundant gene among selected one can achieve a better representation of the characteristic of the targeted phenotype and lead to improved classification accuracy hence we study in this paper the relationship between feature relevance and redundancy and propose an efficient method that can effectively remove redundant gene the efficiency and effectiveness of our method in comparison with representative method ha been demonstrated through an empirical study using public microarray data set 
the overwhelming success of the web a a mechanism for facilitating information retrieval and for conducting business transaction ha ledto an increase in the deployment of complex enterprise application these application typically run on web application server which assume the burden of managing many task such a concurrency memory management database access etc required by these application the performance of an application server depends heavily on appropriate configuration configuration is a difficult and error prone task dueto the large number of configuration parameter and complex interaction between them we formulate the problem of finding an optimal configuration for a given application a a black box optimization problem we propose a smart hill climbing algorithm using idea of importance sampling and latin hypercube sampling lh the algorithm is efficient in both searching and random sampling it consists of estimating a local function and then hill climbing in the steepest descent direction the algorithm also learns from past search and restarts in a smart and selective fashion using the idea of importance sampling we have carried out extensive experiment with an on line brokerage application running in a websphere environment empirical result demonstrate that our algorithm is more efficient than and superior to traditional heuristic method 
this paper proposes a unique map learning method for mobile robot based on the co visibility information of object i e the information on whether two object are visible at the same time or not from the current position this method first estimate empirical distance among the object using a simple heuristic a pair of object observed at the same time more frequently is likely to be located more closely together then it computes all the coordinate of the object by multidimensional scaling md technique in the latter part of this paper it is shown that the proposed method is able to learn qualitatively very accurate map though it us only such primitive information and that it is robust against some kind of object recognition error 
this paper explores the possibility of using a disk based inverted file structure for collaborative filtering our hypothesis is that this allows for faster calculation of prediction and also that early termination heuristic may be used to further speed up the filtering process and perhaps even improve the quality of the prediction in an experiment on the eachmovie dataset this wa tested our result indicate that searching the inverted file structure is many time faster than general in memory vector search even for very large profile the continue termination heuristic produce the best ranked prediction in our experiment and quit is the top performer in term of speed 
web based search engine such a google and northernlight return document that are relevant to a user query not answer to user question we have developed an architecture that augments existing search engine so that they support natural language question answering the process entail five step query modulation document retrieval passage extraction phrase extraction and answer ranking in this paper we describe some probabilistic approach to the last three of these stage we show how our technique apply to a number of existing search engine and we also present result contrasting three different method for question answering our algorithm probabilistic phrase reranking ppr using proximity and question type feature achieves a total reciprocal document rank of on the trec corpus our technique have been implemented a a web accessible system called nsir 
we propose a novel a framework for deriving approximation for intractable probabilistic model this framework is based on a free energy negative log marginal likelihood and can be seen a a generalization of adaptive tap and expectation propagation ep the free energy is constructed from two approximating distribution which encode different aspect of the intractable model such a single node constraint and coupling and are by construction consistent on a chosen set of moment we test the framework on a difficult benchmark problem with binary variable on fully connected graph and d grid graph we find good performance using set of moment which either specify factorized node or a spanning tree on the node structured approximation surprisingly the bethe approximation give very inferior result even on grid 
this paper address the probabilistic inference of geometric structure from image specifically of synthesizing range data to enhance the reconstruction of a d model of an indoor environment by using video image and very partial depth information in our method we interpolate the available range data using statistical inference learned from the concurrently available video image and from those sparse region where both range and intensity information is available the spatial relationship between the variation in intensity and range can be efficiently captured by the neighborhood system of a markov random field mrf in contrast to classical approach to depth recovery i e stereo shape from shading we can afford to make only weak prior assumption regarding specific surface geometry or surface reflectance function since we compute the relationship between existing range data and the image we start with experimental result show the feasibility of our method 
information graphic non pictorial graphic such a bar chart or line graph are an important component of multimedia document often such graphic convey information that is not contained elsewhere in the document thus document summarization must be extended to include summarization of information graphic this paper address our work on graphic summarization it argues that the message that the graphic designer intended to convey must play a major role in determining the content of the summary and it outline our approach to identifying this intended message and using it to construct the summary 
in this paper a new camera calibration algorithm is proposed which is from the quasi affine invariance of two parallel circle two parallel circle here mean two circle in one plane or in two parallel plane they are quite common in our life between two parallel circle and their image under a perspective projection we set up a quasi affine invariance especially if their image under a perspective projection are separate we find out an interesting distribution of the image and the virtual intersection of the image and prove that it is a quasi affine invariance the quasi affine invariance is very useful which is applied to identify the image of circular point after the image of the circular point are identified linear equation on the intrinsic parameter are established from which a camera calibration algorithm is proposed we perform both simulated and real experiment to verify it the result validate this method and show it accuracy and robustness compared with the method in the past literature the advantage of this calibration method are it is from parallel circle with minimal number it is simple by virtue of the proposed quasi affine invariance it doe not need any matching excepting it application on camera calibration the proposed quasiaffine invariance can also be used to remove the ambiguity of recovering the geometry of single axis motion by conic fitting method in and in the two literature three conic are needed to remove the ambiguity of their method while two conic are enough to remove it if the two conic are separate and the quasi affine invariance proposed by u is taken into account 
after many success statistical approach that have been popular in the parsing community are now making headway into natural language generation nlg these system are aimed mainly at surface realization and promise the same advantage that make statistic valuable for parsing robustness wide coverage and domain independence a recent experiment aimed to empirically verify the linguistic coverage for such a statistical surface realization component by generating transformed sentence from the penn treebank corpus this article present the empirical result of a similar experiment to evaluate the coverage of a purely symbolic surface realizer we present the problem facing a symbolic approach on the same task describe the result of it evaluation and contrast them with the result of the statistical method to help quantitatively determine the level of coverage currently obtained by nlg surface realizers 
this paper report on and discus a set of user experiment using the trec web interactive track protocol the focus is on comparing human and machine algorithm in term of performance in a topic distillation task we also investigated the effect of the search result layout in supporting the user effort we have demonstrated that machine can perform nearly a well a people on the topic distillation task given a system tailored to the task there is significant performance improvement and finally given a presentation that support the task there is strong user satisfaction 
we address the problem of camera motion and structurereconstruction from line correspondence across multiple view from initialization to final bundle adjustment one of the maindifficulties when dealing with line feature is their algebraicrepresentation first we consider the triangulation problem basedon pl cker coordinate to represent the line we propose amaximum likelihood algorithm relying on linearising thepl cker constraint and on a pl cker correction procedureto compute the closest pl cker coordinate to a given vector second we consider the bundle adjustment problem previous overparameterizations of d line induce gauge freedomsand or internal consistency constraint we propose the orthonormalrepresentation which allows handy non linear optimization of dlines using the minimum parameter within an unconstrainednon linear optimizer we compare our algorithm to existing one onsimulated and real data 
in statistical machine translation the generation of a translation hypothesis is computationally expensive if arbitrary word reordering are permitted the search problem is np hard on the other hand if we restrict the possible word reordering in an appropriate way we obtain a polynomial time search algorithm in this paper we compare two different reordering constraint namely the itg constraint and the ibm constraint this comparison includes a theoretical discussion on the permitted number of reordering for each of these constraint we show a connection between the itg constraint and the since known schr der number we evaluate these constraint on two task the verbmobil task and the canadian hansard task the evaluation consists of two part first we check how many of the viterbi alignment of the training corpus satisfy each of these constraint second we restrict the search to each of these constraint and compare the resulting translation hypothesis the experiment will show that the baseline itg constraint are not sufficient on the canadian hansard task therefore we present an extension to the itg constraint these extended itg constraint increase the alignment coverage from about to 
the body of work on multi body factorization separate between object whose motion are independent in this work we show that in many case object moving with different d motion will be captured a a single object using these approach we analyze what cause these degeneracy between object and suggest an approach for overcoming some of them we further show that in the case of multiple sequence linear dependency can supply information for temporal synchronization of sequence and for spatial matching of point across sequence 
this paper present a framework for finding point correspondencesin monocular image sequence over multiple frame the generalproblem of multi frame point correspondence is np hard for three ormore frame a polynomial time algorithm for a restriction of thisproblem is presented and is used a the basis of proposed greedyalgorithm for the general problem the greedy nature of theproposed algorithm allows it to be used in real time system fortracking and surveillance etc in addition the proposed algorithmdeals with the problem of occlusion missed detection and falsepositives by using a single non iterative greedy optimizationscheme and hence reduces the complexity of the overall algorithmas compared to most existing approach where multiple heuristicsare used for the same purpose while most greedy algorithm forpoint tracking do not allow for entry and exit of point from thescene this is not a limitation for the proposed algorithm experiment with real and synthetic data show that the proposedalgorithm outperforms the existing technique and is applicable inmore general setting 
abstract wepropose a computational model motivated by human cognitive process for detecting change of driving environment the model call dynamic visual model consists of three major component sensory perceptual and conceptual component the proposed model is used a the underlying framework in which a system for detecting and recognizing road sign is developed 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
the perception of transparent object from image is knownto be a very hard problem in vision given a single image it is difficult to even detect the presence of transparent objectsin the scene in this paper we explore what can be saidabout transparent object by a moving observer we showhow feature that are imaged through a transparent objectbehave differently from those that are rigidly attached to thescene we present a novel model based approach to recoverthe shape and the pose of transparent object from knownmotion the object can be complex in that they may be composedof multiple layer with different refractive index wehave conducted numerous simulation to verify the practicalfeasibility of our algorithm we have applied it to real scenesthat include transparent object and recovered the shape ofthe object with high accuracy 
prior research under a variety of condition ha shown the cori algorithm to be one of the most effective resource selection algorithm but the range of database size studied wa not large this paper show that the cori algorithm doe not do well in environment with a mix of small and very large database a new resource selection algorithm is proposed that us information about database size a well a database content we also show how to acquire database size estimate in uncooperative environment a an extension of the query based sampling used to acquire resource description experiment demonstrate that the database size estimate are more accurate for large database than estimate produced by a competing method the new resource ranking algorithm is always at least a effective a the cori algorithm and the new algorithm result in better document ranking than the cori algorithm 
in this paper we study a family of analytical probability model for image within the spectral representation framework first the input image is decomposed using a bank of filter and probability model are imposed on the filter output or spectral component a two parameter analytical form called a bessel k form derived based on a generator model is used to model the marginal probability of these spectral component the bessel k parameter can be estimated efficiently from the filtered image and extensive simulation using video infrared and range image have demonstrated bessel k form s fit to the observed histogram the effectiveness of bessel k form is also demonstrated through texture modeling and synthesis in contrast to numeric based dimension reduction representation which are derived purely based on numerical method the bessel k representation are derived based on object representation and this enables u to establish relationship between the bessel parameter and certain characteristic of the imaged object we have derived a pseudometric on the image space to quantify image similarity difference using an analytical expression for l metric on the set of bessel k form we have applied the bessel k representation to texture modeling and synthesis clutter classification pruning of hypothesis for object recognition and object classification result show that bessel k representation capture important image feature suggesting it role in building efficient image understanding paradigm and system 
link are one of the most important mean for navigation in the world wide web however the visualization of and the interaction with web link have been scarcely explored although link have severe implication on the appearance and usability of web page and the world wide web a such this paper present two study giving first insight of the effect of link visualization technique on reading habit and performance the first user study compare different highlighting technique for link marker and evaluates their effect on reading performance and user acceptance the second study examines link on demand link that appear when pressing a dedicated key and discus their possible effect on reading and browsing habit the finding of the conducted study imply that the standard appearance of link marker ha seriously underestimated effect on the usability of web page they can significantly reduce the readability of the text and alternative should be carefully considered for the design of future web browser 
we formulate the problem of solving stochastic linear operator equation in a bayesian ganssian process gp framework the solution is obtained in the spirit of a collocation method based on noisy evaluation of the target function at randomly drawn or deliberately chosen point prior knowledge about the solution is encoded by the covariance kernel of the gp a in gp regression analytical expression for the mean and variance of the estimated target function are obtained from which the solution of the operator equation follows by a manipulation of the kernel linear initial and boundary value constraint can be enforced by embedding the non parametric model in a form that automatically satisfies the boundary condition the method is illustrated on a noisy linear first order ordinary differential equation with initial condition and on a noisy second order partial differential equation with dirichlet boundary condition 
we demonstrate an adaptive search system that work proactively to help searcher find relevant information the system observes searcher interaction us what it see to model information need and chooses additional query term the system watch for change in the topic of the search and selects retrieval strategy that reflect the extent to which the topic is seen to change 
the independent lifestyle assistant i l s a is an agent based monitoring and support system to help elderly people to live longer in their home by reducing caregiver burden i l s a is a multiagent system that incorporates a unified sensing model situation assessment response planning real time response and machine learning this paper describes the some of the lesson we learned during the development and six month field study 
this paper describes two study that looked at user ability to formulate visual query with a content based image retrieval system that us dominant image colour a the primary indexing key the first experiment examined user performance with two visual search tool a sketch tool and a structured browsing tool with different type of image query the result showed that while user were able to successfully search on the basis of colour and were able to formulate visual query their ability to do so wa affected by search task type search task type wa also shown to be related to search tool choice however the result of study two showed that while user were able to complete all of the task there wa evidence to suggest that a degree of compromise wa present in the user choice of image that wa largely due to problem relating to query formulation 
visual action recognition is an important problem in computer vision in this paper we propose a new method to probabilistically model and recognize action of articulated object such a hand or bo dy gesture in image sequence our method consists of three level of representation at the low level we first extract a feature vector invar iant to scale and in plane rotation by using the fourier transform of a circular spatial histogram then spectral partitioning is utilized to obtain an initial clustering this clustering is then refined using a temporal smoothness constraint gaussian mixture model gmm based clustering and density estimation in the subspace of linear discriminant analysis lda are then applied to thousand of image feature vector to obtain an intermediate level representation finally at the high level we build a t emporal multiresolution histogram model for each action by aggregating the clustering weight of sampled image belonging to that action we discus how this high level representation can be extended to achieve temporal scaling invariance and to include bi gram or multi gram transition information both image clustering and action recognition segmentation result are given to show the validity of our three tiered representatio n 
we investigate phase transition for the family of bounded satisfiability problem sat b recently introduced by zhang that ask given a cnfformula is there a truth assignment that violates no more than b of it clause zhang s result were experimental and for a fixed number of variable n and suggested that the location of the phase transition for sat b are separated and move significantly a b increase analysis of these location wa posed a an open question we analytically show that the phase transition of all sat problem must occur within a narrow region regardless of how large the value of b is moreover our experiment reveal that the phase transition for these problem occur in a remarkable way specifically unlike sat the probability curve for sat do not have a quasi common intersection point about which they rotate a they become steeper with increasing n instead they move rapidly to the left toward the narrow region that the analysis predicts 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
within five year our personal computer with terabyte disk drive will be able to store everything we read write hear and many of the image we see including video vannevar bush outlined such a system in his famous memex article for the last four year we have worked on mylifebits www mylifebits com http www mylifebits com a system to digitally store everything from one s life including book article personal financial record memorabilia email written correspondence photo time location taken telephone call video television program and web page visited we recently added content from personal device that automatically record photo and audio the project started with the capture of bell s content followed by an effort to explore the use of the sql database for storage and retrieval work ha continued along these line to extend content capture from every useful source e g a meeting capture system the second phase of the project includes the design of tool and link for annotation collection cluster analysis facet for characterizing the content creation of timeline and story and other inherent database related capability e g the ability to pivot on an event or photo or person to retrieve linked information ideally we would like to have a system that would read every document extract meta data e g dublin core and classify it using multiple ontology faceted classification or the relevant while such a system ha implication for future computing device and their user these system will only exist if we can effectively utilize the vast personal store although our system is exploratory the stuff i ve seen system demonstrates the utility and necessity of easy search and access to one s own data other research effort with similar goal relating to personal information include haystack lifestreams and the uk memory for life grand challenge there are serious research issue beyond the problem of making the information useful through rapid and easy retrieval the dear appy problem dear appy my application or platform or medium left me unreadable signed lost data is unsettling to archivist and computer professional and must be solved just navigating the stored life of individual would at first glance appear to take almost a lifetime to sift through while we are making progress in the capture of le traditionally archived content e g meeting phone call video automatic interpretation and index of voice are illusive mylifebits is currently focused on retrieval including the hopefully automatic addition of meta data e g document type identification high level knowledge while such data is essential for the archivist it is unclear how useful such meta data is to a one s own information without such higher level knowledge and concept the vast amount of raw bit may be completely unusable the most cited problem of personal archive is the control of the content including personal security together with joint ownership of content by other individual and organization in many corporation periodic expunging of document is the standard similarly the aspect of a person s life not available in public document is owned by the organization and all document may have to be tagged in such a way that it can be expunged if necessary when an individual is no longer part of the organization the hppa law in the u and even more stringent privacy law in other county have major implication for personal store 
we present a new algorithm gm sarsa for finding approximate solution to multiple goal reinforcement learning problem that are modeled a composite markov decision process according to our formulation different sub goal are modeled a mdps that are coupled by the requirement that they share action existing reinforcement learning algorithm address similar problem formulation by first finding optimal policy for the component mdps and then merging these into a policy for the composite task the problem with such method is that policy that are optimized separately may or may not perform well when they are merged into a composite solution instead of searching for optimal policy for the component mdps in isolation our approach find good policy in the context of the composite task 
tangent distance td is one classical method for invariant pattern classification however conventional td need pre obtain tangent vector which is difficult except for image object this paper extends td to more general pattern classification task the basic assumption is that tangent vector can be approximately represented by the pattern variation we propose three probabilistic subspace model to encode the variation the linear subspace nonlinear subspace and manifold subspace model these three model are addressed in a unified view namely probabilistic tangent subspace pt experiment show that pt can achieve promising classification performance in non image data set 
we aim to infer d body pose directly from human silhouette given a visual input silhouette the objective is to recover the intrinsic body configuration recover the view point reconstruct the input and detect any spatial or temporal outlier in order to recover intrinsic body configuration pose from the visual input silhouette we explicitly learn view based representation of activity manifold a well a learn mapping function between such central representation and both the visual input space and the d body pose space the body pose can be recovered in a closed form in two step by projecting the visual input to the learned representation of the activity manifold i e finding the point on the learned manifold representation corresponding to the visual input followed by interpolating d pose 
we devise and experiment with a dynamical kernel based system for tracking hand movement from neural activity the state of the system corresponds to the hand location velocity and accelerati on while the system s input are the instantaneous spike rate the syste m s state dynamic is defined a a combination of a linear mapping from the previous estimated state and a kernel based mapping tailored for modeling neural activity in contrast to generative model the activity to state mapping is learned using discriminative method by minimizing a noise robust loss function we use this approach to predict hand trajecto ries on the basis of neural activity in motor cortex of behaving monkey and find that the proposed approach is more accurate than both a stati c approach based on support vector regression and the kalman filter 
the organization of html into a tag tree structure which is rendered by browser a roughly rectangular region with embedded text and href link greatly help surfer locate and click on link that best satisfy their information need can an automatic program emulate this human behavior and thereby learn to predict the relevance of an unseen href target page w r t an information need based on information limited to the href source page such a capability would be of great interest in focused crawling and resource discovery because it can fine tune the priority of unvisited url in the crawl frontier and reduce the number of irrelevant page which are fetched and discarded 
we describe the application of plan recognition technique to support human intelligence analyst in processing national security alert set by automatically identifying the hostile intent behind them identifying the intent enables u to both prioritize and explain the alert set for succinct user presentation our empirical evaluation demonstrates that the approach can handle alert set of a many a element and can readily distinguish between false and true alarm we discus the important opportunity for future work that will increase the cardinality of the alert set supported by the system to the level demanded by a deployable application in particular we outline opportunity to bring the analyst into the process and the opportunity for heuristic improvement to the plan recognition algorithm 
designing a hypothesis test to determine the best of two machine learning algorithm with only a small data set available is not a simple task many popular test suer from low power x cv or high type i error weka s x cross validation furthermore many test show a low level of replicability so that test performed by different scientist with the same pair of algorithm the same data set and the same hypothesis test still may present dierent result we show that x cv resampling and fold cv suer from low replicability the main complication is due to the need to use the data multiple time a a consequence independence assumption for most hypothesis test are violated in this paper we pose the case that reuse of the same data cause the eective degree of freedom to be much lower than theoretically expected we show how to calibrate the eective degree of freedom empirically for various test some test are not calibratable indicating another flaw in the design however the one that are calibratable all show very similar behavior moreover the type i error of those test is on the mark for a wide range of circumstance while they show a power and replicability that is a considerably higher than currently popular hypothesis test 
model checking is a promising approach to automatic verification which ha concentrated on specification expressed in temporal logic comparatively little attention ha been given to temporal logic of knowledge although such logic have been proven to be very useful in the specification of protocol for distributed system in this paper we address ourselves to the model checking problem for a temporal logic of knowledge halpern and vardi s logic of c k ln based on the semantics of interpreted system with local proposition we develop an approach to symbolic c k ln model checking via obdds in our approach to model checking specification involving agent knowledge the knowledge modality are eliminated via quantifier over agent non observable variable 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching is one approach that can be used to issue future prediction but it scale poorly with large data source and is unable to make intelligent prediction given previously unseen input data even when there is an obvious relationship between past input and the output it generated in this paper we describe a novel way to combine classification and transduction for a more efficient and accurate value prediction strategy one capable of issuing prediction about previously unseen hint we show how our approach result in significant speedup for plan that query multiple source or source that require multi page navigation 
the bounded hough transform is introduced to track object in a sequence of sparse range image the method is based upon a variation of the general hough transform that exploit the coherence across image frame that result from the relationship between known bound on the object s velocity and the sensor frame rate it is extremely efficient running in o n for n range data point and effectively trade off localization precision for runtime efficiency the method ha been implemented and tested on a variety of object including freeform surface using both simulated and real data from lidar and stereovision sensor the motion bound allow the inter frame transformation space to be reduced to a reasonable and indeed small size containing only possible state in a variation the rotational subspace is projected onto the translational subspace which further reduces the transformation space to only state experimental result confirm that the technique work well with very sparse data possibly comprising only ten of point per frame and that it is also robust to measurement error and outlier 
in this paper we develop a computational learning framework to build a hierarchy of consumer photo category for semantic retrieval two level of visual semantics are learned for image content and image category statistically we evaluate the average precision at top retrieved photo on heterogeneous consumer photo with very good result 
search algorithm in most current text retrieval system use index data structure extracted from the original text document in this paper we focus on reducing the size of the index by reducing the amount of space dedicated to store term frequency in experiment using trec ad hoc corpus and query set we show that it is possible to store the term frequency in only two bit without decreasing retrieval performance 
we show the existence of critical point a line for the likelihood function of mixture type model they are given by embedding of a critical point for model with le component a sufficient condition that the critical line give local maximum or saddle point is also derived based on this fact a component split method is proposed for a mixture of gaussian component and it effectiveness is verified through experiment this paper discus the critical point of the likelihood function for mixture type model by analyzing their hierarchical symmetric structure a generalization of we show that given a critical point of the likelihood for the model with h component duplication of any of the component give critical point a line for the model with h component we call them critical line of mixture model we derive also a sufficient condition that the critical line give maximum or saddle point of the larger model and show that given a maximum of the likelihood for a mixture of gaussian component an appropriate split of any component always give an ascending direction of the likelihood based on this theory we propose a stable method of splitting a component which work effectively with the em optimization for avoiding the dependency on the initial condition and improving the optimization the usefulness of the algorithm is verified through experiment 
paraphrase recognition is a critical step for natural language interpretation accordingly many nlp application would benefit from high coverage knowledge base of paraphrase however the scalability of state of the art paraphrase acquisition approach is still limited we present a fully unsupervised learning algorithm for web based extraction of entailment relation an extended model of paraphrase we focus on increased scalability and generality with respect to prior work eventually aiming at a full scale knowledge base our current implementation of the algorithm take a it input a verb lexicon and for each verb search the web for related syntactic entailment template experiment show promising result with respect to the ultimate goal achieving much better scalability than prior web based method 
a key feature of modern optimal planner such a graphplan and blackbox is their ability to prune large part of the search space previous partial order causal link pocl planner provide an alternative branching scheme but lacking comparable pruning mechanism do not perform a well in this paper a domain independent formulation of temporal planning based on constraint programming is introduced that successfully combine a pocl branching scheme with powerful and sound pruning rule the key novelty in the formulation is the ability to reason about support precedence and causal link involving action that are not in the plan experiment over a wide range of benchmark show that the resulting optimal temporal planner is much faster than current one and is competitive with the best parallel planner in the special case in which action have all the same duration 
we describe experiment carried out with adaptive language and translation model in the context of an interactive computer assisted translation program we developed cache based language model which were then extended to the bilingual case for a cachebased translation model we present the improvement we obtained in two context in a theoretical setting we achieved a drop in perplexity for the new model and in a more practical situation simulating a user working with the system we showed that fewer keystroke would be needed to enter a translation 
machine learning is often used to automatically solve human task in this paper we look for task where machine learning algorithm are not a good a human with the hope of gaining insight into their current limitation we studied various human interactive proof hip on the market because they are system designed to tell computer and human apart by posing challenge presumably too hard for computer we found that most hip are pure recognition task which can easily be broken using machine learning the harder hip use a combination of segmentation and recognition task from this observation we found that building segmentation task is the most effective way to confuse machine learning algorithm this ha enabled u to build effective hip which we deployed in msn passport a well a design challenging segmentation task for machine learning algorithm 
we extend recent work on the connection between loopy belief propagation and the bethe free energy constrained minimization of the bethe free energy can be turned into an unconstrained saddle point problem both converging double loop algorithm and standard loopy belief propagation can be interpreted a attempt to solve this saddle point problem stability analysis then lead u to conclude that stable fixed point of loopy belief propagation must be local minimum of the bethe free energy 
most existing clustering algorithm cluster highly related data object such a web page and web user separately the interrelation among different type of data object is either not considered or represented by a static feature space and treated in the same way a other attribute of the object in this paper we propose a novel clustering approach for clustering multi type interrelated data object recom reinforcement clustering of multi type interrelated data object under this approach relationship among data object are used to improve the cluster quality of interrelated data object through an iterative reinforcement clustering process at the same time the link structure derived from relationship of the interrelated data object is used to differentiate the importance of object and the learned importance is also used in the clustering process to further improve the clustering result experimental result show that the proposed approach not only effectively overcomes the problem of data sparseness caused by the high dimensional relationship space but also significantly improves the clustering accuracy 
recently conitzer and sandholm conitzer sandholm introduced the concept of automated mechanism design whereby mechanism design problem are solved using constraint satisfaction method traditionally mechanism design ha focused on producing game which yield the desired outcome when played by ideal rational player however actual player are never perfectly rational human irrationality ha been exhaustively studied and computational agent have both resource bound and potentially implementation flaw in this paper we discus extension of the technique of automated mechanism design to produce game which are robust in the face of player imperfection we model limited rationality by examining agent which converge on their strategy by using a simple variant of fictitious play simulation of repeated play singh kearns mansour this model associate to each game a system of differential equation describing the trajectory of the agent s strategy we describe additional constraint which guarantee that automated mechanism design search problem yield stable mechanism in particular we present negative result for structural stability and positive result for asymptotic stability by considering strict bayesian nash equilibrium and by employing lyapunov technique 
graphical model are powerful tool for processing image however the large dimensionality of even local image data pose a difficulty representing the range of possible graphical model node variable with discrete state lead to an overwhelmingly large number of state for the model often making both exact and approximate inference computationally intractable we propose a representation that allows a small number of discrete state to represent the large number of possible image value at each pixel or local image patch each node in the graph represents the best regression function chosen from a set of candidate function for estimating the unobserved image pixel from the observed sample this permit a small number of discrete state to summarize the range of possible image value at each point in the image belief propagation is then used to find the best regressor to use at each point to demonstrate the usefulness of this technique we apply it to two problem super resolution and color demosaicing in both case we find our method compare well against other technique for these problem 
we propose a new unsupervised learning technique for extracting information from large text collection we model document a if they were generated by a two stage stochastic process each author is represented by a probability distribution over topic and each topic is represented a a probability distribution over word for that topic the word in a multi author paper are assumed to be the result of a mixture of each author topic mixture the topic word and author topic distribution are learned from data in an unsupervised manner using a markov chain monte carlo algorithm we apply the methodology to a large corpus of abstract and author from the well known citeseer digital library and learn a model with topic we discus in detail the interpretation of the result discovered by the system including specific topic and author model ranking of author by topic and topic by author significant trend in the computer science literature between and parsing of abstract by topic and author and detection of unusual paper by specific author an online query interface to the model is also discussed that allows interactive exploration of author topic model for corpus such a citeseer 
this paper address the problem of applying powerful patternrecognition algorithm based on kernel to efficient visualtracking recently avidan ha shown that object recognizersusing kernel svms can be elegantly adapted to localization by meansof spatial perturbation of the svm using optic flow whereasavidan s svm applies to each frame of a video independently ofother frame the benefit of temporal fusion of data are wellknown this issue is addressed here by using a fully probabilistic relevance vector machine rvm to generate observation withgaussian distribution that can be fused over time to improveperformance further rather than adapting a recognizer webuild alocalizer directly using the regression form of the rvm aclassification svm is used in tandem for object verification andthis provides the capability of automatic initialization andrecovery the approach is demonstrated in real time face andvehicle tracking system the sparsity of the rvms mean thatonly a fraction of cpu time is required to track at frame rate tracker output is demonstrated in a camera management task in whichzoom and pan are controlled inresponse to speaker vehicle positionand orientation over an extended period the advantage oftemporal fusion inthis system are demonstrated 
in this paper we describe an approach to recognizing location from mobile device using image based web search we demonstrate the usefulness of common image search metric applied on image captured with a camera equipped mobile device to find matching image on the world wide web or other general purpose database searching the entire web can be computationally overwhelming so we devise a hybrid image and keyword searching technique first image search is performed over image and link to their source web page in a database that index only a small fraction of the web then relevant keywords on these web page are automatically identified and submitted to an existing text based search engine e g google that index a much larger portion of the web finally the resulting image set is filtered to retain image close to the original query it is thus possible to efficiently search hundred of million of image that are not only textually related but also visually relevant we demonstrate our approach on an application allowing user to browse web page matching the image of a nearby location 
we present a large scale meta evaluation of eight evaluation measure for both single document and multi document summarizers to this end we built a corpus consisting of a million automatic summary using six summarizers and baseline at ten summary length in both english and chinese b more than manual abstract and extract and c million automatic document and summary retrieval using query we present both qualitative and quantitative result showing the strength and draw back of all evaluation method and how they rank the different summarizers 
many problem in computer vision may be considered a low rank approximation problem in which a matrix of measured data must be approximated by a matrix of given low rank if the matrix ha no missing entry then this is easily accomplished by a singular value decomposition svd if some measurement are missing however and the matrix ha hole then the svd method can not be applied we present here a practical iterative method for approximating a data matrix possibly with missing entry with another matrix of small rank r for a complete data matrix the method reduces to the well known power method which is provably convergent to a unique global optimum if the data is well approximated by a matrix of rankr the power method ha rapid convergence our method for incomplete data is applied to several problem of d reconstruction generalizing the tomasi kanade method for orthographic camera and the sturm triggs method for projective camera to missing and uncertain data 
an auditory scene composed of overlapping acoustic source can be viewed a a complex object whose constituent part are the individual source pitch is known to be an important cue for auditory scene analysis in this paper with the goal of building agent that oper ate in human environment we describe a real time system to identify the presence of one or more voice and compute their pitch the signal processing in the front end is based on instantaneous frequency estimation a method for tracking the partial of voiced speech while the pattern m atching in the back end is based on nonnegative matrix factorization an unsupervised algorithm for learning the part of complex object while supporting a framework to analyze complicated auditory scene our system maintains real time operability and state of the art performance i n clean speech 
many enterprise incorporate information gathered from a variety of data source into an integrated input for some learning task for example aiming towards the design of an automated diagnostic tool for some disease one may wish to integrate data gathered in many different hospital a major obstacle to such endeavor is that different data source may vary considerably in the way they choose to represent related data in practice the problem is usually solved by a manual construction of semantic mapping and translation between the different source recently there have been attempt to introduce automated algorithm based on machine learning tool for the construction of such translation in this work we propose a theoretical framework for making classification prediction from a collection of different data source without creating explicit translation between them our framework allows a precise mathematical analysis of the complexity of such task and it provides a tool for the development and comparison of different learning algorithm our main objective at this stage is to demonstrate the usefulness of computational learning theory to this practically important area and to stimulate further theoretical and experimental research of question related to this framework 
we present an algorithm to perform blind one microphone speech separation our algorithm separate mixture of speech without modeling individual speaker instead we formulate the problem of speech separation a a problem in segmenting the spectrogram of the signal into two or more disjoint set we build feature set for our segmenter using classical cue from speech psychophysics we then combine these feature into parameterized affinity matrix we also take adv antage of the fact that we can generate training example for segmentation by artificially superposing separately recorded signal thus the parameter of the affinity matrix can be tuned using recent work on learni ng spectral clustering this yield an adaptive speech specific se gmentation algorithm that can successfully separate one microphone speech mixture 
we investigate data based procedure for selecting the kernel when learning with support vector machine we provide generalization error bound by estimating the rademacher complexity of the corresponding function class in particular we obtain a complexity bound for function class induced by kernel with given eigenvectors i e we allow to vary the spectrum and keep the eigenvectors fix this bound is only a logarithmic factor bigger than the complexity of the function class induced by a single kernel however optimizing the margin over such class lead to overfitting we thus propose a suitable way of constraining the class we use an efficient algorithm to solve the resulting optimization problem present preliminary experimental result and compare them to an alignment based approach 
abstract when clustering a dataset the right number k of cluster to use is often not obvious and choosing k automatically is a hard algorithmic problem in this paper we present an improved algorithm for learning k while clustering the g mean algorithm is based on a statistical test for the hypothesis that a subset of data follows a gaussian distribution g mean run k mean with increasing k in a hierarchical fashion until the test accepts the hypothesis that the data assigned to each 
we developed a robust control policy design method in high dimensional state space by using differential dynamic programming with a minimax criterion a an example we applied our method to a simulated five link biped robot the result show lower joint torque from the optimal control policy compared to a hand tuned pd servo controller result also show that the simulated biped robot can successfully walk with unknown disturbance that cause controller generated by standard differential dynamic programming and the hand tuned pd servo to fail learning to compensate for modeling error and previously unknown disturbance in conjunction with robust control design is also demonstrated 
the tm lpsat planner can construct plan in domain containing atomic action and durative action event and process discrete real valued and interval valued fluents and continuous linear change to quantity it work in three stage in the first stage a representation of the domain and problem in an extended version of pddl is compiled into a system of propositional combination of propositional variable and linear constraint over numeric variable in the second stage the lpsat constraint engine wolfman weld is used to find a solution to the system of constraint in the third stage a correct parallel plan is extracted from this solution we discus the structure of the planner and show how a real time temporal model is compiled into lpsat constraint 
how to determine a priori whether a learning algorithm is suited to a learning problem instance is a major scientific and technological challenge a first step toward this goal inspired by the phase transition pt paradigm developed in the constraint satisfaction domain is presented in this paper based on the pt paradigm extensive and principled experiment allow for constructing the competence map associated to a learning algorithm describing the region where this algorithm on average fails or succeeds the approach is illustrated on the long and widely used c algorithm a non trivial failure region in the landscape of k term dnf language is observed and some interpretation are offered for the experimental result 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
in this paper we present a general machine learning approach to the problem of deciding when to share probabilistic belief between agent for distributed monitoring our approach can generally be applied to domain that use a probabilistic model for evaluating hypothesis and have a method for combining belief from multiple agent we demonstrate the effectiveness of our approach in a concrete application in network intrusion detection a an example of a multi agent monitoring problem based on an evaluation using packet trace data from a real network we demonstrate that our learning approach can reduce both the delay and communication overhead required to detect network intrusion 
this paper present a method for unsupervised discovery of semantic pattern semantic pattern are useful for a variety of text understanding task in particular for locating event in text for information extraction the method build upon previously described approach to iterative unsupervised pattern acquisition one common characteristic of prior approach is that the output of the algorithm is a continuous stream of pattern with gradually degrading precision our method differs from the previous pattern acquisition algorithm in that it introduces competition among several scenario simultaneously this provides natural stopping criterion for the unsupervised learner while maintaining good precision level at termination we discus the result of experiment with several scenario and examine different aspect of the new procedure 
in various application domain including image recognition it is natural to represent each example a a set of vector with a base kernel we can implicitly map these vector to a hilbert space and flt a gaussian distribution to the whole set using kernel pca we deflne our kernel between example a bhattacharyya s measure of a nity between such gaussians the resulting kernel is computable in closed form and enjoys many favorable property including graceful behavior under transformation potentially justifying the vector set representation even in case when more conventional representation also exist 
link detection ha been regarded a a core technology for the topic detection and tracking task of new event detection in this paper we formulate story link detection and new event detection a information retrieval task and hypothesize on the impact of precision and recall on both system motivated by these argument we introduce a number of new performance enhancing technique including part of speech tagging new similarity measure and expanded stop list experimental result validate our hypothesis 
we describe the tivo television show collaborative recommendation system which ha been fielded in over one million tivo client for four year over this install base tivo currently ha approximately million rating by user over approximately distinct tv show and movie tivo us an item item show to show form of collaborative filtering which obviates the need to keep any persistent memory of each user s viewing preference at the tivo server taking advantage of tivo s client server architecture ha produced a novel collaborative filtering system in which the server doe a minimum of work and most work is delegated to the numerous client nevertheless the server side processing is also highly scalable and parallelizable although we have not performed formal empirical evaluation of it accuracy internal study have shown it recommendation to be useful even for multiple user household tivo s architecture also allows for throttling of the server so if more server side resource become available more correlation can be computed on the server allowing tivo to make recommendation for niche audience 
given a set of hidden variable with an a priori markov structure we derive an online algorithm which approximately update the posterior a pairwise measurement between the hidden variable become available the update is performed using assumed density filtering to incorporate each pairwise measurement we compute the optimal markov structure which represents the true posterior and use it a a prior for incorporating the next measurement we demonstrate the resulting algorithm by calculating globally consistent trajectory of a robot a it navigates along a d trajectory to update a trajectory of length t the update take o t when all conditional distribution are linear gaussian the algorithm can be thought of a a kalman filter which simplifies the state covariance matrix after incorporating each measurement 
a unified approach for treating the scale selection problem in the anisotropic scale space is proposed the anisotropic scale space is a generalization of the classical isotropic gaussian scale space by considering the gaussian kernel with a fully parameterized analysis scale bandwidth matrix the maximum over scale and the moststable over scale criterion are constructed by employing the l normalized scale space derivative i e responsenormalized derivative in the anisotropic scale space this extension allows u to directly analyze the anisotropic ellipsoidal shape of local structure the main conclusion are i the norm of the a ndl normalized anisotropic scale space derivative with a constant are maximized regardless of the signal s dimension iff the analysis scale matrix is equal to the signal s covariance and ii the most stable over scale criterion with the isotropic scale space outperforms the maximum over scale criterion in the presence of noise experiment with d and d synthetic data confirm the above finding d implementation of the most stable over scale method are applied to the problem of estimating anisotropic spread of pulmonary tumor shown in high resolution computedtomography hrct image comparison of the firstand second order method show the advantage of exploiting the second order information 
we investigate the following problem given a set of document of a particular topic or class and a large set of mixed document that contains document from class and other type of document identify the document from class in the key feature of this problem is that there is no labeled nondocument which make traditional machine learning technique inapplicable a they all need labeled document of both class we call this problem partially supervised classification in this paper we show that this problem can be posed a a constrained optimization problem and that under appropriate condition solution to the constrained optimization problem will give good solution to the partially supervised classification problem we present a novel technique to solve the problem and demonstrate the effectiveness of the technique through extensive experimentation 
it is well known that occurrence count of word in document are often modeled poorly by standard distribution like the binomial or poisson observed count vary more than simple model predict prompting the use of overdispersed model like gamma poisson or beta binomial mixture a robust alternative another deficiency of standard model is due to the fact that most word never occur in a given document resulting in large amount of zero count we propose using zero inflated model for dealing with this and evaluate competing model on a naive bayes text classification task simple zero inflated model can account for practically relevant variation and can be easier to work with than overdispersed model 
many technique for association rule mining and feature selection require a suitable metric to capture the dependency among variable in a data set for example metric such a support confidence lift correlation and collective strength are often used to determine the interestingness of association pattern however many such measure provide conflicting information about the interestingness of a pattern and the best metric to use for a given application domain is rarely known in this paper we present an overview of various measure proposed in the statistic machine learning and data mining literature we describe several key property one should examine in order to select the right measure for a given application domain a comparative study of these property is made using twenty one of the existing measure we show that each measure ha different property which make them useful for some application domain but not for others we also present two scenario in which most of the existing measure agree with each other namely support based pruning and table standardization finally we present an algorithm to select a small set of table such that an expert can select a desirable measure by looking at just this small set of table 
we describe a tracker that can track moving people in long sequence without manual initialization moving people are modeled with the assumption that while configuration can vary quite substantially from frame to frame appearance doe not this lead to an algorithm that firstly build a model of the appearance of the body of each individual by clustering candidate body segment and then us this model to find all individual in each frame unusually the tracker doe not rely on a model of human dynamic to identify possible instance of people such model are unreliable because human motion is fast and large acceleration are common we show our tracking algorithm can be interpreted a a loopy inference procedure on an underlying bayes net experiment on video of real scene demonstrate that this tracker can a count distinct individual b identify and track them c recover when it loses track for example if individual are occluded or briefly leave the view d identify the configuration of the body largely correctly and e is not dependent on particular model of human motion 
we describe an algorithm for support vector machine svm that can be parallelized efficiently and scale to very large problem with hundred of thousand of training vector instead of analyzing the whole training set in one optimization step the data are split into subset and optimized separately with multiple svms the partial result are combined and filtered again in a cascade of svms until the global optimum is reached the cascade svm can be spread over multiple processor with minimal communication overhead and requires far le memory since the kernel matrix are much smaller than for a regular svm convergence to the global optimum is guaranteed with multiple pass through the cascade but already a single pas provides good generalization a single pas is x x faster than a regular svm for problem of vector when implemented on a single processor parallel implementation on a cluster of processor were tested with over million vector class problem converging in a day or two while a regular svm never converged in over a week 
this paper describes an algorithm for detecting empty node in the penn treebank marcus et al finding their antecedent and assigning them function tag without access to lexical information such a valency unlike previous approach to this task the current method is not corpus based but rather make use of the principle of early government binding theory chomsky the syntactic theory that underlies the annotation using the evaluation metric proposed by johnson this approach outperforms previously published approach on both detection of empty category and antecedent identification given either annotated input stripped of empty category or the output of a parser some problem with this evaluation metric are noted and an alternative is proposed along with the result the paper considers the reason a principle based approach to this problem should outperform corpus based approach and speculates on the possibility of a hybrid approach 
co training is a method for combining labeled and unlabeled data when example can be thought of a containing two distinct set of feature it ha had a number of practical success yet previous theoretical analysis have needed very strong assumption on the data that are unlikely to be satisfied in practice in this paper we propose a much weaker expansion assumption on the underlying data distribution that we prove is sufficient fo r iterative cotraining to succeed given appropriately strong pac learning algorithm on each feature set and that to some extent is necessary a well this expansion assumption in fact motivates the iterative nature of the original co training algorithm unlike stronger assumption s uch a independence given the label that allow a simpler one shot co trai ning to succeed we also heuristically analyze the effect on performance of noise in the data predicted behavior is qualitatively matched in sy nthetic experiment on expander graph 
we consider incorporating action elimination procedure in reinforcement learning algorithm we suggest a framework that is based on learning an upper and a lower estimate of the value function or the q function and eliminating action that are not optimal we provide a model based and a model free variant of the elimination method we further derive stopping condition that guarantee that the learned policy is approximately optimal with high probability simulation demonstrate a considerable speedup and added robustness 
this paper present a novel promising approach that allows greedy decision tree induction algorithm to handle problematic function such a parity function lookahead is the standard approach to addressing difficult function for greedy decision tree learner nevertheless this approach is limited to very small problematic function or subfunctions or variable because the time complexity grows more than exponentially with the depth of lookahead in contrast the approach presented in this paper carry only a constant run time penalty experiment indicate that the approach is effective with only modest amount of data for problematic function or subfunctions of up to six or seven variable where the example themselves may contain numerous other irrelevant variable a well 
abstract we derive an optimal learning rule in the sense of mutual information maximization for a spiking neuron model under the assumption of small fluctuation of the input we find a spike timing dependent plasticity stdp function which depends on the time course of excitatory postsynaptic potential epsps and the autocorrelation function of the postsynaptic neuron we show that the stdp function ha both positive and negative phase the positive phase is related to the shape of the epsp while the negative phase is controlled by neuronal refractoriness 
we solve the problem of obtaining answer to query posed to a mediated integration system under the local a view paradigm that are consistent wrt to certain global integrity constraint for this the query program is combined with logic programming specification under the stable model semantics of the class of minimal global instance and of the class of their repair 
huge amount of data are stored in autonomous geographically distributed source the discovery of previously unknown implicit and valuable knowledge is a key aspect of the exploitation of such source in recent year several approach to knowledge discovery and data mining and in particular to clustering have been developed but only a few of them are designed for distributed data source we propose a novel distributed clustering algorithm based on non parametric kernel density estimation which take into account the issue of privacy and communication cost that arise in a distributed environment 
the modal correspondence method of shapiro and brady aim to match point set by comparing the eigenvectors of a pairwise point proximity matrix although elegant by mean of it matrix representation the method is notoriously susceptible to difference in the relational structure of the point set under consideration in this paper we demonstrate how the method can be rendered robust to structural difference by adopting a hierarchical approach we place the modal matching problem in a probabilistic setting in which the correspondence between pairwise cluster can be used to constrain the individual point correspondence to meet this goal we commence by describing an iterative method which can be applied to the point proximity matrix to identify the location of pairwise modal cluster once we have assigned point to cluster we compute within cluster and between cluster proximity matrix the modal co efficients for these two set of proximity matrix are used to compute cluster correspondence and cluster conditional point correspondence probability a sensitivity study on synthetic point set reveals that the method is considerably more robust than the conventional method to clutter or point set contamination 
a distance based conditional model on the ranking poset is presented for use in classification and ranking the model is an extension of the mallow model and generalizes the classifier combination method used by several ensemble learning algorithm including error correcting output code discrete adaboost logistic regression and cranking the algebraic structure of the ranking poset lead to a simple bayesian interpretation of the conditional model and it special case in addition to a unifying view the framework suggests a probabilistic interpretation for error correcting output code and an extension beyond the binary coding scheme 
autonomous helicopter flight represents a challenging control problem with complex noisy dynamic in this paper we describe a successful application of reinforcement learning to autonomous helicopter flight we first fit a stochastic nonlinear model of the helicopter dynamic we then use the model to learn to hover in place and to fly a number of maneuver taken from an rc helicopter competition 
visual learning is expected to be a continuous and robustprocess which treat input image and pixel selectively in this paper we present a method for subspace learning which take these consideration into account wepresent an incremental method which sequentially updatesthe principal subspace considering weighted influence ofindividual image a well a individual pixel within an image this approach is further extended to enable determinationof consistency in the input data and imputation of thevalues in inconsistent pixel using the previously acquiredknowledge resulting in a novel incremental weighted androbust method for subspace learning 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
when searching the www user often desire result restricted to a particular document category ideally a user would be able to filter result with a text classifier to minimize false positive result however current search engine allow only simple query modification to automate the process of generating effective query modification we introduce a sensitivity analysis based method for extracting rule from nonlinear support vector machine the proposed method allows the user to specify a desired precision while attempting to maximize the recall our method performs several level of dimensionality reduction and is vastly faster than searching the combination feature space moreover it is very effective on real world data 
we present an unsupervised methodfor word sense disambiguation thatexploits translation correspondencesin parallel corpus the techniquetakes advantage of the fact that crosslanguagelexicalizations of the sameconcept tend to be consistent preservingsome core element of it semantics and yet also variable reflecting differingtranslator preference and the influenceof context working with parallelcorpora introduces an extra complicationfor evaluation since it is 
in this paper we propose a novel document clustering method based on the non negative factorization of the term document matrix of the given document corpus in the latent semantic space derived by the non negative matrix factorization nmf each axis capture the base topic of a particular document cluster and each document is represented a an additive combination of the base topic the cluster membership of each document can be easily determined by finding the base topic the axis with which the document ha the largest projection value our experimental evaluation show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clustering method not only in the easy and reliable derivation of document clustering result but also in document clustering accuracy 
we are concerned with the issue of outlier detection and change point detection from a data stream in the area of data mining there have been increased interest in these issue since the former is related to fraud detection rare event discovery etc while the latter is related to event trend by change detection activity monitoring etc specifically it is important to consider the situation where the data source is non stationary since the nature of data source may change over time in real application although in most previous work outlier detection and change point detection have not been related explicitly this paper present a unifying framework for dealing with both of them on the basis of the theory of on line learning of non stationary time series in this framework a probabilistic model of the data source is incrementally learned using an on line discounting learning algorithm which can track the changing data source adaptively by forgetting the effect of past data gradually then the score for any given data is calculated to measure it deviation from the learned model with a higher score indicating a high possibility of being an outlier further change point in a data stream are detected by applying this scoring method into a time series of moving averaged loss for prediction using the learned model specifically we develop an efficient algorithm for on line discounting learning of auto regression model from time series data and demonstrate the validity of our framework through simulation and experimental application to stock market data analysis 
unsupervised clustering can be significantly improved using supervision in the form of pairwise constraint i e pair of instance labeled a belonging to same or different cluster in recent year a number of algorithm have been proposed for enhancing clustering quality by employing such supervision such method use the constraint to either modify the objective function or to learn the distance measure we propose a probabilistic model for semi supervised clustering based on hidden markov random field hmrfs that provides a principled framework for incorporating supervision into prototype based clustering the model generalizes a previous approach that combine constraint and euclidean distance learning and allows the use of a broad range of clustering distortion measure including bregman divergence e g euclidean distance and i divergence and directional similarity measure e g cosine similarity we present an algorithm that performs partitional semi supervised clustering of data by minimizing an objective function derived from the posterior energy of the hmrf model experimental result on several text data set demonstrate the advantage of the proposed framework 
an important task in unsupervised learning is maximum likelihood mixture estimation mlme for exponential family in this paper we prove a mathematical equivalence between this mlme problem and the rate distortion problem for bregman divergence we also present new theoretical result in rate distortion theory for bregman divergence further an analysis of the problem a a trade off between compression and preservation of information is presented that yield the information bottleneck method a an interesting special case 
this paper describes an empirical study of the information synthesis task defined a the process of given a complex information need extracting organizing and inter relating the piece of information contained in a set of relevant document in order to obtain a comprehensive non redundant report that satisfies the information need two main result are presented a the creation of an information synthesis testbed with report manually generated by nine subject for eight complex topic with relevant document each and b an empirical comparison of similarity metric between report under the hypothesis that the best metric is the one that best distinguishes between manual and automatically generated report a metric based on key concept overlap give better result than metric based on n gram overlap such a rouge or sentence overlap 
the vision of the semantic web can only be realized through proliferation of well known ontology describing different domain to enable interoperability in the semantic web it will be necessary to break these ontology down into smaller well focused unit that may be reused currently three problem arise in that scenario firstly it is difficult to locate ontology to be reused thus leading to many ontology modeling the same thing secondly current tool do not provide mean for reusing existing ontology while building new ontology finally ontology are rarely static but are being adapted to changing requirement hence an infrastructure for management of ontology change taking into account dependency between ontology is needed in this paper we present such an infrastructure addressing the aforementioned problem 
sentence ranking is a crucial part of generating text summary we compared human sentence ranking obtained in a psycholinguistic experiment to three different approach to sentence ranking a simple paragraph based approach intended a a baseline two word based approach and two coherence based approach in the paragraph based approach sentence in the beginning of paragraph received higher importance rating than other sentence the word based approach determined sentence ranking based on relative word frequency luhn salton buckley coherence based approach determined sentence ranking based on some property of the coherence structure of a text marcu page et al our result suggest poor performance for the simple paragraph based approach whereas word based approach perform remarkably well the best performance wa achieved by a coherence based approach where coherence structure are represented in a non tree structure most approach also outperformed the commercially available msword summarizer 
in most computer system page fault rate is currently minimized by generic page replacement algorithm which try to model the temporal locality inherent in program in this paper we propose two algorithm one greedy and the other stochastic designed for program specific code restructuring a a mean of increasing spatial locality within a program both algorithm effectively decrease average working set size and hence the page fault rate our method are more effective than traditional approach due to use of domain information we illustrate the efficacy of our algorithm on actual data mining algorithm 
although the owlweb ontology language add considerable expressive power to the semantic web it doe have expressive limitation particularly with respect to what can be said about property wepresent orl owl rule language a horn clause rule extension to owl that overcomes many of these limitation orl extends owl in a syntactically and semantically coherent manner the basic syntax for orl rule is an extension of the abstract syntax for owl dl and owllite orl rule are given formal meaning via an extension of the owldl model theoretic semantics orl rule are given an xml syntax basedon the owl xml presentation syntax and a mapping from orl rule to rdf graph is given based on the owl rdf xml exchange syntax wediscuss the expressive power of orl showing that the ontology consistency problem is undecidable provide several example of orlusage and discus how reasoning support for orl might be provided 
we study the performance of some known algorithm for solving the simple temporal problem stp and the temporal constraint satisfaction problem tcsp in particular we empirically compare the bellman ford bf algorithm and it incremental version incbf by cesta oddi to the stp of xu choueiry a among the tested algorithm we show that stp is the most efficient for determining the consistency of an stp and that incbf combined with the heuristic of xu choueiry b is the most efficient for solving the tcsp we plan to improve stp by exploiting incrementality a in incbf and other new incremental algorithm 
estimation of camera pose from an image of n point or line with known correspondence is a thoroughly studied problem in computer vision most solution are iterative and depend on nonlinear optimization of some geometric constraint either on the world coordinate or on the projection to the image plane for real time application we are interested in linear or closed form solution free of initialization we present a general framework which allows for a novel set of linear solution to the pose estimation problem for both n point and n line we present a number of simulation which compare our result to two other recent linear algorithm a well a to iterative approach we conclude with test on real imagery in an augmented reality setup we also present an analysis of the sensitivity of our algorithm to image noise 
in this paper we present a new algorithm suitable for matching discrete object such a string and tree in linear time thus obviat ing dynamic programming with quadratic time complexity furthermore prediction cost in many case can be reduced to linear cost in the length of the sequence to be classified regardless of the number of support v ectors this improvement on the currently available algorithm make string kernel a viable alternative for the practitioner 
we consider a graph theoretic approach for automatic construction of option in a dynamic environment a map of the environment is generated on line by the learning agent representing the topological structure of the state transition a clustering algorithm is then used to partition the state space to different region policy for reaching the different part of the space are separately learned and added to the model in a form of option macro action the option are used for accelerating the q learning algorithm we extend the basic algorithm and consider building a map that includes preliminary indication of the location of interesting region of the state space where the value gradient is significant and additional exploration might be beneficial experiment indicate significant speedup especially in the initial learning phase 
the context of this work is lateral vehicle control using a camera a a sensor a natural tool for controlling a vehicle is recursive filtering the well known kalman filtering theory relies on gaussian assumption on both the state and measure random variable however image processing algorithm yield measurement that most of the time are far from gaussian a experimentally shown on real data in our application it is therefore necessary to make the approach more robust leading to the so called robust kalman filtering in this paper we review this approach from a very global point of view adopting a constrained least square approach which is very similar to the half quadratic theory and justifies the use of iterative reweighted least square algorithm a key issue in robust kalman filtering is the choice of the prediction error covariance matrix unlike in the gaussian case it computation is not straightforward in the robust case due to the nonlinearity of the involved expectation we review the classical alternative and propose new one a theoretical study of these approximation is out of the scope of this paper however we do provide an experimental comparison on synthetic data perturbed with cauchy distributed noise 
in this paper we describe two new objective automatic evaluation method for machine translation the first method is based on longest common subsequence between a candidate translation and a set of reference translation longest common subsequence take into account sentence level structure similarity naturally and identifies longest co occurring in sequence n gram automatically the second method relaxes strict n gram matching to skip bigram matching skip bigram is any pair of word in their sentence order skip bigram cooccurrence statistic measure the overlap of skip bigram between a candidate translation and a set of reference translation the empirical result show that both method correlate with human judgment very well in both adequacy and fluency 
the integration of data produced and collected across autonomous heterogeneous web service is an increasingly important and challenging problem due to the lack of global identifier the same entity e g a product might have different textual representation across database textual data is also often noisy because of transcription error incomplete information and lack of standard format a fundamental task during data integration is matching of string that refer to the same entity in this paper we adopt the widely used and established cosine similarity metric from the information retrieval field in order to identify potential string match across web source we then use this similarity metric to characterize this key aspect of data integration a a join between relation on textual attribute where the similarity of match exceeds a specified threshold computing an exact answer to the text join can be expensive for query processing efficiency we propose a sampling based join approximation strategy for execution in a standard unmodified relational database management system rdbms since more and more web site are powered by rdbmss with a web based front end we implement the join inside an rdbms using sql query for scalability and robustness reason finally we present a detailed performance evaluation of an implementation of our algorithm within a commercial rdbms using real life data set our experimental result demonstrate the efficiency and accuracy of our technique 
gaussian process provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observation in an empirical model this is of particular importance in identification of nonlinear dynamic system from experimental data it allows u to combine derivative information and associated uncertainty with normal function observation into the learning and inference process this derivative information can be in the form of prior specified by an expert or identified from perturbation data close to equilibrium it allows a seamless fusion of multiple local linear model in a consistent manner inferring consistent model and ensuring that integrability constraint are met it improves dramatically the computational efficiency of gaussian process model for dynamic system identification by summarising large quantity of near equilibrium data by a handful of linearisations reducing the training set size traditionally a problem for gaussian process model 
this paper analyzes the performance of semisupervised learning of mixture model we show that unlabeled data can lead to an increase in classification error even in situation where additional labeled data would decrease classification error this behavior contradicts several empirical result reported in the literature we present a mathematical analysis of this degradation phenomenon and show that it is due to the fact that bias may be adversely affected by unlabeled data we study the impact of these theoretical result to classifier based on bayesian network some situation call for structural learning while others are best handled by relatively simple classifier 
the catchment feature model cfm address two question in multimodal interaction how do we bridge video and audio processing withthe reality of human multimodal communication and how information from the different mode may be fused we discus the need for our model motivate the cfm from psycholinguistic research andpresent the model in contrast to wholegesture recognition the cfm applies a feature decomposition approach that facilitates cross modal fusion at the level of discourse planningand conceptualization we present our experimental framework for cfm based research and cite three concrete example of catchment feature cf and propose new direction of multimodal research based on the model the importance of gesture of hand head face eyebrow eye and body posture in human communication in conjunction with speech is self evident hitherto visionbased gesture research ha by and large ignored the nexus of speech and other multimodal behavior even though such behavior underlies much of human gesture use the key and yet unmet challenge for the field of gesture analysis is how we may be relevant to such real world gesticulation this paper advance a perspective of high level gesture understanding that proceeds from human multimodal language we do not present any particular new algorithm instead we draw our evidence from scientifically proven published research to motivate and derive an overarching model that open the door of discourse understanding for vision speech processing research we shall show that this high level understanding is not inconsequential it ha deep implication on how the entire enterprise of high and low level vision based gesture research may be carried out we present the result of a set of discourse segmentation experiment that support our model 
maximum satisfiability max sat is more general and more difficult to solve than satisfiability sat in this paper we first investigate the effectiveness of walksat one of the best local search algorithm designed for sat on max sat we show that walksat is also effective on max sat while it effectiveness degrades a the problem is more constrained we then develop a novel method that exploit the backbone information in the local minimum from walksat and applies the backbone information in different way to improve the performance of the walksat algorithm we call our new algorithm backbone guided walksat bgwalksat on large random sat and max sat problem a well a instance from the satlib bgwalksat significantly improves walksat s performance 
this paper considers online stochastic optimization problem where time constraint severely limit the number of offline optimization which can be performed at decision time and or in between decision it proposes a novel approach which combine the salient feature of the earlier approach the evaluation of every decision on all sample expectatio n and the ability to avoid distributing the sample among decision consensus the key idea underlying the novel algorithm is to approximate the regret of a decision d the regret algorithm is evaluated on two fundamentally different application online packet scheduling in network and online multiple vehicle routing with time window on both application it produce significant benefit over prior approach 
markov decision process mdps and contingency planning cp are two widely used approach to planning under uncertainty mdps are attractive because the model is extremely general and because many algorithm exist for deriving optimal plan in contrast cp is normally performed using heuristic technique that do not guarantee optimality but the resulting plan are more compact and more understandable the inability to present mdp policy in a clear intuitive way ha limited their applicability in some important domain we introduce an anytime algorithm for deriving contingency plan that combine the advantage of the two approach 
this paper present a new protocol for certified email the protocol aim to combine security scalability easy implementation and viable deployment the protocol relies on a light on line trusted third party it can be implemented without any special software for the receiver beyond a standard email reader and web browser and doe not require any public key infrastructure 
psychophysical study suggest the existence of specialized detector for component motion pat tern radial circular and spiral that are consistent with the visual motion property of cell in the dorsal medial superior t emporal area mstd of non human primate here we use a bi ologically constrained model of visual motion proce ssing i n mstd i n conjunction wi th psychophysical performance on two motion pattern task to elucidate the computational mechanism associated with the processing of widefield motion pattern encountered during self motion in both task discrimination t hresholds varied si gnificantly with the type of motion pat tern presented suggesting perceptual correlate t o the preferred motion bi a repo rted i n mstd through t he model we demonstrate that while independently responding motion pat tern unit are capable of encoding information relevant to the vi sual motion t asks equivalent psychophysical performance can only be achieved using interconnected neural population that systematically i nhibit non responsive unit these result suggest the cyclic trend in psychophysical performance may be mediated in part by recurrent connection within motion pattern responsive area whose structure is a funct ion of the sim ilarity in preferred motion pattern and receptive field location between unit 
this paper present a novel approach for continuous detection and tracking of moving object observed by multiple stationary camera we address the tracking problem by simultaneously modeling motion and appearance of the moving object the object s appearance is represented using color distribution model invariant to d rigid and scale transformation it provides an efficient blob similarity measure for tracking the motion model are obtained using a kalman filter kf process which predicts the position of the moving object in d and d the tracking is performed by the maximization of a joint probability model reflecting object motion and appearance the novelty of our approach consists in integrating multiple cue and multiple view in a jpdaf for tracking a large number of moving people with partial and total occlusion we demonstrate the performance of the proposed method on a soccer game captured by two stationary camera 
the emerging cognitive vision paradigm is concerned with vision system that evaluate gather and integrate contextual knowledge for visual analysis in reasoning about event and structure cognitive vision system should rely on multiple computation in order to perform robustly even in noisy domain action recognition in an unconstrained office environment thus provides an excellent testbed for research on cognitive computer vision in this contribution we present a system that consists of several computational module for object and action recognition it applies attention mechanism visual learning and contextual a well a probabilistic reasoning to fuse individual result and verify their consistency database technology are used for information storage and an xml based communication framework integrates all module into a consistent architecture 
the hypothesis that information on the web can be verified automatically with minimal user interaction will be tested by building and evaluating an interactive system in this paper verification is defined a a reasonable determination of the truth or correctness of a statement by examination research or comparison with similar text the system will contain module for reliability ranking query processing document retrieval and document clustering based on agreement the query processing and document retrieval component will use standard ir technique the reliability module will estimate the likelihood that a statement on the web can be trusted using standard developed by information scientist a well a linguistic aspect of the page and the link structure of associated web page the clustering module will cluster relevant document based on whether or not they agree or disagree with the statement to be verified relevant reference are discussed 
replicability of machine learning experiment measure how likely it is that the outcome of one experiment is repeated when performed with a different randomization of the data in this paper we present an estimator of replicability of an experiment that is efficient more precisely the estimator is unbiased and ha lowest variance in the class of estimator formed by a linear combination of outcome of experiment on a given data set we gathered empirical data for comparing experiment consisting of different sampling scheme and hypothesis test both factor are shown to have an impact on replicability of experiment the data suggests that sign test should not be used due to low replicability ranked sum test show better performance but the combination of a sorted run sampling scheme with a t test give the most desirable performance judged on type i and ii error and replicability 
the intuition that different text classifier behave in qualitatively different way ha long motivated attempt to build a better metaclassifier via some combination of classifier we introduce a probabilistic method for combining classifier that considers the context sensitive reliability of contributing classifier the method harness reliability indicator variable that provide a valuable signal about the performance of classifier in different situation we provide background present procedure for building metaclassifiers that take into consideration both reliability indicator and classifier output and review a set of comparative study undertaken to evaluate the methodology 
this paper concern the discourse understanding process in spoken dialogue system this process enables the system to understand user utterance based on the context of a dialogue since multiple candidate for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding it is not appropriate to decide on a single understanding result after each user utterance by holding multiple candidate for understanding result and resolving the ambiguity a the dialogue progress the discourse understanding accuracy can be improved this paper proposes a method for resolving this ambiguity based on statistical information obtained from dialogue corpus unlike conventional method that use hand crafted rule the proposed method enables easy design of the discourse understanding process experiment result have shown that a system that exploit the proposed method performs sufficiently and that holding multiple candidate for understanding result is effective 
many work have shown that strong connection relate learning from example to regularization technique for ill posed inverse problem nevertheless by now there wa no formal evidence neither that learning from example could be seen a an inverse problem nor that theoretical result in learning theory could be independently derived using tool from regularization theory in this paper we provide a positive answer to both question indeed considering the square loss we translate the learning problem in the language of regularization theory and show that consistency result and optimal regularization parameter choice can be derived by the discretization of the corresponding inverse problem 
we present a complete system for the purpose of automatically assembling d pot given d measurement of their fragment commonly called sherd a bayesian approach is formulated which at present model the data given a set of sherd geometric parameter dense sherd measurement data is obtained by scanning the outside surface of each sherd with a laser scanner mathematical model specied by a set of geometric parameter represent the sherd outer surface and break curve on the outer surface where two sherd have broken apart optimal alignment of assembly of sherd called congur ations is implemented a maximum likelihood estimation mle of the surface and break curve parameter given the measured sherd data for all sherd in a congur ation the assembly process start with a fast clustering scheme which approximates the mle solution for all sherd pair i e congur ations of size using a subspace of the geometric parameter i e the sherd break curve more accurate mle value based on all parameter i e sherd alignment are computed when sherd pair are merged with other sherd congur ations merges take place in order of constant probability starting at the most probable congur ation this method is robust to missing sherd or group of sherd which contain sherd from more than one pot the system represents at least three signicant advance over previous d puzzle solving approach a bayesian framework which allows for easily combining diverse type of information extracted from each sherd a search which reduces comparison on unlikely congur ations and a robust computationally reasonable method for aligning break curve and sherd outer surface simultaneously in addition a number of insight are given which have not previously been discussed and signicantly reduce computation method proposed for and represent important contribution to the eld of puzzle assembly d geometry learning and dataset alignment and are critical to making d puzzle solution tractable to compute result are presented which include assembling a sherd pot where only an incomplete set of sherd is available keywords automatic d puzzle assembly d structure from unorganized d data d alignment geometric learning perceptual grouping hierarchical clustering 
we systematically investigate a new approach to estimating the parameter of language model for information retrieval called parsimonious language model parsimonious language model explicitly address the relation between level of language model that are typically used for smoothing a such they need fewer non zero parameter to describe the data we apply parsimonious model at three stage of the retrieval process at indexing time at search time at feedback time experimental result show that we are able to build model that are significantly smaller than standard model but that still perform at least a well a the standard approach 
surface representation is needed for almost all modelingand visualization application but unfortunately d datafrom a passive vision system are often insufficient for a traditionalsurface reconstruction technique that is designedfor densely scanned d point data in this paper we developa new method for surface reconstruction by combiningboth d data and d image information the silhouetteinformation extracted from d image can also be integratedas an option if it is available the new methodis a variational approach with a new functional integrating d stereo data with d image information this givesa more robust approach than existing method using onlypure d information or d stereo data we also propose abounded regularization method to implement efficiently thesurface evolution by level set method the property ofthe algorithm are discussed proved for some case andempirically demonstrated through intensive experiment onreal sequence 
the tremendous amount of data resulting from the regular usage of tool for automatic presentation recording demand for elaborate search functionality a detailed analysis of the according multimedia document is required to allow search at a very detailed level unfortunately the produced data differs significantly from traditional document in this demo we discus the problem appearing in the presentation retrieval scenario and introduce aofse a search engine to study and illustrate these problem a well a to develop and present according solution and new approach for this task 
large quantity of document in the internet and digital library are simply scanned and archived in image format many of which are packed in pdf file the word search tool provided by adobe reader acrobat doe not work for these imaged document in this paper we present a search engine to deal with this issue for imaged document in pdf file the experimental result show an encouraging performance 
in front of modern database noise tolerance ha become today one of the most studied topic in machine learning many algorithm have been suggested for dealing with noisy data in the case of numerical instance either by filtering them during a preprocess or by treating them during the induction however this research subject remains widely open when one learns from unbounded symbolic sequence which is the aim in grammatical inference in this paper we propose a statistical approach for dealing with noisy data during the inference of automaton by the state merging algorithm rpni our approach is based on a proportion comparison test which relaxes the merging rule of rpni without endangering the generalization error beyond this relevant framework we provide some useful theoretical property about the behavior of our new version of rpni called rpni finally we describe a large comparative study on several datasets 
we present a design technique for realizing given projection a catadioptric sensor in general these problem do not have solution but approximate solution may often be found that are visually acceptable our approach which we call the method of vector field reduces the problem to solving a linear system a given transformation between the image plane object surface is shown to determine a vector field which is normal to the surface in the case where the vector field is a gradient if the vector field is not a gradient we present several functionals that may be minimized to give approximate solution 
the purpose of this paper is to investigate infinity sample property of risk minimization based multi category classification method these method can be considered a natural extension to binary large margin classification we establish condition that guarantee the infinity sample consistency of classifier obtained in the risk minimization framework example are provided for two specific form of the general formulation which extend a number of known method using these example we show that some risk minimization formulation can also be used to obtain conditional probability estimate for the underlying problem such conditional probability information will be useful for statistical inferencing task beyond classification 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
in the present paper we address the problem of recovering the true underlying model of a surface while performing the segmentation a novel criterion for surface model selection is introduced and it performance for selecting the underlying model of various surface ha been tested and compared with many other existing technique using this criterion we then present a range data segmentation algorithm capable of segmenting complex object with planar and curved surface the algorithm simultaneously identifies the type order and geometric shape of surface and separate all the point that are part of that surface from the rest in a range image the paper includes the segmentation result of a large collection of range image obtained from object with planar and curved surface 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this paper address cost sensitive classication in the setting where there are cost for measuring each attribute a well a cost for misclassication error we show how to formulate this a a markov decision process in which the transition model is learned from the training data specically we assume a set of training example in which all attribute and the true class have been measured we describe a learning algorithm based on the ao heuristic search procedure that search for the classication policy with minimum expected cost we provide an admissible heuristic for ao that substantially reduces the number of node that need to be expanded particularly when attribute measurement cost are high to further prune the search space we introduce a statistical pruning heuristic based on the principle that if the value of two policy are statistically indistinguishable on the training data then we can prune one of the policy from the ao search space experiment with realistic and synthetic data demonstrate that these heuristic can substantially reduce the memory needed for ao search without signicantly aecting the quality of the learned policy hence these heuristic expand the range of cost sensitive learning problem for which ao is feasible 
missing data is common in real world datasets and is a problem for many estimation technique we have developed a variational bayesian method to perform independent component analysis ica on high dimensional data containing missing entry missing data are handled naturally in the bayesian framework by integrating the generative density model modeling the distribution of the independent source with mixture of gaussians allows source to be estimated with different kurtosis and skewness the variational bayesian method automatically determines the dimensionality of the data and yield an accurate density model for the observed data without overfitting problem this allows direct probability estimation of missing value in the high dimensional space and avoids dimension reduction preprocessing which is not feasible with missing data 
the problem of extracting the relevant aspect of data in face of multiple conflicting structure is inherent to modeling of complex data extracting structure in one random variable that is relevant for another variable ha been principally addressed recently via the information bottleneck method however such auxiliary variable often contain more information than is actually required due to structure that are irrelevant for the task in many other case it is in fact easier to specify what is irrelevant than what is for the task at hand identifying the relevant structure however can thus be considerably improved by also minimizing the information about another irrelevant variable in this paper we give a general formulation of this problem and derive it formal a well a algorithmic solution it operation is demonstrated in a synthetic example and in the context of text categorization while the original information bottleneck problem is related to rate distortion theory with the distortion measure replaced by the relevant information extracting discriminative relevant feature is formally related to rate distortion with side information 
bayesian network are a powerful probabilistic representation and their use for classification ha received considerable attention however they tend to perform poorly when learned in the standard way this is attributable to a mismatch between the objective function used likelihood or a function thereof and the goal of classification maximizing accuracy or conditional likelihood unfortunately the computational cost of optimizing structure and parameter for conditional likelihood is prohibitive in this paper we show that a simple approximation choosing structure by maximizing conditional likelihood while setting parameter by maximum likelihood yield good result on a large suite of benchmark datasets this approach produce better class probability estimate than naive bayes tan and generatively trained bayesian network 
a bayesian network formulation for relational shapematching is presented the main advantage of the relationalshape matching approach is the obviation ofthe non rigid spatial mapping used by recent non rigidmatching approach the basic variable that need tobe estimated in the relational shape matching objectivefunction are the global rotation and scale and the localdisplacements and correspondence the new bethefree energy approach is used to estimate the pairwisecorrespondences between link of the template graphsand the data the resulting framework is useful inboth registration and recognition context result areshown on hand drawn template and on d transverset weighted mr image 
this paper considers the problem of reconstructing visuallyrealistic d model of fire from a very small setof simultaneous view even two by modeling fire a asemi transparent d density field we show that fire reconstructionis equivalent to a severely under constrained computerizedtomography problem for which traditional methodsbreak down our approach is based on the observationthat every pair of photograph of a semi transparentscene defines a unique density field called a flame sheet that concentrate all it density on one connected semi transparentsurface reproduces the two photo exactly and is the most spatially coherent density field that doesso from this observation we reduce fire reconstruction tothe convex combination of sheet like density field each ofwhich is derived from the flame sheet of two input photo experimental result suggest that this method enables high qualityview extrapolation without over fitting artifact 
we present a novel generative model for natural language tree structure in which semantic lexical dependency and syntactic pcfg structure are scored with separate model this factorization provides conceptual simplicity straightforward opportunity for separately improving the component model and a level of performance comparable to similar non factored model most importantly unlike other modern parsing model the factored model admits an extremely effective a parsing algorithm which enables efficient exact inference 
the problem of establishing image to image correspondence is fundamental in computer vision recently several wide baseline matching algorithm capable of handling large change of viewpoint have appeared by computing feature value from image data these algorithm mainly use appearance a a cue for matching topological information i e spatial relation between feature ha also been used but not nearly to the same extent a appearance in this paper we incorporate topological constraint into an existing matching algorithm which match image intensity profile between interest point we show that the algorithm can be improved by exploiting the constraint that the intensity profile around each interest point should be cyclically ordered string matching technique allows for an efficient implementation of the ordering constraint experiment with real data indicate that the modified algorithm indeed give superior result to the original one the method of enforcing the spatial constraint is not limited to the presented case but can be used on any algorithm where interest point correspondence are sought 
we present a method to distinguish direct connection between two neuron from common input originating from other unmeasured neuron the distinction is computed from the spike time of the two neuron in response to a white noise stimulus although the method is based on a highly idealized linear nonlinear approximation of neural response we demonstrate via simulation that the approach can work with a more realistic integrate and fire neuron model we propose that the approach exemplified by this analysis may yield viable tool for reconstructing stimulus driven neural network from data gathered in neurophysiology experiment 
sketch recognition system are currently being developed for many domain but can be time consuming to build if they are to handle the intricacy of each domain this paper present the first translator that take symbolic shape description written in the ladder sketch language and automatically transforms them into shape recognizers editing recognizers and shape exhibitor for use in conjunction with a domain independent sketch recognition system this transformation allows u to build a single domain independent recognition system that can be customized for multiple domain we have tested our framework by writing several domain description and automatically created a domain specific sketch recognition system for each domain 
in recent year we have witnessed the success of autonomous agent applying machine learning technique across a wide range of application however agent applying the same machine learning technique in online application have not been so successful even agent based hybrid recommender system that combine information filtering technique with collaborative filtering technique have only been applied with considerable success to simple consumer good such a movie book clothing and food complex adaptive autonomous agent system that can handle complex good such a real estate vacation plan insurance mutual fund and mortgage have yet emerged to a large extent the reinforcement learning method developed to aid agent in learning have been more successfully deployed in offline application the inherent limitation in these method have rendered them somewhat ineffective in online application in this paper we postulate that a small amount of prior knowledge and human provided input can dramatically speed up online learning we will demonstrate that our agent humane with it prior knowledge or experience about the real estate domain can effectively assist user in identifying requirement especially unstated one quickly and unobtrusively 
the objective of this paper is to present a new technique for computing term weight for index term which lead to a new ranking mechanism referred to a set based model the component in our model are no longer term but termsets the novelty is that we compute term weight using a data mining technique called association rule which is time efficient and yet yield nice improvement in retrieval effectiveness the set based model function for computing the similarity between a document and a query considers the termset frequency in the document and it scarcity in the document collection experimental result show that our model improves the average precision of the answer set for all three collection evaluated for the trec collection our set based model led to a gain relative to the standard vector space model of in average precision curve and of in average precision for the top document like the vector space model the set based model ha time complexity that is linear in the number of document in the collection 
in recent work we presented a framework for many to many matching of multi scale feature hierarchy in which feature and their relation were captured in a vertex labeled edge weighted directed graph the algorithm wa based on a metric tree representation of labeled graph and their metric embedding into normed vector space using the embedding algorithm of matou sek however the method wa limited by the fact that two graph to be matched were typically embedded into vector space with dieren t dimensionality before the embeddings could be matched a dimensionality reduction technique pca wa required which wa both costly and prone to error in this paper we introduce a more ecien t embedding procedure based on a spherical coding of directed graph the advantage of this novel embedding technique is that it prescribes a single vector space into which both graph are embedded this reduces the problem of directed graph matching to the problem of geometric point matching for which ecien t many to many matching algorithm exist such a the earth mover s distance we apply the approach to the problem of multi scale view based object recognition in which an image is decomposed into a set of blob and ridge with automatic scale selection 
we work with a model of object recognition where word must be placed on image region this approach mean that large scale experiment are relatively easy so we can evaluate the effect of various early and midlevel vision algorithm on recognition performance we evaluate various image segmentation algorithm by determining word prediction accuracy for image segmented in various way and represented by various feature we take the view that good segmentation respect object boundary and so word prediction should be better for a better segmentation however it is usually very difficult in practice to obtain segmentation that do not break up object so most practitioner attempt to merge segment to get better putative object representation we demonstrate that our paradigm of word prediction easily allows u to predict potentially useful segment merges even for segment that do not look similar for example merging the black and white 
this paper present a method for incorporating word pronunciation information in a noisy channel model for spelling correction the proposed method build an explicit error model for word pronunciation by modeling pronunciation similarity between word we achieve a substantial performance improvement over the previous best performing model for spelling correction 
we give a statistical interpretation of proximal support vector machine psvm proposed at kdd a linear approximaters to nonlinear support vector machine svm we prove that psvm using a linear kernel is identical to ridge regression a biased regression method known in the statistical community for more than thirty year technique from the statistical literature to estimate the tuning constant that appears in the svm and psvm framework are discussed better shrinkage strategy that incorporate more than one tuning constant are suggested for nonlinear kernel the minimization problem posed in the psvm framework is equivalent to finding the posterior mode of a bayesian model defined through a gaussian process on the predictor space apart from providing new insight these interpretation help u attach an estimate of uncertainty to our prediction and enable u to build richer class of model in particular we propose a new algorithm called psvmmix which is a combination of ridge regression and a gaussian process model extension to the case of continuous response is straightforward and illustrated with example datasets 
many algorithm rely critically on being given a good metric over their input for instance data can often be clustered in many plausible way and if a clustering algorithm such a k mean initially fails to find one that is meaningful to a user the only recourse may be for the user to manually tweak the metric until sufficiently good cluster are found for these and other application requiring good metric it is desirable that we provide a more systematic way for user to indicate what they consider similar for instance we may ask them to provide example in this paper we present an algorithm that given example of similar and if desired dissimilar pair of point in learns a distance metric over that respect these relationship our method is based on posing metric learning a a convex optimization problem which allows u to give efficient local optimum free algorithm we also demonstrate empirically that the learned metric can be used to significantly improve clustering performance 
we wish to determine the epipolar geometry of a stereo camera pair from image measurement alone this paper describes a solution to this problem which doe not require a parametric model of the camera system and consequently applies equally well to a wide class of stereo configuration example in the paper range from a standard pinhole stereo configuration to more exotic system combining curved mirror and wide angle lens the method described here allows epipolar curve to be learned from multiple image pair presented to the stereo camera by aggregating information over the multiple image a dense map of the epipolar curve can be determined on the image the algorithm requires a large number of image but ha the distinct benefit that the correspondence problem doe not have to be explicitly solved we show that for standard stereo configuration the result are comparable to those obtained from a state of the art parametric model method despite the significantly weaker constraint on the non parametric model the new algorithm is simple to implement so it may easily be employed on a new and possibly complex camera system 
following the tradition of these acceptance talk i will begiving my thought on where our field is going any discussion ofthe future of information retrieval ir research however needsto be placed in the context of it history and relationship toother field although ir ha had a very strong relationship withlibrary and information science it relationship to computerscience c and it relative standing a a sub discipline of cshas been more dynamic ir is quite an old field and when a numberof c department were forming in the s it wa not uncommon forum faculty member to be pursuing research related to ir early acmcurriculum recommendation for c contained course on informationretrieval and encyclopedia described ir and database system asdifferent aspect of the same field by the s there were only a few ir researcher in csdepartments in the u s database system wa a separate andthriving field and many felt that ir had stagnated and waslargely irrelevant the truth in fact wa far from that the irresearch community wa a small but dedicated group of researchersin the u s and europe who were motivated by a desire to understandthe process of information retrieval and to build system thatwould help people find the right information in text database this wa and is a hard goal and led to different evaluationmetrics and methodology than the database community progress inthe field wa hampered by a lack of large scale testbeds and testswere limited to database containing at most a few hundred documentabstracts in the s ai boom ir wa still not a mainstream area despiteits focus on a human task involving natural language ir focused ona statistical approach to language rather than the much morepopular knowledge based approach the fact that ir conference mixpapers on effectiveness a measured by human judgment with papersmeasuring performance of file organization for large scale systemshas meant that ir ha always been difficult to classify into simplecategories such a system or ai that are often used in csdepartments since the early s just about everything ha changed large full text database were finally made available for experimentationthrough darpa funding and trec this ha had an enormous positiveimpact on the quantity and quality of ir research the advent ofthe web search engine ha validated the longstanding claim made byir researcher that simple query and ranking were the righttechniques for information access in a largely unstructuredinformation world what ha not changed is that there are stillrelatively few ir researcher in c department there are however many more people in c department doing ir relatedresearch which is just about the same thing conference indatabases machine learning computational linguistics and datamining publish a number of ir paper done by people who would notprimarily consider themselves a ir researcher given that there is an increasing diffusion of ir idea into thecs community it is worth stating what ir a a field of c hasaccomplished search engine have become the infrastructure for much ofinformation access in our society ir ha provided the basicresearch on the algorithm and data structure for these engine and continues to develop new capability such a cross lingualsearch distributed search question answering and topic detectionand tracking ir championed the statistical approach to language long beforeit wa accepted by other researcher working on languagetechnologies statistical nlp is now mainstream and result fromthat field are being used to improve ir system in questionanswering for example ir focused on evaluation a a research area and developed anevaluation methodology based on large standardized testbeds andcomparison with human judgment that ha been adopted byresearchers in a number of other language technology area ir because of it focus on measuring success based on humanjudgments ha always acknowledged the importance of the user andinteraction a a part of information access this led to a numberof contribution to the design of query and search interface andlearning technique based on user feedback although these achievement are important the long term goalsof the ir field have not yet been met what are those goal onepossibility that is often mentioned is the memex of vannevar bush another more recent statement of long term challenge wasmade in the report of the ir challenge workshop global information access satisfy human information needsthrough natural efficient interaction with an automated systemthat leverage world wide structured and unstructured data in anylanguage contextual retrieval combine search technology and knowledgeabout query and user context into a single framework in order toprovide the most appropriate answer for a user s informationneed these goal are in fact very similar to long term challengescoming out of other c field for example jim gray a turingaward winner from the database area mentioned in his address apersonal and world memex a long term goal for his field and c ingeneral ir s long term goal are clearly important long termgoals for the whole of c and achieving those goal will involveeveryone interested in the general area of information managementand retrieval rather than talking about what ir can do inisolation to progress towards it goal i would prefer to talkabout what ir can do in collaboration with other area there are many example of potential collaborative researchareas collaboration with researcher from the nlp and informationextraction community have been developing for some time in orderto study topic such a advanced question answering on the otherhand not enough ha been done to work with the database communityto develop probabilistic retrieval model for unstructured semi structured and structured data there have been a number ofattempts to combine ir and database functionality none of whichhas been particularly successful most recently some group havebeen working on combining ir search with xml document but what isneeded is a comprehensive examination of the issue and problem byteams from both area working together and the creation of newtestbeds that can be used to evaluate proposed model the time isright for such collaboration another example of where database ir and networking people canwork together is in the development of distributed heterogeneousinformation system this requires significant new research inareas like peer to peer architecture semantic heterogeneity automatic metadata generation and retrieval model if the information system described above are extended toinclude new data type such a video image sound and the wholerange of scientific data such a from the bioscience geoscience and astronomy then a broad range of new challenge are added thatneed to be tackled in collaboration with people who know aboutthese type of data there should also be more cooperation between the data mining ir and summarization community to tackle the core problem ofdefining what is new and interesting in stream of data these and other similar collaboration will the basis for thefuture development of the ir field we will continue to work onresearch problem that specifically interest u but this researchwill increasingly be in the context of larger effort ir conceptsand ir research will be an important part of the evolving mix of csexpertise that will be used to solve the grand challenge 
inspired by tensor voting we present luminance voting a novelapproach for image registration with global and local luminancealignment the key to our modeless approach is the directestimation of replacement function by reducing the complexestimation problem to the robust d tensor voting in thecorresponding voting space no model for replacement function isassumed luminance data are first encoded into d ball tensor subject to the monotonic constraint only we vote for an optimalreplacement function by propagating the smoothness constraint usinga dense tensor field our method effectively infers missing curvesegments and reject image outlier without assuming anysimplifying or complex curve model the voted replacement functionsare used in our iterative registration algorithm for computing thebest warping matrix unlike previous approach our robust methodcorrects exposure disparity even if the two overlapping image areinitially misaligned luminance voting is effective in correctingexposure difference eliminating vignette and thus improvingimage registration we present result on a variety of image 
we present a method for constructing ensemble from library of thousand of model model library are generated using different learning algorithm and parameter setting forward stepwise selection is used to add to the ensemble the model that maximize it performance ensemble selection allows ensemble to be optimized to performance metric such a accuracy cross entropy mean precision or roc area experiment with seven test problem and ten metric demonstrate the benefit of ensemble selection 
simplicity of linear representation make them a popular tool in image analysis the two widely used linear representation are i linear projection of image to low dimensional euclidean subspace and ii linear spectral filtering of image in view of the orthogonality and other constraint imposed on these representation the subspace or the filter they take value on nonlinear manifold grassmann stiefel or rotation group we use a family of stochastic algorithm that exploit the geometry of the underlying manifold to find optimal linear representation for specified task a application we demonstrate the effectiveness of algorithm by finding subspace with optimal generalization through separation maximization and filter that are both sparse and effective for recognition we also show empirically how the learned representation can improve the performance of support vector machine 
visual surveillance using a camera network ha imposed newchallenges to camera calibration an essential problem is that alarge number of camera may not have a common field of view or evenbe synchronized well we propose to use a hybrid camera networkthat consists of catadioptric and perspective camera for a visualsurveillance task the relation between multiple view of a scenecaptured from different camera can be then calibrated under thecatadioptric camera s coordinate system this paper address theimportant issue of how to calibrate the hybrid camera network wecalibrate the hybrid camera network in three step first wecalibrate the catadioptric camera using only the vanishing point in order to reduce computational complexity we calibrate thecamera without the mirror first and then calibrate the catadioptriccamera system second we determine d position of some pointsusingas few a two spatial parallel line and some equidistancepoints finally we calibrate other perspective camera based onthese known spatial point 
the rapid growth of the world wide web ha created many challenge for both general purpose crawling search engine and web directory making it difficult to find index and classify web page based on a topic topic driven crawler can complement search engine because they pre classify the page retrieved by the crawl to implement such a focused crawler a strategy for ordering the crawl frontier is required such a strategy can only use information gleaned from previously crawled page to estimate the relevance of a newly observed url because the best strategy for ranking url in the crawl frontier is not immediately apparent we discover strategy by evolving them using a genetic algorithm strategy are learned by evaluating the result of crawl simulated using a database generated by a previous more general crawl we conclude that a rank function that combine analysis of text and link structure yield effective strategy the evolved strategy perform better than the commonly used best first strategy 
abstract standard value function approach to finding policy for partially observable markov decision process pomdps are intractable for large model the intractability of these algorithm is due to a great extent to their generating an optimal policy over the entire belief space however in real pomdp problem most belief state are unlikely and there is a structured low dimensional manifold of plausible belief embedded in the high dimensional belief space we introduce a new method for solving large scale pomdps by taking advantage of belief space sparsity we reduce the dimensionality of the belief space by exponential family principal component analysis which allows u to turn the sparse highdimensional belief space into a compact low dimensional representation in term of learned feature of the belief state we then plan directly on the low dimensional belief feature by planning in a low dimensional space we can find policy for pomdps that are order of magnitude larger than can be handled by conventional technique we demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task 
a neurotrophic model for the co development of topography and ocular dominance column in the primary visual cortex ha recently been proposed in the present work we test this model by driving it with the output of a pair of neuronal vision sensor stimulated by disparate moving pattern we show that the temporal correlation in the spike train generated by the two sensor elicit the development of refined topography and ocular dominance column even in the presence of significant amount of spontaneous activity and fixed pattern noise in the sensor 
we present a new method for automatically creating useful temporal abstraction in reinforcement learning we argue that state that allow the agent to transition to a different region of the state space are useful subgoals and propose a method for identifying them using the concept of relative novelty when such a state is identified a temporally extended activity e g an option is generated that take the agent efficiently to this state we illustrate the utility of the method in a number of task 
we propose a node centroid method with hill climbing to solve the well known matrix bandwidth minimization problem which is to permute row and column of the matrix to minimize it bandwidth many heuristic have been developed for this np complete problem including the cuthill mckee cm and the gibbs poole and stockmeyer gps algorithm recently heuristic such a simulated annealing tabu search and grasp have been used where tabu search and the grasp with path relinking have achieved significantly better solution quality than the cm and gps algorithm experimentation show that the node centroid method achieves the best solution quality when compared with these while being much faster than the newly developed algorithm 
in this note we present a coupled optimization model forboundary determination one part of the model incorporatesa prior shape into a geometric active contour modelwith a fixed parameter the second part determines the best parameter used in the first part by maximizing the mutualinformation of the image geometry between the priorand an aligned novel image over all the alignment thatare the solution of the first part corresponding to differentparameters we also present an alternative method whichgenerates an intensity model formed a the average of a setof aligned training image experimental result on cardiacultrasound image are presented these result indicatethat the proposed model provides close agreement withexpert traced border and the parameter determined in thismodel for one image can be used for image with similarproperties the existence of a solution to the proposed minimizationproblem is also discussed 
we address the problem of identifying specific instance of a class car from a set of image all belonging to that class although we cannot build a model for any particular instance a we may be provided with only one training example of it we can use information extracted from observing other member of the class we pose this task a a learning problem in which the learner is given image pair labeled a matching or not and must discover which image feature are most consistent for matching instance and discriminative for mismatch we explore a patch based representation where we model the distribution of similarity measurement defined on the patch finally we describe an algorithm that selects the most salient patch based on a mutual information criterion this algorithm performs identification well for our challenging dataset of car image after matching only a few well chosen patch 
naive bayes is often used a a baseline in text classification because it is fast and easy to implement it severe assumption make such efficiency possible but also adversely affect the quality of it result in this paper we propose simple heuristic solution to some of the problem with naive bayes classifier addressing both systemic issue a well a problem that arise because text is not actually generated according to a multinomial model we find that our simple correction result in a fast algorithm that is competitive with state of the art text classification algorithm such a the support vector machine 
in this paper we evaluate the use of implicit interest indicator a the basis for user profiling in the digital tv domain research in more traditional domain such a web browsing or usenet news indicates that some implicit interest indicator e g read time and mouse movement are capable of serving a alternative to explicit profile information such a user rating consequently the key question we wish to answer relates to the type of implicit indicator that can be identified within the dtv domain and the extent to which they can accurately reflect a user s true preference 
a longstanding goal of reinforcement learning is to develop nonparametric representation of policy and value function that support rapid learning without suffering from interference or the curse of dimensionality we have developed a trajectory based approach in which policy and value function are represented nonparametrically along trajectory these trajectory policy and value function are updated a the value function becomes more accurate or a a model of the task is updated we have applied this approach to periodic task such a hopping and walking which required handling discount factor and discontinuity in the task dynamic and using function approximation to represent value function at discontinuity we also describe extension of the approach to make the policy more robust to modeling error and sensor noise 
recent year have witnessed a dramatic increase in the quantity of image data collected due to advance in field such a medical imaging reconnaissance surveillance astronomy multimedia etc with this increase ha come the need to be able to store transmit and query large volume of image data efficiently a common operation on image database is the retrieval of all image that are similar to a query image for this the image in the database are often represented a vector in a high dimensional space and a query is answered by retrieving all image vector that are proximal to the query image in this space under a suitable similarity metric to overcome problem associated with high dimensionality such a high storage and retrieval time a dimension reduction step is usually applied to the vector to concentrate relevant information in a small number of dimension principal component analysis pca is a well known dimension reduction scheme however since it work with vectorized representation of image pca doe not take into account the spatial locality of pixel in image in this paper a new dimension reduction scheme called generalized principal component analysis gpca is presented this scheme work directly with image in their native state a two dimensional matrix by projecting the image to a vector space that is the tensor product of two lower dimensional vector space experiment on database of face image show that for the same amount of storage gpca is superior to pca in term of quality of the compressed image query precision and computational cost 
this paper proposes the use of uncertainty reduction in machine learning method such a co training and bilingual boot strapping which are referred to in a general term a collaborative bootstrapping the paper indicates that uncertainty reduction is an important factor for enhancing the performance of collaborative bootstrapping it proposes a new measure for representing the degree of uncertainty correlation of the two classifier in collaborative bootstrapping and us the measure in analysis of collaborative bootstrapping furthermore it proposes a new algorithm of collaborative bootstrapping on the basis of uncertainty reduction experimental result have verified the correctness of the analysis and have demonstrated the significance of the new algorithm 
this paper describes and evaluates mop an ie system for automatic extraction of metalinguistic information from technical and scientific document we claim that such a system can create special database to bootstrap compilation and facilitate update of the huge and dynamically changing glossary knowledge base and ontology that are vital to modern day research 
this paper describes a jam session system that enables a human player to interplay with virtual player which can imitate the player personality model of various human player previous system have parameter that allow some alteration in the way virtual player react but these system cannot imitate human personality our system can obtain three kind of player personality model from a midi recording of a session in which that player participated a reaction model a phrase model and a groove model the reaction model is the characteristic way that a player reacts to other player and it can be statistically learned from the relationship between the midi data of music the player listens to and the midi data of music improvised by that player the phrase model is a set of player s characteristic phrase it can be acquired through musical segmentation of a midi session recording by using voronoi diagram on a piano roll the groove model is a model that generates onset time deviation it can be acquired by using a hidden markov model experimental result show that the personality model of any player participating in a guitar trio session can be derived from a midi recording of that session 
this paper address the problem of probabilistically modeling d human motion for synthesis and tracking given the high dimensional nature of human motion learning an explicit probabilistic model from available training data is currently impractical instead we exploit method from texture synthesis that treat image a representing an implicit empirical distribution these method replace the problem of representing the probability of a texture pattern with that of searching the training data for similar instance of that pattern we extend this idea to temporal data representing d human motion with a large database of example motion to make the method useful in practice we must address the problem of efficient search in a large training set efficiency is particularly important for tracking towards that end we learn a low dimensional linear model of human motion that is used to structure the example motion database into a binary tree an approximate probabilistic tree search method exploit the coefficient of this low dimensional representation and run in sub linear time this probabilistic tree search return a particular sample human motion with probability approximating the true distribution of human motion in the database this sampling method is suitable for use with particle filtering technique and is applied to articulated d tracking of human within a bayesian framework successful tracking result are presented along with example of synthesizing human motion using the model 
high dimensional directional data is becoming increasingly important in contemporary application such a analysis of text and gene expression data a natural model for multi variate directional data is provided by the von mi fisher vmf distribution on the unit hypersphere that is analogous to the multi variate gaussian distribution in rd in this paper we propose modeling complex directional data a a mixture of vmf distribution we derive and analyze two variant of the expectation maximization em framework for estimating the parameter of this mixture we also propose two clustering algorithm corresponding to these variant an interesting aspect of our methodology is that the spherical kmeans algorithm kmeans with cosine similarity can be shown to be a special case of both our algorithm thus modeling text data by vmf distribution lends theoretical validity to the use of cosine similarity which ha been widely used by the information retrieval community a part of experimental validation we present result on modeling high dimensional text and gene expression data a a mixture of vmf distribution the result indicate that our approach yield superior clustering especially for difficult clustering task in high dimensional space 
the problem of web cache consistency continues to be an important one current web cache use heuristic based policy for determining the freshness of cached object often forcing content provider to unnecessarily mark their content a uncacheable simply to retain control over it server driven invalidation ha been proposed a a mechanism for providing strong cache consistency for web object but it requires server to maintain per client state even for infrequently changing object we propose an alternative approach to strong cache consistency called monarch which doe not require server to maintain per client state in this work we focus on a new approach for evaluation of monarch in comparison with current practice and other cache consistency policy this approach us snapshot of content collected from real web site a input to a simulator result of the evaluation show monarch generates little more request traffic than an optimal cache coherency policy 
abstract we propose the framework of mutual information kernel for learning covariance kernel a used in support vector machine and gaussian process classiers from unlabeled task data using bayesian technique we describe a powerful implementation of this framework which us variational bayesian mixture of factor analyzer in order to attack classication problem in high dimensional space where labeled data is sparse but unlabeled data is abundant content 
abstract keyphrases are useful for a variety of purpose including summarizing indexing labeling categorizing clustering highlighting browsing andsearching the task of automatic keyphrase extractionis to select keyphrases from within the text of a givendocument automatic keyphrase extraction make itfeasible to generate keyphrases for the huge number ofdocuments that do not have manually assignedkeyphrases a limitation of previous keyphraseextraction algorithm is that the 
we propose a framework to incorporate unlabeled data in kernel classifier based on the idea that two point in the same cluster are more likely to have the same label this is achieved by modifying the eigenspectrum of the kernel matrix experimental result ass the validity of this approach 
latent semantic indexing lsi is a well established and effective framework for conceptual information retrieval in traditional implementation of lsi the semantic structure of the collection is projected into the k dimensional space derived from a rank k approximation of the original term by document matrix this paper discus a new way to implement the lsi methodology based on polynomial filtering the new framework doe not rely on any matrix decomposition and therefore it computational cost and storage requirement are low relative to traditional implementation of lsi additionally it can be used a an effective information filtering technique when updating lsi model based on user feedback 
an important class of search on the world wide web ha the goal to find an entry page homepage of an organisation entry page search is quite different from ad hoc search indeed a plain ad hoc system performs disappointingly we explored three non content feature of web page page length number of incoming link and url form especially the url form proved to be a good predictor using url form prior we found over of all entry page at rank and up to in the top non content feature can easily be embedded in a language model framework a a prior probability 
a method is described for real time market intelligence and competitive analysis news story are collected online for a designated group of company the goal is to detect critical difference in the text written about a company versus the text for it competitor a solution is found by mapping the task into a non stationary text categorization model the overall design consists of the following component a a real time crawler that monitor newswires for story about the competitor b a conditional document retriever that selects only those document that meet the indicated condition c text analysis technique that convert the document to a numerical format d rule induction method for finding pattern in data e presentation technique for displaying result the method is extended to combine text with numerical measure such a those based on stock price and market capitalization that allow for more objective evaluation and projection 
in this paper we examine method for comparing human and agent behavior the result of such a comparison can be used to validate a computer model of human behavior score a turning test or guide an intelligent tutoring system we introduce behavior bounding an automated model based approach for behavior comparison we identify how this approach can be used with both human and agent behavior we demonstrate that it requires minimal human effort to use and that it is efficient when working with complex agent finally we show empirical result indicating that this approach is effective at identifying behavioral problem in certain type of agent and that it ha superior performance when compared against two benchmark 
amino acid profile which capture position specific mutation probability are a richer encoding of biological sequence than the individual sequence themselves however profile comparison are much more computationally expensive than discrete symbol comparison making profile impractical for many large datasets furthermore because they are such a rich representation profile can be dicult to visualize to overcome these problem we propose a discretization for profile using an expanded alphabet representing not just individual amino acid but common profile by using an extension of information bottleneck ib incorporating constraint and prior on the class distribution we find an informationally optimal alphabet this discretization yield a concise informative textual representation for profile sequence also alignment between these sequence while nearly a accurate a the full profileprofile alignment can be computed almost a quickly a those between individual or consensus sequence a full pairwise alignment of swissprot would take year using profile but le than day using a discrete ib encoding illustrating how discrete encoding can expand the range of sequence problem to which profile information can be applied 
in spite of the increase in the availability of mobile device in the last few year web information is not yet a accessible from pda or wap phone a it is from the desktop in this paper we propose a solution for supporting one of the most popular information discovery mechanism namely web directory navigation from mobile device our proposed solution consists of caching enough information on the device itself in order to conduct most of the navigation action locally with subsecond response time while intermittently communicating with the server to receive update and additional data requested by the user the cached information is captured in a directory capsule the directory capsule represents only the portion of the directory that is of interest to the user in a given context and is sufficiently rich and consistent to support the information need of the user in disconnected mode we define a novel subscription model specifically geared for web directory and for the special need of pda this subscription model enables user to specify the part of the directory that are of interest to them a well a the preferred granularity we describe a mechanism for keeping the directory capsule in sync over time with the web directory and user subscription request finally we present the pocket directory browser for palm powered computer that we have developed the pocket directory can be used to define view and manipulate the capsule that are stored on the palm we provide several usage example of our system on the open directory project one of the largest and most popular web directory 
we propose to use the community structure of usenet for organizing and retrieving the information stored in newsgroups in particular we study the network formed by cross post message that are posted to two or more newsgroups simultaneously we present what is to our knowledge by far the most detailed data that ha been collected on usenet cross posting we analyze this network to show that it is a small world network with significant clustering we also present a spectral algorithm which cluster newsgroups based on the cross post matrix the result of our clustering provides a topical classification of newsgroups our clustering give many example of significant relationship that would be missed by semantic clustering method 
in this paper we propose a probabilistic model for online document clustering we use non parametric dirichlet process prior to model the growing number of cluster and use a prior of general english language model a the base distribution to handle the generation of novel cluster furthermore cluster uncertainty is modeled with a bayesian dirichletmultinomial distribution we use empirical bayes method to estimate hyperparameters based on a historical dataset our probabilistic model is applied to the novelty detection task in topic detection and tracking tdt and compared with existing approach in the literature 
this paper introduces a new distinctive class of combinatorial auction protocol called price oriented rationing free porf protocol the outline of a porf protocol is a follows i for each bidder the price of each bundle of good is determined independently of his her own declaration while it can depend on the declaration of other bidder ii we allocate each bidder a bundle that maximizes his her utility independently of the allocation of other bidder i e rationing free although a porf protocol appears quite different from traditional protocol description surprisingly it is a sufficient and necessary condition for a protocol to be strategy proof furthermore we show that a porf protocol satisfying additional condition is false name proof at the same time any false name proof protocol can be described a a porf protocol that satisfies the additional condition a porf protocol is an innovative characterization of strategy proof protocol and the first attempt to characterize false name proof protocol such a characterization is not only theoretically significant but also useful in practice since it can serve a a guideline for developing new strategy false name proof protocol we present a new false name proof protocol based on the concept of a porf protocol 
we establish learning rate to the bayes risk for support vector machine svms with hinge loss in particular for svms with gaussian rbf kernel we propose a geometric condition for distribution which can be used to determine approximation property of these kernel finally we compare our method with a recent paper of g blanchard et al 
sentiment classification is the task of labeling a review document according to the polarity of it prevailing opinion favorable or unfavorable in approaching this problem a model builder often ha three source of information available a small collection of labeled document a large collection of unlabeled document and human understanding of language ideally a learning method will utilize all three source to accomplish this goal we generalize an existing procedure that us the latter two we extend this procedure by re interpreting it a a naive bayes model for document sentiment viewed a such it can also be seen to extract a pair of derived feature that are linearly combined to predict sentiment this perspective allows u to improve upon previous method primarily through two strategy incorporating additional derived feature into the model and where possible using labeled data to estimate their relative influence 
we present a framework for mining association rule from transaction consisting of categorical item where the data ha been randomized to preserve privacy of individual transaction while it is feasible to recover association rule and preserve privacy using a straightforward uniform randomization the discovered rule can unfortunately be exploited to find privacy breach we analyze the nature of privacy breach and propose a class of randomization operator that are much more effective than uniform randomization in limiting the breach we derive formula for an unbiased support estimator and it variance which allow u to recover itemset support from randomized datasets and show how to incorporate these formula into mining algorithm finally we present experimental result that validate the algorithm by applying it on real datasets 
page ranking is a fundamental step towards the construction of effective search engine for both generic horizontal and focused vertical search ranking scheme for horizontal search like the pagerank algorithm used by google operate on the topology of the graph regardless of the page content on the other hand the recent development of vertical portal vortals make it useful to adopt scoring system focussed on the topic and taking the page content into account in this paper we propose a general framework for web page scoring system wpss which incorporates and extends many of the relevant model proposed in the literature finally experimental result are given to ass the feature of the proposed scoring system with special emphasis on vertical search 
over the year software engineering researcher have suggested numerous technique for estimating development effort these technique have been classified mainly a algorithmic machine learning and expert judgement several study have compared the prediction accuracy of those technique with emphasis placed on linear regression stepwise regression and case based reasoning cbr to date no converging result have been obtained and we believe they may be influenced by the use of the same cbr configuration the objective of this paper is twofold first to describe the application of case based reasoning for estimating the effort for developing web hypermedia application second comparing the prediction accuracy of different cbr configuration using two web hypermedia datasets result show that for both datasets the best estimation were obtained with weighted euclidean distance using either one analogy dataset or analogy dataset we suggest therefore that case based reasoning is a candidate technique for effort estimation and with the aid of an automated environment can be applied to web hypermedia development effort prediction 
a novel native stochastic local search algorithm for solving k term dnf problem is presented it is evaluated on hard k term dnf problem that lie on the phase transition and compared to the performance of gsat and walksat type algorithm on sat encoding of k term dnf problem we also evaluate state of the art separate and conquer algorithm on these problem finally we demonstrate the practical relevance of our algorithm on a chess endgame database 
introductioninformation retrieval is in large part the study of method for assessing the similarity of pair ofdocuments document similarity metric have been used for many task including ad hocdocument retrieval text classification yc and summarization gc ssmb another problem area in which similarity metric are central is record linkage e g ka where one wish to determine if two database record taken from different source database referto the same 
we present a novel approach to surface reconstruction from multiple image the central idea is to explore the integration of both d stereo data and d calibrated image this is motivated by the fact that only robust and accurate feature point that survived the geometry scrutiny of multiple image are reconstructed in space the density insuciency and the inevitable hole in the stereo data should be lled in by using information from multiple image the idea is therefore to rst construct small surface patch from stereo point then to progressively propagate only reliable patch in their neighborhood from image into the whole surface using a best rst strategy the problem reduces to searching for an optimal local surface patch going through a given set of stereo point from image this constrained optimization for a surface patch could be handled by a local graph cut that we develop real experiment demonstrate the usability and accuracy of the approach 
the problem of detecting atypical object or outlier is one of the classical topic in robust statistic recently it ha been proposed to address this problem by mean of one class svm classifier the main conceptual shortcoming of most one class approach however is that in a strict sense they are unable to detect outlier since the expected fraction of outlier ha to be specified in advance the method presented in this paper overcomes this problem by relating kernelized one class classification to gaussian density estimation in the induced feature space having established this relation it is possible to identify atypical object by quantifying their deviation from the gaussian model for rbf kernel it is shown that the gaussian model is rich enough in the sense that it asymptotically provides an unbiased estimator for the true density in order to overcome the inherent model selection problem a cross validated likelihood criterion for selecting all free model parameter is applied 
an odd cycle of a logic program is a simple cycle that ha an odd number of negative edge in the dependency graph of the program similarly an even cycle is one that ha an even number of negative edge for a normal logic program that ha no odd cycle while it is known that such a program always ha a stable model and such a stable model can be computed in polynomial time we show in this paper that checking whether an atom is in a stable model is np complete and checking whether an atom is in all stable model is co np complete both are the same a in the general case for normal logic program furthermore we show that if a normal logic program ha exactly one odd cycle then checking whether it ha a stable model is np complete again the same a in the general case for normal logic program with a fixed number of even cycle we show that there is a polynomial time algorithm for computing all stable model furthermore this polynomial time algorithm can be improved significantly if the number of odd cycle is also fixed 
algorithm based on following local gradient information are surprisingly effective for certain class of constraint satisfaction problem unfortunately previous local search algorithm are notoriously incomplete they are not guaranteed to find a feasible solution if one exists and they cannot be used to determine unsatisfiability we present an algorithmic framework for complete local search and discus in detail an instantiation for the propositional satisfiability problem sat the fundamental idea is to use constraint learning in combination with a novel objective function that converges during search to a surface without local minimum although the algorithm ha worst case exponential space complexity we present empirical resulls on challenging sat competition benchmark that suggest that our implementation can perform a well a state of the art solver based on more mature technique our framework suggests a range of possible algorithm lying between tree based search and local search 
many common web search by their nature have a very small number of relevant document homepage and namedpage searching are known item search where there is only a single relevant document topic distillation is a special kind of topical relevance search where the user wish to find a few key web site rather than every relevant web page because these type of search are so common web search evaluation have come to focus on task where there are very few relevant document evaluation with few relevant document pose special challenge for current metric in particular the trec topic distillation evaluation is unable to distinguish most submitted run from each other 
we present a new method for calculating approximate marginals for probability distribution defined by graph with cycle based on a gaussian entropy bound combined with a semidefinite outer bound on the marginal polytope this combination lead to a log determinant maximization problem that can be solved by efficient interior point method a with the bethe approximation and it generalization the optimizing argument of this problem can be taken a approximation to the exact marginals in contrast to bethe kikuchi approach our variational problem is strictly convex and so ha a unique global optimum an additional desirable feature is that the value of the optimal solution is guaranteed to provide an upper bound on the log partition function in experimental trial the performance of the log determinant relaxation is comparable to or better than the sum product algorithm and by a substantial margin for certain problem class finally the zero temperature limit of our log determinant relaxation recovers a class of well known semidefinite relaxation for integer programming e g 
due to the high number of inflectional variation of arabic word empirical result suggest that stemming is essential for arabic information retrieval however current light stemming algorithm do not extract the correct stem of irregular so called broken plural which constitute of arabic text and of plural although light stemming in particular ha led to improvement in information retrieval the effect of broken plural on the performance of information retrieval system ha not been examined we propose a light stemmer that incorporates a broken plural recognition component and evaluate it within the context of information retrieval our result show that identifying broken plural and reducing them to their correct stem doe result in a significant improvement in the performance of information retrieval system 
many application in text and speech processing require the analysis of distribution of variable length sequence we recently introduced a general kernel framework rational kernel to extend kernel method to the analysis of such variable length sequence or more generally weighted automaton these kernel are efficient to compute and have been successfully used in application such a spoken dialog classification using support vector machine however the rational kernel previously introduced do not fully encompass distribution over alternate sequence prior similarity measure between two weighted automaton are based only on the expected count of co occurring subsequence and ignore similarity or dissimilarity in higher order moment of the distribution of these count in this paper we introduce a new family of rational kernel moment kernel that precisely exploit this additional information these kernel are distribution kernel based on moment of count of string we describe efficient algorithm to compute moment kernel and apply them to several difficult spoken dialog classification task our experiment show that using the second moment of the count of n gram sequence consistently improves the classification accuracy in these task 
this paper explores the use of multisensory information fusion technique with dynamic bayesian network dbns for modeling and understanding the temporal behavior of facial expression in image sequence our approach to the facial expression understanding lie in a probabilistic framework by integrating the dbns with the facial action unit au from psychological view the dbns provide a coherent and unified hierarchical probabilistic framework to represent spatial and temporal information related to facial expression and to actively select the most informative visual cue from the available information to minimize the ambiguity in recognition the recognition of facial expression is accomplished by fusing not only from the current visual observation but also from the previous visual evidence consequently the recognition becomes more robust and accurate through modeling the temporal behavior of facial expression experimental result demonstrate that our approach is more admissible for facial expression analysis in image sequence 
we present and discus the important business problem of estimating the effect of retention effort on the lifetime value of a customer in the telecommunication industry we discus the component of this problem in particular customer value and length of service or tenure modeling and present a novel segment based approach motivated by the segment level view marketing analyst usually employ we then describe how we build on this approach to estimate the effect of retention on lifetime value our solution ha been successfully implemented in amdocs business insight bi platform and we illustrate it usefulness in real world scenario 
we define noun phrase translation a a subtask of machine translation this enables u to build a dedicated noun phrase translation subsystem that improves over the currently best general statistical machine translation method by incorporating special modeling and special feature we achieved translation accuracy in a german english translation task v with ibm model 
this paper describes seeker a platform for large scale text analytics and semtag an application written on the platform to perform automated semantic tagging of large corpus we apply semtag to a collection of approximately million web page and generate approximately million automatically disambiguated semantic tag published to the web a a label bureau providing metadata regarding the million annotation to our knowledge this is the largest scale semantic tagging effort to date we describe the seeker platform discus the architecture of the semtag application describe a new disambiguation algorithm specialized to support ontological disambiguation of large scale data evaluate the algorithm and present our final result with information about acquiring and making use of the semantic tag we argue that automated large scale semantic tagging of ambiguous content can bootstrap and accelerate the creation of the semantic web 
in most research on concept acquisition from corpus concept are modeled a vector of relation extracted from syntactic structure in the case of modifier these relation often specify value of attribute a in attr red this is unlike what typically proposed in theory of knowledge representation where concept are typically defined in term of their attribute e g color we compared model of concept based on value with model based on attribute using lexical clustering a the basis for comparison we find that attribute based model work better than value based one and result in shorter description but that mixed model including both the best attribute and the best value work best of all 
an additive fluent is a fluent with numerical value such that the effect of several concurrently executed action on it can be computed by adding the effect of the individual action we propose a method for describing effect of action on additive fluents in the declarative language c an implementation of this language called the causal calculator can be used for the automation of example of commonsense reasoning involving additive fluents 
an adaptive semi supervised ensemble method assemble is proposed that construct classification ensemble based on both labeled and unlabeled data assemble alternate between assigning pseudo class to the unlabeled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudolabeled data mathematically this intuitive algorithm corresponds to maximizing the classification margin in hypothesis space a measured on both the labeled and unlabeled of data unlike alternative approach assemble doe not require a semi supervised learning method for the base classifier assemble can be used in conjunction with any cost sensitive classification algorithm for both two class and multi class problem assemble using decision tree won the nip unlabeled data competition in addition strong result on several benchmark datasets using both decision tree and neural network support the proposed method 
in the context of binary classification we define disagreement a a measure of how often two independently trained model differ in their classification of unlabeled data we explore the use of disagreement for error estimation and model selection we call the procedure co validation since the two model effectively in validate one another by comparing result on unlabeled data which we assume is relatively cheap and plentiful compared to labeled data we show that per instance disagreement is an unbiased estimate of the variance of error for that instance we also show that disagreement provides a lower bound on the prediction generalization error and a tight upper bound on the variance of prediction error or the variance of the average error across instance where variance is measured across training set we present experimental result on several data set exploring co validation for error estimation and model selection the procedure is especially effective in active learning setting where training set are not drawn at random and cross validation overestimate error 
this paper report on a family of computationally practical classifier that converge to the bayes error at near minimax optimal rate for a variety of distribution the classifier are based on dyadic classification tree dcts which involve adaptively pruned partition of the feature space a key aspect of dcts is their spatial adaptivity which enables local rather than global fitting of the decision boundary ou r risk analysis involves a spatial decomposition of the usual concentration inequality leading to a spatially adaptive data dependent pruning cr iterion for any distribution on x y whose bayes decision boundary behaves locally like a lipschitz smooth function we show that the dct error converges to the bayes error at a rate within a logarithmic factor of the minimax optimal rate we also study dcts equipped with polynomial classification rule at each leaf and show that a the smoothness of the boundary increase their error converge to the bayes error at a rate a pproaching n the parametric rate we are not aware of any other practical classifier that provide similar rate of convergence guarantee f ast algorithm for tree pruning are discussed 
the coalition search and rescue task support demonstration show cooperative agent supporting a highly dynamic mission in which ai task planning interagent collaboration workflow enactment policy managed communication semantic web query semantic web service matchmaking and knowledge based notification are employed 
particle filter provide a mean to track the state of an object even when the dynamic and the observation are non linear non gaussian however they can be very inefficient when the observation noise is low a compared to the system noise a it is often the case in visual tracking application in this paper we propose a new two stage sampling procedure to boost the performance of particle filter under this condition the new procedure is shown to reduce the variance of the weight by mean of a theoretical analysis this result is confirmed in a series of synthetic and real world visual tracking experiment 
most test collection like trec and clef for experimental research in information retrieval apply binary relevance assessment this paper introduces a four point relevance scale and report the finding of a project in which trec and trec document pool on topic were reassessed the goal of the reassessment wa to build a subcollection of trec for experiment on highly relevant document and to learn about the assessment process a well a the characteristic of a multigraded relevance corpus relevance criterion were defined so that a distinction wa made between document rich in topical information relevant and highly relevant document and poor in topical information marginally relevant document it turned out that about of document assessed a relevant were regarded a marginal the characteristic of the relevance corpus and lesson learned from the reassessment project are discussed the need to develop more elaborated relevance assessment scheme is emphasized 
library have traditionally used manual image annotation for indexing and then later retrieving their image collection however manual image annotation is an expensive and labor intensive procedure and hence there ha been great interest in coming up with automatic way to retrieve image based on content here we propose an automatic approach to annotating and retrieving image based on a training set of image we assume that region in an image can be described using a small vocabulary of blob blob are generated from image feature using clustering given a training set of image with annotation we show that probabilistic model allow u to predict the probability of generating a word given the blob in an image this may be used to automatically annotate and retrieve image given a word a a query we show that relevance model allow u to derive these probability in a natural way experiment show that the annotation performance of this cross medium relevance model is almost six time a good in term of mean precision than a model based on word blob co occurrence model and twice a good a a state of the art model derived from machine translation our approach show the usefulness of using formal information retrieval model for the task of image annotation and retrieval 
the web ontology language owl defines three class of document lite dl and full all rdf xml document are owl full document some owl full document are also owl dl document and some owl dl document are also owl lite document this paper discus parsing and specie recognition that is the process of determining whether a given document fall into the owl lite dl or full class wedescribe two alternative approach to this task one based on abstract syntax tree the other on rdf triple and compare their key characteristic 
a key missing component in information retrieval system is self diagnostic test to establish whether the system can provide reasonable result for a given query on a document collection if we can measure property of a retrieved set of document which allow u to predict average precision we can automate the decision of whether to elicit relevance feedback or modify the retrieval system in other way we use meta data attached to document in the form of time stamp to measure the distribution of document retrieved in response to a query over the time domain to create a temporal profile for a query we define some useful feature over this temporal profile we find that using these temporal feature together with the content of the document retrieved we can improve the prediction of average precision for a query 
this paper describes a novel application of statisticallearning theory slt to control model complexity in flowestimation slt provides analytical generalization boundssuitable for practical model selection from small and noisydata set of image measurement normal flow the methodaddresses the aperture problem by using the penalized risk ridge regression we demonstrate an application of thismethod on both synthetic and real image sequence and useit for motion interpolation and extrapolation our experimentalresults show that our approach compare favorablyagainst alternative model selection method such a theakaike s final prediction error schwartz s criterion generalizedcross validation and shibata s model selector 
recent research on pattern discovery ha progressed form mining frequent itemsets and sequence to mining structured pattern including tree lattice and graph a a general data structure graph can model complicated relation among data with wide application in bioinformatics web exploration and etc however mining large graph pattern in challenging due to the presence of an exponential number of frequent subgraphs instead of mining all the subgraphs we propose to mine closed frequent graph pattern a graph g is closed in a database if there exists no proper supergraph of g that ha the same support a g a closed graph pattern mining algorithm closegraph is developed by exploring several interesting pruning method our performance study show that closegraph not only dramatically reduces unnecessary subgraphs to be generated but also substantially increase the efficiency of mining especially in the presence of large graph pattern 
decoding is a strategy that allows u to ass the amount of information neuron can provide about certain aspect of the visual scene in this study we develop a method based on bayesian sequential updating and the particle filtering algorithm to decode the activity of v neuron in awake monkey a distinction in our method is the use of volterra kernel to filter the particle which live in a high dimensional space this parametric bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder interestingly our result suggest that for decoding in real time spike train of a few a independent but similar neuron would be sufficient for decoding a critical scene variable in a particular class of visual stimulus the reconstructed variable can predict the neural activity about a well a the actual signal with respect to the volterra kernel bayesian decoding scheme which are nonlinear might be useful in this context bayesian sequential updating or belief propagation implemented in the form of particle filtering ha recently been used in estimating the hand trajectory of monkey based on m neuron s response and the location of a rat based on the response of the place cell in the hippocampus however linear method have been shown to be quite adequate for decoding lgn motor cortical or hippocampal place cell signal using population vector or the optimal linear decoder bayesian method with proper probability model assumption could work better than the linear method but they apparently are not critical to solving those problem these method may be more useful or important in the decoding of nonlinear visual neuronal response here we implement an algorithm based on bayesian sequential updating in the form particle filtering to decode nonlinear visual neuron in awake behaving monkey the strategy is similar to the one used by brown et 
active learning al promise to reduce the cost of annotating labeled datasets for trainable human language technology contrary to expectation when creating labeled training material for hpsg parse selection and later reusing it with other model gain from al may be negligible or even negative this ha serious implication for using al showing that additional cost saving strategy may need to be adopted we explore one such strategy using a model during annotation to automate some of the decision our best result show an reduction in annotation cost compared with labeling randomly selected data with a single model 
this paper describes an evaluation method based on term relevance set trels that measure an ir system s quality by examining the content of the retrieved result rather than by looking for pre specified relevant page trels consist of a list of term believed to be relevant for a particular query a well a a list of irrelevant term the proposed method doe not involve any document relevance judgment and a such is not adversely affected by change to the underlying collection therefore it can better scale to very large dynamic collection such a the web moreover this method can evaluate a system s effectiveness on an updatable live collection or on collection derived from different data source our experiment show that the proposed method is very highly correlated with official trec measure 
we introduce a new algorithm based on linear programming that approximates the dieren tial value function of an average cost markov decision process via a linear combination of pre selected basis function the algorithm carry out a form of cost shaping and minimizes a version of bellman error we establish an error bound that scale gracefully with the number of state without imposing the strong lyapunov condition required by it counterpart in we propose a path following method that automates selection of important algorithm parameter which represent counterpart to the state relevance weight studied in 
this paper proposes a flexible architecture for the creation of internet auction it allows the custom definition of the auction parameter and provides a decentralized control of the auction process auction policy are defined a law in the law governed interaction lgi paradigm each of these law specifies not only the auction algorithm itself e g open cry dutch etc but also how to handle the other parameter usually involved in the online auction such a certification auditioning and treatment of complaint lgi is used to enforce the rule established in the auction policy within the agent involved in the process after the agent find out about the action they interact in a peer to peer communication protocol reducing the role of the centralized auction room to an advertising registry and taking profit of the distributed nature of the internet to conduct the auction the paper present an example of an auction law illustrating the use of the proposed architecture 
this paper present an approach to develop bidding agent that participate in multiple alternative auction with the goal of obtaining an item at the lowest price the approach consists of a prediction method and a planning algorithm the prediction method exploit the history of past auction in order to build probability function capturing the belief that a bid of a given price may win a given auction the planning algorithm computes the lowest price such that by sequentially bidding in a subset of the relevant auction the agent can obtain the item at that price with an acceptable probability the approach address the case where the auction are for substitutable item with different value experimental result are reported showing that the approach increase the payoff of their user and the welfare of the market 
visual hull vh construction from silhouette image isa popular method of shape estimation the method alsoknown a shape from silhouette sfs is used in many applicationssuch a non invasive d model acquisition obstacleavoidance and more recently human motion trackingand analysis one of the limitation of sfs however is that the approximated shape can be very coarse whenthere are only a few camera in this paper we proposean algorithm to improve the shape approximation by 
consider a number of moving point where each point is attached to a joint of the human body and projected onto an image plane johannson showed that human can effortlessly detect and recognize the presence of other human from such display this is true even when some of the body point are missing e g because of occlusion and unrelated clutter point are added to the display we are interested in replicating this ability in a machine to this end we present a labelling and detection scheme in a probabilistic framework our method is based on representing the joint probability density of position and velocity of body point with a graphical model and using loopy belief propagation to calculate a likely interpretation of the scene furthermore we introduce a global variable representing the body s centroid experiment on one motion captured sequence suggest that our scheme improves on the accuracy of a previous approach based on triangulated graphical model especially when very few part are visible the improvement is due both to the more general graph structure we use and more significantly to the introduction of the centroid variable 
test collection model use case in way that facilitate evaluation of information retrieval system this paper describes the use of search guided relevance assessment to create a test collection for retrieval of spontaneous conversational speech approximately thematically coherent segment were manually identified in hour of oral history interview with individual automatic speech recognition result manually prepared summary controlled vocabulary indexing and name authority control are available for every segment those feature were leveraged by a team of four relevance assessor to identify topically relevant segment for topic developed from actual user request search guided assessment yielded sufficient inter annotator agreement to support formative evaluation during system development baseline result for ranked retrieval are presented to illustrate use of the collection 
senseclusters is a freely available word sense discrimination system that take a purely unsupervised clustering approach it us no knowledge other than what is available in a raw unstructured corpus and cluster instance of a given target word based only on their mutual contextual similarity it is a complete system that provides support for feature selection from large corpus several different context representation scheme various clustering algorithm and evaluation of the discovered cluster 
we present a class of algorithm for learning the structure of graphical model from data the algorithm are based on a measure known a the kernel generalized variance kgv which essentially allows u to treat all variable on an equal footing a gaussians in a feature space obtained from mercer kernel thus we are able to learn hybrid graph involving discrete and continuous variable of arbitrary type we explore the computational property of our approach showing how to use the kernel trick to compute the relevant statistic in linear time we illustrate our framework with experiment involving discrete and continuous data 
when we learn a new motor skill we have to contend with both the variability inherent in our sensor and the task the sensory uncertainty can be reduced by using information about the distribution of previously experienced task here we impose a distribution on a novel sensorimotor task and manipulate the variability of the sensory feedback we show that subject internally represent both the distribution of the task a well a their sensory uncertainty moreover they combine these two source of information in a way that is qualitatively predicted by optimal bayesian processing we further analyze if the subject can represent multimodal distribution such a mixture of gaussians the result show that the cns employ probabilistic model during sensorimotor learning even when the prior are multimodal 
learning algorithm have enjoyed numerous success in robotic control task in problem with time varying dynamic online learning method have also proved to be a powerful tool for automatically tracking and or adapting to the changing circumstance however for safety critical application such a airplane flight the adoption of these algorithm ha been significantly hampered by their lack of safety such a stability guarantee rather than trying to show difficult a priori stability guarantee for specific learning method in this paper we propose a method for monitoring the controller suggested by the learning algorithm online and rejecting controller leading to instability we prove that even if an arbitrary online learning method is used with our algorithm to control a linear dynamical system the resulting system is stable 
in this paper we propose a novel method to establish temporalcorrespondence between the frame of two video d epipolargeometry is used to eliminate the distortion generated bythe projection from d to d although the fundamental matrixcontains the extrinsic property of the projective geometrybetween view it is sensitive to noise therefore wepropose the use of a rank constraint of corresponding pointsin two view to measure the similarity between trajectory this rank constraint show more robustness and avoids computationof the fundamental matrix a dynamic programmingapproach using the similarity measurement is proposed to findthe non linear time warping function for video containinghuman activity in this way video of different individualstaken at different time and from distinct viewpoint canbe synchronized a temporal pyramid of trajectory is appliedto improve the accuracy of the view invariant dynamictime warping approach we show various application of thisapproach such a video synthesis human action recognition and computer aider training compared to state of the arttechniques our method show a great improvement 
in this paper we investigate the practical applicability of co training for the task of building a classifier for reference resolution we are concerned with the question if co training can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance 
previous approach to pronominalization have largely been theoretical rather than applied in nature frequently such method are based on centering theory which deal with the resolution of anaphoric pronoun but it is not clear that complex theoretical mechanism while having satisfying explanatory power are necessary for the actual generation of pronoun we first illustrate example of pronoun from various domain describe a simple method for generating pronoun in an implemented multi page generation system and present an evaluation of it performance 
this paper explores feature scoring and selection based on weight from linear classification model it investigates how these method combine with various learning model our comparative analysis includes three learning algorithm na ve bayes perceptron and support vector machine svm in combination with three feature weighting method odds ratio information gain and weight from linear model the linear svm and perceptron experiment show that feature selection using weight from linear svms yield better classification performance than other feature weighting method when combined with the three explored learning algorithm the result support the conjecture that it is the sophistication of the feature weighting method rather than it apparent compatibility with the learning algorithm that improves classification performance 
we are interested in finding natural community in large scale linked network our ultimate goal is to track change over time in such community for such temporal tracking we require a clustering algorithm that is relatively stable under small perturbation of the input data we have developed an efficient scalable agglomerative strategy and applied it to the citation graph of the nec citeseer database paper million citation agglomerative clustering technique are known to be unstable on data in which the community structure is not strong we find that some community are essentially random and thus unstable while others are natural and will appear in most clustering these natural community will enable u to track the evolution of community over time 
we overview the development of first order automated reasoning system starting from their early year based on the analysis of current and potential application of such system we also try to predict new trend in first order automated reasoning our presentation will be centered around two main motif efficiency and usefulness for existing and future potential application 
given a database structure mining algorithm search for substructure that satisfy constraint such a minimum frequency minimum confidence minimum interest and maximum frequency example of substructure include graph tree and path for these substructure many mining algorithm have been proposed in order to make graph mining more efficient we investigate the use of the quickstart principle which is based on the fact that these class of structure are contained in each other thus allowing for the development of structure mining algorithm that split the search into step of increasing complexity we introduce the graph sequence tree extraction gaston algorithm that implement this idea by searching first for frequent path then frequent free tree and finally cyclic graph we investigate two alternative for computing the frequency of structure and present experimental result to relate these alternative 
many different high dimensional data set are characterized by the same underlying mode of variability when these mode of variability are continuous and few in number they can be viewed a parameterizing a low dimensional manifold the manifold provides a compact shared representation of the data suggesting correspondence between the high dimensional example from different data set these correspondence though naturally induced by the underlying manifold are difficult to learn using traditional method in supervised learning in this paper we generalize three method in unsupervised learning principal component analysis factor analysis and locally linear embedding to discover subspace and manifold that provide common low dimensional representation of different high dimensional data set we use the shared representation discovered by these algorithm to put high dimensional example from different data set into correspondence finally we show that a notion of self correspondence between example in the same data set can be used to improve the performance of these algorithm on small data set the algorithm are demonstrated on image and text 
the web graph is a giant social network whose property have been measured and modeled extensively in recent year most such study concentrate on the graph structure alone and do not consider textual property of the node consequently web community have been characterized purely in term of graph structure and not on page content we propose that a topic taxonomy such a yahoo or the open directory provides a useful framework for understanding the structure of content based cluster and community in particular using a topic taxonomy and an automatic classifier we can measure the background distribution of broad topic on the web and analyze the capability of recent random walk algorithm to draw sample which follow such distribution in addition we can measure the probability that a page about one broad topic will link to another broad topic extending this experiment we can measure how quickly topic context is lost while walking randomly on the web graph estimate of this topic mixing distance may explain why a global pagerank is still meaningful in the context of broad query in general our measurement may prove valuable in the design of community specific crawler and link based ranking system 
in this paper we address the problem of detecting multiple topic or category of text where each text is not assumed to belong to one of a number of mutually exclusive category conventionally the binary classification approach ha been employed in which whether or not text belongs to a category is judged by the binary classifier for every category in this paper we propose a more sophisticated approach to simultaneously detect multiple category of text using parametric mixture model pmms newly presented in this paper pmms are probabilistic generative model for text that ha multiple category our pmms are essentially different from the conventional mixture of multinomial distribution in the sense that in the former several basis multinomial parameter are mixed in the parameter space while in the latter several multinomial component are mixed we derive efficient learning algorithm for pmms within the framework of the maximum a posteriori estimate we also empirically show that our method can outperform the conventional binary approach when applied to multitopic detection of world wide web page focusing on those from the yahoo com domain 
a new class of nonparametric algorithm for high dimensional binary classification is proposed using cascade of low dimensional polynomial structure construction of polynomial cascade is based on minimax probability machine classification mpmc which result in direct estimate of classification accuracy and provides a simple stopping criterion that doe not require expensive cross validation measure this polynomial mpmc cascade pmc algorithm is constructed in linear time with respect to the input space dimensionality and linear time in the number of example making it a potentially attractive alternative to algorithm like support vector machine and standard mpmc experimental evidence is given showing that compared to state of the art classifier pmcs are competitive inherently fast to compute not prone to overfitting and generally yield accurate estimate of the maximum error rate on unseen data 
this paper provides an extensive analysis of pre stored streaming medium workload focusing on the client interactive behavior we analyze four workload that fall into three different domain namely education entertainment video and entertainment audio our main goal are a to identify qualitative similarity and difference in the typical client behavior for the three workload class and b to provide data for generating realistic synthetic workload 
we present a discriminative part based approach for the recognition of object class from unsegmented cluttered scene object are modeled a flexible constellation of part conditioned on local observation found by an interest operator for each object class the probability of a given assignment of part to local feature is modeled by a conditional random field crf we propose an extension of the crf framework that incorporates hidden variable and combine class conditional crfs into a unified framework for part based object recognition the parameter of the crf are estimated in a maximum likelihood framework and recognition proceeds by finding the most likely class under our model the main advantage of the proposed crf framework is that it allows u to relax the assumption of conditional independence of the observed data i e local feature often used in generative approach an assumption that might be too restrictive for a considerable number of object class 
we follow searle s contention that speaking a natural language is to engage in a rule governed form of behaviour and that those rule are conventional institutional rather than natural or physical we show how this analysis can also be used to specify rule of interaction for system of electronic agent communicating with an artificial language we conclude that using constitutive rule to define the semantics of an agent communication language not only distinguishes agent communication from method invocation but also offer significant computational advantage over using intentional state 
abstract bayesian classi er such a naive bayes or tree augmented naive bayes tan have shown excellent performance given their sim plicity and heavy underlying independence assumption in this paper we introduce a classi er taking a basis the tan model and taking into account uncertainty in model se lection to do this we introduce decompos able distribution over tan and show that they allow the expression resulting from the bayesian model averaging of tan model to be integrated into closed form with this re sult we construct a classi er with a shorter learning time and a longer classi cation time than tan empirical result show that the classi er is most of the case more accurate than tan and approximates better the class probability 
this paper describes a novel approach for obtaining semantic interoperability among data source in a bottom up semi automatic manner without relying on pre existing global semantic model we assume that large amount of data exist that have been organized and annotated according to local schema seeing semantics a a form of agreement our approach enables the participating data source to incrementally develop global agreement in an evolutionary and completely decentralized process that solely relies on pair wise local interaction participant provide translation between schema they are interested in and can learn about other translation by routing query gossiping to support the participant in assessing the semantic quality of the achieved agreement we develop a formal framework that take into account both syntactic and semantic criterion the assessment process is incremental and the quality rating are adjusted along with the operation of the system ultimately this process result in global agreement i e the semantics that all participant understand we discus strategy to efficiently find translation and provide result from a case study to justify our claim our approach applies to any system which provides a communication infrastructure existing website or database decentralized system p p system and offer the opportunity to study semantic interoperability a a global phenomenon in a network of information sharing party 
in this article we present a novel approach to solving the localization problem in cellular network the goal is to estimate a mobile user s position based on measurement of the signal strength received from network base station our solution work by building gaussian process model for the distribution of signal strength a obtained in a series of calibration measurement in the localization stage the user s position can be estimated by maximizing the likelihood of received signal strength with respect to the position we investigate the accuracy of the proposed approach on data obtained within a large indoor cellular network 
surveillance system have long been used to monitor industrial process and are becoming increasingly popular in public health and anti terrorism application most early detection system produce a time series of p value or some other statistic a their output typically the decision to signal an alarm is based on a threshold or other simple algorithm such a cusum that accumulates detection information temporally we formulate a pomdp model of underlying event and observation from a detector we solve the model and show how it is used for single output detector when dealing with spatio temporal data scan statistic are a popular method of building detector we describe the use of scan statistic in surveillance and how our pomdp model can be used to perform alarm signaling with them we compare the result obtained by our method with simple thresholding and cusum on synthetic and semi synthetic health data 
in this paper we describe a new approach for recovering d geometry from an uncalibrated image sequence of a single axis turntable motion unlike previous method the computation of multiple view encoded by the fundamental matrix or trifocal tensor is not required instead the new approach is based on fitting a conic locus to corresponding image point over multiple view it is then shown that the geometry of single axis motion can be recovered given at least two such conic in the case of two conic the reconstruction may have a two fold ambiguity but this ambiguity is removed if three conic are used the approach enables the geometry of the single axis motion the d rotation axis and euclidean geometry in plane perpendicular to this axis to be estimated using the minimal number of parameter it is demonstrated that a maximum likelihood estimation result in measurement that are a good a or superior to those obtained by previous method and with a far simpler algorithm example are given on various real sequence which show the accuracy and robustness of the new algorithm 
web search engine help user find useful information on the world wide web www however when the same query is submitted by different user typical search engine return the same result regardless of who submitted the query generally each user ha different information need for his her query therefore the search result should be adapted to user with different information need in this paper we first propose several approach to adapting search result according to each user s need for relevant information without any user effort and then verify the effectiveness of our proposed approach experimental result show that search system that adapt to each user s preference can be achieved by constructing user profile based on modified collaborative filtering with detailed analysis of user s browsing history in one day 
noun phrase in query are identified and classified into four type proper name dictionary phrase simple phrase and complex phrase a document ha a phrase if all content word in the phrase are within a window of a certain size the window size for different type of phrase are different and are determined using a decision tree phrase are more important than individual term consequently document in response to a query are ranked with matching phrase given a higher priority we utilize wordnet to disambiguate word sens of query term whenever the sense of a query term is determined it synonym hyponym word from it definition and it compound word are considered for possible addition to the query experimental result show that our approach yield between and improvement over the best known result on the trec and collection for short title only query without using web data 
convolution kernel such a sequence and tree kernel are advantageous for both the concept and accuracy of many natural language processing nlp task experiment have however shown that the over fitting problem often arises when these kernel are used in nlp task this paper discus this issue of convolution kernel and then proposes a new approach based on statistical feature selection that avoids this issue to enable the proposed method to be executed efficiently it is embedded into an original kernel calculation process by using sub structure mining algorithm experiment are undertaken on real nlp task to confirm the problem with a conventional method and to compare it performance with that of the proposed method 
synapsis are a critical element of biologically realistic spike based neural computation serving the role of communication computation and modification many different circuit implementation of sy napse function exist with different computational goal in mind in th is paper we describe a new cmos synapse design that separately control quiescent leak current synaptic gain and time constant of decay this circuit implement part of a commonly used kinetic model of synaptic conductance we show a theoretical analysis and experimental data for prototype fabricated in a commercially available m cmos process 
abstract in this paper we consider formulation of multi class probl em based on a generalized notion of a margin and using output coding this includes but is not restricted to standard multi class svm formulat ion differently from many previous approach we learn the code a well a the embedding function we illustrate how this can lead to a formulation that allows for solving a wider range of problem with e g many class or even missing class to keep our optimization problem tractable we propose an algorithm capable of solving them using two class classifier similar in spirit to boosting 
abstract at a cocktail party a listener can selectively attend to a single voice and filter out other acoustical interference how to simulate this perceptual ability remains a great challenge this paper describes a novel supervised learning approach to speech segregation in which a target speech signal is separated from interfering sound using spatial location cue interaural time difference itd and interaural intensity difference iid motivated by the auditory masking effect we employ the notion of an ideal time frequency binary mask which selects the target if it is stronger than the interference in a local time frequency unit within a narrow frequency band modification to the relative strength of the target source with respect to the interference trigger systematic change for estimated itd and iid for a given spatial configuration this interaction produce characteristic clustering in the binaural feature space consequently we perform pattern classification in order to estimate ideal binary mask a systematic evaluation in term of signal to noise ratio a well a automatic speech recognition performance show that the resulting system produce mask very close to ideal binary one a quantitative comparison show that our model yield significant improvement in performance over an existing approach furthermore under certain condition the model produce large speech intelligibility improvement with normal listener 
we propose in this paper a probabilistic approach for adaptive inference of generalized nonlinear classification that combine the computational advantage of a parametric solution with the flexibility of sequential sampling technique we regard the parameter of the classifier a latent state in a first order markov process and propose an algorithm which can be regarded a variational generalization of standard kalman filtering the variational kalman filter is based on two novel lower bound that enable u to use a non degenerate distribution over the adaptation rate an extensive empirical evaluation demonstrates that the proposed method is capable of infering competitive classifier both in stationary and non stationary environment although we focus on classification the algorithm is easily extended to other generalized nonlinear model 
relevance feedback rf is an interactive process which refines theretrievals by utilizing user s feedback history most researchersstrive to develop new rf technique and ignore the advantage ofexisting one in this paper we propose an image relevancereinforcement learning irrl model for integrating existing rftechniques various integration scheme are presented and along term shared memory is used to exploit the retrieval experiencefrom multiple user also a concept digesting method is proposedto reduce the complexity of storage demand the experimentalresults manifest that the integration of multiple rf approachesgives better retrieval performance than using one rf techniquealone and that the sharing of relevance knowledge between multiplequery session also provides significant contribution forimprovement further the storage demand is significantly reducedby the concept digesting technique this show the scalability ofthe proposed model against a growing size database 
animal s rhythmic movement such a locomotion are considered to be controlled by neural circuit called central pattern generator cpgs this article present a reinforcement learning rl method for a cpg controller which is inspired by the control mechanism of animal because the cpg controller is an instance of recurrent neural network a naive application of rl involves difficulty in addition since state and action space of controlled system are very large in real problem such a robot control the learning of the value function is also difficult in this study we propose a learning scheme for a cpg controller called a cpgactor critic model whose learning algorithm is based on a policy gradient method we apply our rl method to autonomous acquisition of biped locomotion by a biped robot simulator computer simulation show our method is able to train a cpg controller such that the learning process is stable 
facial motion produce not only facial feature point motion but also subtle appearance change such a wrinklesand shading change these subtle change are importantyet difficult issue for both analysis tracking and synthesis animation previous approach were mostly basedon model learned from extensive training appearance example however the space of all possible facial motionappearance is huge thus it is not feasible to collect samplescovering all possible variation due to lighting condition individuality and head pose therefore it is difficultto adapt such model to new condition in this paper we present an adaptive technique for analyzing subtle facialappearance change we propose a new ratio imagebased appearance feature which is independent of a person sface albedo this feature is used to track face appearancevariations based on exemplar to adapt the exemplarappearance model to new people and lighting condition we develop an online em based algorithm experimentsshow that the proposed method improves classification resultsin a facial expression recognition task where a varietyof people and lighting condition are involved 
we study the problem of computing classification rule set from relational database so that accurate prediction can be made on test data with missing attribute value traditional classifier perform badly when test data are not a complete a the training data because they tailor a training database too much we introduce the concept of one rule set being more robust than another that is able to make more accurate prediction on test data with missing attribute value we show that the optimal class association rule set is a robust a the complete class association rule set we then introduce the k optimal rule set which provides prediction exactly the same a the optimal class association rule set on test data with up to k missing attribute value this lead to a hierarchy of k optimal rule set in which decreasing size corresponds to decreasing robustness and they all more robust than a traditional classification rule set we introduce two method to find k optimal rule set i e an optimal association rule mining approach and a heuristic approximate approach we show experimentally that a k optimal rule set generated by the optimal association rule mining approach performs better than that by the heuristic approximate approach and both rule set perform significantly better than a typical classification rule set c rule on incomplete test data 
we present the result of an experiment on extending the automatic method of machine translation evaluation blue with statistical weight for lexical item such a tf idf score we show that this extension give additional information about evaluated text in particular it allows u to measure translation adequacy which for statistical mt system is often overestimated by the baseline bleu method the proposed model us a single human reference translation which increase the usability of the proposed method for practical purpose the model suggests a linguistic interpretation which relates frequency weight and human intuition about translation adequacy and fluency 
in today s industry the design of software test is mostly based on the tester expertise while test automation tool are limited to execution of pre planned test only evaluation of test output is also associated with a considerable effort by human tester who often have imperfect knowledge of the requirement specification not surprisingly this manual approach to software testing result in heavy loss to the world s economy the cost of the so called catastrophic software failure such a mar polar lander shutdown in are even hard to measure in this paper we demonstrate the potential use of data mining algorithm for automated induction of functional requirement from execution data the induced data mining model of tested software can be utilized for recovering missing and incomplete specification designing a minimal set of regression test and evaluating the correctness of software output when testing new potentially flawed release of the system to study the feasibility of the proposed approach we have applied a novel data mining algorithm called info fuzzy network ifn to execution data of a general purpose code for solving partial differential equation after being trained on a relatively small number of randomly generated input output example the model constructed by the ifn algorithm ha shown a clear capability to discriminate between correct and faulty version of the program 
this paper describes a novel algorithm and deployed system golden path analyzer gpa that analyzes clickstreams of people trying to complete the same task on a website it find the shortest successful path taken by user golden path and us these a seed for clickstream cluster other user are assigned to a cluster if their clickstream is a supersequence of the golden path the advantage of this approach are that the resulting cluster are easily comprehended they are few in number correspond to semantically different strategy used by the user and jointly partition all the clickstreams gpa s key contribution over prior work in process funnel is that by not excluding user that make diversion from the golden path gpa is able to assign more user to fewer cluster another key contribution is to use actual full clickstreams a cluster seed to which supersequences of other user are added golden path correspond to complete clickstreams that are based on actual user page transition gpa is particularly useful for site designer to improve process such a shopping return and registration it analysis identify which web page cause many user to deviate from a golden path which link distract user and the percentage of user taking each golden path gpa ha demonstrated value on more than twenty client project in diverse industry 
a new method for classiflcation is proposed this is based on kernel orthonormalized partial least square pls dimensionality reduction of the original data space followed by a support vector classifler unlike principal component analysis pca which ha previously served a a dimension reduction step for discrimination problem orthonormalized pls is closely related to fisher s approach to linear discrimination or equivalently to canonical correlation analysis for this reason orthonormalized pls is preferable to pca for discrimination good behavior of the proposed method is demonstrated on difierent benchmark data set and on the real world problem of classifying flnger movement period from non movement period based on electroencephalogram 
abstract in the last decade there ha been an explosion of interest in mining time series data literally hundred of paper have introduced new algorithm to index clas sify cluster and segment time series in this work we make the following claim much of this work ha very little utility because the contribution made speed in the case of indexing accuracy in the case of classification and clustering model accuracy in the case of segmentation offer an amount of improvement that would have been completely dwarfed by the variance that would have been observed by testing on many real world datasets or the variance that would have been observed by changing minor unstated implementation detail to illustrate our point we have undertaken the most exhaustive 
this paper explores the large scale acquisition of sense tagged example for word sense disambiguation wsd we have applied the wordnet monosemous relative method to construct automatically a web corpus that we have used to train disambiguation system the corpus building process ha highlighted important factor such a the distribution of sens bias the corpus ha been used to train wsd algorithm that include supervised method combining automatic and manuallytagged example minimally supervised requiring sense bias information from hand tagged corpus and fully unsupervised these method were tested on the senseval lexical sample test set and compared successfully to other system with minimum or no supervision 
in this paper we propose a competition learning approach to coreference resolution traditionally supervised machine learning approach adopt the single candidate model nevertheless the preference relationship between the antecedent candidate cannot be determined accurately in this model by contrast our approach adopts a twin candidate learning model such a model can present the competition criterion for antecedent candidate reliably and ensure that the most preferred candidate is selected furthermore our approach applies a candidate filter to reduce the computational cost and data noise during training and resolution the experimental result on muc and muc data set show that our approach can outperform those based on the single candidate model 
we present an improved method for clustering in the presence of very limited supervisory information given a pairwise instance constraint by allowing instance level constraint to have spacelevel inductive implication we are able to successfully incorporate constraint for a wide range of data set type our method greatly improves on the previously studied constrained mean algorithm generally requiring le than half a many constraint to achieve a given accuracy on a range of real world data while also being more robust when over constrained we additionally discus an active learning algorithm which increase the value of constraint even further 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
an object tracking algorithm using a novel simple symmetric similarity function between spatially smoothed kernel density estimate of the model and target distribution is proposed and tested the similarity measure is based on the expectation of the density estimate over the model or target image the density is estimated using radial basis kernel function that measure the affinity between point and pr ovide a better outlier rejection property the mean shift algorithm i s used to track object by iteratively maximizing this similarity functio n to alleviate the quadratic complexity of the density estimation we employ gaussian kernel and the fast gauss transform to reduce the computation to linear order this lead to a very efficient and robust nonparametri c tracking algorithm the proposed algorithm is tested with several image sequence and shown to achieve robust and reliable real time tracking several sequence are placed at http www c umd edu user yangcj node html 
we present a novel algorithm for the fast computation of pagerank a hyperlink based estimate of the importance of web page the original pagerank algorithm us the power method to compute successive iterates that converge to the principal eigenvector of the markov matrix representing the web link graph the algorithm presented here called quadratic extrapolation accelerates the convergence of the power method by periodically subtracting off estimate of the nonprincipal eigenvectors from the current iterate of the power method in quadratic extrapolation we take advantage of the fact that the first eigenvalue of a markov matrix is known to be to compute the nonprincipal eigenvectors using successive iterates of the power method empirically we show that using quadratic extrapolation speed up pagerank computation by on a web graph of million node with minimal overhead our contribution is useful to the pagerank community and the numerical linear algebra community in general a it is a fast method for determining the dominant eigenvector of a matrix that is too large for standard fast method to be practical 
we compare and contrast two different model for detecting sentence like unit in continuous speech using both acoustic and lexical information the first approach is based on hidden markov sequence model based on n gram us maximum likelihood estimation and model interpolation to combine different representation of the data the second approach model the posterior probability of the target class is therefore discriminative and integrates multiple knowledge source in the maximum entropy maxent framework both model combine lexical syntactic and prosodic information we develop a technique for integrating pretrained probability model into the maxent framework and show that this approach can improve if only slightly on an hmm based state of the art system for the sentence boundary detection task a much more substantial improvement is obtained by combining the posterior probability of the two system 
a lot of learning machine with hidden variable used in information science have singularity in their parameter space at singularity the fisher information matrix becomes degenerate resulting that the learning theory of regular statistical model doe not hold recently it wa proven that if the true parameter is contained in singularity then the coecien t of the bayes generalization error is equal to the pole of the zeta function of the kullback information in this paper under the condition that the true parameter is almost but not contained in singularity we show two result if the dimension of the parameter from input to hidden unit is not larger than three then there exit a region of true parameter where the generalization error is larger than those of regular model however if otherwise then for any true parameter the generalization error is smaller than those of regular model the symmetry of the generalization error and the training error doe not hold in singular model in general 
text classification is the process of classifying document into predefined category based on their content existing supervised learning algorithm to automatically classify text need sufficient labeled document to learn accurately applying the expectation maximization em algorithm to this problem is an alternative approach that utilizes a large pool of unlabeled document to augment the available labeled document unfortunately the time needed to learn with these large unlabeled document is too high this paper introduces a novel parallel learning algorithm for text classification task the parallel algorithm is based on the combination of the em algorithm and the naive bayes classifier our goal is to improve the computational time in learning and classifying process we studied the performance of our parallel algorithm on a large linux pc cluster called pirun cluster we report both timing and accuracy result these result indicate that the proposed parallel algorithm is capable of handling large document collection 
optimal solution to markov decision problem mdps are very sensitive with respect to the state transition probability in many practical problem the estimation of those probability is far from accurate hence estimation error are limiting factor in applying mdps to realworld problem we propose an algorithm for solving finite state and finite action mdps where the solution is guaranteed to be robust with respect to estimation error on the state transition probability our algorithm involves a statistically accurate yet numerically efficient representation of uncertainty via kullback leibler divergence bound the worst case complexity of the robust algorithm is the same a the original bellman recursion hence robustness can be added at practically no extra computing cost 
we present a method for very high dimensional correlation analysis the method relies equally on rigorous search strategy and on human interaction at each step the method conservatively shave off a fraction of the database tuples and attribute so that most of the correlation present in the data are not affected by the decomposition instead the correlation become more obvious to the user because they are hidden in a much smaller portion of the database this process can be repeated iteratively and interactively until only the most important correlation remain the main technical difficulty of the approach is figuring out how to shave off part of the database so a to preserve most correlation we develop an algorithm for this problem that ha a polynomial running time and guarantee result quality 
we study generalization property of linear learning algorithm and develop a data dependent approach that is used to derive generalization bound that depend on the margin distribution our method make use of random projection technique to allow the use of existing vc dimension bound in the eective lower dimension of the data comparison with existing generalization bound show that our bound are tighter and meaningful in case existing bound are not 
the concept of data squashing wa introduced by dumouchel et al a a method of summarizing massive data set that preserve statistical relationship among variable the idea is to create a smaller data set that allows statistical modeling to take place using in memory algorithm and to preserve the modeling result more accurately than would a same size random sample from the massive data set this research attempt to avoid several limitation of previous approach to data squashing our method avoids the curse of dimensionality by a double use of principal component transformation that make computing time linear in the number of case and quadratic in the number of variable categorical and continuous variable are smoothly integrated because the binning is based on principal component which are uncorrelated we can use fractional factorial design that sample le than one point per bin we also investigate various weighting scheme for the squashed sample to see whether matching moment or matching subregion data count is more effective finally previous work required the specification of a statistical model either to perform the squashing algorithm or to compare the worth of different squashing method our approach to evaluation is model free and doe not even require the specification of variable a response or predictor instead we develop a chi squared like measure of accuracy to compare the closeness of various discrete density the squashed data set to the discrete massive data set 
in a variety of modern mining application data are commonly viewed a infinite time ordered data stream rather a finite data set stored on disk this view challenge fundamental assumption commonly made in the context of several data mining algorithm in this paper we study the problem of identifying correlation between multiple data stream in particular we propose algorithm capable of capturing correlation between multiple continuous data stream in a highly efficient and accurate manner our algorithm and technique are applicable in the case of both synchronous and asynchronous data streaming environment we capture correlation between multiple stream using the well known technique of singular value decomposition svd correlation between data item and the svd technique in particular have been repeatedly utilized in an off line non stream data mining problem for example forecasting approximate query answering and data reduction we propose a methodology based on a combination of dimensionality reduction and sampling to make the svd technique suitable for a data stream context our technique are approximate trading accuracy with performance and we analytically quantify this tradeoff we present a through experimental evaluation using both real and synthetic data set from a prototype implementation of our technique investigating the impact of various parameter in the accuracy of the overall computation our result indicate that correlation between multiple data stream can be identified very efficiently and accurately the algorithm proposed herein are presented a generic tool with a multitude of application on data stream mining problem 
information extraction method can be used to automatically fill in database form from unstructured data such a web document or email state of the art method have achieved low error rate but invariably make a number of error the goal of an interactive information extraction system is to assist the user in filling in database field while giving the user confidence in the integrity of the data the user is presented with an interactive interface that allows both the rapid verification of automatic field assignment and the correction of error in case where there are multiple error our system take into account user correction and immediately propagates these constraint such that other field are often corrected automatically linear chain conditional random field crfs have been shown to perform well for information extraction and other language modelling task due to their ability to capture arbitrary overlapping feature of the input in a markov model we apply this framework with two extension a constrained viterbi decoding which find the optimal field assignment consistent with the field explicitly specified or corrected by the user and a mechanism for estimating the confidence of each extracted field so that low confidence extraction can be highlighted both of these mechanism are incorporated in a novel user interface for form filling that is intuitive and speed the entry of data providing a reduction in error due to automated correction 
this paper present a novel graph theoretic approach named ratio contour to extract perceptually salient boundary from a set of noisy boundary fragment detected in real image the boundary saliency is defined using the gestalt law of closure proximity and continuity this paper first construct an undirected graph with two different set of edge solid edge and dashed edge the weight of solid and dashed edge measure the local saliency in and between boundary fragment respectively then the most salient boundary is detected by searching for an optimal cycle in this graph with minimum average weight the proposed approach guarantee the global optimality without introducing any bias related to region area or boundary length we collect a variety of image for testing the proposed approach with encouraging result 
one major problem of existing method to mine data stream is that it make ad hoc choice to combine most recent data with some amount of old data to search the new hypothesis the assumption is that the additional old data always help produce a more accurate hypothesis than using the most recent data only we first criticize this notion and point out that using old data blindly is not better than gambling in other word it help increase the accuracy only if we are lucky we discus and analyze the situation where old data will help and what kind of old data will help the practical problem on choosing the right example from old data is due to the formidable cost to compare different possibility and model this problem will go away if we have an algorithm that is extremely efficient to compare all sensible choice with little extra cost based on this observation we propose a simple efficient and accurate cross validation decision tree ensemble method 
we show that temporal logic and combination of temporal logic and modal logic of knowledge can be efiectively represented in artiflcial neural network we present a translation algorithm from temporal rule to neural network and show that the network compute a flxed point semantics of the rule we also apply the translation to the muddy child puzzle which ha been used a a testbed for distributed multi agent system we provide a complete solution to the puzzle with the use of simple neural network capable of reasoning about time and of knowledge acquisition through inductive learning 
a reactive environment is one that responds to the action of an agent rather than evolving obliviously in reactive environment expert algorithm must balance exploration and exploitation of expert more carefully than in oblivious one in addition a more subtle denition of a learnable value of an expert is required a general exploration exploitation expert method is presented along with a proper denition of value the method is shown to asymptotically perform a well a the best available expert several variant are analyzed from the viewpoint of the exploration exploitation tradeoff including explore then exploit polynomially vanishing exploration constant frequency exploration and constant size exploration phase complexity and performance bound are proven 
there is a notable interest in extending probabilistic generative modeling principle to accommodate for more complex structured data type in this paper we develop a generative probabilistic model for visualizing set of discrete symbolic sequence the model a constrained mixture of discrete hidden markov model is a generalization of density based visualization method previously developed for static data set we illustrate our approach on sequence representing web log data and choral by j s bach 
when the training instance of the target class are heavily outnumbered by non target training instance svms can be ineffective in determining the class boundary to remedy this problem we propose an adaptive conformal transformation act algorithm act considers feature space distance and the class imbalance ratio when it performs conformal transformation on a kernel function experimental result on uci and real world datasets show act to be effective in improving class prediction accuracy 
the barn owl is a nocturnal hunter capable of capturing prey using auditory information alone the neural basis for this localization behavior is the existence of auditory neuron with spatial receptive field we provide a mathematical description of the operation performed on auditory input signal by the barn owl that facilitate the creation of a representation of auditory space to develop our model we first formulate the sound localization problem solved by the barn owl a a statistical estimation problem the implementation of the solution is constrained by the known neurobiology 
how a robot should grasp an object depends on it size and shape such parameter can be estimated visually but this is fallible particularly for unrecognized unfamiliar object failure will result in a clumsy grasp or glancing blow against the object if the robot doe not learn something from the encounter then it will be apt to repeat the same mistake again and again this paper show how to recover information about an object s extent by poking it either accidentally or deliberately poking an object make it move and motion is a powerful cue for visual segmentation the period immediately before and after the moment of impact turn out to be particularly informative and give visual evidence for the boundary of the object that is well suited to segmentation using graph cut the segmentation algorithm is shown to produce result consistent enough to support autonomous collection of datasets for object recognition which enables often encountered object to be segmented without the need for further poking figure a motivating scenario the robot left reach towards an object in it environment while fixating it with a camera the robot s view is shown on the right the boundary between the cube and the table it is sitting on is clear to human eye but too subtle to be reliably segmented by current automatic method but once the robot arm come in contact with the object it can be easily segmented from the background using the motion due to the impact 
learning from ambiguous training data is highly relevant in many application we present a new learning algorithm for classification problem where label are associated with set of pattern instead of individual pattern this encompasses multiple instance learning a a special case our approach is based on a generalization of linear programming boosting and us result from disjunctive programming to generate successively stronger linear relaxation of a discrete non convex problem 
supertagging is the tagging process of assigning the correct elementary tree of ltag or the correct supertag to each word of an input sentence in this paper we propose to use supertags to expose syntactic dependency which are unavailable with po tag we first propose a novel method of applying sparse network of winnow snow to sequential model then we use it to construct a supertagger that us long distance syntactical dependency and the supertagger achieves an accuracy of we apply the supertagger to np chunking the use of supertags in np chunking give rise to almost absolute increase from to in f score under transformation based learning tbl frame the surpertagger described here provides an effective and efficient way to exploit syntactic information 
recent research ha had some success using the length of time a user display a document in their web browser a implicit feedback for document preference however most study have been confined to specific search domain such a news and have not considered the effect of task on display time and the potential impact of this relationship on the effectiveness of display time a implicit feedback we describe the result of an intensive naturalistic study of the online information seeking behavior of seven subject during a fourteen week period throughout the study subject online information seeking activity were monitored with various piece of logging and evaluation software subject were asked to identify the task with which they were working classify the document that they viewed according to these task and evaluate the usefulness of the document result of a user centered analysis demonstrate no general direct relationship between display time and usefulness and that display time differ significantly according to specific task and according to specific user 
one current explanation of the view independent representation of space by the place cell of the hippocampus is that they arise out of the summation of view dependent gaussians this proposal assumes that visual representation show bounded invariance here we investigate whether a recently proposed visual encoding scheme called the temporal population code can provide such representation our analysis is based on the behavior of a simulated robot in a virtual environment containing specic visual cue our result show that the temporal population code provides a representational substrate that can naturally account for the formation of place eld 
hand crafting effective visual presentation is time consuming and requires design skill here we present a case based graphic sketch generation algorithm which us a database of existing graphic example case to automatically create a sketch of a presentation for a new user request a the first case based learning approach to graphic generation our work offer three unique contribution first we augment a similarity metric with a set of adequacy evaluation criterion to retrieve a case that is most similar to the request and is also usable in sketch synthesis to facilitate the retrieval of case fragment we develop a systematic approach to case request decomposition when a usable case cannot be found second we improve case retrieval speed by organizing case into hierarchical cluster based on their similarity distance and by using dynamically selected cluster representative third we develop a general case composition method to synthesize a new sketch from multiple retrieved case furthermore we have implemented our casebased sketch generation algorithm in a user system cooperative graphic design system called improvise which help user to generate creative and tailored presentation 
we address three crucial issue encountered in dt mri diffusiontensor magnetic resonance imaging diffusion tensor estimation regularization and fiber bundle visualization we first reviewrelated algorithm existing in the literature and propose thenalternative variational formalism that lead to new and improvedschemes thanks to the preservation of important tensor constraint positivity symmetry we illustrate how our complete dt mriprocessing pipeline can be successfully used to construct and drawfiber bundle in the white matter of the brain from a set of noisyraw mri image 
a commonly used representation of a visual pattern is the set ofmarginal probability distribution of the output of a bank offilters gaussian laplacian gabor etc this representationhas been used effectively for a variety of vision task includingtexture classification texture synthesis object detection andimage retrieval this paper examines the ability of thisrepresentation to discriminate between an arbitrary pair of visualstimuli example of pattern are derived that provably posse thesame marginal statistical property yet are visually distinct these result suggest the need for either employing a large anddiverse filter bank or incorporating joint statistic in order torepresent a large class of visual pattern 
an analog system on chip for kernel based pattern classification and sequence estimation is presented state transition probability conditioned on input data are generated by an integrated support vector machine dot product based kernel and support vector coefficient are implemented in analog programmable floating gate translinear circuit and probability are propagated and normalized using sub threshold current mode circuit a input state and support vector forward decoding kernel machine is integrated on a mm mm chip in m cmos technology experiment with the processor trained for speaker verification and phoneme sequence estimation demonstrate real time recognition accuracy at par with floating point software at sub microwatt power 
in this demo we focus on the engineering of open multi agent system a electronic institution electronic institution are a formalism to define the rule which structure agent interaction establishing what agent are permitted and forbidden to do we present a set of tool that support the specification analysis and execution of institution a well a the implementation of agent our methodology allows for a successive refinement approach to multi agent system engineering 
society need humor not just for entertainment in the web age presentation become more and more flexible and personalized and they will require humor contribution for electronic commerce development e g product promotion getting selective attention help in memorizing name etc more or le a it happened in the world of broadcasted advertisement even if deep modeling of humor in all of it facet is not something for the near future there is something concrete that ha been achieved and that can help in providing attention to the field the paper refers to the result of hahacronym a project devoted to humorous acronym production a circumscribed task that nonetheless requires various generic component the project open the way to development for creative language with application in the world of advertisement 
this paper describes a new and simple method of recovering thegeometry of uncalibrated circular motion or single axis motionusing a minimal data set of point in image this problem hasbeen solved using non minimal data either by computing thefundamental matrix and trifocal tensor in image or by fittingconics to tracked point in image our new method first computesa planar homography from a minimum of point in image it isshown that two eigenvectors of this homography are the image ofthe circular point then other fixed image entity and rotationangles can be straightforwardly computed the crux of the methodlies in relating this planar homography from two different pointsto a homology naturally induced by corresponding point ondifferent conic locus from a circular motion the experiment onreal image sequence demonstrate the simplicity accuracy androbustness of the new method 
a crucial step toward the goal of automatic extraction of propositional information from natural language text is the identification of semantic relation between constituent in sentence we examine the problem of distinguishing among seven relation type that can occur between the entity treatment and disease in bioscience text and the problem of identifying such entity we compare five generative graphical model and a neural network using lexical syntactic and semantic feature finding that the latter help achieve high classification accuracy 
the maximisation of information transmission over noisy channel is a common albeit generally computationally difficult problem we approach the difficulty of computing the mutual information for noisy channel by using a variational approximation the resulting im algorithm is analagous to the em algorithm yet maximises mutual information a opposed to likelihood we apply the method to several practical example including linear compression population encoding and cdma 
this paper proposes a novel data envelopment analysis dea based approach for model combination we first prove that for the class classification problem dea model identify the same convex hull a the popular roc analysis used for model combination for general k class classifier we then develop a dea based method to combine multiple classifier experiment show that the method outperforms other benchmark method and suggest that dea can be a promising tool for model combination 
the problem of learning a semantic representation of a text document from data is addressed in the situation where a corpus of unlabeled paired document is available each pair being formed by a short english document and it french translation this representation can then be used for any retrieval categorization or clustering task both in a standard and in a cross lingual setting by using kernel function in this case simple bag of word inner product each part of the corpus is mapped to a high dimensional space the correlation between the two space are then learnt by using kernel canonical correlation analysis a set of direction is found in the first and in the second space that are maximally correlated since we assume the two representation are completely independent apart from the semantic content any correlation between them should reflect some semantic similarity certain pattern of english word that relate to a specific meaning should correlate with certain pattern of french word corresponding to the same meaning across the corpus using the semantic representation obtained in this way we first demonstrate that the correlation detected between the two version of the corpus are significantly higher than random and hence that a representation based on such feature doe capture statistical pattern that should reflect semantic information then we use such representation both in cross language and in single language retrieval task observing performance that is consistently and significantly superior to lsi on the same data 
dynamic probabilistic network dpns are exploited for modeling the temporal relationship among a set of different object temporal event in the scene for a coherent and robust scene level behaviour interpretation in particular we develop a dynamically multi linked hidden markov model dml hmm to interpret group activity involving multiple object captured in an outdoor scene the model is based on the discovery of salient dynamic interlinks among multiple temporal event using dpns object temporal event are detected and labeled using gaussian mixture model with automatic model order selection a dml hmm is built using schwarz s bayesian information criterion based factorisation resulting in it topology being intrinsically determined by the underlying causality and temporal order among different object event our experiment demonstrate that it performance on modelling group activity in a noisy outdoor scene is superior compared to that of a multi observation hidden markov model mohmm a parallel hidden markov model pahmm and a coupled hidden markov model chmm 
hunter gatherer is an interface that let web user carry out three main task collect component from within web page represent those component in a collection edit those component collection our research show that while the practice of making collection of content from within web page is common it is not frequent due in large part to poor interaction support in existing tool we engaged with user in task analysis a well a iterative design review in order to understand the interaction issue that are part of within web page collection making and to design an interaction that would support that process we report here on that design development a well a on the evaluation of the tool that evolved from that process and the future work stemming from these result in which our critical question is what happens to user perception and expectation of web based information their web based information management practice when they can treat this information a harvestable recontextualizable data rather than a fixed page 
this paper discus the use of statistical word alignment over multiple parallel text for the identification of string span that cannot be constituent in one of the language this information is exploited in monolingual pcfg grammar induction for that language within an augmented version of the inside outside algorithm besides the aligned corpus no other resource are required we discus an implemented system and present experimental result with an evaluation against the penn tree bank 
graph are an increasingly important data source with such important graph a the internet and the web other familiar graph include cad circuit phone record gene sequence city street social network and academic citation any kind of relationship such a actor appearing in movie can be represented a a graph this work present a data mining tool called anf that can quickly answer a number of interesting question on graph represented data such a the following how robust is the internet to failure what are the most influential database paper are there gender difference in movie appearance pattern at it core anf is based on a fast and memory efficient approach for approximating the complete neighbourhood function for a graph for the internet graph k node anf s highly accurate approximation is more than time faster than the exact computation this reduces the running time from nearly a day to a matter of a minute or two allowing user to perform ad hoc drill down task and to repeatedly answer question about changing data source to enable this drill down anf employ new technique for approximating neighbourhood type function for graph with distinguished node and or edge when compared to the best existing approximation anf s approach is both faster and more accurate given the same resource additionally unlike previous approach anf scale gracefully to handle disk resident graph finally we present some of our result from mining large graph using anf 
we examine the approach of encoding planning problem a csps more closely first we present a simple csp encoding for planning problem and then a set of transformation that can be used to eliminate variable and add new constraint to the encoding we show that our transformation uncover additional structure in the planning problem structure that subsumes the structure uncovered by graphplan planning graph we solve the csp encoded planning problem by using standard csp algorithm empirical evidence is presented to validate the effectiveness of this approach to solving planning problem and to show that even a prototype implementation is more effective than standard graphplan our prototype is even competitive with far more optimized planning graph based implementation we also demonstrate that this approach can be more easily lifted to more complex type of planning than can planning graph in particular we show that the approach can be easily extended to planning with resource 
a behavior based architecture that enables a simulated agent to exist and navigate in an artificial environment without any kind of spatial representation is presented hebbian learning is used to combine reactive behavior that enable the agent to exploit spatial and temporal regularity in the environment the agent is then able to apply it innate behavior in situation that were not initially designed to trigger these reactive behavior the system can also accommodate change in the environment simulation result that measure the performance of the system are also presented is easier to recognize location of landmark than location of food in this case no representation of the environment need be used but based on it past experience the agent can generate a new method for finding food the architecture of the system presented in this report is connectionist and it us hebbian learning to build new combination of innate behavior thus enabling the agent to exploit regularity in it environment the connectionist nature of the system also enables the agent to adapt it internal representation to reflect change in it surroundings 
in this paper we present discriminative random field drf a discriminative framework for the classification of natural image region by incorporating neighborhood spatial dependency in the label a well a the observed data the proposed model exploit local discriminative model and allows to relax the assumption of conditional independence of the observed data given the label commonly used in the markov random field mrf framework the parameter of the drf model are learned using penalized maximum pseudo likelihood method furthermore the form of the drf model allows the map inference for binary classification problem using the graph min cut algorithm the performance of the model wa verified on the synthetic a well a the real world image the drf model outperforms the mrf model in the experiment 
this paper describes the user expertise model in athosmail a mobile speech based e mail system the model encodes the system s assumption about the user expertise and give recommendation on how the system should respond depending on the assumed competence level of the user the recommendation are realized a three type of explicitness in the system response the system monitor the user s competence with the help of parameter that describe e g the success of the user s interaction with the system the model consists of an online and an offline version the former taking care of the expertise level change during the same session the latter modelling the overall user expertise a a function of time and repeated interaction 
phrasal verb are an important feature of the english language properly identifying them provides the basis for an english parser to decode the related structure phrasal verb have been a challenge to natural language processing nlp because they sit at the borderline between lexicon and syntax traditional nlp framework that separate the lexicon module from the parser make it difficult to handle this problem properly this paper present a finite state approach that integrates a phrasal verb expert lexicon between shallow parsing and deep parsing to handle morpho syntactic interaction with precision recall combined performance benchmarked consistently at the phrasal verb identification problem ha basically been solved with the presented method 
we present an alternative technique for discovering aggregate usage profile from web access log the technique is based on clustering information need inferred from user browsing path browsing path are extracted from user access log information need is inferred from each browsing path by using the ostensive model the technique is evaluated in a document recommendation application we compare the performance of our technique against the well established transaction based technique proposed in based on an initial evaluation the result are encouraging 
from a computational perspective there is a close connection between various probabilistic reasoning task and the problem of counting or sampling satisfying assignment of a propositional theory we consider the question of whether state of the art satisfiability procedure based on random walk strategy can be used to sample uniformly or nearuniformly from the space of satisfying assignment we first show that random walk sat procedure often do reach the full set of solution of complex logical theory moreover by interleaving random walk step with metropolis transition we also show how the sampling becomes near uniform 
label ranking is the task of inferring a total order over a predefined set of label for each given instance we present a general framework for batch learning of label ranking function from supervised data we assume that each instance in the training data is associated with a list of preference over the label set however we do not assume that this list is either complete or consistent this enables u to accommodate a variety of ranking problem in contrast to the general form of the supervision our goal is to learn a ranking function that induces a total order over the entire set of label special case of our setting are multilabel categorization and hierarchical classification we present a general boosting based learning algorithm for the label ranking problem and prove a lower bound on the progress of each boosting iteration the applicability of our approach is demonstrated with a set of experiment on a large scale text corpus 
automatically acquiring synonymous collocation pair such a and from corpus is a challenging task for this task we can in general have a large monolingual corpus and or a very limited bilingual corpus method that use monolingual corpus alone or use bilingual corpus alone are apparently inadequate because of low precision or low coverage in this paper we propose a method that us both these resource to get an optimal compromise of precision and coverage this method first get candidate of synonymous collocation pair based on a monolingual corpus and a word thesaurus and then selects the appropriate pair from the candidate using their translation in a second language the translation of the candidate are obtained with a statistical translation model which is trained with a small bilingual corpus and a large monolingual corpus the translation information is proved a effective to select synonymous collocation pair experimental result indicate that the average precision and recall of our approach are and respectively which outperform those method that only use monolingual corpus and those that only use bilingual corpus 
it ha been suggested that the primary goal of the sensory system is to represent input in such a way a to reduce the high degree of redundancy given a noisy neural representation however solely reducing redundancy is not desirable since redundancy is the only clue to reduce the effect of noise here we propose a model that best balance redundancy reduction and redundant representation like previous model our model account for the localized and oriented structure of simple cell but it also predicts a different organization for the population with noisy limited capacity unit the optimal representation becomes an overcomplete multi scale representation which compared to previous model is in closer agreement with physiological data these result offer a new perspective on the expansion of the number of neuron from retina to v and provide a theoretical model of incorporating useful redundancy into efficient neural representation 
we investigate the criterion used by online searcher when assessing the relevance of web page to information seeking task twenty four searcher were given three task each and indicated the feature of web page which they employed when deciding about the usefulness of the page these task were presented within the context of a simulated work task situation the result of this study provide a set of criterion used by searcher to decide about the utility of web page such criterion have implication for the design of system that use or recommend web page a well a to author of web page 
several approach have been described for the automatic unsupervised acquisition of pattern for information extraction each approach is based on a particular model for the pattern to be acquired such a a predicate argument structure or a dependency chain the effect of these alternative model ha not been previously studied in this paper we compare the prior model and introduce a new model the subtree model based on arbitrary subtrees of dependency tree we describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using subtree pattern 
we propose a variable ordering heuristic for sat which is based on a structural analysis of the sat problem we show that when the heuristic is used by a davis putnam sat solver that employ conflict directed backtracking it produce a divide and conquer behavior in which the sat problem is recursively decomposed into smaller problem that are solved independently we discus the implication of this divide and conquer behavior on our ability to provide structure based guarantee on the complexity of davis putnam sat solver we also report on the integration of this heuristic with zchaffa state of the art sat solver showing experimentally that it significantly improves performance on a range of benchmark problem that exhibit structure 
hand gesture are example of fast and complex motion computer fail to track these in fast video but sleight ofhand fool human a well what happens too quickly wejust cannot see we show a d tracker for these type ofmotions that relies on the recognition of familiar configurationsin d image classification and fill the gapsin between interpolation we illustrate this idea with experimentson hand motion similar to finger spelling thepenalty for a recognition failure is often small if two configurationsare confused they are often similar to eachother and the illusion work well enough for instance todrive a graphic animation of the moving hand we contributeadvances in both feature design and classifier training our image feature are invariant to image scale translation and rotation and we propose a classification methodthat combine vqpca with discrimination tree 
a novel algorithm for actively trading stock is presented while tradi tional universal algorithm and technical trading heuristic attempt to predict winner or trend our approach relies on predictable statistical relation between all pair of stock in the market our empirical result on historical market provide strong evidence that this type of techni cal trading can beat the market and moreover can beat the best stock in the market in doing so we utilize a new idea for smoothing critical parameter in the context of expert learning 
in this paper we provide a polynomial time algorithm for solving an important class of metric temporal problem that involve simple temporal constraint between various event variable and piecewise constant preference function over variable domain we are given a graph g where x x xn is a set of event x is the beginning of the world node and is set to by convention and e xi xj annotated with the bound lb e ub e is a simple temporal constraint between xi and xj indicating that xj must be scheduled between lb e and ub e second after xi is scheduled lb e ub e a family of stepwise constant preference function f fxi t r r specifies the preference attached with scheduling xi at time t the goal is to find a schedule for all the event such that all the temporal constraint are satisfied and the sum of the preference is maximized our polynomial time algorithm for solving such problem which we refer to a extended simple temporal problem estps ha important consequence in dealing with limited form of disjunction and preference in metric temporal reasoning that would otherwise require an exponential search space 
in this paper we describe a novel technique for detecting salient region in an image the detector is a generalization to a ne invariance of the method introduced by kadir and brady the detector deems a region salient if it exhibit unpredictability in both it attribute and it spatial scale 
this paper present a novel approach for detecting affine invariant interest point our method can deal with significant affine transformation including large scale change such transformation introduce significant change in the point location a well a in the scale and the shape of the neighbourhood of an interest point our approach allows to solve for these problem simultaneously it is based on three key idea the second moment matrix computed in a point can be used to normalize a region in an affine invariant way skew and stretch the scale of the local structure is indicated by local extremum of normalized derivative over scale an affine adapted harris detector determines the location of interest point a multi scale version of this detector is used for initialization an iterative algorithm then modifies location scale and neighbourhood of each point and converges to affine invariant point for matching and recognition the image is characterized by a set of affine invariant point the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination change a quantitative comparison of our detector with existing one show a significant improvement in the presence of large affine deformation experimental result for wide baseline matching show an excellent performance in the presence of large perspective transformation including significant scale change result for recognition are very good for a database with more than image 
identifier resolution is presented a a way to link the physical world with virtual web resource in this paradigm designed to support nomadic user the user employ a handheld wirelessly connected sensor equipped device to read identifier associated with physical entity the identifier are resolved into virtual resource or action related to the physical entity a though the user clicked on a physical hyperlink we have integrated identifier resolution with the web so that it can be deployed a ubiquitously a the web in the infrastructure and on wirelessly connected handheld device we enable user to capture resolution service and application a web resource in their local context we use the web to invoke resolution service with a model of physical web form filling we propose a scheme for binding identifier to resource to promote service and application linking the physical and virtual world 
we consider the problem of structured classification where the task is to predict a label y from an input x and y ha meaningful internal structure our framework includes supervised training of markov random field and weighted context free grammar a special case we describe an algorithm that solves the large margin optimization problem defined in using an exponential family gibbs distribution representation of structured object the algorithm is efficient even in case where the number of label y is exponential in size provided that certain expectation under gibbs distribution can be calculated efficiently the method for structured label relies on a more general result specifically the application of exponentiated gradient update to quadratic program 
we study the problem of agent negotiating period of time during which they can have use of resource thus allowing for the sharing of resource we define a multi stage negotiation framework where agent in order to obtain resource step through a sequence of stage each characterised by an increased chance of a mutually agreeable deal but at the price of disclosing more and more information in the sequence the agent may agree to move to the next stage if the previous stage fails to produce a deal amongst them in this paper we concentrate on two early negotiation stage characterised by minimal disclosure of information thus the agent negotiating at these stage can be thought of a minimally intrusive 
a new algorithm is proposed for novel view generation in one to oneteleconferencing application given the video stream acquired bytwo camera placed on either side of a computer monitor theproposed algorithm synthesis image from a virtual camera inarbitrary position typically located within the monitor tofacilitate eye contact our technique is based on an improved dynamic programming stereo algorithm for efficient novel viewgeneration the two main contribution of this paper are i a newtype of three plane graph for dense stereo dynamic programming that encourages correct occlusion labeling ii a compact geometricderivation for novel view synthesis by direct projection of theminimum cost surface furthermore this paper present a novelalgorithm for the temporal maintenance of a background model toenhance the rendering of occlusion and reduce temporal artefact flicker and a cost aggregation algorithm that act directly onour three dimensional matching cost space example are given thatdemonstrate the robustness of the new algorithm to spatial andtemporal artefact for long stereo video stream these includedemonstrations of synthesis of cyclopean view of extendedconversational sequence we further demonstrate synthesis from afreely translating virtual camera 
the morphology of semitic language is unique in the sense that the major word formation mechanism is an inherently non concatenative process of interdigitation whereby two morpheme a root and a pattern are interwoven identifying the root of a given word in a semitic language is an important task in some case a crucial part of morphological analysis it is also a non trivial task which many human find challenging we present a machine learning approach to the problem of extracting root of hebrew word given the large number of potential root thousand we address the problem a one of combining several classifier each predicting the value of one of the root s consonant we show that when these predictor are combined by enforcing some fairly simple linguistics constraint high accuracy which compare favorably with human performance on this task can be achieved 
increasingly the world wide web is being viewed a a mean of creating web community rather than simply a a mean of publishing and delivering document and service in this research we develop the concept of federated information sharing community fisc and associated architecture that enables community centred information system to be constructed such system provide a way for organisation distributed workgroups and individual to build up a federated community based on their common interest over the world wide web to support community we develop capability that go beyond the generic retrieval of document to include the ability to retrieve people their interest and inter relationship we focus on providing social awareness in the large to help user understand the member within a community and the relationship between them who is working on what topic and who is working with whom within the fisc framework we provide a viewpoint retrieval service to enable a user to construct visual contextual view of the community from the perspective of any community member to evaluate these idea we develop test bed to compare individual component technology such a user and group profile construction and similarity matching and we develop prototype web network and citeseer community to explore the broader architecture and usage issue 
burst detection is the activity of finding abnormal aggregate in data stream such aggregate are based on sliding window over data stream in some application we want to monitor many sliding window size simultaneously and to report those window with aggregate significantly different from other period we will present a general data structure for detecting interesting aggregate over such elastic window in near linear time we present application of the algorithm for detecting gamma ray burst in large scale astrophysical data detection of period with high volume of trading activity and high stock price volatility is also demonstrated using real time trade and quote taq data from the new york stock exchange nyse our algorithm beat the direct computation approach by several order of magnitude 
one of the most well studied problem in data mining is computing the collection of frequent item set in large transactional database one obstacle for the applicability of frequent set mining is that the size of the output collection can be far too large to be carefully examined and understood by the user even restricting the output to the border of the frequent item set collection doe not help much in alleviating the problem in this paper we address the issue of overwhelmingly large output size by introducing and studying the following problem what are the k set that best approximate a collection of frequent item set our measure of approximating a collection of set by k set is defined to be the size of the collection covered by the the k set i e the part of the collection that is included in one of the k set we also specify a bound on the number of extra set that are allowed to be covered we examine different problem variant for which we demonstrate the hardness of the corresponding problem and we provide simple polynomial time approximation algorithm we give empirical evidence showing that the approximation method work well in practice 
we discus the automatic generation of thematic lexicon by mean of term categorization a novel task employing technique from information retrieval ir and machine learning ml specifically we view the generation of such lexicon a an iterative process of learning previously unknown association between term and theme i e discipline or field of activity the process is iterative in that it generates for each ci in a set c c cm of theme a sequence li li lin of lexicon bootstrapping from an initial lexicon li and a set of text corpus thgr thgr thgr n given a input the method is inspired by text categorization the discipline concerned with labelling natural language text with label from a predefined set of theme or category however while text categorization deal with document represented a vector in a space of term term categorization deal dually with term represented a vector in a space of document and label term instead of document with theme a a learning device we adopt boosting since a it ha demonstrated state of the art effectiveness in a variety of text categorization application and b it naturally allows for a form of data cleaning thereby making the process of generating a thematic lexicon an iteration of generate and test step 
a mixed signal image filtering vlsi ha been developed aiming at real time generation of edge based image vector for robust image recognition a four stage asynchronous median detection architecture based on analog digital mixed signal circuit ha been introduced to determine the threshold value of edge detection the key processing parameter in vector generation a a result a fully seamless pipeline processing from threshold detection to edge feature map generation ha been established a prototype chip wa designed in a m double polysilicon three metal layer cmos technology and the concept wa verified by the fabricated chip the chip generates a dimension feature vector from a x pixel gray scale image every sec this is about time faster than the software computation making a real time image recognition system feasible 
scheduling observation by coordinated fleet of earth observing satellite eos involves large search space complex constraint and poorly understood bottleneck condition where stochastic algorithm are often effective however there are many such algorithm and the best one to use is not obvious here we compare multiple variant of the genetic algorithm hill climbing simulated annealing squeaky wheel optimization and iterated sampling on ten realistically sized model eos scheduling problem schedule are represented by a permutation non temperal ordering of the observation request a simple greedy deterministic scheduler assigns time and resource to each observation request in the order indicated by the permutation discarding those that violate the constraint created by previously scheduled observation simulated annealing performs best and random mutation outperforms a more intelligent mutator furthermore the best mutator by a small margin wa a novel approach we call temperature dependent swap that make large change in the early stage of the search and smaller change towards the end 
we have developed a robot controller based upon a neural implementation of norman and shallice s model of executive attentional control in human a simulation illustrates how attentional control lead to the suppression of action selection error in neurally controlled robot a related demonstration illustrates how lesioning of the control architecture lead to behavioural pathology that resemble those seen in human patient with damage to the prefrontal cortex 
we propose a novel projection based visualization method for high dimensional datasets by combining concept from md and the geometry of the hyperbolic space our approach hyperbolic multi dimensional scaling h md extends earlier work using hyperbolic space for visualization of tree structure data hyperbolic tree browser by borrowing concept from multi dimensional scaling we map proximity data directly into the dimensional hyperbolic space h this remove the restriction to quasihierarchical graph based data limiting previous work since a suitable distance function can convert all kind of data to proximity or distance based data this type of data can be considered the most general we used the circular poincar model of the h which allows effective human computer interaction by moving the focus via mouse the user can navigate in the data without loosing the context in h the fish eye behavior originates not simply by a non linear view transformation but rather by extraordinary non euclidean property of the h especially the exponential growth of length and area of the underlying space make the h a prime target for mapping hierarchical and now also high dimensional data we present several high dimensional mapping example including synthetic and real world data and a successful application for unstructured text by analyzing and integrating multiple film critique from news rec art movie review and the internet movie database each movie becomes placed within the h here the idea is that related film share more word in their review than unrelated their semantic proximity lead to a closer arrangement the result is a kind of high level content structured display allowing the user to explore the space of movie 
we develop an exact dynamic programming algorithm for partially observable stochastic game posgs the algorithm is a synthesis of dynamic programming for partially observable markov decision process pomdps and iterated elimination or dominated strategy in normal form game we prove that when applied to finite horizon posgs the algorithm iteratively eliminates very weakly dominated strategy without first forming a normal form representation of the game for the special case in which agent share the same payoff the algorithm can be used to find an optimal solution we present preliminary empirical result and discus way to further exploit pomdp theory in solving posgs 
we approximate arabic s rich morphology by a model that a word consists of a sequence of morpheme in the pattern prefix stem suffix denotes zero or more occurrence of a morpheme our method is seeded by a small manually segmented arabic corpus and us it to bootstrap an unsupervised algorithm to build the arabic word segmenter from a large unsegmented arabic corpus the algorithm us a trigram language model to determine the most probable morpheme sequence for a given input the language model is initially estimated from a small manually segmented corpus of about word to improve the segmentation accuracy we use an unsupervised algorithm for automatically acquiring new stem from a million word unsegmented corpus and re estimate the model parameter with the expanded vocabulary and training corpus the resulting arabic word segmentation system achieves around exact match accuracy on a test corpus containing word token we believe this is a state of the art performance and the algorithm can be used for many highly inflected language provided that one can create a small manually segmented corpus of the language of interest 
when rule of transfer based machine translation mt are automatically acquired from bilingual corpus incorrect redundant rule are generated due to acquisition error or translation variety in the corpus a a new countermeasure to this problem we propose a feedback cleaning method using automatic evaluation of mt quality which remove incorrect redundant rule a a way to increase the evaluation score bleu is utilized for the automatic evaluation the hill climbing algorithm which involves feature of this task is applied to searching for the optimal combination of rule our experiment show that the mt quality improves by in test sentence according to a subjective evaluation this is considerable improvement over previous method 
we study two apparently different but formally similar scheduling problem the first problem involves contract algorithm which can trade off run time for solution quality a long a the amount of available run time is known in advance the problem is to schedule contract algorithm to run on parallel processor under the condition that an interruption can occur at any time and upon interruption a solution to any one of a number of problem can be requested schedule are compared in term of acceleration ratio which is a worst case measure of efficiency we provide a schedule and prove it optimality among a particular class of schedule our second problem involves multiple robot searching for a goal on one of multiple ray search strategics are compared in term of time competitive ratio the ratio of the total search time to the time it would take for one robot to traverse directly to the goal we demonstrate that search strategy and contract schedule are formally equivalent in addition for our class of schedule we derive a formula relating the acceleration ratio of a schedule to the time competitive ratio of the corresponding search strategy 
most of the work on d object recognition from rangedata ha used an alignment verification approach in whicha specific d object is matched to an exact instance of thesame object in a scene this approach ha been successfullyused in industrial machine vision but it is not capable ofdealing with the complexity of recognizing class of similarobjects this paper undertakes this task by proposingand testing a component based methodology encompassingthree main ingredient a new way of learning and extractingshape class component from surface shape information a new shape representation called a symbolicsurface signature that summarizes the geometric relationshipsamong component and an abstract representationof shape class formed by a hierarchy of classifiersthat learn object class part and their spatial relationshipsfrom example 
this paper present a multi layered question answering q a architecture suitable for enhancing current q a capability with the possibility of processing complex question that is question whose answer need to be gathered from piece of factual information scattered in different document specifically we have designed a layer oriented to process the different type of temporal question complex temporal question are first decomposed into simpler one according to the temporal relationship expressed in the original question in the same way the answer of each simple question are re composed fulfilling the temporal restriction of the original complex question using this architecture a temporal q a system ha been developed in this paper we focus on explaining the first part of the process the decomposition of the complex question furthermore it ha been evaluated with the terqas question corpus of temporal question for the task of question splitting our system ha performed in term of precision and recall and respectively 
this paper present a novel statistical model for cross language information retrieval given a written query in the source language document in the target language are ranked by integrating probability computed by two statistical model a query translation model which generates most probable term by term translation of the query and a query document model which evaluates the likelihood of each document and translation integration of the two score is performed over the set of n most probable translation of the query experimental result with value n are presented on the italian english bilingual track data used in the clef and evaluation campaign 
youserv is a system that allows it user to pool existing desktop computing resource for high availability web hosting and file sharing by exploiting standard web and internet protocol e g http and dns youserv doe not require those who access youserv published content to install special purpose software because it requires minimal server side resource and administration youserv can be provided at a very low cost we describe the design implementation and a successful intranet deployment of the youserv system and compare it with several alternative 
abstract the application of kernel based learning algorithm ha so far largely been confined to realvalued data and a few special data type such a string in this paper we propose a general method of constructing natural family of kernel over discrete structure based on the matrix exponentiation idea in particular we focus on generating kernel on graph for which we propose a special class of exponential kernel based on the heat equation called diffusion kernel and show that these can be regarded a the discretization of the familiar gaussian kernel of euclidean space 
we propose a solution to the problem of inferring the depth map radiance and motion of a scene from a collection of motion blurred and defocused image we model motion blur and defocus a an anisotropic diusion process whose initial condition depend on the radiance and whose diusion tensor encodes the shape of the scene the motion field and the optic parameter we show that this model is well posed and propose an ecient algorithm to infer the unknown of the model inference is performed by minimizing the discrepancy between the measured blurred image and the one synthesized via forward diusion since the problem is ill posed we also introduce additional tikhonov regularization term the resulting method is fast and robust to noise a shown by experiment with both synthetic and real data 
method based on local viewpoint invariant feature have proven capable of recognizing object in spite of viewpoint change occlusion and clutter however these approach fail when these factor are too strong due to the limited repeatability and discriminative power of the feature a additional shortcoming the object need to be rigid and only their approximate location is found we present a novel object recognition approach which overcomes these limitation an initial set of feature correspondence is first generated the method anchor on it and then gradually explores the surrounding area trying to construct more and more matching feature increasingly farther from the initial one the resulting process cover the object with match and simultaneously separate the correct match from the wrong one hence recognition and segmentation are achieved at the same time only very few correct initial match suffice for reliable recognition the experimental result demonstrate the stronger power of the presented method in dealing with extensive clutter dominant occlusion large scale and viewpoint change moreover non rigid deformation are explicitly taken into account and the approximative contour of the object are produced the approach can extend any viewpoint invariant feature extractor 
we present a data and error analysis for semantic role labelling in a first experiment we build a generic statistical model for semantic role assignment in the framenet paradigm and show that there is a high variance in performance across frame the main hypothesis of our paper is that this variance is to a large extent a result of difference in the underlying argument structure of the predicate in different frame in a second experiment we show that frame uniformity which measure argument structure variation correlate well with the performance figure effectively explaining the variance 
the structural similarity of neural network and genetic regulatory network to digital circuit and hence to each other wa noted from the very beginning of their study in this work we propose a simple biochemical system whose architecture mimic that of genetic regulation and whose component allow for in vitro implementation of arbitrary circuit we use only two enzyme in addition to dna and rna molecule rna polymerase rnap and ribonuclease rnase we develop a rate equation for in vitro transcriptional network and derive a correspondence with general neural network rate equation a proof of principle demonstration an associative memory task and a feedforward network computation are shown by simulation a difference between the neural network and biochemical model is also highlighted global coupling of rate equation through enzyme saturation can lead to global feedback regulation thus allowing a simple network without explicit mutual inhibition to perform the winner take all computation thus the full complexity of the cell is not necessary for biochemical computation a wide range of functional behavior can be achieved with a small set of biochemical component 
previous research ha demonstrated the utility of clustering in inducing semantic verb class from undisambiguated corpus data we describe a new approach which involves clustering subcategorization frame scf distribution using the information bottleneck and nearest neighbour method in contrast to previous work we particularly focus on clustering polysemic verb a novel evaluation scheme is proposed which account for the effect of polysemy on the cluster offering u a good insight into the potential and limitation of semantically classifying undisambiguated scf data 
in this paper we address the problem of searching schema database for semantically related schema we first give a method of finding semantic similarity between pair wise schema based on tokenization part of speech tagging word expansion and ontology matching we then address the problem of indexing the schema database through a semantic hash table matching schema in the database are found by hashing the query attribute and recording peak in the histogram of schema hit result indicated a improvement in search performance while maintaining high precision and recall 
class membership probability estimate are important for many application of data mining in which classification output are combined with other source of information for decision making such a example dependent misclassification cost the output of other classifier or domain knowledge previous calibration method apply only to two class problem here we show how to obtain accurate probability estimate for multiclass problem by combining calibrated binary probability estimate we also propose a new method for obtaining calibrated two class probability estimate that can be applied to any classifier that produce a ranking of example using naive bayes and support vector machine classifier we give experimental result from a variety of two class and multiclass domain including direct marketing text categorization and digit recognition 
we consider how to use external memory such a disk storage to improve the scalability of heuristic search in state space graph to limit the number of slow disk i o operation we develop a new approach to duplicate detection in graph search that localizes memory reference by partitioning the search graph based on an abstraction of the state space and expanding the frontier node of the graph in an order that respect this partition we demonstrate the effectiveness of this approach both analytically and empirically 
this paper proposes an image matching method that is robust to illumination variation and affine distortion our idea is to do image matching through establishing an imaging function that describes the functional relationship relating intensity value between two image similar methodology ha been proposed by viola and lai fang viola proposed to do image matching through establishment of an imaging function based on a consistency principle lai fang proposed a parametric form of the imaging function in case where the illumination variation is not globally uniform and the parametric form of imaging function is not obvious one need to have a more robust method our method aim to take care of spatially non uniform illumination variation and affine distortion central to our method is the proposal of a localized consistency principle implemented through a non parametric way of estimating the imaging function the estimation is effected through optimizing a similarity measure that is robust under spatially non uniform illumination variation and affine distortion experimental result are presented from both synthetic and real data encouraging result were obtained 
we investigate the calculation of a bound for sequence and tree model which are the explicit intersection of a set of simpler model or can be bounded by such an intersection we provide a natural viewpoint which unifies various instance of factored a model for tree and sequence some previously known and others novel including multiple sequence alignment weighted finite state transducer composition and lexicalized statistical parsing the specific case of parsing with a product of syntactic pcfg and semantic lexical dependency component is then considered in detail we show that this factorization give a modular lexicalized parser which is simpler than comparably accurate non factored model and which allows efficient exact inference with large treebank grammar 
in this paper we propose to combine an ecien t image representation based on local descriptor with a support vector machine classier in order to perform object categorization for this purpose we apply kernel dened on set of vector after testing dieren t combination of kernel local descriptor we have been able to identify a very performant one 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
in this paper we argue that cache consistency mechanism designed for stand alone proxy do not scale to the large number of proxy in a content distribution network and are not flexible enough to allow consistency guarantee to be tailored to object need to meet the twin challenge of scalability and flexibility we introduce the notion of cooperative consistency along with a mechanism called cooperative lease to achieve it by supporting dgr consistency semantics and by using a single lease for multiple proxy cooperative lease allows the notion of lease to be applied in a flexible scalable manner to cdns further the approach employ application level multicast to propagate server notification to proxy in a scalable manner we implement our approach in the apache web server and the squid proxy cache and demonstrate it efficacy using a detailed experimental evaluation our result show a factor of reduction in server message overhead and a reduction in server state space overhead when compared to original lease albeit at an increased inter proxy communication overhead 
resource description framework rdf is a general description technology that can be applied to many application domain redland is a flexible and efficient rdf system that complement this power and provides high level interface allowing instance of the model to be stored queried and manipulated in c perl python tcl java and other language it is implemented using an object based api providing several of the implementation class a module which can be added removed or replaced to allow different functionality or application specific optimisation the framework provides the core technology for developing new rdf application experimenting with implementation technique apis and representation 
abstract we present a modified version of the perceptron learning algorithm pla which solves semidefinite program sdps in polynomial time the algorithm is based on the following three observation i semidefinite program are linear program with infinitely many linear constraint ii every linear program can be solved by a sequence of constraint satisfaction problem with linear constraint iii in general the perceptron learning algorithm solves a constraint satisfaction problem with linear constraint in finitely many update combining the pla with a probabilistic rescaling algorithm which on average increase the size of the feasable region result in a probabilistic algorithm for solving sdps that run in polynomial time we present preliminary result which demonstrate that the algorithm work but is not competitive with state of the art interior point method 
in this paper we introduce an information theoretic method for estimating the usefulness of the hyperlink structure induced from the set of retrieved document we evaluate the effectiveness of this method in the context of an optimal bayesian decision mechanism which selects the most appropriate retrieval approach on a per query basis for two trec task the estimation of the hyperlink structure s usefulness is stable when we use different weighting scheme or when we employ sampling of document to reduce the computational overhead next we evaluate the effectiveness of the hyperlink structure s usefulness in a realistic setting by setting the threshold of a decision mechanism automatically our result show that improvement over the baseline are obtained 
in this paper we introduce and study shortest path discovery spd problem a generalization of shortest path problem in spd one is given a directed edge weighted graph and the task is to find a the shortest path for fixed source and target node such that initially the edge weight are unknown but they can be queried querying the cost of an edge is expensive and hence the goal is to minimize the total number of edge cost query executed in this article we characterize some common property of sound spd algorithm propose a particular algorithm that is shown to be sound and effective experimental result on real world ocr task demonstrate the usefulness of the approach whereas the proposed algorithm is shown to yield a substantial speed up of the recognition process 
for most web based application content are created dynamically based on the current state of a business such a product price and inventory stored in database system these application demand personalized content and track user behavior while maintaining application integrity many of such practice are not compatible with web acceleration solution consequently although many web acceleration solution have shown promising performance improvement and scalability architecting and engineering distributed enterprise web application to utilize available content delivery network remains a challenge in this paper we examine the challenge to accelerate j ee based enterprise web application we list obstacle and recommend some practice to transform typical database driven j ee application to cache friendly web application where web acceleration solution can be applied furthermore such transformation should be done without modification to the underlying application business logic and without sacrificing function that are essential to e commerce we take the j ee reference software the java petstore a a case study by using the proposed guideline we are able to cache more than of the content in the petstore and scale up the web site more than time 
abstract recent algorithm for sparse coding and independent component analysis ica have demonstrated how localized feature can be learned from natural image however these approach do not take image transformation into account a a result they produce image code that are redundant because the same feature is learned at multiple location we describe an algorithm for sparse coding based on a bilinear generative model of image by explicitly modeling the interaction between image feature and their transformation the bilinear approach help reduce redundancy in the image code and provides a basis for transformationinvariant vision we present result demonstrating bilinear sparse coding of natural image we also explore an extension of the model that can capture spatial relationship between the independent feature of an object thereby providing a new framework for part based object recognition 
in this paper we investigate whether paragraph can be identified automatically in different language and domain we propose a machine learning approach which exploit textual and discourse cue and we ass how well human perform on this task our best model achieve an accuracy that is significantly higher than the best baseline and for most data set come to within of human performance 
real world application often require the classification of document under situation of small number of feature mi labeled document and rare positive example this paper investigates the robustness of three regularized linear classification method svm ridge regression and logistic regression under above situation we compare these method in term of their loss function and score distribution and establish the connection between their optimization problem and generalization error bound several set of controlled experiment on the reuters corpus are conducted to investigate the robustness of these method our result show that ridge regression seems to be the most promising candidate for rare class problem 
a wealth of information relevant for e commerce often appears intext form this includes specification and performance data sheetsof product financial statement product offering etc typicallythese type of product and financial data are published in tabularform the only separator between item in the table are whitespaces and line separator we will refer to such table a texttables due to the lack of structure in such table theinformation present is not readily queriable using traditionaldatabase query language like sql one way to make it amenable tostandard database querying technique is to extract the data itemsin the table and create a database out of the extracted data butextraction from text table pose difficulty due to theirregularity of the data in the column existing technique like and are based on finding fixedseparators between successive column however it is not alwayspossible to find fixed separator even if fixed separator existthey may not unambiguously separate column that have multiworditems another set of technique are based on regular expression the problem here are i they are difficult to construct and ii they depend on lexical similarity between column item note that by visual inspection a casual observer can correctlyassociate every item in a text table to it corresponding column this is because all the item belonging to a column appear clustered more closely to each other than to item in differentcolumns whereas such cluster can be clearly discerned by a humanobserver making them machine recognizable is the key to robustautomated extraction of data item from text based table clustering enables u to make association between item in acolumn based not merely on examining item in adjacent row butacross all the row in the table we have designed and implemented the cutex system forextracting data from irregular text table the input is a filecontaining only text table the output produced by cutex isan association between every item in a column note thatcutex doe not do table detection in text the innovativeaspect of cutex is it clustering based algorithm thatdrives the extraction process in cutex each line is brokendown into a set of token each token is a contiguous sequence ofnon white space character the center of any token in a cluster iscloser to the center of some other token in the same cluster inter cluster gap are gap between the extremal token in theclusters starting with an initial set of cluster adjacentclusters are merged into bigger cluster based on the inter clustergaps the algorithm terminates when no more cluster can be merged we have formalized the notion of a correct extraction and developeda syntactic characterization of table on which this algorithm willalways produce a correct extraction detail appear in anunique aspect of the algorithm is it robustness in the presence ofmisalignments precision of extraction can be improved by supplying the minimumseparation between column a a parameter such a separator isestimated by sampling a few input table the clustering algorithmdoes not merge adjacent cluster if the gap between them is largerthan this parameter value note though that the minimum column gapcannot be used a a fixed separator since doing so amount to doinglocalized determination making it brittle to misalignment cutex is implemented in java and is approximately about line of code the system automatically partition the set ofinput text table into directory containing correct and incorrectextractions at the end of an extraction the user can examine thedirectory containing incorrectly extracted table sample a few ofthem identify if it wa caused by an erroneous estimate of theminimum column gap re adjust the configuration parameter and starta new extraction on all these table successive iteration cangenerate a higher extraction yield the primary focus of the demonstration will be on illustratingthe robustness and the iterative process of improving theextraction yield of the clustering algorithm 
abstract we describe a mixture density propagation algorithm to estimate d human motion in monocular videosequences based on observation encoding the appearance of image silhouette our approach is discriminative rather than generative therefore it doe not require the probabilistic inversion of a predictive observationmodel instead it us a large human motion capture data base and a d computer graphic humanmodel to synthesize training pair of typical human configuration together with 
the larger amount of information on the web is stored in document database and is not indexed by general purpose search engine i e google and yahoo such information is dynamically generated through querying database which are referred to a hidden web database document returned in response to a user query are typically presented using template generated web page this paper proposes a novel approach that identifies web page template by analysing the textual content and the adjacent tag structure of a document in order to extract query related data preliminary result demonstrate that our approach effectively detects template and retrieves data with high recall and precision 
feature extraction is important in many application such a text and image retrieval because of high dimensionality uncorrelated linear discriminant analysis ulda wa recently proposed for feature extraction the extracted feature via ulda were shown to be statistically uncorrelated which is desirable for many application in this paper we will first propose the ulda qr algorithm to simplify the previous implementation of ulda then we propose the ulda gsvd algorithm based on a novel optimization criterion to address the singularity problem it is applicable for undersampled problem where the data dimension is much larger than the data size such a text and image retrieval the novel criterion used in ulda gsvd is the perturbed version of the one from ulda qr while surprisingly the solution to ulda gsvd is shown to be independent of the amount of perturbation applied we did extensive experiment on text and face image data to show the effectiveness of ulda gsvd and compare with other popular feature extraction algorithm 
since the state space of most game is a directed graph many game playing system detect repeated position with a transposition table this approach can reduce search effort by a large margin however it suffers from the so called graph history interaction ghi problem which cause error in game containing repeated position this paper present a practical solution to the ghi problem that combine and extends previous technique because our scheme is general it is applicable to different game tree search algorithm and to different domain a demonstrated with the two algorithm and df pn in the two game checker and go our scheme incurs only a very small overhead while guaranteeing the correctness of solution 
preview and overview of large heterogeneous information resource help user comprehend the scope of collection and focus on particular subset of interest for narrative document question of what happened where and when are natural point of entry building on our earlier work at the perseus project with detecting term place name and date we have exploited co occurrence of date and place name to detect and describe likely event in document collection we compare statistical measure for determining the relative significance of various event we have built interface that help user preview likely region of interest for a given range of space and time by plotting the distribution and relevance of various collocation user can also control the amount of collocation information in each view once particular collocation are selected the system can identify key phrase associated with each possible event to organize browsing of the document themselves 
we propose a functional mixture model for simultaneous clustering and alignment of set of curve measured on a discrete time grid the model is specifically tailored to gene expression time course data each functional cluster center is a nonlinear combination of solution of a simple linear differential equation that describes the change of individual mrna level when the synthesis and decay rate are constant the mixture of continuoustimeparametricfunctionalformsallowsoneto a accountfor the heterogeneity in the observed profile b align the profile in time by estimating real valued time shift c capture the synthesis and decay of mrna in the course of an experiment and d regularize noisy profile by enforcing smoothness in the mean curve we derive an em algorithm for estimating the parameter of the model and apply the proposed approach to the set of cycling gene in yeast the experiment show consistent improvement in predictive power and within cluster variance compared to regular gaussian mixture 
a bal anced network lead to contradictory constraint on memory model a e xemplified in previous work on accomm odation of synfire chai n here we show that these constraint can be overcome by introducing a shadow inhibitory pattern for each excitatory p attern of the mo del this is interpreted a a d oublebalance principle whereby there exists both global balance between average excitatory and inhibitory current and local balance between the current carrying coherent activity at any given time frame thi s principle can be appl ied t o network with hebbian cell assembly leading to a high capacity of the associative memory the number of possible pattern is limited by a com binatorial constraint that turn out to be p n within the specific model that we employ th is limit is reac hed by the hebbian cell assembly network to the best of our knowledge this is the first time that such high memory capacity are demonstrated in the asynchronous state of model of spiking neuron 
an increasingly large amount of web application employ serviceobjects such a servlets to generate dynamic and personalizedcontent existing caching infrastructure are not well suited forcaching such content in mobile environment because ofdisconnection and weak connection one possible approach to thisproblem is to replicate web related application logic to clientdevices the challenge to this approach are to deal with clientdevices that exhibit huge divergence in resource availability tosupport application that have different data sharing and coherencyrequirements and to accommodate the same application underdifferent deployment environment the replet system target these challenge it us client server and application capability and preference information cpi to direct the replication of service object to client device from the selection of a device for replication and populating thedevice with client specific data to choosing an appropriatereplica to serve a given request and maintaining the desired stateconsistency among replica the replet system exploit on devicereplication to enable client serverand application specificcost metric for replica invocation and synchronization we haveimplemented a prototype in the context of servlet based webapplications our experiment and simulation result demonstrate theviability and significant benefit of cpi driven on device serviceobject replication 
in this paper we analyze the most popular evaluation metric for separate and conquer rule learning algorithm our result show that all commonly used heuristic including accuracy weighted relative accuracy entropy gini index and information gain are equivalent to one of two fundamental prototype precision which try to optimize the area under the roc curve for unknown cost and a cost weighted difference between covered positive and negative example which try to find the optimal point under known or assumed cost we also show that a straightforward generalization of the m estimate trade off these two prototype 
p nbsp p div a key challenge for neural modeling is to explain how a continuous stream of multi modal input from a rapidly changing environment can be processed by stereotypical recurrent circuit of integrate and fire neuron in real time we propose a new computational model that doe not require a task dependent construction of neural circuit instead it is based on principle of high dimensional dynamical system in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry div p nbsp p 
this poster session examines a probabilistic approach to distributed information retrieval using a logistic regression algorithm for estimation of collection relevance the algorithm is compared to other method for distributed search using test collection developed for distributed search evaluation 
recently several lower bound function are proposed for solving the max sat problem optimally in a branch and bound algorithm these lower bound improve significantly the performance of these algorithm based on the study of these lower bound function we propose a new linear time lower bound function we show that the new lower bound function is admissible and it is consistently and substantially better than other known lower bound function the result of this study is a high performance implementation of an exact algorithm for max sat which outperforms any implementation of the same class 
this paper discus the challenge and proposes a solution to performing information retrieval on the web using chinese natural language speech query the main contribution of this research is in devising a divide and conquer strategy to alleviate the speech recognition error it us the query model to facilitate the extraction of main core semantic string cs from the chinese natural language speech query it then break the cs into basic component corresponding to phrase and us a multi tier strategy to map the basic component to known phrase in order to further eliminate the error the resulting system ha been found to be effective 
using language technology for text analysis and light weight ontology a a content mediating level we acquire indexing pattern from vast amount of indexing data for english language medical document this is achieved by statistically relating interlingual representation of these document based on text token bigram to their associated index term from these english indexing pattern we then induce the associated index term for german and portuguese document when their interlingual representation match those of english document thus we learn from past english indexing experience and transfer it in an unsupervised way to non english text without ever having seen concrete indexing data for language other than english 
saliency mechanism play an important role when visual recognition must be performed in cluttered scene we propose a computational definition of saliency that deviate from existing model by equating saliency to discrimination in particular the salient attribute of a given visual class are defined a the feature that enable best discrimination between that class and all other class of recognition interest it is shown that this definition lead to saliency algorithm of low complexity that are scalable to large recognition problem and is compatible with existing model of early biological vision experimental result demonstrating success in the context of challenging recognition problem are also presented 
in this paper we introduce methodology to determine the bifurcation structure of optimum for a class of similar cost function from rate distortion theory deterministic annealing information distortion and the information bottleneck method we also introduce a numerical algorithm which us the explicit form of the bifurcating branch to find optimum at a bifurcation point 
this prototype system demonstrates a novel sentence alignment method for bilingual text based on adaptive learning and lexical information the system aligns bilingual text at the paragraph level first and acquires length related statistic for the subsequent sentence alignment process in addition to length a probabilistic translation lexicon is utilized to further enhance the precision the system is especially effective in the case of noisy translation produced in either translation direction that may involve different domain 
we address the problem of learning topic hierarchy from data the model selection problem in this domain is daunting which of the large collection of possible tree to use we take a bayesian approach generating an appropriate prior via a distribution on partition that we refer to a the nested chinese restaurant process this nonparametric prior allows arbitrarily large branching factor and readily accommodates growing data collection we build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent dirichlet allocation we illustrate our approach on simulated data and with an application to the modeling of nip abstract 
while monochrome unformatted text and richly colored graphical content are both capable of conveying a message well designed graphical content ha the potential for better engaging the human sensory system it is our contention that the author of an audio presentation should be afforded the benefit of judiciously exploiting the human aural perceptual ability to deliver content in a more compelling concise and realistic manner while contemporary streaming medium player and voice browser share the ability to render content non textually neither technology is currently capable of rendering three dimensional medium the contribution described in this paper are proposed d audio extension to smil and a server based framework able to receive a request and on demand process such a smil file and dynamically create the multiple simultaneous audio object spatialize them in d space multiplex them into a single stereo audio and prepare it for transmission over an audio stream to a mobile device to the knowledge of the author this is the first reported solution for delivering and rendering on a commercially available wireless handheld device a rich d audio listening experience a described by a markup language naturally in addition to mobile device this solution also work with desktop streaming medium player 
recent research ha demonstrated that useful pomdp solution do not require consideration of the entire belief space we extend this idea with the notion of temporal abstraction we present and explore a new reinforcement learning algorithm over grid point in belief space which us macro action and monte carlo update of the q value we apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space we can learn pomdp policy faster and we can do information gathering more efficiently 
the mantra of every experienced web application developer is the same thou shalt separate business logic from display ironically almost all template engine allow violation of this separation principle which is the very impetus for html template engine development this situation is due mostly to a lack of formal definition of separation and fear that enforcing separation emasculates a template s power i show that not only is strict separation a worthy design principle but that we can enforce separation while providing a potent template engine i demonstrate my stringtemplate engine used to build jguru com and other commercial site at work solving some nontrivial generational task my goal is to formalize the study of template engine thus providing a common nomenclature a mean of classifying template generational power and a way to leverage interesting result from formal language theory i classify three type of restricted template analogous to chomsky s type grammar class and formally define separation including the rule that embody separation because this paper provides a clear definition of model view separation template engine designer may no longer blindly claim enforcement of separation moreover given theoretical argument and empirical evidence programmer no longer have an excuse to entangle model and view 
director musices is a program that transforms notated score into musical performance it implement the performance rule emerging from research project at the royal institute of technology kth rule in the program model performance aspect such a phrasing articulation and intonation and they operate on performance variable such a tone inter onset duration amplitude and pitch by manipulating rule parameter the user can act a a metaperformer controlling different feature of the performance leaving the technical execution to the computer different interpretation of the same piece can easily be obtained feature of director musices include midi file input and output rule palette graphical display of all performance variable along with the notation and userdefined performance rule the program is implemented in common lisp and is available free a a stand alone application both for macintosh and window platform further information including music example publication and the program itself is located online at http www speech kth se music performance this paper is a revised and updated version of a previous paper published in the computer music journal in year that wa mainly written by anders friberg friberg colombo fryd n and sundberg 
this paper describes a new qualitative quantitative simulator to help buyer learn how to make decision when they purchase good in this paper we propose an e learning support system lsdm for assisting user decision making by applying artificial intelligence technology when buyer purchase expensive item they must carefully select these item from many alternative the learning support system provides useful information that assist consumer in purchasing good we employ qualitative simulation because the output simulation result are useful our system consists of a qualitative processing system and a quantitative calculation system when user use the system they first input information on the good they want to purchase the information input by user is used in the qualitative simulation next they supply the detail of their budget the rate of loan and several other factor on a form the system then integrates the result of simulation and the user s input data and proposes plan to aid in their decision process the system ha several advantage it can be used by simple input the process of simulation is easy to understand user can learn how to make decision by trial and error and the user can base their decision making on synthetic result 
we suggest a way for locating duplicate and plagiarism in a text collection using an r measure which is the normalized sum of the length of all suffix of the text repeated in other document of the collection the r measure can be effectively computed using the suffix array data structure additionally the computation procedure can be improved to locate the set of duplicate or plagiarised document we applied the technique to several standard text collection and found that they contained a significant number of duplicate and plagiarised document another reformulation of the method lead to an algorithm that can be applied to supervised multi class categorization we illustrate the approach using the recently available reuters corpus volume rcv the result show that the method outperforms svm at multi class categorization and interestingly that result correlate strongly with compression based method 
many sequential prediction task involve locating instance of pattern in sequence generative probabilistic language model such a hidden markov model hmms have been successfully applied to many of these task a limitation of these model however is that they cannot naturally handle case in which pattern instance overlap in arbitrary way we present an alternative approach based on conditional markov network that can naturally represent arbitrarily overlapping element we show how to ecien tly train and perform inference with these model experimental result from a genomics domain show that our model are more accurate at locating instance of overlapping pattern than are baseline model based on hmms 
we examine the use of hidden markov and hidden semi markov model for automatically segmenting an electrocardiogram waveform into it constituent waveform feature an undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling we show that the state duration implicit in a standard hidden markov model are ill suited to those of real ecg feature and we investigate the use of hidden semi markov model for improved state duration modelling 
the rapid growth of the world wide web had made the problem of topic specific resource discovery an important one in recent year in this problem it is desired to find web page which satisfy a predicate specified by the user such a predicate could be a keyword query a topical query or some arbitrary contraint several technique such a focussed crawling and intelligent crawling have recently been proposed for topic specific resource discovery all these crawler are linkage based since they use the hyperlink behavior in order to perform resource discovery recent study have shown that the topical correlation in hyperlink are quite noisy and may not always show the consistency necessary for a reliable resource discovery process in this paper we will approach the problem of resource discovery from an entirely different perspective we will mine the significant browsing pattern of world wide web user in order to model the likelihood of web page belonging to a specified predicate this user behavior can be mined from the freely available trace of large public domain proxy on the world wide web we refer to this technique a collaborative crawling because it mine the collective user experience in order to find topical resource such a strategy is extremely effective because the topical consistency in world wide web browsing pattern turn out to very reliable in addition the user centered crawling system can be combined with linkage based system to create an overall system which work more effectively than a system based purely on either user behavior or hyperlink 
this paper focus on temporal constraint problem where the objective is to optimize a set of local preference for when event occur in previous work a subclass of these problem ha been formalized a a generalization of temporal csps and a tractable strategy for optimization ha been proposed where global optimality is defined a maximizing the minimum of the component preference value this criterion for optimality which we call weakest link optimization wlo is known to have limited practical usefulness because solution are compared only on the basis of their worst value thus there is no requirement to improve the other value to address this limitation we introduce a new algorithm that rc applies wlo iteratively in a way that lead to improvement of all the value we show the value of this strategy by proving that with suitable preference function the resulting solution are pareto optimal 
spike timing plasticity stdp is a special form of synaptic plasticity where the relative timing of postand presynaptic activity determines the change of the synaptic weight on the postsynaptic side active backpropagating spike in dendrite seem to play a crucial role in the induction of spike timing dependent plasticity we argue that postsynaptically the temporal change of the membrane potential determines the weight change coming from the presynaptic side induction of stdp is closely related to the activation of nmda channel therefore we will calculate analytically the change of the synaptic weight by correlating the derivative of the membrane potential with the activity of the nmda channel thus for this calculation we utilise biophysical variable of the physiological cell the final result show a weight change curve which conforms with measurement from biology the positive part of the weight change curve is determined by the nmda activation the negative part of the weight change curve is determined by the membrane potential change therefore the weight change curve should change it shape depending on the distance from the soma of the postsynaptic cell we find temporally asymmetric weight change close to the soma and temporally symmetric weight change in the distal dendrite 
if the cortex us spike timing to compute the timing of the spike must be robust to perturbation based on a recent framework that provides a simple criterion to determine whether a spike sequence produced by a generic network is sensitive to initial condition and numerical simulation of a variety of network architecture we argue within the limit set by our model of the neuron that it is unlikely that precise sequence of spike timing are used for computation under condition typically found in the cortex 
we aim at using color information to classify the physicalnature of edge in video to achieve physic based edgeclassification we first propose a novel approach to coloredge detection by automatic noise adaptive thresholdingderived from sensor noise analysis then we present a taxonomyon color edge type a a result a parameter freeedge classifier is obtained labeling color transition intoone of the following type shadow geometry highlightedges material edge the proposed method isempirically verified on image showing complex real worldscenes 
passage retrieval is an important component common to many question answering system because most evaluation of question answering system focus on end to end performance comparison of common component becomes difficult to address this shortcoming we present a quantitative evaluation of various passage retrieval algorithm for question answering implemented in a framework called pauchok we present three important finding boolean querying scheme perform well in the question answering task the performance difference between various passage retrieval algorithm vary with the choice of document retriever which suggests significant interaction between document retrieval and passage retrieval the best algorithm in our evaluation employ density based measure for scoring query term our result reveal future direction for passage retrieval and question answering 
this paper take a critical look at the feature used in the semantic role tagging literature and show that the information in the input generally a syntactic parse tree ha yet to be fully exploited we propose an additional set of feature and our experiment show that these feature lead to fairly signicant improvement in the task we performed we further show that dierent feature are needed for dierent subtasks finally we show that by using a maximum entropy classier and fewer feature we achieved result comparable with the best previously reported result obtained with svm model we believe this is a clear indication that developing feature that capture the right kind of information is crucial to advancing the stateof the art in semantic analysis 
one of the major problem in question answering qa is that the query are either too brief or often do not contain most relevant term in the target corpus in order to overcome this problem our earlier work integrates external knowledge extracted from the web and wordnet to perform event based qa on the trec task this paper extends our approach to perform event based qa by uncovering the structure within the external knowledge the knowledge structure loosely model different facet of qa event and is used in conjunction with successive constraint relaxation algorithm to achieve effective qa our result obtained on trec qa corpus indicate that the new approach is more effective and able to attain a confidence weighted score of above 
when searching large hypertext document collection it is often possible that there are too many result available for ambiguous query query refinement is an interactive process of query modification that can be used to narrow down the scope of search result we propose a new method for automatically generating refinement or related term to query by mining anchor text for a large hypertext document collection we show that the usage of anchor text a a basis for query refinement produce high quality refinement suggestion that are significantly better in term of perceived usefulness compared to refinement that are derived using the document content furthermore our study suggests that anchor text refinement can also be used to augment traditional query refinement algorithm based on query log since they typically differ in coverage and produce different refinement our result are based on experiment on an anchor text collection of a large corporate intranet 
this paper present an efficient framework for error bounded compression of high dimensional discrete attributed datasets such datasets which frequently arise in a wide variety of application pose some of the most significant challenge in data analysis subsampling and compression are two key technology for analyzing these datasets proximus provides a technique for reducing large datasets into a much smaller set of representative pattern on which traditional expensive analysis algorithm can be applied with minimal loss of accuracy we show desirable property of proximus in term of runtime scalability to large datasets and performance in term of capability to represent data in a compact form we also demonstrate application of proximus in association rule mining in doing so we establish proximus a a tool for preprocessing data before applying computationally expensive algorithm or a a tool for directly extracting correlated pattern our experimental result show that use of the compressed data for association rule mining provides excellent precision and recall value near across a range of support threshold while reducing the time required for association rule mining drastically 
we propose a general model for joint inference in correlated natural language processing task when fully annotated training data is not available and apply this model to the dual task of word sense disambiguation and verb subcategorization frame determination the model us the em algorithm to simultaneously complete partially annotated training set and learn a generative probabilistic model over multiple annotation when applied to the word sense and verb subcategorization frame determination task the model learns sharp joint probability distribution which correspond to linguistic intuition about the correlation of the variable use of the joint model lead to error reduction over competitive independent model on these task 
we introduce a computationally efficient method to estimate the validity of the bp method a a function of graph topology the connectivity strength frustration and network size we present numerical result that demonstrate the correctness of our estimate for the uniform random model and for a real world network c elegans although the method is restricted to pair wise interaction no local evidence zero bias and binary variable we believe that it prediction correctly capture the limitation of bp for inference and map estimation on arbitrary graphical model using this approach we find that bp always performs better than mf especially for large network with broad degree distribution such a scale free network bp turn out to significantly outperform mf 
the empirical investigation of the effectiveness of information retrieval ir system requires a test collection a set of query topic and a set of relevance judgment made by human assessor for each query previous experiment show that difference in human relevance assessment do not affect the relative performance of retrieval system based on this observation we propose and evaluate a new approach to replace the human relevance judgment by an automatic method ranking of retrieval system with our methodology correlate positively and significantly with that of human based evaluation in the experiment we assume a web like imperfect environment the indexing information for all document is available for ranking but some document may not be available for retrieval such condition can be due to document deletion or network problem our method of simulating imperfect environment can be used for web search engine assessment and in estimating the effect of network condition e g network unreliability on ir system performance 
we introduce an emergent collaborative filing system in such a system an individual is allowed to organize a subset of document in a repository into a personal hierarchy and share the hierarchy with others the system generates a consensus hierarchy from all user personal hierarchy which provides a full common and emergent view of all document we believe that collaborative filing help translate personal tacit knowledge into sharable structure which help the user a well a community of which he or she is a part our filing system is suitable for any document from text to multimedia file initial result on an experimental website show promise for a knowledge task involving extensive document retrieval hierarchy are not only used frequently but are also effective in identifying high quality document one surprising finding is how often subject use others personal hierarchy and upon close examination social network play a key role a well 
our knowitall system aim to automate the tedious process of extracting large collection of fact e g name of scientist or politician from the web in an autonomous domain independent and scalable manner in it first major run knowitall extracted over fact with high precision but suggested a challenge how can we improve knowitall s recall and extraction rate without sacrificing precision this paper present three distinct way to address this challenge and evaluates their performance rule learning learns domain specific extraction rule subclass extraction automatically identifies sub class in order to boost recall list extraction locates list of class instance learns a wrapper for each list and extract element of each list since each method bootstrap from knowitall s domain independent method no hand labeled training example are required experiment show the relative coverage of each method and demonstrate their synergy in concert our method gave knowitall a fold to fold increase in recall while maintaining high precision and discovered city missing from the tipster gazetteer 
we propose a convex optimization based strategy to deal with uncertainty in the observation of a classification problem we assume that instead of a sample xi yi a distribution over xi yi is specified in particular we derive a robust formulation when the distribution is given by a normal distribution it lead to second order cone programming formulation our method is applied to the problem of missing data where it outperforms direct imputation 
text classifier that give probability estimate are more readily applicable in a variety of scenario for example rather than choosing one set decision threshold they can be used in a bayesian risk model to issue a run time decision which minimizes a user specified cost function dynamically chosen at prediction time however the quality of the probability estimate is crucial we review a variety of standard approach to converting score and poor probability estimate from text classifier to high quality estimate and introduce new model motivated by the intuition that the empirical score distribution for the extremely irrelevant hard to discriminate and obviously relevant item are often significantly different finally we analyze the experimental performance of these model over the output of two text classifier the analysis demonstrates that one of these model is theoretically attractive introducing few new parameter while increasing flexibility computationally efficient and empirically preferable 
we use unsupervised probabilistic machine learning idea to try to explain the kind of learning observed in real neuron the goal being to connect abstract principle of self organisation to known biophysical process for example we would like to explain spike timingdependent plasticity see and figure a in term of information theory starting out we explore the optimisation of a network sensitivity measure related to maximising the mutual information between input spike timing and output spike timing our derivation are analogous to those in ica except that the sensitivity of output timing to input timing is maximised rather than the sensitivity of output firing rate to input ica and related approach have been successful in explaining the learning of many property of early visual receptive field in rate coding model and we are hoping for similar gain in understanding of spike coding in network and how this is supported in principled probabilistic way by cellular biophysical process for now in our initial simulation we show that our derived rule can learn synaptic weight which can unmix or demultiplex mixed spike train that is it can recover independent point process embedded in distributed correlated input spike train using an adaptive single layer feedforward spiking network 
this poster motivates ai research in the area of real time strategy rts game and describes the current status of a project whose goal are to implement an rts game programming environment and to build ai that eventually can outperform human expert in this challenging and popular domain 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this paper describes a novel application of support vector machinesand multiscale texture and color invariant to a problem inbiological oceanography the identification of specie of bivalvelarvae our data consists of polarized color image of scallop andother bivalve larva between and day old collected from theocean by a shipboard optical imaging system of our design larvaeof scallop clam and oyster are small micron with fewdistinguishing feature when observed under standard lightmicroscopy however the use of polarized light with a full waveretardation plate produce a vivid color bi refringence pattern the pattern display very subtle difference between specie oftennot discernable to human observer we show that a soft marginsupport vector machine with gaussian rbf kernel is a gooddiscriminator on a feature set extracted from gabor wavelettransforms and color distribution angle of each image byconstraining the gabor center frequency to be low the resultingsystem can attain classification accuracy in excess of forvertically oriented image and in excess of for randomlyoriented image 
abstract the basic tool of machine learning appear inthe inner loop of most reinforcement learning algorithm typically in the form of monte carlomethods or function approximation technique 
with the huge amount of information available electronically there is an increasing demand for automatic text summarization system the use of machine learning technique for this task allows one to adapt summary to the user need and to the corpus characteristic these desirable property have motivated an increasing amount of work in this field over the last few year most approach attempt to generate summary by extracting sentence segment and adopt the supervised learning paradigm which requires to label document at the text span level this is a costly process which put strong limitation on the applicability of these method we investigate here the use of semi supervised algorithm for summarization these technique make use of few labeled data together with a larger amount of unlabeled data we propose new semi supervised algorithm for training classification model for text summarization we analyze their performance on two data set the reuters news wire corpus and the computation and language cmp lg collection of tipster summac we perform comparison with a baseline non learning system and a reference trainable summarizer system 
current tool for interactive knowledge capture have little or no learning aptitude they are mostly oblivious to the process or strategy that the user may be following in entering new knowledge unaware of their progress during a session and ignorant of typical skill expected from a good student we present an approach to make acquisition interface more proactive by extending them with goal that represent what remains to be learned strategy to achieve these goal and acquire further knowledge and awareness of the current status of the body of knowledge learned the resulting interaction show that the system is aware of it progress towards acquiring the new knowledge and move forward by understanding what acquisition goal and strategy to pursue 
abstract using an wireless client a a location sensor is an increasingly popular way of enabling location based service triangulation on signal strength from multiple access point can be used to pinpoint location down to a few meter however this level of accuracy come at the price of a manual tedious spatially highdensity calibration of signal strength a a function of location this paper present a new location algorithm based on a relatively coarse calibration this help answer the question of how accurate location can 
existing patient record are a valuable resource for automated outcome analysis and knowledge discovery however key clinical data in these record is typically recorded in unstructured form a free text and image and most structured clinical information is poorly organized time consuming interpretation and analysis is required to convert these record into structured clinical data thus only a tiny fraction of this resource is utilized we present remind a bayesian framework for reliable extraction and meaningful inference from nonstructured data remind integrates and blend the structured and unstructured clinical data in patient record to automatically created high quality structured clinical data this structuring allows existing patient record to be mined for quality assurance regulatory compliance and to relate financial and clinical factor we demonstrate remind on two medical application a extract recurrence the key outcome for measuring treatment effectiveness for colon cancer patient ii extract key diagnosis and complication for acute myocardial infarction heart attack patient and demonstrate the impact of these clinical factor on financial outcome 
we extend the applicability of impact transformation which is a technique for adjusting the term weight assigned to document so a to boost the effectiveness of retrieval when short query are applied to large document collection in conjunction with technique called quantization and thresholding impact transformation allows improved query execution rate compared to traditional vector space similarity computation a the number of arithmetic operation can be reduced the transformation also facilitates a new dynamic query pruning heuristic we give result based upon the trec web data that show the combination of these various technique to yield highly competitive retrieval in term of both effectiveness and efficiency for both short and long query 
prototype based algorithm are commonly used to reduce the computational complexity of nearest neighbour nn classifier in this paper we discus theoretical and algorithmical aspect of such algorithm on the theory side we present margin based generalization bound that suggest that these kind of classifier can be more accurate then the nn rule furthermore we derived a training algorithm that selects a good set of prototype using large margin principle we also show that the year old learning vector quantization lvq algorithm emerges naturally from our framework 
natural image are the composite consequence of multiple factor related to scene structure illumination and imaging multilinear algebra the algebra of higher order tensor offer a potent mathematical framework for analyzing the multifactor structure of image ensemble and for addressing the difficult problem of disentangling the constituent factor or mode our multilinear modeling technique employ a tensor extension of the conventional matrix singular value decomposition svd known a the n mode svd a a concrete example we consider the multilinear analysis of ensemble of facial image that combine several mode including different facial geometry people expression head pose and lighting condition our resulting tensorfaces representation ha several advantage over conventional eigenfaces more generally multilinear analysis show promise a a unifying framework for a variety of computer vision problem 
a method for automatic plot analysis of narrative text that us component of both traditional symbolic analysis of natural language and statistical machine learning is presented for the story rewriting task in the story rewriting task an exemplar story is read to the pupil and the pupil rewrite the story in their own word this allows them to practice language skill such a spelling diction and grammar without being stymied by content creation often the pupil improperly recall the story our method of automatic plot analysis enables the tutoring system to automatically analyze the student s story for both general coherence and specific missing event 
multiple realization of continuous valued time series fr om a stochastic process often contain systematic variation in rate and amp litude to leverage the information contained in such noisy replicate set we need to align them in an appropriate way for example to allow the data to be properly combined by adaptive averaging we present the continuous profile model cpm a generative model in which each observe d time series is a non uniformly subsampled version of a single lat ent trace to which local rescaling and additive noise are applied after unsupervised training the learned trace represents a canonical high re solution fusion of all the replicates a well an alignment in time and scale of each observation to this trace can be found by inference in the model we apply cpm to successfully align speech signal from multiple speaker and set of liquid chromatography mass spectrometry proteomic data 
one of the most robust finding of experimental psycholinguistics is that the context in which a word is presented influence the effort involved in processing that word we present a computational model of contextual facilitation based on word co occurrence vector and empirically validate the model through simulation of three representative type of context manipulation single word priming multiple priming and contextual constraint the aim of our study is to find out whether special purpose mechanism are necessary in order to capture the pattern of the experimental result 
motivated by the particular problem involved in communicating with locked in paralysed patient we aim to develop a braincomputer interface that us auditory stimulus we describe a paradigm that allows a user to make a binary decision by focusing attention on one of two concurrent auditory stimulus sequence using support vector machine classification and recursive channel elimination on the independent component of averaged eventrelated potential we show that an untrained user s eeg data can be classified with an encouragingly high level of accuracy this suggests that it is possible for user to modulate eeg signal in a single trial by the conscious direction of attention well enough to be useful in bci 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
much attention ha been paid to the relative effectiveness of interactive query expansion versus automatic query expansion although interactive query expansion ha the potential to be an effective mean of improving a search in this paper we show that on average human searcher are le likely than system to make good expansion decision to enable good expansion decision searcher must have adequate instruction on how to use interactive query expansion functionality we show that simple instruction on using interactive query expansion do not necessarily help searcher make good expansion decision and discus difficulty found in making query expansion decision 
an optoelectronic implementation of a spiking neuron model based on the fitzhugh nagumo equation is presented a tunable semiconductor laser source and a spectral filter provide a nonlinear mapping from driver voltage to detected signal linear electronic feedback completes the implementation which allows either electronic or optical input signal experimental result for a single system and numeric result of model interaction confirm that important feature of spiking neural model can be implemented through this approach 
a method for solving binocular and multi view stereomatching problem is presented in this paper a weakconsistency constraint is proposed which express thevisibility constraint in the image space it can be provedthat the weak consistency constraint hold for scene thatcan be represented by a set of d point a well alsoproposed is a new reliability measure for dynamicprogramming technique which evaluates the reliability ofa given match a novel reliability based dynamicprogramming algorithm is derived accordingly which canselectively assign disparity value to pixel when thereliabilities of the corresponding match exceed a giventhreshold consistency constraint and the new reliability baseddynamic programming algorithm can be combinedin an iterative approach the experimental result showthat the iterative approach can produce dense and reliable total error rate of matching forbinocular stereo datasets it can also generate promisingdisparity map for trinocular and multi view stereodatasets 
the world wide web is emerging not only a an infrastructure for data but also for a broader variety of resource that are increasingly being made available a web service relevant current standard like uddi wsdl and soap are in their fledgling year and form the basis of making web service a workable and broadly adopted technology however realizing the fuller scope of the promise of web service and associated service oriented architecture will requite further technological advance in the area of service interoperation service discovery service composition and process orchestration semantics especially a supported by the use of ontology and related semantic web technology are likely to provide better qualitative and scalable solution to these requirement just a semantic annotation of data in the semantic web is the first critical step to better search integration and analytics over heterogeneous data semantic annotation of web service is an equally critical first step to achieving the above promise our approach is to work with existing web service technology and combine them with idea from the semantic web to create a better framework for web service discovery and composition in this paper we present mwsaf meteor s web service annotation framework a framework for semi automatically marking up web service description with ontology we have developed algorithm to match and annotate wsdl file with relevant ontology we use domain ontology to categorize web service into domain an empirical study of our approach is presented to help evaluate it performance 
we consider the problem of multi step ahead prediction in time series analysis using the non parametric gaussian process model step ahead forecasting of a discrete time non linear dynamic system can be performed by doing repeated one step ahead prediction for a state space model of the form the prediction of at time is based on the estimate of the previous output in this paper we show how using an analytical gaussian approximation we can formally incorporate the uncertainty about future regressor value thus updating the uncertainty on the current pre diction 
in this paper we propose an efficient algorithm for reducing a large mixture of gaussians into a smaller mixture while still preserving the component structure of the original model this is achieved by clustering grouping the component the method minimizes a new easily computed distance measure between two gaussian mixture that can be motivated from a suitable stochastic model and the iteration of the algorithm use only the model parameter avoiding the need for explicit resampling of datapoints we demonstrate the method by performing hierarchical clustering of scenery image and handwritten digit 
to support more efficient video database management this paper explores the concept of video association mining with which the association pattern are characterized by sequentially associated video shot and their cluster information given a continuous video sequence v the video shot segmentation mechanism is first introduced to parse it into discrete shot we then cluster shot into visually distinct group and construct a shot cluster sequence by using the class label of each shot an association mining scheme is designed to mine sequentially associated cluster from the sequence those detected association will convey valuable knowledge for video database management the experimental result demonstrate the effectiveness of our design 
the model used by the ccg parser of hockenmaier and steedman b would fail to capture the correct bilexical dependency in a language with freer word order such a dutch this paper argues that probabilistic parser should therefore model the dependency in the predicate argument structure a in the model of clark et al and defines a generative model for ccg derivation that capture these dependency including bounded and unbounded long range dependency 
the problem of measuring similarity of object arises in many application and many domain specific measure have been developed e g matching text across document or computing overlap among item set we propose a complementary approach applicable in any domain with object to object relationship that measure similarity of the structural context in which object occur based on their relationship with other object effectively we compute a measure that say two object are similar if they are related to similar object this general similarity measure called simrank is based on a simple and intuitive graph theoretic model for a given domain simrank can be combined with other domain specific similarity measure we suggest technique for efficient computation of simrank score and provide experimental result on two application domain showing the computational feasibility and effectiveness of our approach 
plan recognition ha traditionally been developed for logically encoded application domain with a focus on logical reasoning in this paper we present an integrated plan recognition model that combine low level sensory reading with high level goal inference a two level architecture is proposed to infer a user s goal in a complex indoor environment using an rf based wireless network the novelty of our work derives from our ability to infer a user s goal from sequence of signal trajectory and the ability for u to make a trade off between model accuracy and inference efficiency the model relies on a dynamic bayesian network to infer a user s action from raw signal and an n gram model to infer the user goal from action we present a method for constructing the model from the past data and demonstrate the effectiveness of our proposed solution through empirical study using some real data that we have collected 
under the lambertian reflectance model uncalibrated photometricstereo with unknown light source is inherentlyambiguous in this paper we consider the use of a moregeneral reflectance model namely the torrance and sparrowmodel in uncalibrated photometric stereo we demonstratethat this can not only resolve the ambiguity when thelight source are unknown but can also result in more accuratesurface reconstruction and can capture the reflectanceproperties of a large number of non lambertian surface our method us single light source image with unknownlighting and no knowledge about the parameter of the reflectancemodel it can recover the d shape of surface up to the binary convex concave ambiguity together withtheir reflectance property we have successfully tested ouralgorithm on a variety of non lambertian surface demonstratingthe effectiveness of our approach in the case ofhuman face the estimated skin reflectance ha been shownto closely resemble the measured skin reflectance reportedin the literature we also demonstrate improved recognitionresults on image of face with variable lightingand viewpoint when the synthetic image based representationsof the face are generated using the surface reconstructionsand reflectance property recovered while assumingthe extended reflectance model 
this paper present a framework for texture recognitionbased on local affine invariant descriptor and their spatiallayout at modeling time a generative model of localdescriptors is learned from sample image using the em algorithm the em framework allows the incorporation ofunsegmented multi texture image into the training set thesecond modeling step consists of gathering co occurrencestatistics of neighboring descriptor at recognition time initial probability computed from the generative modelare refined using a relaxation step that incorporates co occurrencestatistics performance is evaluated on imagesof an indoor scene and picture of wild animal 
we propose a novel approach on how to rectify the photo image of the bound document the surface of the document is modeled by a cylindrical surface by the geometry of camera image formation the equation using the cue of directrixes to map the point on the surface in the d scene to the point on the image plane are achieved baseline of the horizontal text line are extracted a projection of directrixes to estimate the bending extent of the surface and then the image are rectified the proposed method need no auxiliary device experimental result are presented to demonstrate the feasibility and the application of the method 
relational reinforcement learning rrl is a q learning technique which us first order regression technique to generalize the qfunction both the relational setting and the q learning context introduce a number of difficulty which must be dealt with in this paper we investigate a few dierent method that do incremental relational instance based regression and can be used for rrl this lead u to dierent approach which limit both memory consumption and processing time we implemented a number of these approach and experimentally evaluated and compared them to each other and an existing rrl algorithm these experiment show relational instance based regression to work well and to add robustness to rrl 
area of the brain involved in various form of memory exhibit pattern of neural activity quite unlike those in canonical computational model we show how to use well founded bayesian probabilistic autoassociative recall to derive biologically reasonable neuronal dynamic in recurrently coupled model together with appropriate value for parameter such a the membrane time constant and inhibition we explicitly treat two case one arises from a standard hebbian learning rule and involves activity pattern that are coded by graded firing rate the other arises from a spike timing dependent learning rule and involves pattern coded by the phase of spike time relative to a coherent local field potential oscillation our model offer a new and more complete understanding of how neural dynamic may support autoassociation 
most recent research of scalable inductive learning on very large dataset decision tree construction in particular focus on eliminating memory constraint and reducing the number of sequential data scan however state of the art decision tree construction algorithm still require multiple scan over the data set and use sophisticated control mechanism and data structure we first discus a general inductive learning framework that scan the dataset exactly once then we propose an extension based on hoeffding s inequality that scan the dataset le than once our framework are applicable to a wide range of inductive learner 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we address the problem of integrating object from a source taxonomy into a master taxonomy this problem is not only currently pervasive on the web but also important to the emerging semantic web a straightforward approach to automating this process would be to learn a classifier that can classify object from the source taxonomy into category of the master taxonomy the key insight is that the availability of the source taxonomy data could be helpful to build better classifier for the master taxonomy if their categorization have some semantic overlap in this paper we propose a new approach co bootstrapping to enhance the classification by exploiting such implicit knowledge our experiment with real world web data show substantial improvement in the performance of taxonomy integration 
human are able to detect blurring of visual image but the mechanism by which they do so is not clear a traditional view is that a blurred image look unnatural because of the reduction in energy either glob ally or locally at high frequency in this paper we propose that the disruption of local phase can provide an alternative explanation for blur perception we show that precisely localized feature such a step edge result in strong local phase coherence structure across scale and space in the complex wavelet transform domain and blurring cause loss of such phase coherence we propose a technique for coarse to fine phase pre diction of wavelet coefficient and observe that such prediction are highly effective in natural image phase coherence increase with the strength of image feature and blurring disrupts the phase coherence relationship in image we thus lay the groundwork for a new theory of perceptual blur estimation a well a a variety of algorithm for restora tion and manipulation of photographic image 
we propose a probabilistic algorithm able to detect the curve thatare unexpectedy smooth in a set of digital curve the onlyparameter is a false alarme rate influencing the detection only byits logarithm we experiment the good continuation criterion onimage level line one of the conclusion is that accordingly togestalt theory onecan detect egdes in a way that is widelyindependent of contrast we also use the same kind of method todetect corner and junction 
our demonstration show the hierarchy system working on a locally run search engine hierarchy are dynamically generated from the retrieved document and visualised on the menu when a user selects a term from the hierarchy the document linked to the term are listed and the term is then added to the initial query to rerun a search through the demonstration we illustrate how hierarchical presentation of expansion term is achieved and how our approach support user to articulate their information need using the hierarchy 
an image sequence based framework for appearance based object recognition is proposed in this paper compared with the method of using a single view for object recognition inter frame consistency can be exploited in a sequence based method so that a better recognition performance can be achieved we use the nearest feature line method nfl to model each object the nfl method is extended in this paper by further integrating motion continuity information between feature line in a probabilistic framework the associated recognition task is formulated a maximizing an a posteriori probability measure the recognition problem is then further transformed to a shortest path searching problem and a dynamic programming technique is used to solve it 
we present a new approach to modeling and processing multimedia data this approach is based on graphical model that combine audio and video variable we demonstrate it by developing a new algorithm for tracking a moving object in a cluttered noisy scene using two microphone and a camera our model us unobserved variable to describe the data in term of the process that generates them it is therefore able to capture and exploit the statistical structure of the audio and video data separately a well a their mutual dependency model parameter are learned from data via an em algorithm and automatic calibration is performed a part of this procedure tracking is done by bayesian inference of the object location from data we demonstrate successful performance on multimedia clip captured in real world scenario using off the shelf equipment 
policy gradient method have received increased attention recently a a mechanism for learning to act in partially observable environment they have shown promise for problem admitting memoryless policy but have been le successful when memory is required in this paper we develop several improved algorithm for learning policy with memory in an innite horizon setting directly when a known model of the environment is available and via simulation otherwise we compare these algorithm on some large pomdps including noisy robot navigation and multi agent problem 
occlusion are common place in man made and natural environment they often result in photometric feature where a line terminatesat an occluding boundary resembling a t we show that the dmotion of such t junction in multiple view carry non trivialinformation on the d structure of the scene and it motionrelative to the camera we show how the constraint among multipleviews of t junction can be used to reliably detect them anddifferentiate them from ordinary point feature finally wepropose an integrated algorithm to recursively and causallyestimate structure and motion in the presence of t junction alongwith other point feature 
policy gradient method based on reinforce are model free in the sense that they estimate the gradient using only online experience executing the current stochastic policy this is extremely wasteful of training data a well a being computationally inefficient this paper present a new modelbased policy gradient algorithm that us training experience much more efficiently our approach construct a series of incomplete model of the mdp and then applies these model to compute the policy gradient in closed form the paper describes an algorithm that alternate between pruning to remove irrelevant part of the incomplete mdp model exploration to gather training data in the relevant part of the state space and gradient ascent search we show experimental result on several benchmark problem including resource constrained scheduling the overall feasibility of this approach depends on whether a sufficiently informative partial model can fit into available memory 
in this paper we study a logistics problem arising in military transport planning a military organization operates a large fleet of vehicle in a depot to serve the request of various operational unit each request ha a fixed start and end time and is served by a prescribed number of vehicle we address the following two problem how many vehicle are at least needed to meet a given service level of request and suppose we allow each request to shift it start time by a constant duration call all the request be met a niche genetic algorithm together with a hybridized variant are applied to the problem 
the propose of this paper is to introduce a new regularization formulation for inverse problem in computer vision and image processing that allows one to reconstruct second order piece wise smooth image that is image consisting of an assembly of region with almost constant value almost constant slope or almost constant curvature this formulation is based on the idea of using potential function that correspond to spring or thin plate with an adaptive rest condition efficient algorithm for computing the solution and example illustrating the performance of this scheme compared with other known regularization scheme are presented a well 
this paper present japanese morphological analysis based on conditional random field crfs previous work in crfs assumed that observation sequence word boundary were fixed however word boundary are not clear in japanese and hence a straightforward application of crfs is not possible we show how crfs can be applied to situation where word boundary ambiguity exists crfs offer a solution to the long standing problem in corpus based or statistical japanese morphological analysis first flexible feature design for hierarchical tagsets become possible second influence of label and length bias are minimized we experiment crfs on the standard testbed corpus used for japanese morphological analysis and evaluate our result using the same experimental dataset a the hmms and memms previously reported in this task our result confirm that crfs not only solve the long standing problem but also improve the performance over hmms and memms 
this paper proposes a new method for word translation disambiguation using a machine learning technique called bilingual bootstrapping bilingual bootstrapping make use of in learning a small number of classified data and a large number of unclassified data in the source and the target language in translation it construct classifier in the two language in parallel and repeatedly boost the performance of the classifier by further classifying data in each of the two language and by exchanging between the two language information regarding the classified data experimental result indicate that word translation disambiguation based on bilingual bootstrapping consistently and significantly outperforms the existing method based on monolingual bootstrapping 
we describe a markov chain method for sampling from the distribution of the hidden state sequence in a non linear dynamical system given a sequence of observation this method update all state in the sequence simultaneously using an embedded hidden markov model hmm an update begin with the creation of pool of candidate state at each time we then define an embedded hmm whose state are index within these pool using a forward backward dynamic programming algorithm we can efficiently choose a state sequence with the appropriate probability from the exponentially large number of state sequence that pas through state in these pool we illustrate the method in a simple one dimensional example and in an example showing how an embedded hmm can be used to in effect discretize the state space without any discretization error we also compare the embedded hmm to a particle smoother on a more substantial problem of inferring human motion from d trace of marker 
here we derive optimal gain function for minimum mean square reconstruction from neural rate response subjected to poisson noise the shape of these function strongly depends on the length of the time window within which spike are counted in order to estimate the underlying firing rate a phase transition towards pure binary encoding occurs if the maximum mean spike count becomes smaller than approximately three provided the minimum firing rate is zero for a particular function class we were able to prove the existence of a second order phase transition analytically the critical decoding time window length obtained from the analytical derivation is in precise agreement with the numerical result we conclude that under most circumstance relevant to information processing in the brain rate coding can be better ascribed to a binary low entropy code than to the other extreme of rich analog coding 
we present an unsupervised algorithm for registering d surface scan of an object undergoing signicant deformation our algorithm doe not use marker nor doe it assume prior knowledge about object shape the dynamic of it deformation or scan alignment the algorithm register two mesh by optimizing a joint probabilistic model over all point topoint correspondence between them this model enforces preservation of local mesh geometry a well a more global constraint that capture the preservation of geodesic distance between corresponding point pair the algorithm applies even when one of the mesh is an incomplete range scan thus it can be used to automatically ll in the remaining surface for this partial scan even if those surface were previously only seen in a different conguration we evaluate the algorithm on several real world datasets where we demonstrate good result in the presence of signicant movement of articulated part and non rigid surface deformation finally we show that the output of the algorithm can be used for compelling computer graphic task such a interpolation between two scan of a non rigid object and automatic recovery of articulated object model 
this paper present an application of boosting for classifying labeled graph general structure for modeling a number of real world data such a chemical compound natural language text and bio sequence the proposal consists of i decision stump that use subgraph a feature and ii a boosting algorithm in which subgraph based decision stump are used a weak learner we also discus the relation between our algorithm and svms with convolution kernel two experiment using natural language data and chemical compound show that our method achieves comparable or even better performance than svms with convolution kernel a well a improves the testing efficiency 
we establish a mistake bound for an ensemble method for classification based on maximizing the entropy of voting weight subject to margin constraint the bound is the same a a general bound proved for the weighted majority algorithm and similar to bound for other variant of winnow we prove a more refined bound that lead to a nearly optimal algorithm for learning disjunction again based on the maximum entropy principle we describe a simplification of the on line maximum entropy method in which after each iteration the margin constraint are replaced with a single linear inequality the simplified algorithm which take a similar form to winnow achieves the same mistake bound 
this paper proposes a novel unified and systematic approach to combine collaborative and content based filtering for ranking and user preference prediction the framework incorporates all available information by coupling together multiple learning problem and using a suitable kernel or similarity function between user item pair we propose and evaluate an on line algorithm jrank that generalizes perceptron learning using this framework and show significant improvement over other approach 
web page classification is one of the essential technique for web mining specifically classifying web page of a user interesting class is the first step of mining interesting information from the web however constructing a classifier for an interesting class requires laborious pre processing such a collecting positive and negative training example for instance in order to construct a homepage classifier one need to collect a sample of homepage positive example and a sample of non homepage negative example in particular collecting negative training example requires arduous work and special caution to avoid biasing them we introduce in this paper the positive example based learning pebl framework for web page classification which eliminates the need for manually collecting negative training example in pre processing we present an algorithm called mapping convergence m c that achieves classification accuracy with positive and unlabeled data a high a that of traditional svm with positive and negative data our experiment show that when the m c algorithm us the same amount of positive example a that of traditional svm the m c algorithm performs a well a traditional svm 
locally weighted projection regression is a new algorithm that achieves nonlinear function approximation in high dimensional space with redundant and irrelevant input dimension at it core it us locally linear model spanned by a small number of univariate regression in selected direction in input space this paper evaluates different method of projection regression and derives a nonlinear function approximator based on them this nonparametric local learning system i learns rapidly with second order learning method based on incremental training ii us statistically sound stochastic cross validation to learn iii adjusts it weighting kernel based on local information only iv ha a computational complexity that is linear in the number of input and v can deal with a large number of possibly redundant input a shown in evaluation with up to dimensional data set to our knowledge this is the first truly incremental spatially localized learning method to combine all these property 
we consider data which are image containing view of multiple object our task is to learn about each of the object present in the image this task can be approached a a factorial learning problem where each image must be explained by instantiating a model for each of the object present with the correct instantiation parameter a major problem with learning a factorial model is that a the number of object increase there is a combinatorial explosion of the number of configuration that need to be considered we develop a method to extract object model sequentially from the data by making use of a robust statistical method thus avoiding the combinatorial explosion and present result showing successful extraction of object from real image 
the paper report on progress in building computational model of a constructivist approach to language development it introduces a formalism for construction grammar and learning strategy based on invention abduction and induction example are drawn from experiment exercising the model in situated language game played by embodied artificial agent 
this paper present a method for evaluating multiple feature spaceswhile tracking and for adjusting the set of feature used toimprove tracking performance our hypothesisis that the featuresthat best discriminate between object and background are also bestfor tracking the object we develop an on line feature selectionmechanism based on the two class variance ratio measure applied tolog likelihood distribution computed with respect to a givenfeature from sample of object and background pixel this featureselection mechanism is embedded in a tracking system thatadaptively selects the top ranked discriminative feature fortracking example are presented to illustrate how the methodadapts to changing appearance of both tracked object and scenebackground 
query by melody is the problem of retrieving musical performance from melody retrieval of real performance is complicated due to the large number of variation in performing a melody and the presence of colored accompaniment noise we describe a simple yet effective probabilistic model for this task we describe a generative model that is rich enough to capture the spectral and temporal variation of musical performance and allows for tractable melody retrieval while most of previous study on music retrieval from melody were performed with either symbolic e g midi data or with monophonic single instrument performance we performed experiment in retrieving live and studio recording of opera that contain a leading vocalist and rich instrumental accompaniment our result show that the probabilistic approach we propose is effective and can be scaled to massive datasets 
in labelling or prediction task a trained model s test performance is often based on the quality of it single time marginal distribution over label rather than it joint distribution over label sequence we propose using a new cost function for discriminative learning that more accurately reflects such test time condition we present an efficient method to compute the gradient of this cost for maximum entropy markov model conditional random field and for an extension of these model 
in this paper we describe a cross document summarizer xdox designed specifically to summarize large document set document and more such set of document are typically obtained from routing or filtering system run against a continuous stream of data such a a newswire xdox work by identifying the most salient theme within the set at the granularity level that is regulated by the user and composing an extraction summary which reflects these main theme in the current version xdox is not optimized to produce a summary based on a few unrelated document indeed such summary are best obtained simply by concatenating summary of individual document we show example of summary obtained in our test a well a from our participation in the first document understanding conference duc 
we present a coherent framework for data clustering starting with a hopfield network we show the solution for several well motivated clustering objective function are principal component for minmaxcut objective motivated for ensuring cluster balance the solution are the nonlinearly scaled principal component using scaled pc a we generalize to multi way clustering constructing a self aggregation network where connection weight between different cluster are automatically suppressed while connection weight within same cluster are automatically enhanced 
several important time series data mining problem reduce to the core task of finding approximately repeated subsequence in a longer time series in an earlier work we formalized the idea of approximately repeated subsequence by introducing the notion of time series motif two limitation of this work were the poor scalability of the motif discovery algorithm and the inability to discover motif in the presence of noise here we address these limitation by introducing a novel algorithm inspired by recent advance in the problem of pattern discovery in biosequences our algorithm is probabilistic in nature but a we show empirically and theoretically it can find time series motif with very high probability even in the presence of noise or don t care symbol not only is the algorithm fast but it is an anytime algorithm producing likely candidate motif almost immediately and gradually improving the quality of result over time 
existing data mining algorithm on graph look for node satisfying specific property such a specific notion of structural similarity or specific measure of link based importance while such analysis for predetermined property can be effective in well understood domain sometimes identifying an appropriate property for analysis can be a challenge and focusing on a single property may neglect other important aspect of the data in this paper we develop a foundation for mining the property themselves we present a theoretical framework defining the space of graph property a variety of mining query enabled by the framework technique to handle the enormous size of the query space and an experimental system called f miner that demonstrates the utility and feasibility of property mining 
model theoretic semantics is a formal account of the interpretation of legitimate expression of a language it is increasingly being used to provide web markup language with well defined semantics but a discussion of it role and limitation for the semantic web ha not yet received a coherent and detailed treatment this paper take the first step towards such a treatment the major result is an introductory explication of key idea that are usually only implicit in existing account of semantics for the web reference to more detailed account of these idea are also provided the benefit of this explication is increased awareness among web user of some important issue inherent in using model theoretic semantics for web markup language 
the temporal coding hypothesis of miller and colleague suggests that animal integrate related temporal pattern of stimulus into single memory representation we formalize this concept using quasi bayes estimation to update the parameter of a constrained hidden markov model this approach allows u to account for some surprising temporal eects in the second order conditioning experiment of miller et al which other model are unable to explain 
a recommender system suggests the item expected to be preferred by the user recommender system use collaborative filtering to recommend item by summarizing the preference of people who have tendency similar to the user preference traditionally the degree of preference is represented by a scale for example one that range from one to five this type of measuring technique is called the semantic differential sd method web adopted the ranking method however rather than the sd method since the sd method is intrinsically not suited for representing individual preference in the ranking method the preference are represented by order which are sorted item sequence according to the user preference we here propose some method to recommed item based on these order response and carry out the comparison experiment of these method 
although text categorization is a burgeoning area of ir research readily available test collection in this field are surprisingly scarce we describe a methodology and system named accio for automatically acquiring labeled datasets for text categorization from the world wide web by capitalizing on the body of knowledge encoded in the structure of existing hierarchical directory such a the open directory we define parameter of category that make it possible to acquire numerous datasets with desired property which in turn allow better control over categorization experiment in particular we develop metric that estimate the difficulty of a dataset by examining the host directory structure these metric are shown to be good predictor of categorization accuracy that can be achieved on a dataset and serve a efficient heuristic for generating datasets subject to user s requirement a large collection of automatically generated datasets are made available for other researcher to use 
conditional exponential model ha been one of the widely used conditional model in machine learning field and improved iterative scaling ii ha been one of the major algorithm for finding the optimal parameter for the conditional exponential model in this paper we proposed a faster iterative algorithm named fis that is able to find the optimal parameter faster than the ii algorithm the theoretical analysis show that the proposed algorithm yield a tighter bound than the traditional ii algorithm empirical study on the text classification over three different datasets showed that the new iterative scaling algorithm converges substantially faster than both the ii algorithm and the conjugate gradient algorithm cg furthermore we examine the quality of the optimal parameter found by each learning algorithm in the case of incomplete training experiment have shown that when only a limited amount of computation is allowed e g no convergence is achieved the new algorithm fis is able to obtain lower testing error than both the ii method and the cg method 
this paper focus on the optimization of the navigation through voluminous subsumption hierarchy of topic employed by portal catalog like netscape open directory odp we advocate for the use of labeling scheme for modeling these hierarchy in order to efficiently answer query such a subsumption check descendant ancestor or nearest common ancestor which usually require costly transitive closure computation we first give a qualitative comparison of three main family of scheme namely bit vector prefix and interval based scheme we then show that two labeling scheme are good candidate for an efficient implementation of label querying using standard relational dbms namely the dewey prefix scheme and an interval scheme by agrawal borgida and jagadish we compare their storage and query evaluation performance for the odp hierarchy using the postgresql engine 
in this work we study an information filtering model where the relevance label associated to a sequence of feature vector are realization of an unknown probabilistic linear function building on the analysis of a restricted version of our model we derive a general filtering rule based on the margin of a ridge regression estimator while our rule may observe the label of a vector only by classfying the vector a relevant experiment on a real world document filtering problem show that the performance of our rule is close to that of the on line classifier which is allowed to observe all label these empirical result are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that it expected number of mistake is never much larger than that of the optimal filtering rule which know the hidden linear model 
the problem of interactive foreground background segmentation in still image is of great practical importance in image editing the state of the art in interactive segmentation is probably represented by the graph cut algorithm of boykov and jolly iccv it underlying model us both colour and contrast information together with a strong prior for region coherence estimation is performed by solving a graph cut problem for which very efcient algorithm have recently been developed however the model depends on parameter which must be set by hand and the aim of this work is for those constant to be learned from image data first a generative probabilistic formulation of the model is set out in term of a gaussian mixture markov random field gmmrf secondly a pseudolikelihood algorithm is derived which jointly learns the colour mixture and coherence parameter for foreground and background respectively error rate for gmmrf segmentation are calculated throughout using a new image database available on the web with ground truth provided by a human segmenter the graph cut algorithm using the learned parameter generates good object segmentation with little interaction however pseudolikelihood learning prof to be frail which limit the complexity of usable model and hence also the achievable error rate 
mining maximal frequent itemsets is one of the most fundamental problem in data mining in this paper we study the complexity theoretic aspect of maximal frequent itemset mining from the perspective of counting the number of solution we present the first formal proof that the problem of counting the number of distinct maximal frequent itemsets in a database of transaction given an arbitrary support threshold is p complete thereby providing strong theoretical evidence that the problem of mining maximal frequent itemsets is np hard this result is of particular interest since the associated decision problem of checking the existence of a maximal frequent itemset is in p we also extend our complexity analysis to other similar data mining problem dealing with complex data structure such a sequence tree and graph which have attracted intensive research interest in recent year normally in these problem a partial order among frequent pattern can be defined in such a way a to preserve the downward closure property with maximal frequent pattern being those without any successor with respect to this partial order we investigate several variant of these mining problem in which the pattern of interest are subsequence subtrees or subgraphs and show that the associated problem of counting the number of maximal frequent pattern are all either p complete or p hard 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
abstract introduction text summarizationtext summarization is the process of taking a textdocument and creating a compressed version thatconsists of the most useful information for the user one distinguishes between single document summarizers sd and multi document summarizers md multi document summarization is muchmore complicated than single document summarization factor that make multi document summarizationmore difficult include multiple article can be written by different 
internet search engine and comparison shopping have recently begun implementing a paid placement strategy where some content provider are given prominent positioning in return for a placement fee this bias generates placement revenue but creates a disutility to user thus reducing user based revenue we formulate the search engine design problem a a tradeoff between these two type of revenue we demonstrate that the optimal placement strategy depends on the relative benefit to provider and disutilities to user of paid placement we compute the optimal placement fee characterize the optimal bias level and analyze sensitivity of the placement strategy to various factor in the optimal paid placement strategy the placement revenue are set below the monopoly level due to it negative impact on advertising revenue an increase in the search engine s quality of service allows it to improve profit from paid placement moving it closer to the ideal however an increase in the value per user motivates the gatekeeper to increase market share by reducing further it reliance on paid placement and fraction of paying provider 
given an n x n grid of square where each square ha a count cij and an underlying population pij our goal is to find the rectangular region with the highest density and to calculate it significance by randomization an arbitrary density function d dependent on a region s total count c and total population p can be used for example if each count represents the number of disease case occurring in that square we can use kulldorff s spatial scan statistic dk to find the most significant spatial disease cluster a naive approach to finding the maximum density region requires o n time and is generally computationally infeasible we present a multiresolution algorithm which partition the grid into overlapping region using a novel overlap kd tree data structure bound the maximum score of subregions contained in each region and prune region which cannot contain the maximum density region for sufficiently dense region this method find the maximum density region in o n log n time in practice resulting in significant x speedup on both real and simulated datasets 
we present the bump mixture model a statistical model for analog data where the probabilistic semantics inference and learning rule derive from low level transistor behavior the bump mixture model relies on translinear circuit to perform probabilistic inference and floating gate device to perform adaptation this system is low power asynchronous and fully parallel and support various on chip learning algorithm in addition the mixture model can perform several task such a probability estimation vector quantization classification and clustering we tested a fabricated system on clustering quantization and classification of handwritten digit and show performance comparable to the e m algorithm on mixture of gaussians 
in this paper we present a mathematical theory for marr s primal sketch the theory ha four component the first component is a primal sketch model for natural image which integrates the descriptive markov random field model and the generative wavelet model the former is applied to textural location without distinguishable element called non sketchable and the latter is applied to geometric location called sketchable the second component is a sketching pursuit process which coordinate the competition between two pursuit algorithm the matching pursuit and the filter pursuit or the competition of the two family of model that seek to explain the image by base and filter respectively the third component is a theoretical definition of sketchability which state a critical condition and thus a dividing point for our perceptual jump between texture and geometry the fourth component is to learn a generic dictionary of image primitive or textons in julesz s term for natural image our dictionary is found to be far more effective than conventional gabor and log base our model is not only extremely parsimonious for image representation but produce meaningful sketch a marr hoped for over a large number of generic image 
this paper present a chinese word segmentation system that us improved source channel model of chinese sentence generation chinese word are defined a one of the following four type lexicon word morphologically derived word factoid and named entity our system provides a unified approach to the four fundamental feature of word level chinese language processing word segmentation morphological analysis factoid detection and named entity recognition the performance of the system is evaluated on a manually annotated test set and is also compared with several state of the art system taking into account the fact that the definition of chinese word often varies from system to system 
metadata for the world wide web is important but metadata for peer to peer p p network is absolutely crucial in this paper we discus the open source project edutella which build upon metadata standard defined for the www and aim to provide an rdf based metadata infrastructure for p p application building on the recently announced jxta framework we describe the goal and main service this infrastructure will provide and the architecture to connect edutella peer based on exchange of rdf metadata a the query service is one of the core service of edutella upon which other service are built we specify in detail the edutella common data model ecdm a basis for the edutella query exchange language rdf qel i and format implementing distributed query over the edutella network finally we shortly discus registration and mediation service and introduce the prototype and application scenario for our current edutella aware peer 
recent text and speech processing application such a speech mining raise new and more general problem related to the construction of language model we present and describe in detail several new and efficient algorithm to address these more general problem and report experimental result demonstrating their usefulness we give an algorithm for computing efficiently the expected count of any sequence in a word lattice output by a speech recognizer or any arbitrary weighted automaton describe a new technique for creating exact representation of n gram language model by weighted automaton whose size is practical for offline use even for a vocabulary size of about word and an n gram order n and present a simple and more general technique for constructing class based language model that allows each class to represent an arbitrary weighted automaton an efficient implementation of our algorithm and technique ha been incorporated in a general software library for language modeling the grm library that includes many other text and grammar processing functionality 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
first order markov model have been successfully applied to many problem for example in modeling sequential data using markov chain and modeling control problem using the markov decision process mdp formalism if a first order markov model s parameter are e timated from data the standard maximum likelihood estimator considers only the first order single step transition but for many pro blems the firstorder conditional independence assumption are not satisfi ed and a a result the higher order transition probability may be poorl y approximated motivated by the problem of learning an mdp s parameter for control we propose an algorithm for learning a first order markov mod el that explicitly take into account higher order interaction duri ng training our algorithm us an optimization criterion different from maximum likelihood and allows u to learn model that capture longer range effect but without giving up the benefit of using first order markov mod el our experimental result also show the new algorithm outperforming conventional maximum likelihood estimation in a number of control problem where the mdp s parameter are estimated from data 
we are interested in enabling a generic sketch recognition system that would allow more natural interaction with design tool in various domain such a mechanical engineering military planning logic design etc we would like to teach the system the symbol for a particular domain by simply drawing an example of each one a easy a it is to teach a person study in cognitive science suggest that when shown a symbol people attend preferentially to certain geometric feature relying on such bias we built a system capable of learning description of hand drawn symbol from a single example the generalization power is derived from a qualitative vocabulary reflecting human perceptual category and a focus on perceptually relevant global property of the symbol our user study show that the system agrees with the subject majority classification about a often a any individual subject did 
we describe a computer system that play a responsive and sensitive accompaniment to a live musician in a piece of non improvised music the system of composed of three component listen anticipate and synthesize listen analyzes the soloist s acoustic signal and estimate note onset time using a hidden markov model synthesize play a prerecorded audio file back at variable rate using a phase vocoder anticipate creates a bayesian network that mediates between listen and synthesize the system ha a learning phase analogous to a series of rehearsal in which model parameter for the network are estimated from training data in performance the system synthesizes the musical score the training data and the on line analysis of the soloist s acoustic signal using a principled decision making engine based on the bayesian network a live demonstration will be given using the aria mi chiamano mimi from puccini s opera la boheme 
since many arabic document are available only in print automating retrieval from collection of scanned arabic document image using optical character recognition ocr is an interesting problem arabic combine rich morphology with a writing system that present unique challenge to ocr system these factor must be considered when selecting term for automatic indexing in this paper alternative choice of indexing term are explored using both an existing electronic text collection and a newly developed collection built from image of actual printed arabic document character n gram or lightly stemmed word were found to typically yield near optimal retrieval effectiveness and combining both type of term resulted in robust performance across a broad range of condition 
we review here the result of one of the experiment performed at the reliable information access ria workshop hosted by mitre corporation and the northeast regional research center nrrc the experiment concentrate on query expansion using relevance feedback and explores the behaviour of several information retrieval system using variable number of relevant document 
planning algorithm designed for deterministic world such a a search usually run much faster than algorithm designed for world with uncertain action outcome such a value iteration real world planning problem often exhibit uncertainty which force u to use the slower algorithm to solve them many real world planning problem exhibit sparse uncertainty there are long sequence of deterministic action which accomplish task like moving sensor platform into place interspersed with a small number of sensing action which have uncertain outcome in this paper we describe a new planning algorithm called mcp short for mdp compression planning which combine a search with value iteration for solving stochastic shortest path problem in mdps with sparse stochasticity we present experiment which show that mcp can run substantially faster than competing planner in domain with sparse uncertainty these experiment are based on a simulation of a ground robot cooperating with a helicopter to fill in a partial map and move to a goal location in deterministic planning problem optimal path are acyclic no state is visited more than once because of this property algorithm like a search can guarantee that they visit each state in the state space no more than once by visiting the state in an appropriate order it is possible to ensure that we know the exact value of all of a state s possible successor before we visit that state so the first time we visit a state we can compute it correct value by contrast if action have uncertain outcome optimal path may contain cycle some state will be visited two or more time with positive probability because of these cycle there is no way to order state so that we determine the value of a state s successor before we visit the state itself instead the only way to compute state value is to solve a set of simultaneous equation in problem with sparse stochasticity only a small fraction of all state have uncertain outcome it is these few state that cause all of the cycle while a deterministic state s may participate in a cycle the only way it can do so is if one of it successor ha an action with a stochastic outcome and only if this stochastic action can lead to a predecessor of s in such problem we would like to build a smaller mdp which contains only state which are related to stochastic action we will call such an mdp a compressed mdp and we will call it state distinguished state we could then run fast algorithm like a search to plan path between distinguished state and reserve slower algorithm like value iteration for deciding how to deal with stochastic outcome 
grid technology provide a robust infrastructure for distributed computing and are widely used in large scale scientific application that generate terabyte soon petabyte of data this data is described with metadata attribute about the data property and provenance and is organized in a variety of metadata catalog distributed over the grid in order to find a collection of data that share certain property these metadata catalog need to be identified and queried on an individual basis this paper introduces artemis a system developed to integrate distributed metadata catalog on the grid artemis exploit several ai technique including a query mediator a query planning and execution system ontology and semantic web tool to model metadata attribute and an intelligent user interface that guide user through these ontology to formulate query we describe our experience using artemis with large metadata catalog from two project in the physic domain 
we show a close relationship between the expectation maximization em algorithm and direct optimization algorithm such a gradientbased method for parameter learning we identify analytic condition under which em exhibit newton like behavior and condition under which it posse poor first order convergence based on this analysis we propose two novel algorithm for maximum likelihood estimation of latent variable model and report empirical result showing that a predicted by theory the proposed new algorithm can substantially outperform standard em in term of speed of convergence in certain case 
previous discretization technique have discretized numeric attribute into disjoint interval we argue that this is neither necessary nor appropriate for naive bayes classifier the analysis lead to a new discretization method non disjoint discretization ndd ndd form overlapping interval for a numeric attribute always locating a value toward the middle of an interval to obtain more reliable probability estimation it also adjusts the number and size of discretized interval to the number of training instance seeking an appropriate trade o between bias and variance of probability estimation we justify ndd in theory and test it on a wide cross section of datasets our experimental result suggest that for naivebayes classifier ndd work better than alternative discretization approach 
approximation structure play an important role in inference on loopy graph a a tractable structure tree approximation have been utilized in the variational method of ghahramani jordan and the sequential projection method of frey et al however belief propagation represents each factor of the graph with a product of single node message in this paper belief propagation is extended to r epresent factor with tree approximation by way of the expectation propagation framework that is each factor sends a message to all pair of node in a tree structure the result is more accurate inference a nd more frequent convergence than ordinary belief propagation at a lower cost than variational tree or double loop algorithm 
the problem of learning with positive and unlabeled example arises frequently in retrieval application we transform the problem into a problem of learning with noise by labeling all unlabeled example a negative and use a linear function to learn from the noisy example to learn a linear function with noise we perform logistic regression after weighting the example to handle noise rate of greater than a half with appropriate regularization the cost function of the logistic regression problem is convex allowing the problem to be solved efficiently we also propose a performance measure that can be estimated from positive and unlabeled example for evaluating retrieval performance the measure which is proportional to the product of precision and recall can be used with a validation set to select regularization parameter for logistic regression experiment on a text classification corpus show that the method proposed are effective 
this paper aim at providing a view of text recycled within a short time by the author themselves we first present a simple and general method for extracting reused term sequence and then analyze several author identified text collection to compare the statistical quantity the ratio of recycling is also measured for each collection finally related research topic are introduced together with some discussion of future research direction 
in this contribution we present an approach for d d pose estimation of d free form surface model in our scenario we observe a free form object in an image of a calibrated camera pose estimation mean to estimate the relative position and orientation of the d object to the reference camera system the object itself is modeled a a two parametric d surface and extended by one parametric contour part of the object a twist representation which is equivalent to a fourier representation allows for a low pas approximation of the object model which is advantageously applied to regularize the pose problem the experiment show that our developed algorithm are fast m frame and accurate o rotational error frame 
in this paper we use the cumulative distribution of a randomvariable to define the information content in it and use it todevelop a novel measure of information that parallel shannonentropy which we dub cumulative residual entropy cre the keyfeatures of cre may be summarized a it definition is valid inboth the continuous and discrete domain it is mathematicallymore general than the shannon entropy and it computation fromsample data is easy and these computation converge asymptoticallyto the true value we define the cross cre ccre between tworandom variable and apply it to solve the uni multi modalimage alignment problem for parameterized rigid affine andprojective transformation the key strength of the ccre overusing the now popular mutual information method based on shannon sentropy are that the former ha significantly larger noiseimmunity and a much larger convergence range over the field ofparameterized transformation these strength of ccre aredemonstrated via experiment on synthesized and real image data 
bilingual dictionary have been commonly used for query translation in cross language information retrieval clir however we are faced with the problem of translation selection several recent study suggested the utilization of term co occurrence in this selection this paper present two extension to improve them first we extend the basic co occurrence model by adding a decaying factor that decrease the mutual information when the distance between the term increase second we incorporate a triple translation model in which syntactic dependence relation represented a triple are integrated our evaluation on translation accuracy show that translating triple a unit is more precise than a word by word translation our clir experiment show that the addition of the decaying factor lead to substantial improvement of the basic co occurrence model and the triple translation model brings further improvement 
we explore approximate policy iteration api replacing the usual costfunction learning step with a learning step in policy space we give policy language bias that enable solution of very large relational markov decision process mdps that no previous technique can solve in particular we induce high quality domain specific plan ners for classical planning domain both deterministic and stochastic variant by solving such domain a extremely large mdps 
wide area database replication technology and the availability of content delivery network allow web application to be hosted and served from powerful data center this form of application support requires a complete web application suite to be distributed along with the database replica a major advantage of this approach is that dynamic content is served from location closer to user leading into reduced network latency and fast response time however this is achieved at the expense of overhead due to a invalidation of cached dynamic content in the edge cache and b synchronization of database replica in the data center these have adverse effect on the freshness of delivered content in this paper we propose a freshness driven adaptive dynamic content caching which monitor the system status and adjusts caching policy to provide content freshness guarantee the proposed technique ha been intensively evaluated to validate it effectiveness the experimental result show that the freshness driven adaptive dynamic content caching technique consistently provides good content freshness furthermore even a web site that enables dynamic content caching can further benefit from our solution which improves content freshness up to time especially under heavy user request traffic and long network latency condition our approach also provides better scalability and significantly reduced response time up to in the experiment 
in many application that track and analyze spatiotemporal data movement obey periodic pattern the object follow the same route approximately over regular time interval for example people wake up at the same time and follow more or le the same route to their work everyday the discovery of hidden periodic pattern in spatiotemporal data apart from unveiling important information to the data analyst can facilitate data management substantially based on this observation we propose a framework that analyzes manages and query object movement that follow such pattern we define the spatiotemporal periodic pattern mining problem and propose an effective and fast mining algorithm for retrieving maximal periodic pattern we also devise a novel specialized index structure that can benefit from the discovered pattern to support more efficient execution of spatiotemporal query we evaluate our method experimentally using datasets with object trajectory that exhibit periodicity 
introductionthe design of the askmsr question answering system ismotivated by recent observation in natural languageprocessing that for many application significantimprovements in accuracy can be attained simply byincreasing the amount of data used for learning e g banko amp brill by taking advantage of the vastamount of online text available via the worldwide web rather than relying on an approach that depends heavily onnatural language intensive technique we developed 
for probabilistic reasoning one often need to sample from a combinatorial space for example one may need to sample uniformly from the space of all satisfying assignment can state of the art search procedure for sat be used to sample effectively from the space of all solution and if so how uniformly do they sample our initial result find that on the positive side one can find all solution to a given instance nevertheless sampling can be highly biased this research provides a starting point for the development of more balanced procedure 
in vision and graphic there is a sustained interest incapturing accurate d shape with various scanning device however the resulting geometric representation isonly part of the story surface texture of real object isalso an important component of the representation and fine scalesurface geometry such a surface marking roughness and imprint are essential in highly realistic renderingand accurate prediction we present a novel approachfor measuring the fine scale surface shape of specular surfacesusing a curved mirror to view multiple angle in asingle image a distinguishing aspect of our method is thatit is designed for specular surface unlike many method e g laser scanning which cannot handle highly specularobjects also the spatial resolution is very high so that itcan resolve very small surface detail that are beyond theresolution of standard device furthermore our approachincorporates the simultaneous use of a bidirectional texturemeasurement method so that spatially varying bidirectionalreflectance is measured at the same time a surfaceshape 
photometric invariance is used in many computer vision application the advantage of photometric invariance is therobustness against shadow shading and illumination condition however the drawback of photometric invarianceis the loss of discriminative power and the inherent instabilitiescaused by the non linear transformation to computethe invariant in this paper we propose a new class of derivativeswhich we refer to a photometric quasi invariant thesequasi invariant share with full invariant the nice propertythat they are robust against photometric edge such asshadows or specular edge further these quasi invariantsdo not have the inherent instability of full photometric invariant we will apply these quasi invariant derivativesin the context of photometric invariant edge detection andclassification experiment show that the quasi invariantderivatives are stable and they significantly outperform thefull invariant derivative in discriminative power 
qa by dossier with constraint is a new approach to question answering whereby candidate answer confidence are adjusted by asking auxiliary question whose answer constrain the original answer these constraint emerge naturally from the domain of interest and enable application of real world knowledge to qa we show that our approach significantly improves system performance relative improvement in f measure on select question type and can create a dossier of information about the subject matter in the original question 
reluctance of data owner to share their possibly confidential or proprietary data with others who own related database is a serious impediment to conducting a mutually beneficial data mining analysis we address the case of vertically partitioned data multiple data owner agency each posse a few attribute of every data record we focus on the case of the agency wanting to conduct a linear regression analysis with complete record without disclosing value of their own attribute this paper describes an algorithm that enables such agency to compute the exact regression coefficient of the global regression equation and also perform some basic goodness of fit diagnostics while protecting the confidentiality of their data in more general setting beyond the privacy scenario this algorithm can also be viewed a method for the distributed computation for regression analysis 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we present a novel approach to embedding data represented by a network into a lowdimensional euclidean space unlike existing method the proposed method attempt to minimize an energy function based on the cross entropy between desirable and embedded node congurations without directly utilizing pairwise distance between node we also propose a natural criterion to effectively evaluate an embedded network layout in term of how well node connectivity are preserved experimental result show that the proposed method provides better layout than those produced by some of the well known embedding method in term of the proposed criterion we believe that our method produce a natural embedding of a large scale network suitable for analyzing by manual browsing in a twoor threedimensional euclidean space 
we are going to present an implementation of an ai system cabma built on top of a commercial project management tool m project project planning is a business process for successfully delivering one of a kind product and service under real world time and resource constraint cabma for case based project management assistant provides the following functionality it capture case from project plan it reuses captured case to refine project plan and generate project plan from the scratch it maintains consistency of piece of a project plan obtained by case reuse it refines the case base to cope with inconsistency resulting from capturing case over a period of time cabma add a knowledge layer on top of m project to assist the user with his project management task 
we present a probabilistic model for a document corpus that combine many of the desirable feature of previous model the model is called gap for gamma poisson the distribution of the first and last random variable gap is a factor model that is it give an approximate factorization of the document term matrix into a product of matrix and x these factor have strictly non negative term gap is a generative probabilistic model that assigns finite probability to document in a corpus it can be computed with an efficient and simple em recurrence for a suitable choice of parameter the gap factorization maximizes independence between the factor so it can be used a an independent component algorithm adapted to document data the form of the gap model is empirically a well a analytically motivated it give very accurate result a a probabilistic model measured via perplexity and a a retrieval model the gap model project document and term into a low dimensional space of theme and model text a passage of term on the same theme 
multi level spatial aggregate are important for data mining in a variety of scientific and engineering application from analysis of weather data aggregating temperature and pressure data into ridge and front to performance analysis of wireless system aggregating simulation result into configuration space region exhibiting particular performance characteristic in many of these application data collection is expensive and time consuming so effort must be focused on gathering sample at location that will be most important for the analysis this requires that we be able to functionally model a data mining algorithm in order to ass the impact of potential sample on the mining of suitable spatial aggregate this paper describes a novel gaussian process approach to modeling multi layer spatial aggregation algorithm and demonstrates the ability of the resulting model to capture the essential underlying qualitative behavior of the algorithm by helping cast classical spatial aggregation algorithm in a rigorous quantitative framework the gaussian process model support diverse us such a directed sampling characterizing the sensitivity of a mining algorithm to particular parameter and understanding how variation in input data field percolate up through a spatial aggregation hierarchy 
we describe an empirical evaluation of the utility of thumbnail preview in web search result result page were constructed to show text only summary thumbnail preview only or the combination of text summary and thumbnail preview we found that in the combination case user were able to make more accurate decision about the potential relevance of result than in either of the other version with hardly any increase in speed of processing the page a a whole 
this paper is concerned with learning categorial grammar in gold s model in contrast to k valued classical categorial grammar k valued lambek grammar are not learnable from string this result wa shown for several variant but the question wa left open for the weakest one the non associative variant nl we show that the class of rigid and k valued nl grammar is unlearnable from string for each k this result is obtained by a specific construction of a limit point in the considered class that doe not use product operator another interest of our construction is that it provides limit point for the whole hierarchy of lambek grammar including the recent pregroup grammar such a result aim at clarifying the possible direction for future learning algorithm it express the difficulty of learning categorial grammar from string and the need for an adequate structure on example 
the successful implementation of machine learning in autonomous rover traverse science requires addressing challenge that range from the analytical technical realm to the fuzzy philosophical domain of entrenched belief system within scientist and mission manager these challenge are many they include helping scientist understand the benefit and risk of using machine leaming onboard and guiding them to distill and translate science goal into clear task that can be accomplished by algorithm the technical challenge include among other thing developing robust verification and validation plan the ultimate test of onboard machine learning acceptance is if it fly and is executed onboard and delivers successful result during the mission this last hurdle can be overcome by developing an implementation plan that pose an acceptable risk v reward scenario for mission manager we are working on rover traverse science application that address each of these issue in this paper we will describe what we are doing and how we approach these challenge 
abstract awide range of supervised learning algorithm ha been applied to text categorization however the supervised learning approach have some problem one ofthem is that they require a large often prohibitive number of labeled training document for accurate learning generally acquiring class label for training data is costly while gathering a large quantity of unlabeled data is cheap we here propose a new automatic text categorization method for learning from only unlabeled data using a bootstrapping framework and a feature projection technique from result of our experiment our method showed reasonably comparable performance compared with a supervised method if our method is used in a text categorization task building text categorization system will become significantly faster and le expensive 
predictive accuracy ha been used a the main and often only evaluation criterion for the predictive performance of classification learning algorithm in recent year the area under the roc receiver operating characteristic curve or simply auc ha been proposed a an alternative single number measure for evaluating learning algorithm in this paper we prove that auc is a better measure than accuracy more specifically we present rigourous definition on consistency and discriminancy in comparing two evaluation measure for learning algorithm we then present empirical evaluation and a formal proof to establish that auc is indeed statistically consistent and more discriminating than accuracy our result is quite significant since we formally prove that for the first time auc is a better measure than accuracy in the evaluation of learning algorithm 
often the training procedure for statistical machine translation model is based on maximum likelihood or related criterion a general problem of this approach is that there is only a loose relation to the final translation quality on unseen text in this paper we analyze various training criterion which directly optimize translation quality these training criterion make use of recently proposed automatic evaluation metric we describe a new algorithm for efficient training an unsmoothed error count we show that significantly better result can often be obtained if the final evaluation criterion is taken directly into account a part of the training procedure 
we describe the problem of scheduling the television broadcast of the u s national football league nfl unlike traditional round robin tournament scheduling the nfl problem involves assigning game to broadcast slot under various complex constraint while attempting to satisfy a set of user preference a well a mixed initiative functionality wa required to allow the user to control and assist in the scheduling process a prototype system wa developed for the nfl which produced schedule satisfying many of these constraint and preference in this paper we provide an overview of the constraint solving methodology employed and the implementation of the nfl prototype system 
survey propagation is a powerful technique from statistical physic that ha been applied to solve the sat problem both in principle and in practice we give using only probability argument a common derivation of survey propagation belief propagation and several interesting hybrid method we then present numerical experiment which use wsat a widely used random walk based sat solver to quantify the complexity of the sat formula a a function of their parameter both a randomly generated and after simpli cation guided by survey propagation some property of wsat which have not previously been reported make it an ideal tool for this purpose it mean cost is proportional to the number of variable in the formula at a xed ratio of clause to variable in the easy sat regime and slightly beyond and it behavior in the hardsat regime appears to reect the underlying structure of the solution space that ha been predicted by replica symmetry breaking argument an analysis of the tradeoff between the various method of search for satisfying assignment show wsat to be far more powerful than ha been appreciated and suggests some interesting new direction for practical algorithm development 
recent work ha demonstrated that the assessment of pairwise object similarity can be approached in an axiomatic manner using information theory we extend this concept specifically to document similarity and test the effectiveness of an information theoretic measure for pairwise document similarity we adapt query retrieval to rate the quality of document similarity measure and demonstrate that our proposed information theoretic measure for document similarity yield statistically significant improvement over other popular measure of similarity 
previous research on cluster based retrieval ha been inconclusive a to whether it doe bring improved retrieval effectiveness over document based retrieval recent development in the language modeling approach to ir have motivated u to re examine this problem within this new retrieval framework we propose two new model for cluster based retrieval and evaluate them on several trec collection we show that cluster based retrieval can perform consistently across collection of realistic size and significant improvement over document based retrieval can be obtained in a fully automatic manner and without relevance information provided by human 
we present a method capable of extracting parallel sentence from far more disparate very non parallel corpus than previous comparable corpus method by exploiting bootstrapping on top of ibm model em step of our method like previous method us similarity measure to find matching document in a corpus first and then extract parallel sentence a well a new word translation from these document but unlike previous method we extend this with an iterative bootstrapping framework based on the principle of find one get more which claim that document found to contain one pair of parallel sentence must contain others even if the document are judged to be of low similarity we re match document based on extracted sentence pair and refine the mining process iteratively until convergence this novel find one get more principle allows u to add more parallel sentence from dissimilar document to the baseline set experimental result show that our proposed method is nearly more effective than the baseline method without iteration we also show that our method is effective in boosting the performance of the ibm model em lexical learner a the latter though stronger than model used in previous work doe not perform well on data from very non parallel corpus 
asymmetric lateral connection are one possible mechanism that can account for the direction selectivity of cortical neuron we present a mathematical analysis for a class of these model contrasting with earlier theoretical work that ha relied on method from linear system theory we study the network s nonlinear dynamic property that arise when the threshold nonlinearity of the neuron is taken into account we show that such network have stimulus locked traveling pulse solution that are appropriate for modeling the response of direction selective cortical neuron in addition our analysis show that outside a certain regime of stimulus speed the stability of this solution break down giving rise to another class of solution that are characterized by specific spatiotemporal periodicity this predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connection lurching activity wave might be observable in ensemble of direction selective cortical neuron within appropriate regime of the stimulus speed 
new feature selection algorithm for linear threshold function are described which combine backward elimination with an adaptive regularization method this make them particularly suitable to the classification of microarray expression data where the goal is to obtain accurate rule depending on few gene only our algorithm are fast and easy to implement since they center on an incremental large margin algorithm which allows u to avoid linear quadratic or higher order programming method we report on preliminary experiment with five known dna microarray datasets these experiment suggest that multiplicative large margin algorithm tend to outperform additive algorithm such a svm on feature selection task 
labeling video data is an essential prerequisite for many visionapplications that depend on training data such a visualinformation retrieval object recognition and human activitymodeling however manually creating label is not onlytime consuming but also subject to human error and eventually becomes impossible for a very large amount of data e g surveillance video to minimize the human effort in labeling wepropose a unified multi class active learning approach forautomatically labeling video data the contribution of this paperinclude extending active learning from binary class to multipleclasses and evaluating several practical sample selectionstrategies the experimental result show that the proposedapproach work effectively even with a significantly reduced amountof labeled data the best sample selection strategy can achievemore than a error reduction over random sample selection 
while electronic music archive are gaining popularity access to and navigation within these archive is usually limited to text based query or manually predefined genre category browsing we present a system that automatically organizes a music collection according to the perceived sound similarity resembling genre or style of music audio signal are processed according to psychoacoustic model to obtain a time invariant representation of it characteristic subsequent clustering provides an intuitive interface where similar piece of music are grouped together on a map display 
we propose several context based method for text categorization one method a small modification to the ppm compression based model which is known to significantly degrade compression performance counter intuitively ha the opposite effect on categorization performance another method called c measure simply count the presence of higher order character context and outperforms all other approach investigated 
we apply a novel variant of random forest breiman to the shallow semantic parsing problem and show extremely promising result the final system ha a semantic role classification accuracy of using propbank gold standard par these result are better than all others published except those of the support vector machine svm approach implemented by pradhan et al and random forest have numerous advantage over svms including simplicity faster training and classification easier multi class classification and easier problem specific customization we also present new feature which result in a gain in classification accuracy and describe a technique that result in a reduction in the feature space with no significant degradation in accuracy 
a large amount of information on the web is contained in regularly structured object which we call data record such data record are important because they often present the essential information of their host page e g list of product or service it is useful to mine such data record in order to extract information from them to provide value added service existing automatic technique are not satisfactory because of their poor accuracy in this paper we propose a more effective technique to perform the task the technique is based on two observation about data record on the web and a string matching algorithm the proposed technique is able to mine both contiguous and non contiguous data record our experimental result show that the proposed technique outperforms existing technique substantially 
we present the first application of the head driven statistical parsing model of collins a a simultaneous language model and parser for large vocabulary speech recognition the model is adapted to an online left to right chart parser for word lattice integrating acoustic n gram and parser probability the parser us structural and lexical dependency not considered by n gram model conditioning recognition on more linguistically grounded relationship experiment on the wall street journal treebank and lattice corpus show word error rate competitive with the standard n gram language model while extracting additional structural information useful for speech understanding 
in this paper we present a method for unsupervised clustering ofimage database the method is based on a recently introducedinformation theoretic principle the information bottleneck ib principle image archive are clustered such that the mutualinformation between the cluster and the image content is maximallypreserved the ib principle is applied to both discrete andcontinuous image representation using discrete image histogramsand probabilistic continuous image modeling based on mixture ofgaussian density respectively experimental result demonstratethe performance of the proposed method forimage clustering on alarge image database several clustering algorithm derived fromthe ib principle are explored and compared 
we compute approximate analytical bootstrap average for support vector classification using a combination of the replica method of statistical physic and the tap approach for approximate inference we test our method on a few datasets and compare it with exact average obtained by extensive monte carlo sampling 
selective sampling a part of the active learning method reduces the cost of labeling supplementary training data by asking for the label only of the most informative unlabeled example this additional information added to an initial randomly chosen training set is expected to improve the generalization performance of a learning machine we investigate some method for a selection of the most informative example in the context of one class classification problem occ i e problem where only or nearly only the example of the so called target class are available we applied selective sampling algorithm to a variety of domain including realworld problem mine detection and texture segmentation the goal of this paper is to show why the best or most often used selective sampling method for twoor multi class problem are not necessarily the best one for the one class classification problem by modifying the sampling method we present a way of selecting a small subset from the unlabeled data to be presented to an expert for labeling such that the performance of the retrained one class classifier is significantly improved 
document centric xml collection contain text rich document marked up with xml tag the tag add lightweight semantics to the text querying such collection call for a hybrid query language the text rich nature of the document suggest a content oriented ir approach while the mark up allows user to add structural constraint to their ir query we will show how evidence for relevancy from different source help to answer such hybrid query we evaluate our method using the inex test set and show that structural hint in hybrid query help to improve retrieval effectiveness 
we present a novel connectionist model for acquiring the semantics of language through the behavioral experience of a real robot we focus on the compositionality of semantics which is a fundamental characteristic of human language namely the fact that we can understand the meaning of a sentence a a combination of the meaning of word the essential claim is that a compositional semantic representation can be self organized by generalizing correspondence between sentence and behavioral pattern this claim is examined and confirmed through simple experiment in which a robot generates corresponding behavior from unlearned sentence by analogy with the correspondence between learned sentence and behavior 
relational markov model rmms are a generalization of markov model where state can be of different type with each type described by a different set of variable the domain of each variable can be hierarchically structured and shrinkage is carried out over the cross product of these hierarchy rmms make effective learning possible in domain with very large and heterogeneous state space given only sparse data we apply them to modeling the behavior of web site user improving prediction in our proteus architecture for personalizing web site we present experiment on an e commerce and an academic web site showing that rmms are substantially more accurate than alternative method and make good prediction even when applied to previously unvisited part of the site 
in this paper we address the issue of using local embeddings for data visualization in two and three dimension and for classification we advocate their use on the basis that they provide an efficient mapping procedure from the original dimension of the data to a lower intrinsic dimension we depict how they can accurately capture the user s perception of similarity in high dimensional data for visualization purpose moreover we exploit the low dimensional mapping provided by these embeddings to develop new classification technique and we show experimentally that the classification accuracy is comparable albeit using fewer dimension to a number of other classification procedure 
we train a decision tree inducer cart and a memory based classifier mbl on predicting prosodic pitch accent and break in dutch text on the basis of shallow easy to compute feature we train the algorithm on both task individually and on the two task simultaneously the parameter of both algorithm and the selection of feature are optimized per task with iterative deepening an efficient wrapper procedure that us progressive sampling of training data result show a consistent significant advantage of mbl over cart and also indicate that task combination can be done at the cost of little generalization score loss test on cross validated data and on held out data yield f score of mbl on accent placement of and respectively and on break of and respectively accent placement is shown to outperform an informed baseline rule reliably predicting break other than those already indicated by intra sentential punctuation however appears to be more challenging 
previous algorithm to compute lexical chain suffer either from a lack of accuracy in word sense disambiguation wsd or from computational inefficiency in this paper we present a new linear time algorithm for lexical chaining that adopts the assumption of one sense per discourse our result show an improvement over previous algorithm when evaluated on a wsd task 
in this paper we propose a new language model namely a title language model for information retrieval different from the traditional language model used for retrieval we define the conditional probability p q d a the probability of using query q a the title for document d we adopted the statistical translation model learned from the title and document pair in the collection to compute the probability p q d to avoid the sparse data problem we propose two new smoothing method in the experiment with four different trec document collection the title language model for information retrieval with the new smoothing method outperforms both the traditional language model and the vector space model for ir significantly 
web service web accessible program and device are a key application area for the semantic web with the proliferation of web service and the evolution towards the semantic web come the opportunity to automate various web service task our objective is to enable markup and automated reasoning technology to describe simulate compose test and verify composition of web service we take a our starting point the daml s daml oil ontology for describing the capability of web service we define the semantics for a relevant subset of daml s in term of a first order logical language with the semantics in hand we encode our service description in a petri net formalism and provide decision procedure for web service simulation verification and composition we also provide an analysis of the complexity of these task under different restriction to the daml s composite service we can describe finally we present an implementation of our analysis technique this implementation take a input a daml s description of a web service automatically generates a petri net and performs the desired analysis such a tool ha broad applicability both a a back end to existing manual web service composition tool and a a stand alone tool for web service developer 
this paper compare a number of generative probability model for a wide coverage combinatory categorial grammar ccg parser these model are trained and tested on a corpus obtained by translating the penn treebank tree into ccg normal form derivation according to an evaluation of unlabeled word word dependency our best model achieves a performance of comparable to the figure given by collins for a linguistically le expressive grammar in contrast to gildea we find a significant improvement from modeling word word dependency 
in this paper a general method is given for reconstruction of a set of feature point in an arbitrary dimensional projective space from their projection into lower dimensional space the method extends the method applied in the well studied problem of reconstruction of a set of scene point in p given their projection in a set of image in this case the bifocal trifocal and quadrifocal tensor are used to carry out this computation it is shown that similar method will apply in a much more general context and hence may be applied to projection from pn to pm which have been used in the analysis of dynamic scene for sufficiently many generic projection reconstruction of the scene is shown to be unique up to projectivity except in the case of projection onto one dimensional image space line 
in this paper we introduce the language golog htnti for specifying control using procedural and htn based construct together with deadline and time restriction our language start with feature from golog and htn and extends them so that we can deal with action with duration by being able to specify time interval between the start or end of an action or a program and the start or end of another action or program we then discus an off line interpreter based on the answer set planning paradigm such that the answer set of the logic program have a one to one correspondence with the trace of the golog htnti specification 
we present metric for measuring the similarity of state in a finite markov decision process mdp the formulation of our metric is based on the notion of bisimulation for mdps with an aim towards solving discounted infinite horizon reinforcement learning task such metric can be used to aggregate state a well a to better structure other value function approximators e g memory based or nearest neighbor approximators we provide bound that relate our metric distance to the optimal value of state in the given mdp 
in many real world planning scenario agent often do not have enough resource to achieve all of their goal consequently they are forced to find plan that satisfy only a subset of the goal solving such partial satisfaction planning psp problem pose several challenge including an increased emphasis on modeling and handling plan quality in term of action cost and goal utility despite the ubiquity of such psp problem very little attention ha been paid to them in the planning community in this paper we start by describing a spectrum of psp problem and focus on one of the more general psp problem termed psp net benefit we develop three technique i one based on integer programming called optiplan ii the second based on regression planning with reachability heuristic called altaltps and iii the third based on anytime heuristic search for a forward state space heuristic planner called sapaps our empirical study with these planner show that the heuristic planner generate plan that are comparable to the quality of plan generated by optiplan while incurring only a small fraction of the cost 
it is often useful to classify email according to the intent of the sender e g propose a meeting deliver information we present experimental result in learning to classify email in this fashion where each class corresponds to a verb noun pair taken from a predefined ontology describing typical email speech act we demonstrate that although this categorization problem is quite different from topical text classification certain category of message can nonetheless be detected with high precision above and reasonable recall above using existing text classification learning method this result suggests that useful task tracking tool could be constructed based on automatic classification into this taxonomy 
an interesting alternative to domain independent planning is to provide example plan to demonstrate how to solve problem in a particular domain and to use that information to learn domainspecific planner others have used example plan for case based planning but the retrieval and adaptation mechanism for the inevitably large case library raise efficiency issue of concern in this paper we introduce dsplanners or automatically generated domain specific planner we present the distill algorithm for learning dsplanners automatically from example plan distill convert a plan into a dsplanner and then merges it with previously learned dsplanners our result show that the dsplanners automatically learned by distill compactly represent it domain specific planning experience furthermore the dsplanners situationally generalize the given example plan thus allowing them to efficiently solve problem that have not previously been encountered finally we present the distill procedure to automatically acquire one step loop from example plan which permit experience acquired from small problem to be applied to solving arbitrarily large one 
topic detection and tracking tdt task are evaluated using a cost function the standard tdt cost function assumes a constant probability of relevance p rel across all topic in practice p rel varies widely across topic we argue using both theoretical and experimental evidence that the cost function should be modified to account for the varying p rel 
we establish a new hardness result that show that the difficulty of planning in factored markov decision process is representational rather than just computational more precisely we give a fixed family of factored mdps with linear reward whose optimal policy and value function simply cannot be represented succinctly in any standard parametric form previous hardness result indicated that computing good policy from the mdp parameter wa difficult but left open the possibility of succinct function approximation for any fixed factored mdp our result applies even to policy which yield a polynomially poor approximation to the optimal value and highlight interesting connection with the complexity class of arthur merlin game 
x ray crystallography is currently the most common way protein structure are elucidated one of the most time consuming step in the crystallographic process is interpretation of the electron density map a task that involves finding pattern in a three dimensional picture of a protein this paper describes deft deformable template an algorithm using pictorial structure to build a flexible protein model from the protein s amino acid sequence matching this pictorial structure into the density map is a way of automating density map interpretation also described are several extension to the pictorial structure matching algorithm necessary for this automated interpretation deft is tested on a set of density map ranging from to resolution producing rootmean squared error ranging from to 
multi level annotation of image is a promising solution to enable more effective semantic image retrieval by using various keywords at different semantic level in this paper we propose a multi level approach to annotate the semantics of natural scene by using both the dominant image component and the relevant semantic concept in contrast to the well known image based and region based approach we use the salient object a the dominant image component to achieve automatic image annotation at the content level by using the salient object for image content representation a novel image classification technique is developed to achieve automatic image annotation at the concept level to detect the salient object automatically a set of detection function are learned from the labeled image region by using support vector machine svm classifier with an automatic scheme for searching the optimal model parameter to generate the semantic concept finite mixture model are used to approximate the class distribution of the relevant salient object an adaptive em algorithm ha been proposed to determine the optimal model structure and model parameter simultaneously we have also demonstrated that our algorithm are very effective to enable multi level annotation of natural scene in a large scale dataset 
most foreign name are transliterated into chinese japanese or korean with approximate phonetic equivalent the transliteration is usually achieved through intermediate phonemic mapping this paper present a new framework that allows direct orthographical mapping dom between two different language through a joint source channel model also called n gram transliteration model tm with the n gram tm model we automate the orthographic alignment process to derive the aligned transliteration unit from a bilingual dictionary the n gram tm under the dom framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state of the art machine learning algorithm the modeling framework is validated through several experiment for english chinese language pair 
a central problem of word sense disambiguation wsd is the lack of manually sense tagged data required for supervised learning in this paper we evaluate an approach to automatically acquire sense tagged training data from english chinese parallel corpus which are then used for disambiguating the noun in the senseval english lexical sample task our investigation reveals that this method of acquiring sense tagged data is promising on a subset of the most difficult senseval noun the accuracy difference between the two approach is only and the difference could narrow further to if we disregard the advantage that manually sense tagged data have in their sense coverage our analysis also highlight the importance of the issue of domain dependence in evaluating wsd program 
abstract we present a novel flexible statistical approach for modelling music and text jointly the approach is based on multi modal mixture model and maximum a posteriori estimation the learned model can be used to browse database with document containing music and text to search for music using query consisting of music and text lyric and other contextual information to annotate text document with music and to automatically recommend or identify similar song 
this paper present a document representation improvement technique named the relevance feedback accumulation rfa algorithm using prior relevance feedback assessment and a data mining measure called support this algorithm improves document representation and generates higher quality index at the same time the algorithm is efficient and scalable suited for retrieval system managing large document collection the result of the preliminary evaluation reveal that the rfa algorithm is able to reduce the index dimensionality while improving retrieval effectiveness 
this paper describes a new paradigm for modeling traffic level on the world wide web www using a method of entropy maximization this traffic is subject to the conservation condition of a circulation flow in the entire www an aggregation of the www or a subgraph of the www such a an intranet or extranet we specifically apply the primal and dual solution of this model to the static ranking of web site the first of these us an imputed measure of total traffic through a web page the second provides an analogy of local temperature allowing u to quantify the hotness of a page 
distance function computation is a key subtask in many data mining algorithm and application the most effective form of the distance function can only be expressed in the context of a particular data domain it is also often a challenging and non trivial task to find the most effective form of the distance function for example in the text domain distance function design ha been considered such an important and complex issue that it ha been the focus of intensive research over three decade the final design of distance function in this domain ha been reached only by detailed empirical testing and consensus over the quality of result provided by the different variation with the increasing ability to collect data in an automated way the number of new kind of data continues to increase rapidly this make it increasingly difficult to undertake such effort for each and every new data type the most important aspect of distance function design is that since a human is the end user for any application the design must satisfy the user requirement with regard to effectiveness this creates the need for a systematic framework to design distance function which are sensitive to the particular characteristic of the data domain in this paper we discus such a framework the goal is to create distance function in an automated waywhile minimizing the work required from the user we will show that this framework creates distance function which are significantly more effective than popularly used function such a the euclidean metric 
semantic video classification ha become an active research topic to enable more effective video retrieval and knowledge discovery from large scale video database however most existing technique for classifier training require a large number of hand labeled sample to learn correctly to address this problem we have proposed a semi supervised framework to achieve incremental classifier training by integrating a limited number of labeled sample with a large number of unlabeled sample specifically this emi supervised framework includes a modeling the semantic video concept by using the finite mixture model to approximate the class distribution of the relevant salient object b developing an adaptive em algorithm to integrate the unlabeled sample to achieve parameter estimation and model selection simultaneously the experimental result in a certain domain of medical video are also provided 
the structure of the web is increasingly being used to improve organization search and analysis of information on the web for example google us the text in citing document document that link to the target document for search we analyze the relative utility of document text and the text in citing document near the citation for classification and description result show that the text in citing document when available often ha greater discriminative and descriptive power than the text in the target document itself the combination of evidence from a document and citing document can improve on either information source alone moreover by ranking word and phrase in the citing document according to expected entropy loss we are able to accurately name cluster of web page even with very few positive example our result confirm quantify and extend previous research using web structure in these area introducing new method for classification and description of page 
this paper introduces an approach for clustering classification which is based on the use of local high order structure present in the data for some problem this local structure might be more relevant for classification than other measure of point similarity used by popular unsupervised and semi supervised clustering method under this approach change in the class label are associated to change in the local property of the data using this idea we also pursue to learn how to cluster given example of clustered data including from different datasets we make these concept formal by presenting a probability model that capture their fundamental and show that in this setting learning to cluster is a well defined and tractable task based on probabilistic inference method we then present an algorithm for computing the posterior probability distribution of class label for each data point experiment in the domain of spatial grouping and functional gene classification are used to illustrate and test these concept 
we present a new method for transductive learning which can be seen a a transductive version of the k nearest neighbor classifier unlike for many other transductive learning method the training problem ha a meaningful relaxation that can be solved globally optimally using spectral method we propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently a key advantage of the algorithm is that it doe not require additional heuristic to avoid unbalanced split furthermore we show a connection to transductive support vector machine and that an effective co training algorithm arises a a special case 
test collection for the filtering track in trec have typically used either past set of relevance judgment or categorized collection such a reuters corpus volume or ohsumed because filtering system need relevance judgment during the experiment for training and adaptation for trec we constructed an entirely new set of search topic for the reuters corpus for measuring filtering system our method for building the topic involved multiple iteration of feedback from assessor and fusion of result from multiple search system using different search algorithm we also developed a second set of inexpensive topic based on category in the document collection we found that the initial judgment made for the experiment were sufficient subsequent pooled judging changed system ranking very little we also found that system performed very differently on the category topic than on the assessor built topic 
computing least common subsumers ic and most specific concept msc are inference task that can support the bottom up construction of knowledge base in description logic in description logic with existential restriction the most specific concept need not exist if one restricts the attention to concept description or acyclic tboxes in this paper we extend the notion le and msc to cyclic tboxes for the description logic ec which allows for conjunction existential restriction and the top concept we show that the le and msc always exist and can be computed in polynomial time if we interpret cyclic definition with greatest fixpoint semantics 
the fast spreading of electronic business to business procurement system ha led to the development of new standard for the exchange of electronic product catalog e catalog e catalog contain various information about product essential is price information price are used for buying decision and following order transaction while simple price model are often sufficient for the description of indirect good e g office supply other good and line of business make higher demand in this paper we examine what price information is contained in commercial xml standard for the exchange of product catalog data for that purpose we bring the different implicit price model of the examined catalog standard together and provide a generalized model 
wepresentan optimalgridbasedalgorithm for pomdps that is tractable in the discountfactorandthemaximumabsolutevalue of the cost function but exponential in the dimension of the state space to the best of our knowledge this is the flrst optimal grid based algorithm for pomdps all other optimal algorithm that we know are based onsondik srepresentationofthevaluefunction we also propose a robustness criterion for grid based algorithm and show that the new algorithm is robust in such sense 
the support vector machine svm is a widely used tool for classification many efficient implementation exist for fitting a two class svm model the user ha to supply value for the tuning parameter the regularization cost parameter and the kernel parameter it seems a common practice is to use a default value for the cost parameter often leading to the least restrictive model in this paper we argue that the choice of the cost parameter can be critical we then derive an algorithm that can fit the entire path of svm solution for every value of the cost parameter with essentially the same computational cost a fitting one svm model we illustrate our algorithm on some example and use our representation to give further insight into the range of svm solution 
we propose a dynamic bayesian model for motif in biopolymer sequence which capture rich biological prior knowledge and positional dependency in motif structure in a principled way our model posit that the position specific multinomial parameter for mono mer distribution are distributed a a latent dirichlet mixture random variable and the position specific dirichlet component is determined by a hi dden markov process model parameter can be fit on training motif using a variational em algorithm within an empirical bayesian framework variational inference is also used for detecting hidden motif o ur model improves over previous model that ignore biological prior and positional dependence it ha much higher sensitivity to motif during detection and a notable ability to distinguish genuine motif from false recurring pattern 
best first search is limited by the memory needed to store the open and closed list primarily to detect duplicate node magnetic disk provide vastly more storage but random access of a disk is extremely slow instead of checking generated node immediately against existing node in a hash table delayed duplicate detection ddd appends them to a file then periodically remove the duplicate node using only sequential disk access frontier search save storage in a best first search by storing only the open list and not the closed list the main contribution of this paper are to provide a scalable implementation of ddd to combine it with frontier search and to extend it to more general best first search such a a we illustrate these idea by performing complete breadth first search of sliding tile puzzle up to the fourteen puzzle for the peg tower of hanoi problem we perform complete search with up to disk searching a space of over a trillion node and discover a surprising anomaly concerning the problem space diameter of the and disk problem we also verify the presumed optimal solution length for up to disk in addition we implement a with ddd on the fifteen puzzle finally we present a scalable implementation of ddd based on hashing rather than sorting 
we present an unsupervised methodfor word sense disambiguation thatexploits translation correspondencesin parallel corpus the techniquetakes advantage of the fact that crosslanguagelexicalizations of the sameconcept tend to be consistent preservingsome core element of it semantics and yet also variable reflecting differingtranslator preference and the influenceof context working with parallelcorpora introduces an extra complicationfor evaluation since it is 
we propose a sequential information maximization model a a general strategy for programming eye movement the model reconstructs high resolution visual information from a sequence of fixation taking into account the fall off in resolution from the fovea to the periphery from this framework we get a simple rule for predicting fixation sequence after each fixation fixate next at the location that minimizes uncertainty maximizes information about the stimulus by comparing our model performance to human eye movement data and to prediction from a saliency and random model we demonstrate that our model is best at predicting fixation location modeling additional biological constraint will improve the prediction of fixation sequence our result suggest that information maximization is a useful principle for programming eye movement 
in this paper we present the rwth fsa toolkit an efficient implementation of algorithm for creating and manipulating weighted finite state automaton the toolkit ha been designed using the principle of on demand computation and offer a large range of widely used algorithm to prove the superior efficiency of the toolkit we compare the implementation to that of other publically available toolkits we also show that on demand computation help to reduce memory requirement significantly without any loss in speed to increase it flexibility the rwth fsa toolkit support high level interface to the programming language python a well a a command line tool for interactive manipulation of fsas furthermore we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system future extensibility of the toolkit is ensured a it will be publically available a open source software 
many real life optimization problem contain bothhard and soft constraint a well a qualitative conditionalpreferences however there is no singleformalism to specify all three kind of information 
according to a series of influential model dopamine da neuron signal reward prediction error using a temporal difference td algorithm we address a problem not convincingly solved in these account how to maintain a representation of cue that predict delayed consequence our new model us a td rule grounded in partially observable semi markov process a formalism that capture two largely neglected feature of da experiment hidden state and temporal variability previous model predicted reward using a tapped delay line representation of sensory input we replace this with a more active process of inference about the underlying state of the world the da system can then learn to map these inferred state to reward prediction using td the new model can explain previously vexing data on the response of da neuron in the face of temporal variability by combining statistical model based learning with a physiologically grounded td theory it also brings into contact with physiology some insight about behavior that had previously been confined to more abstract psychological model 
given a user specified minimum correlation threshold and a market basket database with n item and t transaction an all strong pair correlation query find all item pair with correlation above the threshold however when the number of item and transaction are large the computation cost of this query can be very high in this paper we identify an upper bound of pearson s correlation coefficient for binary variable this upper bound is not only much cheaper to compute than pearson s correlation coefficient but also exhibit a special monotone property which allows pruning of many item pair even without computing their upper bound a two step all strong pair correlation que ry taper algorithm is proposed to exploit these property in a filter and refine manner furthermore we provide an algebraic cost model which show that the computation saving from pruning is independent or improves when the number of item is increased in data set with common zipf or linear rank support distribution experimental result from synthetic and real data set exhibit similar trend and show that the taper algorithm can be an order of magnitude faster than brute force alternative 
formalism and axiomatic theory are designed to support reasoning they are often intended with a preferred interpretation and a targeted ontology question of proper interpretation and of the possible challenge of an intended interpretation arise when integrating a particular theory in pre existing formal and ontological setting this paper report on an instance of this general problem of ontological engineering the case study is that of the integration of the region connection calculus for spatial reasoning in the cyc knowledge base we show that given the assumption on the cyc ontology rcc had to be interpreted within a substantivalist metaphysic of space a a boolean algebra of spatial region which are distinct from their occupant the rcc literature suggests such an intended interpretation and this paper intends to show that this wa a necessary condition of integration in cyc s ontology this led to the enrichment of the cyc knowledge base rather than to a radical modification of the upper level ontology 
a critical problem in cluster ensemble research is how to combine multiple clustering to yield a final superior clustering result leveraging advanced graph partitioning technique we solve this problem by reducing it to a graph partitioning problem we introduce a new reduction method that construct a bipartite graph from a given cluster ensemble the resulting graph model both instance and cluster of the ensemble simultaneously a vertex in the graph our approach retains all of the information provided by a given ensemble allowing the similarity among instance and the similarity among cluster to be considered collectively in forming the final clustering further the resulting graph partitioning problem can be solved efficiently we empirically evaluate the proposed approach against two commonly used graph formulation and show that it is more robust and achieves comparable or better performance in comparison to it competitor 
we consider a general model of stochastic discrete event system with asynchronous event and propose to develop efficient algorithm for verification and control of such system 
intelligent agent often need to ass user utility function in order to make decision on their behalf or predict their behavior when uncertainty exists over the precise nature of this utility function one can model this uncertainty using a distribution over utility function this view lie at the core of game with incomplete information and more recently several proposal for incremental preference elicitation in such case decision or predicted behavior are based on computing the expected expected utility eeu of decision with respect to the distribution over utility function unfortunately decision made under eeu are sensitive to the precise representation of the utility function we examine the condition under which eeu provides for sensible decision by appeal to the foundational axiom of decision theory we also discus the impact these condition have on the enterprise of preference elicitation more broadly 
the problem of analyzing microarray data became one of important topic in bioinformatics over the past several year and different data mining technique have been proposed for the analysis of such data in this paper we propose to use association rule discovery method for determining association among expression level of different gene one of the main problem related to the discovery of these association is the scalability issue microarrays usually contain very large number of gene that are sometimes measured in s therefore analysis of such data can generate a very large number of association that can often be measured in million the paper address this problem by presenting a method that enables biologist to evaluate these very large number of discovered association rule during the post analysis stage of the data mining process this is achieved by providing several rule evaluation operator including rule grouping filtering browsing and data inspection operator that allow biologist to validate multiple individual gane regulation pattern at a time by iteratively applying these operator biologist can explore a significant part of all the initially generated rule in an acceptable period of time and thus answer biological question that are of a particular interest to him or her to validate our method we tested our system on the microarray data pertaining to the study of environmental hazard and their influence of gane expression process a a result we managed to answer several question that were of interest to the biologist that had collected this data 
user of the world wide web are not only confronted by an immense overabundance of information but also by a plethora of tool for searching for the web page that suit their information need web search engine differ widely in interface feature coverage of the web ranking method delivery of advertising and more in this paper we present a method for comparing search engine automatically based on how they rank known item search result because the engine perform their search on overlapping but different subset of the web collected at different point in time evaluation of search engine pose significant challenge to the traditional information retrieval methodology our method us known item searching comparing the relative rank of the item in the search engine ranking our approach automatically construct known item query using query log analysis and automatically construct the result via analysis of editor comment from the odp open directory project additionally we present our comparison on five lycos netscape fast google hotbot well known search service and find that some service perform known item search better than others but the majority are statistically equivalent 
we describe an innovative solution to the problem of scheduling astronomy observation for the stratospheric observatory for infrared astronomy an airborne observatory the problem contains complex constraint relating the feasibility of an astronomical observation to the position and time at which the observation begin telescope elevation limit and available fuel solving the problem requires making discrete choice e g selection and sequencing of observation and continuous one e g takeoff time and setting up observation by repositioning the aircraft the problem also includes optimization criterion such a maximizing observing time while simultaneously minimizing total flight time we describe a method to search for good flight plan that satisfy all constraint this novel approach combine heuristic search biased stochastic sampling continuous optimization technique and well founded approximation that eliminate feasible solution but greatly reduce computation time 
we consider the problem of multi step ahead prediction in time series analysis using the non parametric gaussian process model step ahead forecasting of a discrete time non linear dynamic system can be performed by doing repeated one step ahead prediction for a state space model of the form the prediction of at time is based on the point estimate of the previous output in this paper we show how using an analytical gaussian approximation we can formally incorporate the uncertainty about intermediate regressor value thus updating the uncertainty on the current prediction 
we describe a method for enriching the output of a parser with information available in a corpus the method is based on graph rewriting using memory based learning applied to dependency structure this general framework allows u to accurately recover both grammatical and semantic information a well a non local dependency it also facilitates dependency based evaluation of phrase structure parser our method is largely independent of the choice of parser and corpus and show state of the art performance 
coreferential information of a candidate such a the property of it antecedent is important for pronoun resolution because it reflects the salience of the candidate in the local discourse such information however is usually ignored in previous learning based system in this paper we present a trainable model which incorporates coreferential information of candidate into pronoun resolution preliminary experiment show that our model will boost the resolution performance given the right antecedent of the candidate we further discus how to apply our model in real resolution where the antecedent of the candidate are found by a separate noun phrase resolution module the experimental result show that our model still achieves better performance than the baseline 
we introduce an incremental singular value decomposition svd of incomplete data the svd is developed a data arrives and can handle arbitrary missing untrusted value correlated uncertainty across row or column of the measurement matrix and user prior since incomplete data doe not uniquely specify an svd the procedure selects one having minimal rank for a dense p q matrix of low rank r the incremental method ha time complexity o pqr and space complexity o p q r better than highly optimized batch algorithm such a matlab s svd in case of missing data it produce factoring of lower rank and residual than batch svd algorithm applied to standard missing data imputation we show application in computer vision and audio feature extraction in computer vision we use the incremental svd to develop an efficient and unusually robust subspace estimating flow based tracker and to handle occlusion missing point in structure from motion factorization 
developing regression model for large datasets that are both accurate and easy to interpret is a very important data mining problem regression tree with linear model in the leaf satisfy both these requirement but thus far no truly scalable regression tree algorithm is known this paper proposes a novel regression tree construction algorithm secret that produce tree of high quality and scale to very large datasets at every node secret us the em algorithm for gaussian mixture to find two cluster in the data and to locally transform the regression problem into a classification problem based on closeness to these cluster goodness of split measure like the gini gain can then be used to determine the split variable and the split point much like in classification tree construction scalability of the algorithm can be achieved by employing scalable version of the em and classification tree construction algorithm an experimental evaluation on real and artificial data show that secret ha accuracy comparable to other linear regression tree algorithm but take order of magnitude le computation time for large datasets 
an important issue in tracking is how to incorporate an appropriate degree of adaptivity into the observation model without any adaptivity tracking fails when object property change for example when illumination change affect surface colour conversely if an observation model adapts too readily then during some transient failure of tracking it is liable to adapt erroneously to some part of the background the approach proposed here is to adapt selectively allowing adaptation only during period when two particular condition are met that the object should be both present and in motion the proposed mechanism for adaptivity is tested here with a foreground colour and motion model the experimental setting itself is novel in that it us combined colour and motion observation from a fixed filter bank with motion used also for initialisation via a monte carlo proposal distribution adaptation is performed using a stochastic em algorithm during period that meet the condition above test verify the value of such adaptivity in that immunity to distraction from clutter of similar colour to the object is considerably enhanced 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we present a new method for learning good strategy in zero sum markov game in which each side is composed of multiple agent collaborating against an opposing team of agent our method requires full observability and communication during learning but the learned policy can be executed in a distributed manner the value function is represented a a factored linear architecture and it structure determines the necessary computational resource and communication bandwidth this approach permit a tradeoff between simple representation with little or no communication between agent and complex computationally intensive representation with extensive coordination between agent thus we provide a principled mean of using approximation to combat the exponential blowup in the joint action space of the participant the approach is demonstrated with an example that show the efficiency gain over naive enumeration 
we show how a conceptually simple search operator called optimal reinsertion can be applied to learning bayesian network structure from data on each step we pick a node called the target we delete all arc entering or exiting the target we then find subject to some constraint the globally optimal combination of in arc and out arc with which to reinsert it the heart of the paper is a new algorithm called orsearch which allows each optimal reinsertion step to be computed efficiently on large datasets our empirical result compare optimal reinsertion against a highly tuned implementation of multi restart hill climbing the result typically show one to two order of magnitude speed up on a variety of datasets they usually show better final result both in term of bdeu score and in modeling of future data drawn from the same distribution 
we augment a model of translation based on re ordering node in syntactic tree in order to allow alignment not conforming to the original tree structure while keeping computational complexity polynomial in the sentence length this is done by adding a new subtree cloning operation to either tree to string or tree to tree alignment algorithm 
we introduce a new algorithm for mining sequential pattern our algorithm is especially efficient when the sequential pattern in the database are very long we introduce a novel depth first search strategy that integrates a depth first traversal of the search space with effective pruning mechanism our implementation of the search strategy combine a vertical bitmap representation of the database with efficient support counting a salient feature of our algorithm is that it incrementally output new frequent itemsets in an online fashion in a thorough experimental evaluation of our algorithm on standard benchmark data from the literature our algorithm outperforms previous work up to an order of magnitude 
color detection can be seriously affected by lighting condition and other variation in the environment the robot vision system need to be recalibrated a lighting condition change otherwise they fail to recognize object or classify them incorrectly this paper describes experiment toward object recognition under different lightning condition we propose to train the vision system to recognize object under different lightning condition using machine learning learned knowledge is then used for object recognition having attached leaning module to the vision system facilitates the object recognition and provides condition for automatic adaptation of the vision system to new environment 
microarray data provides a powerful basis for analysis of gene expression data mining method such a clustering have been widely applied to microarray data to link gene that show similar expression pattern however this approach usually fails to unveil multiple interaction by the same gene association rule mining ha been used for this purpose but the inherent limitation of association rule limit the applicability of the result in this paper we use a combination of association rule mining and loglinear modeling to discover k gene interaction using this technique we can discover interaction among k gene that cannot be explained by the combined eects of any of the subset of those gene we test our technique experimentally using yeast microarray data our result reveal some previously unknown association that have solid biological explanation 
it ha been proven that a catadioptric projection can be modeled by an equivalent spherical projection in this paper we present an extension and improvement of those idea using the conformal geometric algebra a modern framework for the projective space of hyper sphere using this mathematical system the analysis of diverse catadioptric mirror becomes transparent and computationally simpler a a result the algebraic burden is reduced allowing the user to work in a much more effective framework for the development of algorithm for omnidirectional vision this paper includes complementary experimental analysis related to omnidirectional vision guided robot navigation 
empirical study of information retrieval method show that good retrieval performance is closely related to the use of various retrieval heuristic such a tf idf weighting one basic research question is thus what exactly are these necessary heuristic that seem to cause good retrieval performance in this paper we present a formal study of retrieval heuristic we formally define a set of basic desirable constraint that any reasonable retrieval function should satisfy and check these constraint on a variety of representative retrieval function we find that none of these retrieval function satisfies all the constraint unconditionally empirical result show that when a constraint is not satisfied it often indicates non optimality of the method and when a constraint is satisfied only for a certain range of parameter value it performance tends to be poor when the parameter is out of the range in general we find that the empirical performance of a retrieval formula is tightly related to how well it satisfies these constraint thus the proposed constraint provide a good explanation of many empirical observation and make it possible to evaluate any existing or new retrieval formula analytically 
one of the key benefit of xml is it ability to represent a mix of structured and unstructured text data although current xml query language such a xpath and xquery can express rich query over structured data they can only express very rudimentary query over text data we thus propose texquery which is a powerful full text search extension to xquery texquery provides a rich set of fully composable full text search primitive such a boolean connective phrase matching proximity distance stemming and thesaurus texquery also enables user to seamlessly query over both structured and text data by embedding texquery primitive in xquery and vice versa finally texquery support a flexible scoring construct that can be used toscore query result based on full text predicate texquery is the precursor ofthe full text language extension to xpath and xquery currently being developed by the w c 
in this work we quantitatively investigate the way in which a given person influence the joint turn taking behavior in a conversation after collecting an auditory database of social interaction among a group of twenty three people via wearable sensor hour of data each over two week we apply speech and conversation detection method to the auditory stream these method automatically locate the conversation determine their participant and mark which participant wa speaking when we then model the joint turn taking behavior a a mixed memory markov model that combine the statistic of the individual subject self transition and the partner cross transition the mixture parameter in this model describe how much each person s individual behavior contributes to the joint turn taking behavior of the pair by estimating these parameter we thus estimate how much influence each participant ha in determining the joint turntaking behavior we show how this measure correlate significantly with betweenness centrality an independent measure of an individual s importance in a social network this result suggests that our estimate of conversational influence is predictive of social influence 
we have created ladder the first language to describe how sketched diagram in a domain are drawn displayed and edited the difficulty in creating such a language is choosing a set of predefined entity that is broad enough to support a wide range of domain while remaining narrow enough to be comprehensible the language consists of predefined shape constraint editing behavior and display method a well a a syntax for specifying a domain description sketch grammar and extending the language ensuring that shape and shape group from many domain can be described the language allows shape to be built hierarchically e g an arrow is built out of three line and includes the concept of abstract shape analogous to abstract class in an object oriented language shape group describe how multiple domain shape interact and can provide the sketch recognition system with information to be used in top down recognition shape group can also be used to describe chain reaction editing command that effect multiple shape at once to test that recognition is feasible using this language we have built a simple domain independent sketch recognition system that par the domain description and generates the code necessary to recognize the shape 
web intelligence wi shed new light on direction for scientific research and development which explores the fundamental role a well a practical impact of artificial intelligence ai and advanced information technology it on the next generation of web empowered product system service and activity this paper give new perspective on the future wi research and highlight some of the research challenge and initiative 
representing lexicon and sentence with the subsymbolic approach using technique such a self organizing map som or artificial neural network ann is a relatively new but important research area in natural language processing the performance of this approach however is highly dependent on whether representation are well formed so that member within each cluster are corresponding to sentence or phrase of similar meaning despite the moderate success and the rapid advancement of contemporary computing power it is still difficult to establish an efficient learning method so that natural language can be represented in a way close to the benchmark exhibited by human being one of the major problem is due to the general lack of effective method s to encapsulate semantic information into quantitative expression or structure in this paper we propose to alleviate this problem with a novel technique based on tensor product representation and non linear compression the method is capable of encoding sentence into distributed representation that are closely associated with the semantic content being more comprehensible and analyzable from the perspective of human intelligence 
we show that a practical translation of mr description into normal dominance constraint is feasible we start from a recent theoretical translation and verify it assumption on the output of the english resource grammar erg on the redwood corpus the main assumption of the translation that all relevant underspecified description are net is validated for a large majority of case all non net computed by the erg seem to be systematically incomplete 
we consider the problem of learning to classify partially specified instance i e instance that are described in term of attribute value at different level of precision using user supplied attribute value taxonomy avt we formalize the problem of learning from avt and data and present an avt guided decision tree learning algorithm avt dtl to learn classification rule at multiple level of abstraction the proposed approach generalizes existing technique for dealing with missing value to handle instance with partially missing value we present experimental result that demonstrate that avt dtl is able to effectively learn robust high accuracy classifier from partially specified example our experiment also demonstrate that the use of avt dtl outperforms standard decision tree algorithm c and it variant when applied to data with missing attribute value and produce substantially more compact decision tree than those obtained by standard approach 
this paper proposes the application of finite state approximation technique on a unification based grammar of word formation for a language like german a refinement of an rtn based approximation algorithm is proposed which extends the state space of the automaton by selectively adding distinction based on the parsing history at the point of entering a context free rule the selection of history item exploit the specific linguistic nature of word formation a experiment show this algorithm avoids an explosion of the size of the automaton in the approximation construction 
we study the relationship among structural method for identifying and solving tractable class of constraint satisfaction problem csps in particular we first answer a long standing question about the notion of biconnected component applied to an optimal reduct of the dual constraint graph by showing that this notion is in fact equivalent to the hinge decomposition method then we give a precise characterization of the relationship between the treewidth notion applied to the hidden variable encoding of a csp and the same notion applied to some optimal reduct of the dual constraint graph finally we face the open problem of computing such an optimal reduct we provide an algorithm that output an approximation of an optimal tree decomposition and give a qualitative explanation of the difference between this graph based method and more general hypergraph based method 
in the last several year large olap database have become common in a variety of application such a corporate data warehouse and scientific computing to support interactive analysis many of these database are augmented with hierarchical structure that provide meaningful level of abstraction that can be leveraged by both the computer and analyst this hierarchical structure generates many challenge and opportunity in the design of system for the query analysis and visualization of these database in this paper we present an interactive visual exploration tool that facilitates exploratory analysis of data warehouse with rich hierarchical structure such a might be stored in data cube we base this tool on polaris a system for rapidly constructing table based graphical display of multidimensional database polaris build visualization using an algebraic formalism derived from the interface and interpreted a a set of query to a database we extend the user interface algebraic formalism and generation of data query in polaris to expose and take advantage of hierarchical structure in the resulting system analyst can navigate through the hierarchical projection of a database rapidly and incrementally generating visualization for each projection 
we propose a non linear canonical correlation analysis cca method which work by coordinating or aligning mixture of linear model in the same way that cca extends the idea of pca our work extends recent method for non linear dimensionality reduction to the case where multiple embeddings of the same underlying low dimensional coordinate are observed each lying on a different high dimensional manifold we also show that a special case of our method when applied to only a single manifold reduces to the laplacian eigenmaps algorithm a with previous alignment scheme once the mixture model have been estimated all of the parameter of our model can be estimated in closed form without local optimum in the learning experimental result illustrate the viability of the approach a a non linear extension of cca 
we describe a d layered representation for visual motion analysis the representation provides a global interpretation of image motion in term of several spatially localized foreground region along with a background region each of these region comprises a parametric shape model and a parametric motion model the representation also contains depth ordering so visibility and occlusion are rightly included in the estimation of the model parameter finally because the number of object their position shape and size and their relative depth are all unknown initial model are drawn from a proposal distribution and then compared using a penalized likelihood criterion this allows u to automatically initialize new model and to compare different depth ordering 
this work proposes a model for video retrieval based upon the inference network model the document network is constructed using video metadata encoded using mpeg and capture information pertaining to the structural aspect video breakdown into shot and scene conceptual aspect video scene and shot content and contextual aspect context information about the position of conceptual content within the document the retrieval process a exploit the distribution of evidence among the shot to perform ranking of different level of granularity b address the idea that evidence may be inherited during evaluation and c exploit the contextual information to perform constrained query 
abstract loopy belief propagation bp ha been successfully used in a numberof di cult graphical model to find the most probable configurationof the hidden variable in application ranging from proteinfolding to image analysis one would like to find not just the bestconfiguration but rather the top m while this problem ha beensolved using the junction tree formalism in many real world problemsthe clique size in the junction tree is prohibitively large inthis work we address the 
the hepatitis temporal database collected at chiba university hospital between wa recently given to challenge the kdd research the database is large where each patient corresponds to test represented a sequence of irregular timestamp point with different length this paper present a temporal abstraction approach to mining knowledge from this hepatitis database exploiting hepatitis background knowledge and data analysis we introduce new notion and method for abstracting short term changed and long term changed test the abstracted data allow u to apply different machine learning method for finding knowledge part of which is considered a new and interesting by medical doctor 
many autonomous system such a mobile robot uavs or spacecraft have limited resource capacity and move in dynamic environment performing on board mission planning and execution in such a context requires deliberative capability to generate plan achieving mission goal while respecting deadline and resource constraint a well a run time plan adaption mechanism during execution in this paper we propose a framework to integrate deliberative planning plan repair and execution control in a dynamic environment with stringent temporal constraint it is based on lifted partial order temporal planning technique which produce flexible plan and allow under certain condition discussed in the paper plan repair interleaved with plan execution this framework ha been implemented using the ixtet planner and used to control a robotic platform 
the sub symbolic approach on natural language processing nlp is one of the mainstream in artificial intelligence indeed we have plenty of algorithm for variation of nlp such a syntactic structure representation or lexicon classification theoretically the goal of these research is obviously for developing a hybrid architecture which can process natural language a what human doe thus we propose an online intelligent system to extract the semantics utterance interpretation by applying a layer back propagation neural network to classify the encoded syntactic structure into corresponding semantic frame type e g agent action patient the result are generated dynamically according to training set and user input in webpage form it can diminish the manipulating time while using extra tool and share the statistical result with colleague in clear and standard form 
this paper describes the gerona knowledge ontology and the way it support spoken dialogue tutoring of crisis decision making skill gerona allows domain ontology model to be encoded and reasoned over this paper describes how it ha been used to encode three type of tutoring model that are relevant to simulator based training an expert model a student critiquing model and a question answer model these model provide scot dc a spoken conversational tutor with domain specific knowledge needed for reflective tutoring 
in this thesis we document the result of our investigation of four important problem in the domain of telecommunication network routing and traffic engineering the technique we use involve stochastic learning and learning automaton la our first contribution consists of two efficient solution for maintaining single source shortest path routing tree in network where the weight of the link connecting the node of the network change continuously in a random manner following an unknown stochastic distribution in the second problem we were interested in maintaining shortest path between all pair of node in a dynamically changing network again for this problem we have proposed two la based efficient solution our third contribution wa in the area of qos routing and traffic engineering in network more specifically we have proposed an adaptive online routing algorithm that computes bandwidth guaranteed path in mpls based network using a random race based learning scheme that computes an optimal ordering of route the last problem that we studied wa that of designing routing scheme that would successfully operate in the presence of adversarial environment in mobile ad hoc network manet the need for fault tolerant routing protocol for manet wa identified recently by xue and nahrstedt and in our research we have proposed a new fault tolerant routing scheme that us a stochastic learning based weak estimation procedure all our proposed solution have been demonstrated to be superior to the state of the art 
for the discovery of similar pattern in d time series it is very typical to perform a normalization of the data for example a transformation so that the data follow a zero mean and unit standard deviation such transformation can reveal latent pattern and are very commonly used in datamining application however when dealing with multidimensional time series which appear naturally in application such a video tracking motion capture etc similar motion pattern can also be expressed at different orientation it is therefore imperative to provide support for additional transformation such a rotation in this work we transform the positional information of moving data into a space that is translation scale and rotation invariant our distance measure in the new space is able to detect elastic match and can be efficiently lower bounded thus being computationally tractable the proposed method are easy to implement fast to compute and can have many application for real world problem in area such a handwriting recognition and posture estimation in motion capture data finally we empirically demonstrate the accuracy and the efficiency of the technique using real and synthetic handwriting data 
we compute a common feature selection or kernel selection configuration for multiple support vector machine svms trained on different yet inter related datasets the method is advantageous when multiple classification task and differently labeled datasets exist over a common input space different datasets can mutually reinforce a common choice of representation or relevant feature for their various classifier we derive a multi task representation learning approach using the maximum entropy discrimination formalism the resulting convex algorithm maintain the global solution property of support vector machine however in addition to multiple svm classification regression parameter they also jointly estimate an optimal subset of feature or optimal combination of kernel experiment are shown on standardized datasets 
amilcare is a tool for adaptive information extraction ie designed for supporting active annotation of document for the semantic web sw it can be used either for unsupervised document annotation or a a support for human annotation amilcare is portable to new application domain without any knowledge of ie a it just requires user to annotate a small training corpus with the information to be extracted it is based on lp a supervised learning strategy for ie able to cope with different text type from newspaper like text to rigidly formatted web page and even a mixture of them adaptation start with the definition of a tag set for annotation possibly organized a an ontology then user have to manually annotate a small training corpus amilcare provides a default mouse based interface called melita where annotation are inserted by first selecting a tag from the ontology and then identifying the text area to annotate with the mouse differently from similar annotation tool melita actively support training corpus annotation while user annotate text amilcare run in the background learning how to reproduce the inserted annotation induced rule are silently applied to new text and their result are compared with the user annotation when it rule reach a user defined level of accuracy melita present new text with a preliminary annotation derived by the rule application in this case user have just to correct mistake and add missing annotation user correction are inputted back to the learner for retraining this technique focus the slow and expensive user activity on uncovered case avoiding requiring annotating case where a satisfying effectiveness is already reached moreover validating extracted information is a much simpler task than tagging bare text and also le error prone speeding up the process considerably at the end of the corpus annotation process the system is trained and the application can be delivered mnm and ontomat annotizer are two annotation tool adopting amilcare s learner in this demo we simulate the annotation of a small corpus and we show how and when amilcare is able to support user in the annotation process focusing on the way the user can control the tool s proactivity and intrusivity we will also quantify such support with data derived from a number of experiment on corpus we will focus on training corpus size and correctness of suggestion when the corpus is increased 
we describe an evaluation of result set filtering technique for providing ultra high precision in the task of presenting related news for general web query in this task the negative user experience generated by retrieving non relevant document ha a much worse impact than not retrieving relevant one we adapt cost based metric from the document filtering domain to this result filtering problem in order to explicitly examine the tradeoff between missing relevant document and retrieving non relevant one a large manual evaluation of three simple threshold filter show that the basic approach of counting matching title term outperforms also incorporating selected abstract term based on part of speech or higher level linguistic structure simultaneously leveraging these cost based metric allows u to explicitly determine what other task would benefit from these alternative technique 
principal component analysis pca ha been successfully applied to construct linear model of shape graylevel and motion in particular pca ha been widely used to model the variation in the appearance of people s face we extend previous work on facial modeling for tracking face in video sequence a they undergo significant change due to facial expression here we develop person specific facial appearance model psfam which use modular pca to model complex intra person appearance change such model require aligned visual training data in previous work this ha involved a time consuming and errorprone hand alignment and cropping process instead we introduce parameterized component analysis to learn a subspace that is invariant to affine or higher order geometric transformation the automatic learning of a psfam given a training image sequence is posed a a continuous optimization problem and is solved with a mixture of stochastic and deterministic technique achieving sub pixel accuracy we illustrate the use of the d psfam model with several application including video conferencing realistic avatar animation and eye tracking 
this paper present a new hybrid method for solving constraint optimization problem in anytime context discrete optimization problem are modelled a valued csp our method vns lds cp combine a variable neighborhood search and limited discrepancy search with constraint propagation to efficiently guide the search experiment on the celar benchmark demonstrate significant improvement over other competing method vns lds cp ha been successfully applied to solve a real life anytime resource allocation problem in computer network 
it ha been demonstrated that basic aspect of human visual motion perception are qualitatively consistent with a bayesian estimation framework where the prior probability distribution on velocity favor slow speed here we present a refined probabilistic model that can account for the typical trial to trialvariabilities observedin psychophysicalspeed perception experiment we also show that data from such experiment can be used to constrain both the likelihood and prior function of the model specifically we measured matching speed and threshold in a two alternative forced choice speed discrimination task parametric fit to the data reveal that the likelihood function is well approximated by a lognormal distribution with a characteristic contrast dependent variance and that the prior distribution on velocity exhibit significantly heavier tail than a gaussian and approximately follows a power law function 
we present a technique for performing the tracking stage of optical motion capture which retains at each time frame multiple marker association hypothesis and estimate of the subject s position central to this technique are the equation for calculating the likelihood of a sequence of association hypothesis which we develop using a bayesian approach the system is able to perform motion capture using fewer camera and a lower frame rate than ha been used previously and doe not require the assistance of a human operator we conclude by demonstrating the tracker on real data and provide an example in which our technique is able to correctly determine all marker association and standard tracking technique fail 
in this paper we have designed and experimented novel convolution kernel for automatic classification of predicate argument their main property is the ability to process structured representation support vector machine svms using a combination of such kernel and the flat feature kernel classify prop bank predicate argument with accuracy higher than the current argument classification state of the art additionally experiment on framenet data have shown that svms are appealing for the classification of semantic role even if the proposed kernel do not produce any improvement 
this paper report on theoretical investigation about the assumption underlying the inverse document frequency idf we show that an intuitive idf based probability function for the probability of a term being informative assumes disjoint document event by assuming document to be independent rather than disjoint we arrive at a poisson based probability of being informative the framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval model 
learning general functional dependency is one of the main goal in machine learning recent progress in kernel based method ha focused on designing flexible and powerful input representation this paper address the complementary issue of problem involving complex output such a multiple dependent output variable and structured output space we propose to generalize multiclass support vector machine learning in a formulation that involves feature extracted jointly from input and output the resulting optimization problem is solved efficiently by a cutting plane algorithm that exploit the sparseness and structural decomposition of the problem we demonstrate the versatility and effectiveness of our method on problem ranging from supervised grammar learning and named entity recognition to taxonomic text classification and sequence alignment 
multi view algorithm reduce the amount of required training data by partitioning the domain feature into separate subset or view that are sufficient to learn the target concept such algorithm rely on the assumption that the view are sufficiently compatible for multi view learning i e most example are labeled identically in all view in practice it is unclear whether or not two view are sufficiently compatible for solving a new unseen learning task in order to cope with this problem we introduce a view validation algorithm given a learning task the algorithm predicts whether or not the view are sufficiently compatible for solving that particular task we use information acquired while solving several exemplar learning task to train a classifier that discriminates between the task for which the view are sufficiently and insufficiently compatible for multi view learning our experiment on wrapper induction and text classification show that view validation requires only a modest amount of training data to make high accuracy prediction 
we focus on the problem of efficient learning of dependency tree it is well known that given the pairwise mutual information coefficient a minimum weight spanning tree algorithm solves this problem exactly and in polynomial time however for large data set it is the construction of the correlation matrix that dominates the running time we have developed a new spanning tree algorithm which is capable of exploiting partial knowledge about edge weight the partial knowledge we maintain is a probabilistic confidence interval on the coefficient which we derive by examining just a small sample of the data the algorithm is able to flag the need to shrink an interval which translates to inspection of more data for the particular attribute pair experimental result show running time that is near constant in the number of record without significant loss in accuracy of the generated tree interestingly our spanning tree algorithm is based solely on tarjan s red edge rule which is generally considered a guaranteed recipe for bad performance 
mining frequent structural pattern from graph database is an interesting problem with broad application most of the previous study focus on pruning unfruitful search subspace effectively but few of them address the mining on large disk based database a many graph database in application cannot be held into main memory scalable mining of large disk based graph database remains a challenging problem in this paper we develop an effective index structure adi for adjacency index to support mining various graph pattern over large database that cannot be held into main memory the index is simple and efficient to build moreover the new index structure can be easily adopted in various existing graph pattern mining algorithm a an example we adapt the well known gspan algorithm by using the adi structure the experimental result show that the new index structure enables the scalable graph pattern mining over large database in one set of the experiment the new disk based method can mine graph database with one million graph while the original gspan algorithm can only handle database of up to thousand graph moreover our new method is faster than gspan when both can run in main memory 
recently a widespread interest ha emerged in using ontology on the web resource description framework schema rdfs is a basic tool that enables user to define vocabulary structure and constraint for expressing meta data about web resource however it includes no provision for formal semantics and it expressivity is not sufficient for full fledged ontological modeling and reasoning in this paper we will show how rdfs can be extended to include a more expressive knowledge representation language that in turn would enrich it with the required additional expressivity and the semantics of that language we do this by describing the ontology language ontology inference layer oil a an extension of rdfs an important advantage to our approach is that it ensures maximal sharing of meta data on the web even partial interpretation of an oil ontology by le semantically aware processor will yield a correct partial interpretation of the meta data 
feature space analysis is the main module in many computer visiontasks the most popular technique k mean clustering however hastwo inherent limitation the cluster are constrained to bespherically symmetric and their number ha to be known a priori innonparametric clustering method like the one based on mean shift these limitation are eliminated but the amount of computationbecomes prohibitively large a the dimension of the spaceincreases we exploit a recently proposed approximation technique locality sensitive hashing lsh to reduce the computationalcomplexity of adaptive mean shift in our implementation of lsh theoptimal parameter of the data structure are determined by a pilotlearning procedure and the partition are data driven a anapplication the performance of mode and k mean based textons arecompared in a texture classification study 
chemoinformatics is the generic name for the technique used to represent store and process information about the two dimensional d and three dimensional d structure of chemical molecule chemoinformatics ha attracted much recent prominence a a result of development in the method that are used to synthesize new molecule and then to test them for biological activity these development have resulted in a massive increase in the amount of structural and biological information that is available to support discovery programme in the pharmaceutical and agrochemical industry chemoinformatics may appear to be far removed from information retrieval ir and there are indeed many significant difference most notably in the use of graph representation to encode chemical molecule rather than the string that are used to encode text however there are also many similarity between the two field and this paper will exemplify some of these relationship the most obvious area of similarity is in the principal type of database search that are carried out with both application domain making extensive use of exact match partial match and best match searching procedure in the ir context these are known item searching boolean searching and ranked output searching in the chemical context these are structure searching substructure searching and similarity searching in ir there is a natural distinction between an initial ranked output search and one in which relevance feedback can be employed where the keywords in the query statement are assigned weight based on their differential occurrence in known relevant and known nonrelevant document in the chemoinformatics technique called substructural analysis substructural fragment are assigned weight based on their occurrence in molecule that do posse and molecule that do not posse some desired biological activity the analogy between relevance and biological activity ha also resulted in the development of measure to quantify the effectiveness of chemical searching procedure that are based on the standard ir concept of recall and precision analogy such a these have provided the basis for some of the chemoinformatics research carried out in sheffield the starting point wa the recognition that technique applicable to document represented by keywords might also be applicable to molecule represented by substructural fragment this led directly to the introduction of similarity searching something that is now a standard tool in chemoinformatics software system in particular it use for virtual screening i e the ranking of a database in order of decreasing probability of activity so a to maximize the cost effectiveness of biological testing measure of inter molecular structural similarity also lie at the heart of system for clustering chemical database just a ir ha the cluster hypothesis similar document tend to be relevant to the same request a a basis for document clustering so the similar property principle similar molecule tend to have similar property ha led to clustering becoming a well established tool for the organization of large chemical database more recently we have applied another ir technique the use of data fusion to combine different ranking of a database to chemoinformatics and again found that it is equally applicable in this new domain the many similarity between ir and chemoinformatics that have already been identified suggest that chemoinformatics is a domain of which ir researcher should be aware when considering the applicability of new technique that they have developed 
we present a major variant of the graphplan algorithm that employ available memory to transform the depth first nature of graphplan s search into an iterative state space view in which heuristic can be used to traverse the search space when the planner pegg is set to conduct exhaustive search it produce guaranteed optimal parallel plan to time faster than a version of graphplan enhanced with csp speedup method by heuristically pruning this search space pegg produce plan comparable to graphplan s in makespan at speed approaching state of the art heuristic serial planner 
we derive the limiting form of the eigenvalue spectrum for sample covariance matrix produced from non isotropic data for the analysis of standard pca we study the case where the data ha increased variance along a small number of symmetry breaking direction the limiting form of the eigenvalue spectrum depends on the strength of the symmetry breaking signal and on a parameter which is the ratio of sample size to data dimension result are derived in the limit of large data dimension while keeping fixed a increase there are transition in which delta function emerge from the upper end of the bulk spectrum corresponding to the symmetry breaking direction in the data and we calculate the bias in the corresponding eigenvalue for kernel pca the covariance matrix in feature space may contain symmetrybreaking structure even when the data component are independently distributed with equal variance we show example of phase transition behaviour analogous to the pca result in this case 
many different metric are used in machine learning and data mining to build and evaluate model however there is no general theory of machine learning metric that could answer question such a when we simultaneously want to optimise two criterion how can or should they be traded off some metric are inherently independent of class and misclassification cost distribution while other are not can this be made more precise this paper provides a derivation of roc space from first principle through d roc space and the skew ratio and redefines metric in these dimension the paper demonstrates that the graphical depiction of machine learning metric by mean of roc isometric give many useful insight into the characteristic of these metric and provides a foundation on which a theory of machine learning metric can be built 
the problem of inferring haplotype from genotype of single nucleotide polymorphism snp is essential for the understanding of genetic variation within and among population with important application to the genetic analysis of disease propensity and other complex trait the problem can be formulated a a mixture model where the mixture component correspond to the pool of haplotype in the population the size of this pool is unknown indeed knowing the size of the pool would correspond to knowing something significant about the genome and it history thus method for fitting the genotype mixture must crucially address the problem of estimating a mixture with an unknown number of mixture component in this paper we present a bayesian approach to this problem based on a nonparametric prior known a the dirichlet process the model also incorporates a likelihood that capture statistical error in the haplotype genotype relationship we apply our approach to the analysis of both simulated and real genotype data and compare to extant method 
a novel maximal figure of merit mfom learning approach to text categorization is proposed different from the conventional technique the proposed mfom method attempt to integrate any performance metric of interest e g accuracy recall precision or f measure into the design of any classifier the corresponding classifier parameter are learned by optimizing an overall objective function of interest to solve this highly nonlinear optimization problem we use a generalized probabilistic descent algorithm the mfom learning framework is evaluated on the reuters task with lsi based feature extraction and a binary tree classifier experimental result indicate that the mfom classifier give improved f and enhanced robustness over the conventional one it also outperforms the popular svm method in micro averaging f other extension to design discriminative multiple category mfom classifier for application scenario with new performance metric could be envisioned too 
photometric stereo algorithm use a lambertian reflectance model with a varying albedo field and involve the appearance of only one object this paper extends photometric stereo algorithm to handle all the appearance of all the object in a class in particular the class of human face similarity among all facial appearance motivates a rank constraint on the albedo and surface normal in the class this lead to a factorization of an observation matrix that consists of exemplar image of different object under different illumination which is beyond what can be analyzed using bilinear analysis bilinear analysis requires exemplar image of different object under same illumination to fully recover the class specific albedo and surface normal integrability and face symmetry constraint are employed the proposed linear algorithm take into account the effect of the varying albedo field by approximating the integrability term using only the surface normal a an application face recognition under illumination variation is presented the rank constraint enables an algorithm to separate the illumination source from the observed appearance and keep the illuminant invariant information that is appropriate for recognition good recognition result have been obtained using the pie dataset 
a given entity representing a person a location or an organization may be mentioned in text in multiple ambiguous way understanding natural language requires identifying whether different mention of a name within and across document represent the same entity we present two machine learning approach to this problem which we call the robust reading problem our first approach is a discriminative approach trained in a supervised way our second approach is a generative model at the heart of which is a view on how document are generated and how name of different entity type are sprinkled into them in it most general form our model assumes a joint distribution over entity e g a document that mention president kennedy is more likely to mention oswald or white house than roger clemens an author model that assumes that at least one mention of an entity in a document is easily identifiable and then generates other mention via an appearance model governing how mention are transformed from tile representative mention we show that both approach perform very accurately in the range of f measure for different entity type much better than previous approach to some aspect of this problem our extensive experiment exhibit the contribution of relational and structural feature and somewhat surprisingly that the assumption made within our generative model are strong enough to yield a very powerful approach that performs better than a supervised approach with limited supervised information 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
distance based method in pattern recognition and machine learning have to rely on a similarity or dissimilarity measure between pattern in the input space for many application euclidean distance in the input space is not a good choice and hence more complicated distance metric have to be used in this paper we propose a parametric method for metric learning based on class label information we first define a dissimilarity measure that can be proved to be metric it ha the favorable property that between class dissimilarity is always larger than within class dissimilarity we then perform parametric learning to find a regression mapping from the input space to a feature space such that the dissimilarity between pattern in the input space is approximated by the euclidean distance between point in the feature space parametric learning is performed using the iterative majorization algorithm experimental result on real world benchmark data set show that this approach is promising 
this paper present an unsupervised learning approach to building a non english arabic stemmer the stemming model is based on statistical machine translation and it us an english stemmer and a small k sentence parallel corpus a it sole training resource no parallel text is needed after the training phase monolingual unannotated text can be used to further improve the stemmer by allowing it to adapt to a desired domain or genre example and result will be given for arabic but the approach is applicable to any language that need affix removal our resource frugal approach result in agreement with a state of the art proprietary arabic stemmer built using rule affix list and human annotated text in addition to an unsupervised component task based evaluation using arabic information retrieval indicates an improvement of in average precision over unstemmed text and of the performance of the proprietary stemmer above 
this paper illustrates how a multi agent system implement and governs a computational linguistic model of phonology for syllable recognition we describe how the time map model can be recast a a multi agent architecture and discus how constraint relaxation output extrapolation parse tree pruning clever task allocation and distributed processing are all achieved in this new architcture 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
detecting the dominant normal direction to the decisionsurface is an established technique for feature selectionin high dimensional classification problem several approacheshave been proposed to render this strategy moreamenable to practice but they still show a number of importantshortcomings from a pragmatic point of view this paperintroduces a novel such approach which combine thenormal direction idea with support vector machine classifier the two make a natural and powerful match a svsare located nearby and fully describe the decision surface the approach can be included elegantly into the training ofperformant classifier from extensive datasets the potentialis corroborated by experiment both on synthetic andreal data the latter on a face detection experiment in thisexperiment we demonstrate how our approach can lead to asignificant reduction of cpu time with neglectable loss ofclassification performance 
analyzing gene expression data from microarray device ha many important application in medicine and biology but present significant challenge to data mining microarray data typically ha many attribute gene and few example sample making the process of correctly analyzing such data difficult to formulate and prone to common mistake for this reason it is unusually important to capture and record good practice for this form of data mining this paper present a process for analyzing microarray data including pre processing gene selection randomization testing classification and clustering this process is captured with clementine application template the paper describes the process in detail and includes three case study showing how the process is applied to class classification multi class classification and clustering analysis for publicly available microarray datasets 
most information retrieval technology are designed to facilitate information discovery however much knowledge work involves finding and re using previously seen information we describe the design and evaluation of a system called stuff i ve seen si that facilitates information re use this is accomplished in two way first the system provides a unified index of information that a person ha seen whether it wa seen a email web page document appointment etc second because the information ha been seen before rich contextual cue can be used in the search interface the system ha been used internally by more than employee we report on both qualitative and quantitative aspect of system use initial finding show that time and people are important retrieval cue user find information more easily using si and use other search tool le frequently after installation 
we present a new approach to intrinsic summary evaluation based on initial experiment in van halteren and teufel which combine two novel aspect comparison of information content rather than string similarity in gold standard and system summary measured in shared atomic information unit which we call factoid and comparison to more than one gold standard summary in our data and summary respectively in this paper we show that factoid annotation is highly reproducible introduce a weighted factoid score estimate how many summary are required for stable system ranking and show that the factoid score cannot be su ciently approximated by unigrams and the duc information overlap measure 
an approach to semi supervised learning is proposed that is based on a gaussian random field model labeled and unlabeled data are represented a vertex in a weighted graph with edge weight encoding the similarity between instance the learning problem is then formulated in term of a gaussian random field on this graph where the mean of the field is characterized in term of harmonic function and is efficiently obtained using matrix method or belief propagation the resulting learning algorithm have intimate connection with random walk electric network and spectral graph theory we discus method to incorporate class prior and the prediction of classifier obtained by supervised learning we also propose a method of parameter learning by entropy minimization and show the algorithm s ability to perform feature selection promising experimental result are presented for synthetic data digit classification and text classification task 
this paper present a novel approach for landmark basedshape deformation in which fitting error and shapedifference are formulated into a support vector machine svm regression problem to well describe nonrigid shapedeformation this paper measure the shape difference usinga thin plate spline model the proposed approach iscapable of preserving the topology of the template shape inthe deformation this property is achieved by inserting aset of additional point and imposing a set of linear equalityand or inequality constraint the underlying optimizationproblem is solved using a quadratic programming algorithm the proposed method ha been tested using practicaldata in the context of shape based image segmentation some relevant practical issue such a missing detectedlandmarks and selection of the regularization parameter arealso briefly discussed 
one challenge for developing spoken dialogue system in multiple domain is facilitating system component communication using a shared domain ontology since each domain come with it own set of concept and action relevant to the application adapting a system to a new domain requires customizing component to use the ontological representation required for that domain our research in multiple domain development ha highlighted difference in the ontological need of a generalpurpose language interface and a task specific reasoning application although different domain application have their own ontology many aspect of spoken dialogue interaction are common across domain in this paper we present a new method of customizing a broad coverage parser to different domain by maintaining two ontology one that is generalized for language representation and another that is customized to the domain and defining mapping between them in this way we preserve the broad coverage language component across domain a well a produce semantic representation that are optimally suited to the domain reasoner 
web search engine provide a large scale text document retrieval service by processing huge inverted file index inverted file index allow fast query resolution and good memory utilization since their d gap representation can be effectively and efficiently compressed by using variable length encoding method this paper proposes and evaluates some algorithm aimed to find an assignment of the document identifier which minimizes the average value of d gap thus enhancing the effectiveness of traditional compression method we ran several test over the google contest collection in order to validate the technique proposed the experiment demonstrated the scalability and effectiveness of our algorithm using the proposed algorithm we were able to sensibly improve up to the compression ratio of several encoding scheme 
this paper present a multi scale generative model for representinganimate shape and extracting meaningful part of object themodel assumes that animate shape d simple closed curve areformed by a linear superposition of a number of shape base theseshapebases resemble the multi scale gabor base in image pyramidrepresentation are well localized in both spatial and frequencydomains and form an over complete dictionary this model issimpler than the popular b spline representation since it doe notengage a domainpartition thus it eliminates the interferencebetween adjacent b spline base and becomes a true linear additivemodel we pursue the base by reconstructing the shape in acoarse to fine procedure through curve evolution these shape basesare further organized ina tree structure where the base in eachsubtree sum up to an intuitive part of the object to buildprobabilistic model for a class of object we propose a markovrandom field model at each level of the tree representation toaccount for the spatial relationship between base thus the finalmodel integrates a markov tree generative model over scale and amarkov random field over space we adopt em type algorithm forlearning the meaningful part for a shape class and show someresults on shape synthesis 
in response to attack against enterprise network administrator increasingly deploy intrusion detection system these system monitor host network and other resource for sign of security violation the use of intrusion detection ha given rise to another difficult problem namely the handling of a generally large number of alarm in this paper we mine historical alarm to learn how future alarm can be handled more efficiently first we investigate episode rule with respect to their suitability in this approach we report the difficulty encountered and the unexpected insight gained in addition we introduce a new conceptual clustering technique and use it in extensive experiment with real world data to show that intrusion detection alarm can be handled efficiently by using previously mined knowledge 
this poster report upon the ongoing effort being made to establish trec like and other comprehensive evaluation paradigm within the music ir mir and music digital library mdl research community the proposed research task are based upon expert opinion garnered from member of the information retrieval ir mdl and mir community with regard to the construction and implementation of scientifically valid evaluation framework 
an approach for model free markerless motion capture of human is presented this approach is centered on generating underlying nonlinear ax or a skeleton curve from a volume of a human subject human volume are captured from multiple calibrated camera we describe the use of skeleton curve for determining the kinematic posture of a human captured volume our motion capture us a skeleton curve found in each frame of a volume sequence to automatically produce kinematic motion we apply several type of motion to our capture approach and use the result to actuate a dynamically simulated humanoid robot 
future manned space operation are expected to include a greater use of automation cooke and hine this automation will function without human intervention most of the time however human will be required to supervise the automation and they must be on call to respond to anomaly or to perform related task that are not easily automated in such an environment human perform other task most of the time and their interaction with the automation may be remote and asynchronous a automation becomes more prevalent better support for such interaction is needed the distributed collaboration and interaction dci environment being developed at nasa investigates the use of software agent to assist human in this type of remote distributed space operation the dci approach ha been applied for use by control engineer at the johnson space center jsc who are investigating advanced technology for life support such a the water recovery system or wrs schreckenghost et al the wrs recycles wastewater through biological and chemical process to remove impurity and produce potable water managed by an autonomous control program called t bonasso et al the wrs ran unattended in a continuous integrated test from january through april bonasso et al wrs control engineer periodically monitored for network hardware or power failure from remote location while spending the majority of their time carrying out their daily task on unrelated project the current prototype of the dci environment us a simulation of the wrs t system for both demonstration and continuing development the dci implementation creates an environment in which human and the t control automation together form an integrated team to ensure efficient effective operation of the wrs 
we applied statistical mechanic to an inverse problem of linear mapping to investigate the physic of optimal lossy compression we used the replica symmetry breaking technique with a toy model to demonstrate shannon s result the rate distortion function which is widely known a the theoretical limit of the compression with a fidelity criterion is derived numerical study show that sparse construction of the model provide suboptimal compression 
in the task of adaptive information filtering a system receives a stream of document but delivers only those that match a person s information need a the system filter it also refines it knowledge about the user s information need based on relevance feedback from the user delivering a document thus ha two effect i it satisfies the user s information need immediately and ii it help the system better satisfy the user in the future by improving it model of the user s information need the traditional approach to adaptive information filtering fails to recognize and model this second effect this paper proposes utility divergence a the measure of model quality unlike the model quality measure used in most active learning method utility divergence is represented on the same scale a the filtering system s target utility function thus it is meaningful to combine the expected immediate utility with the model quality and to quantitatively manage the trade off between exploitation and exploration the proposed algorithm is implemented for setting the filtering system s dissemination threshold a major problem for adaptive filtering system experiment with trec and trec filtering data demonstrate that the proposed method is effective 
in this paper we overcome a major drawback of the levelset framework the lack of point correspondence we maintainexplicit backward correspondence from the evolvinginterface to the initial one by advecting the initial point coordinateswith the same speed a the level set function ourmethod lead to a system of coupled eulerian partial differentialequations we show in a variety of numerical experimentsthat it can handle both normal and tangential velocity large deformation shock rarefaction and topologicalchanges application are many in computer vision andelsewhere since our method can upgrade virtually any levelset evolution we complement our work with the design ofnon zero tangential velocity that preserve the relative areaof interface patch this feature may be crucial in such applicationsas computational geometry grid generation orunfolding of the organ surface e g brain in medicalimaging 
the paper study machine learning problem where each example is described using a set of boolean feature and where hypothesis are represented by linear threshold element one method of increasing the expressiveness of learned hypothesis in this context is to expand the feature set to include conjunction of basic feature this can be done explicitly or where possible by using a kernel function focusing on the well known perceptron and winnow algorithm the paper demonstrates a tradeofi between the computational e ciency with which the algorithm can be run over the expanded feature space and the generalization ability of the corresponding learning algorithm we flrst describe several kernel function which capture either limited form of conjunction or all conjunction we show that these kernel can be used to e ciently run the perceptron algorithm over a feature space of exponentially many conjunction however we also show that using such kernel the perceptron algorithm can provably make an exponential number of mistake even when learning simple function we then consider the question of whether kernel function can analogously be used to run the multiplicative update winnow algorithm over an expanded feature space of exponentially many conjunction known upper bound imply that the winnow algorithm can learn disjunctive normal form dnf formula with a polynomial mistake bound in this setting however we prove that it is computationally hard to simulate winnow s behavior for learning dnf over such a feature set this implies that the kernel function which correspond to running winnow for this problem are not e ciently computable and that there is no general construction that can run winnow with kernel 
data cube pre computation is an important concept for supporting olap online analytical processing and ha been studied extensively it is often not feasible to compute a complete data cube due to the huge storage requirement recently proposed quotient cube addressed this issue through a partitioning method that group cube cell into equivalence partition such an approach is not only useful for distributive aggregate function such a sum but can also be applied to the holistic aggregate function like median maintaining a data cube for holistic aggregation is a hard problem since it difficulty lie in the fact that history tuple value must be kept in order to compute the new aggregate when tuples are inserted or deleted the quotient cube make the problem harder since we also need to maintain the equivalence class in this paper we introduce two technique called addset data structure and sliding window to deal with this problem we develop efficient algorithm for maintaining a quotient cube with holistic aggregation function that take up reasonably small storage space performance study show that our algorithm are effective efficient and scalable over large database 
in this paper we discus the role of emotion in ai and possible way to determine their utility for the design of artificial agent we propose a research methodology for determining the utility of emotional control and apply it to the study of autonomous agent that compete for resource in an artificial life environment the result show that the emotional control can improve performance in some circumstance 
we analyze the formal grounding behind negative correlation nc learning an ensemble learning technique developed in the evolutionary computation literature we show that by removing an assumption made in the original work nc can be seen to be exploiting the well known ambiguity decomposition of the ensemble error grounding it in a statistic framework around the biasvariance decomposition we use this grounding to find bound for the parameter and provide insight into the behaviour of the optimal parameter value these observation allow u understand how nc relates to other algorithm identifying a group of paper spread over the last decade that have all exploited the ambiguity decomposition for machine learning problem when taking into account our new understanding of the algorithm significant reduction in error rate were observed in empirical test 
we give a fast rejection scheme that is based on image segment and demonstrate it on the canonical example of face detection however instead of focusing on the detection step we focus on the rejection step and show that our method is simple and fast to be learned thus making it an excellent pre processing step to accelerate standard machine learning classifier such a neural network bayes classifier or svm we decompose a collection of face image into region of pixel with similar behavior over the image set the relationship between the mean and variance of image segment are used to form a cascade of rejectors that can reject over of image patch thus only a small fraction of the image patch must be passed to a full scale classifier moreover the training time for our method is much le than an hour on a standard pc the shape of the feature i e image segment we use is data driven they are very cheap to compute and they form a very low dimensional feature space in which exhaustive search for the best feature is tractable 
this paperpresentsan adaptivediscriminativegenerativemodelthat generalizes the conventional fisher linear discriminant algorithm and render a proper probabilistic interpretation within the context of object tracking we aim to find a discriminative generative model that best separate the target from the background we present a computationally efficient algorithm to constantly update this discriminative model a time progress while most tracking algorithm operate on the premise that the objectappearanceorambientlightingconditiondoesnot significantly change a time progress our method adapts a discriminative generative model to reflect appearance variation of the target and background thereby facilitating the tracking task in ever changingenvironments numerous experiment show that our method is able to learn a discriminative generative model for tracking target object undergoing large pose and lighting change 
continuous query are query for which response given to user must be continuously updated a the source of interest get updated such query occur for instance during on line decision making e g traffic flow control weather monitoring etc the problem of keeping the response current reduces to the problem of deciding how often to visit a source to determine if and how it ha been modified in order to update earlier response accordingly on the surface this seems to be similar to the crawling problem since crawler attempt to keep index up to date a page change and user pose search query we show that this is not the case both due to the inherent difference between the nature of the two problem a well a the performance metric we propose develop and evaluate a novel multi phase continuous adaptive monitoring cam solution to the problem of maintaining the currency of query result some of the important phase are the tracking phase in which change to an initially identified set of relevant page are tracked from the observed change characteristic of these page a probabilistic model of their change behavior is formulated and weight are assigned to page to denote their importance for the current query during the next phase the resource allocation phase based on these statistic resource needed to continuously monitor these page for change are allocated given these resource allocation the scheduling phase produce an optimal achievable schedule for the monitoring task an experimental evaluation of our approach compared to prior approach for crawling dynamic web page show the effectiveness of cam for monitoring dynamic change for example by monitoring just of the page change cam is able to return of the changed information to the user the experiment also produce some interesting observation pertaining to the difference between the two problem of crawling to build an index and the problem of change tracking to respond to continuous query 
secure multiparty computation allows party to jointly compute a function of their private input without revealing anything but the output theoretical result provide a general construction of such protocol for any function protocol obtained in this way are however inefficient and thus practically speaking useless when a large number of participant are involved the contribution of this paper is to define a new privacy model k privacy by mean of an innovative yet natural generalization of the accepted trusted third party model this allows implementing cryptographically secure efficient primitive for real world large scale distributed system a an example for the usefulness of the proposed model we employ k privacy to introduce a technique for obtaining knowledge by way of an association rule mining algorithm from large scale data grid while ensuring that the privacy is cryptographically secure 
a mobile computing and communication have become popular predictive text entry system have become an increasingly important technology existing method still need refinement though with respect to personalization especially how to acquire vocabulary not pre registered in the system dictionary in this paper we report on an automatic method that dynamically obtains a user specific vocabulary from the user s unanalyzed document when a user make an entry the system dynamically extract the corresponding chunk from the user text and suggests them along with word suggested by the dictionary with our method text in a particular style or concerning a specific domain can be entered using a predictive text entry system we verified that a large amount of word not registered in the dictionary can be entered using our method 
loopybeliefpropagation bp hasbeensuccessfullyusedinanumberofdi cultgraphicalmodelstoflndthemostprobableconflgurationofthehiddenvariables inapplicationsrangingfromprotein folding to image analysis one would like to flnd not just the best conflguration but rather the top m while this problem ha been solvedusingthejunctiontreeformalism inmanyrealworldproblems the clique size in the junction tree is prohibitively large in thisworkweaddresstheproblemofflndingthe m bestconflgurations when exact inference is impossible westartbydevelopinganewexactinferencealgorithmforcalculating the best conflgurations that us only max marginals for approximateinference wereplacethemax marginalswiththebeliefs calculatedusingmax productbpandgeneralizedbp weshowempiricallythatthealgorithmcanaccuratelyandrapidlyapproximate the m best conflgurations in graph with hundredsof variable 
privacy and security concern can prevent sharing of data derailing data mining project distributed knowledge discovery if done correctly can alleviate this problem the key is to obtain valid result while providing guarantee on the non disclosure of data we present a method for k mean clustering when different site contain different attribute for a common set of entity each site learns the cluster of each entity but learns nothing about the attribute at other site 
an essential step in understanding the function of sensory nervous system is to characterize a accurately a possible the stimulus response function srf of the neuron that relay and process sensory information one increasingly common experimental approach is to present a rapidly varying complex stimulus to the animal while recording the response of one or more neuron and then to directly estimate a functional transformation of the input that account for the neuronal firing the estimation technique usually employed such a wiener filtering or other correlation based estimation of the wiener or volterra kernel are equivalent to maximum likelihood estimation in a gaussian output noise regression model we explore the use of bayesian evidence optimization technique to condition these estimate we show that by learning hyperparameters that control the smoothness and sparsity of the transfer function it is possible to improve dramatically the quality of srf estimate a measured by their success in predicting response to novel input 
reactive planning using assumption is a well known approach to tackle complex planning problem for nondeterministic partially observable domain however assumption may be wrong this may cause an assumption based plan to fail in general it is not possible to decide at runtime whether an assumption ha failed and is putting at danger the success of the plan thus plan execution ha to be controlled taking into account every possible success endangering assumption failure the possibility of tracing such failure strongly depends on the action performed by the plan in this paper focusing on a simple assumption language we provide two main contribution first we formally characterize safe assumption based plan i e plan that not only succeed whenever the assumption hold but also guarantee that any success endangering assumption failure is traced by a suitable monitor in this way replanning may be triggered only when actually needed second we extend the planner in a reactive platform in order to produce safe assumption based plan we experimentally show that safe assumption based re planning is a good alternative to it unsafe counterpart minimizing the need for replanning while retaining the efficiency in plan generation 
the ability to determine what day to day activity such a cooking pasta taking a pill or watching a video a person is performing is of interest in many application domain a system that can do this requires model of the activity of interest but model construction doe not scale well human must specify low level detail such a segmentation and feature selection of sensor data and high level structure such a spatio temporal relation between state of the model for each and every activity a a result previous practical activity recognition system have been content to model a tiny fraction of the thousand of human activity that are potentially useful to detect in this paper we present an approach to sensing and modeling activity that scale to a much larger class of activity than before we show how a new class of sensor based on radio frequency identification rfid tag can directly yield semantic term that describe the state of the physical world these sensor allow u to formulate activity model by translating labeled activity such a cooking pasta into probabilistic collection of object term such a pot given this view of activity model a text translation we show how to mine definition of activity in an unsupervised manner from the web we have used our technique to mine definition for over activity we experimentally validate our approach using data gathered from actual human activity a well a simulated data 
a generative probabilistic model for object in image is presented an object consists of a constellation of feature feature appearance and pose are modeled probabilistically scene image are generated by drawing a set of object from a given database with random clutter sprinkled on the remaining image surface occlusion is allowed we study the case where feature from the same object share a common reference frame moreover parameter for shape and appearance density are shared across feature this is to be contrasted with previous work on probabilistic constellation model where feature depend on each other and each feature and model have different pose and appearance statistic these two difference allow u to build model containing hundred of feature a well a to train each model from a single example our model may also be thought of a a probabilistic revisitation of lowe s model we propose an efficient entropy minimization inference algorithm that construct the best interpretation of a scene a a collection of object and clutter we test our idea with experiment on two image database we compare with lowe s algorithm and demonstrate better performance in particular in presence of large amount of background clutter 
the goal of this work is to accurately detect and localize boundary in natural scene using local image measurement we formulate feature that respond to characteristic change in brightness and texture associated with natural boundary in order to combine the information from these feature in an optimal way a classifier is trained using human labeled image a ground truth we present precision recall curve showing that the resulting detector outperforms existing approach 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
in this paper we propose a new type of web browser called the comparative web browser cwb which concurrently present multiple web page in a way that enables the content of the web page to be automatically synchronized the ability to view multiple web page at one time is useful when we wish to make a comparison on the web such a when we compare similar product or news article from different newspaper the cwb is characterized by automatic content based retrieval of passage from another web page based on a passage of the web page the user is reading and automatic transformation of a user s behavior scrolling clicking or moving backward or forward on a web page into a series of behavior on the other web page the cwb try to concurrently present similar passage from different web page and for this purpose our cwb automatically navigates web page that contain passage similar to those of the initial web page furthermore we propose an enhancement to the cwb which enables it to use linkage information to find related document based on link structure 
this paper is concerned with transductive learning although transduction appears to be an easier task than induction there have not been many provably useful algorithm and bound for transduction we present explicit error bound for transduction and derive a general technique for devising bound within this setting the technique is applied to derive error bound for compression scheme such a transductive svms and for transduction algorithm based on clustering 
reinforcement learning deal with learning optimal or near optimal policy while interacting with the environment application domain with many continuous variable are difficult to solve with existing reinforcement learning method due to the large search space in this paper we use a relational representation to define powerful abstraction that allow u to incorporate domain knowledge and re use previously learned policy in other similar problem we also describe how to learn useful action from human trace using a behavioural cloning approach combined with an exploration phase since several conflicting action may be induced for the same abstract state reinforcement learning is used to learn an optimal policy over this reduced space it is shown experimentally how a combination of behavioural cloning and reinforcement learning using a relational representation is powerful enough to learn how to fly an aircraft through different point in space and different turbulence condition 
a good distance function is an essential tool in application which involve querying large database such a image retrieval and bioinformatics we describe a non parametric algorithm for distance function learning which is based on the boosting of low grade weak learner in a product space the algorithm learns a function defined over pair of point using supervision in the form of equivalence constraint the weak learner are based on partitioning the original feature space using a generic density estimation generative model gmm augmented by equivalence constraint on pair of datapoints using a number of database from the uci repository we show significantly improved result over method which learn the parametric mahalanobis distance we also show initial result of image retrieval using a large database of facial image yaleb 
abstract a part of an environmental observation and forecasting system sensor deployed in the columbia river estuary corie gatherinformation on physical dynamic and change in estuary habitat 
this paper present a general family of algebraic positive definite similarity function over space of matrix with varying column rank the column can represent local region in an image whereby image have varying number of local part image of an image sequence motion trajectory in a multibody motion and so forth the family of set kernel we derive is based on a group invariant tensor product lifting with parameter that can be naturally tuned to provide a cook book of sort covering the possible wish list from similarity measure over set of varying cardinality we highlight the strength of our approach by demonstrating the set kernel for visual recognition of pedestrian using local part representation 
protein interaction typically arise from a physical inter action of one or more small site on the surface of the two protein identify ing these site is very important for drug and protein design in this paper we propose a computational method based on probabilistic relational model that attempt to address this task using high throughput protein i nteraction data and a set of short sequence motif we learn the model using the em algorithm with a branch and bound algorithm a an approximate inference for the e step our method search for motif whose presence in a pair of interacting protein can explain their observed int eraction it also try to determine which motif pair have high affinity and c an therefore lead to an interaction we show that our method is more accurate than others at predicting new protein protein interaction mo re importantly by examining solved structure of protein complex we find that of the predicted active motif correspond to actual interacti on site 
most text categorization system use simple model of document and document collection in this paper we describe a technique that improves a simple web page classifier s performance on page from a new unseen web site by exploiting link structure within a site a well a page structure within hub page on real world test case this technique significantly and substantially improves the accuracy of a bag of word classifier reducing error rate by about half on average the system us a variant of co training to exploit unlabeled data from a new site page are labeled using the base classifier the result are used by a restricted wrapper learner to propose potential main category anchor wrapper and finally these wrapper are used a feature by a third learner to find a categorization of the site that implies a simple hub structure but which also largely agrees with the original bag of word classifier 
a number of vertical mining algorithm have been proposed recently for association mining which have shown to be very effective and usually outperform horizontal approach the main advantage of the vertical format is support for fast frequency counting via intersection operation on transaction id tids and automatic pruning of irrelevant data the main problem with these approach is when intermediate result of vertical tid list become too large for memory thus affecting the algorithm scalability in this paper we present a novel vertical data representation called diffset that only keep track of difference in the tids of a candidate pattern from it generating frequent pattern we show that diffsets drastically cut down the size of memory required to store intermediate result we show how diffsets when incorporated into previous vertical mining method increase the performance significantly 
analyzing data to find trend correlation and stable pattern is an important task in many industrial application this paper proposes a new technique based on parallel coordinate visualization previous work on parallel coordinate method ha shown that they are effective only when variable that are correlated and or show similar pattern are displayed adjacently although current parallel coordinate tool allow the user to manually rearrange the order of variable this process is very time consuming when the number of variable is large automated assistance is required this paper introduces an edit distance based technique to rearrange variable so that interesting change pattern can be easily detected visually the visual miner v miner software includes both automated method for visualizing common pattern and a query tool that enables the user to describe specific target pattern to be mined or displayed by the system in addition the system can filter data according to rule set imported from other data mining tool this feature wa found very helpful in practice because it enables decision maker to visually identify interesting rule and data segment for further analysis or data mining this paper begin with an introduction to the proposed technique and the v miner system next a case study illustrates how v miner ha been used at motorola to guide product design and test decision 
an approach to recognizing human hand gesture from a monocular temporal sequence of image is presented of particular concern is the representation and recognition of hand movement that are used in single handed american sign language asl the approach exploit previous linguistic analysis of manual language that decompose dynamic gesture into their static and dynamic component the first level of decomposition is in term of three set of primitive hand shape location and movement further level of decomposition involve the lexical and sentence level and are part of our plan for future work we propose and subsequently demonstrate that given a monocular gesture sequence kinematic feature can be recovered from the apparent motion that provide distinctive signature for primitive movement of asl the approach ha been implemented in software and evaluated on a database of gesture sequence with an overall recognition rate of for fully automated processing and for manually initialized processing 
text clustering method can be used to structure large set of text or hypertext document the well known method of text clustering however do not really address the special problem of text clustering very high dimensionality of the data very large size of the database and understandability of the cluster description in this paper we introduce a novel approach which us frequent item term set for text clustering such frequent set can be efficiently discovered using algorithm for association rule mining to cluster based on frequent term set we measure the mutual overlap of frequent set with respect to the set of supporting document we present two algorithm for frequent term based text clustering ftc which creates flat clustering and hftc for hierarchical clustering an experimental evaluation on classical text document a well a on web document demonstrates that the proposed algorithm obtain clustering of comparable quality significantly more efficiently than state of theart text clustering algorithm furthermore our method provide an understandable description of the discovered cluster by their frequent term set 
dimensionality reduction technique such a principal component analysis and factor analysis are used to discover a linear mapping between high dimensional data sample and point in a lower dimensional subspace in jojic and frey introduced mixture of transformation invariant component analyzer mtca that can account for global transformation such a translation and rotation perform clustering and learn local appearance deformation by dimensionality reduction however due to enormous computational requirement of the em algorithm for learning the model o where is the dimensionality of a data sample mtca wa not practical for most application in this paper we demonstrate how fast fourier transforms can reduce the computation to the order of log with this speedup we show the effectiveness of mtca in various application tracking video texture clustering video sequence object recognition and object detection in image 
in this paper we propose a novel face photo retrievalsystem using sketch drawing by transforming a photoimage into a sketch we reduce the difference betweenphoto and sketch significantly thus allow effectivematching between the two to improve the synthesisperformance we separate shape and texture informationin a face photo and conduct transformation on themrespectively finally a bayesian classifier is used torecognize the probing sketch from the synthesizedpseudo sketch experiment on a data set containing people clearly demonstrate the efficacy of thealgorithm 
xml document have recently become ubiquitous because of their varied applicability in a number of application classification is an important problem in the data mining domain but current classification method for xml document use ir based method in which each document is treated a a bag of word such technique ignore a significant amount of information hidden inside the document in this paper we discus the problem of rule based classification of xml data by using frequent discriminatory substructure within xml document such a technique is more capable of finding the classification characteristic of document in addition the technique can also be extended to cost sensitive classification we show the effectiveness of the method with respect to other classifier we note that the methodology discussed in this paper is applicable to any kind of semi structured data 
abstract feature selection a a preprocessing step tomachine learning ha been shown very effectivein reducing dimensionality removingirrelevant data increasing learning accuracy and improving comprehensibility in this paper we consider the problem of active featureselection in alter model setting wedescribe a formalism of active feature selectioncalled selective sampling demonstrate itby applying it to a widely used feature selectionalgorithm relief and show how it 
numerous application of data mining to scientific data involve the induction of a classification model in many case the collection of data is not performed with this task in mind and therefore the data might contain irrelevant or redundant feature that affect negatively the accuracy of the induction algorithm the size and dimensionality of typical scientific data make it difficult to use any available domain information to identify feature that discriminate between the class of interest similarly exploratory data analysis technique have limitation on the amount and dimensionality of the data they can process effectively in this paper we describe application of efficient feature selection method to data set from astronomy plasma physic and remote sensing we use variation of recently proposed filter method a well a traditional wrapper approach where practical we discus the general challenge of feature selection in scientific datasets the strategy for success that were common among our diverse application and the lesson learned in solving these problem 
we present a bayesian approach to color constancy which utilizes a nongaussian probabilistic model of the image formation process the parameter of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameter are chosen using cross validation the algorithm is empirically shown to exhibit rms error lower than other color constancy algorithm based on the lambertian surface reflectance model when estimating the illuminant of a set of test image this is demonstrated via a direct performance comparison utilizing a publicly available set of real world test image and code base 
a novel approach to combining clustering and feature selection is presented it implement a wrapper strategy for feature selection in the sense that the feature are directly selected by optimizing the discriminative power of the used partitioning algorithm on the technical side we present an efficient optimization algorithm with guaranteed local convergence property the only free parameter of this method is selected by a resampling based stability analysis experiment with real world datasets demonstrate that our method is able to infer both meaningful partition and meaningful subset of feature 
abstract the need to discretize a numerical rangeinto class coherent interval is a problemfrequently encountered training set error tse is one of the commonly used impurityfunctions in this task 
spatial collocation pattern associate the co existence of non spatial feature in a spatial neighborhood an example of such a pattern can associate contaminated water reservoir with certain decease in their spatial neighborhood previous work on discovering collocation pattern convert neighborhood of feature instance to itemsets and applies mining technique for transactional data to discover the pattern we propose a method that combine the discovery of spatial neighborhood with the mining process our technique is an extension of a spatial join algorithm that operates on multiple input and count long pattern instance a demonstrated by experimentation it yield significant performance improvement compared to previous approach 
the flow pattern of ridge in a fingerprint is unique to the person in that no two people with the same fingerprint have yet been found fingerprint have been in use in forensic application for many year and more recently in computer automated identification and authentication for automated fingerprint image matching a machine representation of a fingerprint image is often a set of minutia in the print a minimal but fundamental representation is just a set of ridge ending and bifurcation oddly however after all the year of using minutia a precise definition of minutia ha never been formulated we provide a formal definition of a minutia based on the gray scale image this definition is constructive in that given a minutia image the minutia location and orientation can be uniquely determined 
computer experiment often require dense sweep over input parameter to obtain a qualitative understanding of their response such sweep can be prohibitively expensive and are unnecessary in region where the response is easy predicted well chosen design could allow a mapping of the response with far fewer simulation run thus there is a need for computationally inexpensive surrogate model and an accompanying method for selecting small design we explore a general methodology for addressing this need that us non stationary gaussian process binary tree partition the input space to facilitate non stationarity and a bayesian interpretation provides an explicit measure of predictive uncertainty that can be used to guide sampling our method are illustrated on several example including a motivating example involving computational fluid dynamic simulation of a nasa reentry vehicle 
we are interested in the problem of modeling and evaluating spoken language system in the context of human machine dialog spoken dialog corpus allow for a multidimensional analysis of speech recognition and language understanding model of dialog system therefore language model can be directly trained based either on the dialog history or it equivalence class or cluster in this paper we propose an algorithm to mine dialog trace which exhibit similar pattern and are identied by the same class for this purpose we apply data clustering method to large human machine spoken dialogue corpus the resulting cluster can be used for system evaluation and language modeling by clustering dialog trace we expect to learn about the behavior of the system with regard to not only the automation rate but the nature of the interaction e g easy v dicult dialog the equivalence class can also be used in order to automatically adapt the language model the understanding module and the dialogue strategy to better t the kind of interaction detected this paper investigates different way for encoding dialogue into multidimensional structure and dieren t clustering method preliminary result are given for cluster interpretation and dynamic model adaptation using the cluster obtained 
well calibrated probability are necessary in many application like probabilistic framework or cost sensitive task based on previous success of asymmetric laplace method in calibrating text classifier score we propose to use piecewise logistic regression which is a simple extension of standard logistic regression a an alternative method in the discriminative family we show that both method have the flexibility to be piecewise linear function in log odds but they are based on quite different assumption we evaluated asymmetric laplace method piecewise logistic regression and standard logistic regression over standard text categorization collection reuters and trecap with three classifier svm naive bayes and logistic regression classifier and observed that piecewise logistic regression performs significantly better than the other two method in the log loss metric 
natural eye design are optimized with regard to the tasksthe eye carrying organism ha to perform for survival thisoptimization ha been performed by the process of naturalevolution over many million of year every eye capturesa subset of the space of light ray the information containedin this subset and the accuracy to which the eye canextract the necessary information determines an upper limiton how well an organism can perform a given task in thiswork we propose a new methodology for camera design byinterpreting eye a sample pattern in light ray space wecan phrase the problem of eye design in a signal processingframework this allows u to develop mathematical criteriafor optimal eye design which in turn enables u to build thebest eye for a given task without the trial and error phase ofnatural evolution the principle is evaluated on the task of d ego motion estimation 
we describe a new framework based on boosting algorithm and cascade structure to efficiently detect object face with occlusion while our approach is motivated by the work of viola and jones several technique have been developed for establishing a more general system including i a robust boosting scheme to select useful weak learner and to avoid overfitting ii reinforcement training to reduce false positive rate via a more effective training procedure for boosted cascade and iii cascading with evidence to extend the system to handle occlusion without compromising in detection speed experimental result on detecting face under various situation are provided to demonstrate the performance of the proposed method 
lasso least absolute shrinkage and selection operator is a useful tool to achieve the shrinkage and variable selection simultaneously since lasso us the l penalty the optimization should rely on the quadratic program qp or general non linear program which is known to be computational intensive in this paper we propose a gradient descent algorithm for lasso even though the final result is slightly le accurate the proposed algorithm is computationally simpler than qp or non linear program and so can be applied to large size problem we provide the convergence rate of the algorithm and illustrate it with simulated model a well a real data set 
abstract robustness is a key requirement in spoken languageunderstanding slu system humanspeech is often ungrammatical and ill formed and there will frequently be a mismatch betweentraining and test data this paper discussesrobustness and adaptation issue in astatistically based slu system which is entirelydata driven to test robustness the systemhas been tested on data from the air travelinformation service atis domain which hasbeen artificially corrupted with varying 
i will discus the use of graphical model for data mining i will review key research area including structure learning variational method a relational modeling and describe application ranging from web traffic analysis to aid vaccine design 
abstract pairwise data in empirical science typically violate metricity eitherdue to noise or due to fallible estimate and therefore arehard to analyze by conventional machine learning technology inthis paper we therefore study way to work around this problem 
this article describes the development of a free test collection for chinese text categorization a novel retrieval based approach wa developed to detect duplicate and label inconsistency in this corpus and in reuters for comparison the method wa able to detect certain type of similar and or duplicated document that were overlooked by an alternative repetition based method experiment showed that effectiveness wa not affected by the confusing document 
peer to peer information sharing environment are increasingly gaining acceptance on the internet a they provide an infrastructure in which the desired information can be located and downloaded while preserving the anonymity of both requestors and provider a recent experience with p p environment such a gnutella show anonymity open the door to possible misuse and abuse by resource provider exploiting the network a a way to spread tampered with resource including malicious program such a trojan horse and virus in this paper we propose an approach to p p security where servents can keep track and share with others information about the reputation of their peer reputation sharing is based on a distributed polling algorithm by which resource requestors can ass the reliability of perspective provider before initiating the download the approach nicely complement the existing p p protocol and ha a limited impact on current implementation furthermore it keep the current level of anonymity of requestors and provider a well a that of the party sharing their view on others reputation 
we exploit some useful property of gaussian process gp regression model for reinforcement learning in continuous state space and discrete time we demonstrate how the gp model allows evaluation of the value function in closed form the resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space further we speculate that the intrinsic ability of gp model to characterise distribution of function would allow the method to capture entire distribution over future value instead of merely their expectation which ha traditionally been the focus of much of reinforcement learning 
large and complex graph representing relationship among set of entity are an increasingly common focus of interest in data analysis example include social network web graph telecommunication network and biological network in interactive analysis of such data a natural query is which entity are most important in the network relative to a particular individual or set of individual we investigate the problem of answering such query in this paper focusing in particular on defining and computing the importance of node in a graph relative to one or more root node we define a general framework and a number of different algorithm building on idea from social network graph theory markov model and web graph analysis we experimentally evaluate the different property of these algorithm on toy graph and demonstrate how our approach can be used to study relative importance in real world network including a network of interaction among september th terrorist a network of collaborative research in biotechnology among company and university and a network of co authorship relationship among computer science researcher 
by comparison to some other sensory cortex the functional property of cell in the primary auditory cortex are not yet well understood recent attempt to obtain a generalized description of auditory cortical response have often relied upon characterization of the spectrotemporal receptive field strf which amount to a model of the stimulusresponse function srf that is linear in the spectrogram of the stimulus how well can such a model account for neural response at the very first stage of auditory cortical processing to answer this question we develop a novel methodology for evaluating the fraction of stimulus related response power in a population that can be captured by a given type of srf model we use this technique to show that in the thalamo recipient layer of primary auditory cortex strf model account for no more than of the stimulus related power in neural response 
we report a system that classifies and can learn to classify pattern of visual motion on line the complete system is described by the dynamic of it physical network architecture the combination of the following property make the system novel firstly the front end of the system consists of an avlsi optical flow chip that collectively computes d global visual motion in real time secondly the complexity of the classification task is significantly reduced by mapping the continuous motion trajectory to sequence of motion event and thirdly all the network structure are simple and with the exception of the optical flow chip based on a winner take all wta architecture we demonstrate the application of the proposed generic system for a contactless man machine interface that allows to write letter by visual motion regarding the low complexity of the system it robustness and the already existing front end a complete avlsi system on chip implementation is realistic allowing various application in mobile electronic device 
we present a new tool for gathering textual information according to a query text on arbitrary web site specified by an information seeking user this tool is helpful in any knowledge intensive area it technology is based on the vector space model with optimized feature definition 
learning to fly an aircraft is a complex task that requires the development of control skill and goal achievement strategy this paper present a behavioural cloning system that learns to successfully fly manoeuvre in turbulence of a realistic aircraft simulation a hierarchical decomposition of the problem is employed where goal setting and the control skill to achieve them are learnt the benefit of this goal directed approach is demonstrated in the reuse of the learnt manoeuvre to a flight path that includes novel manoeuvre the system is based on an error minimisation technique that benefit from the use of model tree learner the model tree provide a compact and comprehensible representation of the underlying control skill and goal structure the performance of the system wa improved by compensating for human reaction time and goal anticipation we conclude that our system address current limitation of behavioural cloning by producing robust reusable and readable clone 
organizing web search result into a hierarchy of topic and sub topic facilitates browsing the collection and locating result of interest in this paper we propose a new hierarchical monothetic clustering algorithm to build a topic hierarchy for a collection of search result retrieved in response to a query at every level of the hierarchy the new algorithm progressively identifies topic in a way that maximizes the coverage while maintaining distinctiveness of the topic we refer the proposed algorithm to a discover evaluating the quality of a topic hierarchy is a non trivial task the ultimate test being user judgment we use several objective measure such a coverage and reach time for an empirical comparison of the proposed algorithm with two other monothetic clustering algorithm to demonstrate it superiority even though our algorithm is slightly more computationally intensive than one of the algorithm it generates better hierarchy our user study also show that the proposed algorithm is superior to the other algorithm a a summarizing and browsing tool 
collaborative filtering cf system have been researched for over a decade a a tool to deal with information overload at the heart of these system are the algorithm which generate the prediction and recommendation in this article we empirically demonstrate that two of the most acclaimed cf recommendation algorithm have flaw that result in a dramatically unacceptable user experience in response we introduce a new belief distribution algorithm that overcomes these flaw and provides substantially richer user modeling the belief distribution algorithm retains the quality of nearest neighbor algorithm which have performed well in the past yet produce prediction of belief distribution across rating value rather than a point rating value in addition we illustrate how the exclusive use of the mean absolute error metric ha concealed these flaw for so long and we propose the use of a modified precision metric for more accurately evaluating the user experience 
the boundary of image region necessarily consist of edge inparticular step and roof edge corner and junction currently different algorithm are used to detect each boundarytype separately but the integration of the result into a singleboundary representation is difficult therefore a method for thesimultaneous detection of all boundary type is needed we proposeto combine response of suitable polar separable filter into whatwe will call the boundary tensor the trace of this tensor is ameasure of boundary strength while the small eigenvalue and itsdifference to the large one represent corner junction and edgestrengths respectively we prove that the edge strength measurebehaves like a rotationally invariant quadrature filter a numberof example demonstrate the property of the new method andillustrate it application to image segmentation 
inpainting is the problem of filling in hole in image considerable progress ha been made by technique that use theimmediate boundary of the hole and some prior information on imagesto solve this problem these algorithm successfully solve thelocal inpainting problem but they must by definition give thesame completion to any two hole that have the same boundary evenwhen the rest of the image is vastly different in this paper weaddress a different more global inpainting problem how can we usethe rest of the image in order to learn how to inpaint we approachthis problem from the context of statistical learning given atraining image we build an exponential family distribution overimages that is based on the histogram of local feature we thenuse this image specific distribution to in paint the hole byfinding the most probable image given the boundary and thedistribution the optimization is done using loopy beliefpropagation we show that our method can successfully completeholes while taking into account the specific image statistic inparticular it can give vastly different completion even when thelocal neighborhood are identical 
we describe a statistical approach for modeling agreement and disagreement in conversational interaction our approach first identifies adjacency pair using maximum entropy ranking based on a set of lexical durational and structural feature that look both forward and backward in the discourse we then classify utterance a agreement or disagreement using these adjacency pair and feature that represent various pragmatic influence of previous agreement or disagreement on the current utterance our approach achieves accuracy a increase over previous work 
we consider the general problem of learning from labeled and unlabeled data which is often called semi supervised learning or transductive inference a principled approach to semi supervised learning is to design a classifying function which is sufciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled point we present a simple algorithm to obtain such a smooth solution our method yield encouraging experimental result on a number of classication problem and demonstrates effective use of unlabeled data 
we introduce anytime mechanism for distributed optimization with self interested agent anytime mechanism retain good incentive property even when interrupted before the optimal solution is computed and provide better quality solution when given additional time anytime mechanism can solve easy instance of a hard problem quickly and optimally while providing approximate solution on very hard instance in a particular instantiation growrange we successively expand the range of outcome considered computing the optimal solution for each range truth revelation remains a dominant strategy equilibrium with a stage based interruption and is a best response with high probability when the interruption is time based 
in the selective dissemination of information or publish subscribe paradigm client subscribe to a server with continuous query or profile that express their information need client can also publish document to server whenever a document is published the continuous query satisfying this document are found and notification are sent to appropriate client this paper deal with the filtering problem that need to be solved effciently by each server given a database of continuous query db and a document d find all query q db that match d we present data structure and indexing algorithm that enable u to solve the filtering problem efficiently for large database of query expressed in the model awp which is based on named attribute with value of type text and word proximity operator 
we present an analysis of concentration of expectation phenomenon in layered bayesian network that use generalized linear model a the local conditional probability this framework encompasses a wide variety of probability distribution including both discrete and continuous random variable we utilize idea from large deviation analysis and the delta method to devise and evaluate a class of approximate inference algorithm for layered bayesian network that have superior asymptotic error bound and very fast computation time 
we present an extension to the jojic and frey layered sprite model which allows for layer to undergo afne transformation this extension allows for afne object pose to be inferred whilst simultaneously learning the object shape and appearance learning is carried out by applying an augmented variational inference algorithm which includes a global search over a discretised transform space followed by a local optimisation to aid correct convergence we use bottom up cue to restrict the space of possible afne transformation we present result on a number of video sequence and show how the model can be extended to track an object whose appearance change throughout the sequence 
the use of domain knowledge in a learner can greatly improve the model it produce however high quality expert knowledge is very difficult to obtain traditionally researcher have assumed that knowledge come from a single self consistent source a little explored but often more feasible alternative is to use multiple weaker source in this paper we take a step in this direction by developing a method for learning the structure of a bayesian network from multiple expert data is then used to refine the structure and estimate parameter a simple analysis show that even relatively few noisy expert can produce high quality knowledge when combined experiment with real and simulated expert in a variety of domain show the benefit of this approach 
web application are becoming increasingly popular for mobile wireless pda however web browsing on these system can be quite slow an alternative approach is handheld thin client computing in which the web browser and associated application logic run on a server which then sends simple screen update to thepda for display to ass the viability of this thin client approach we compare the web browsing performance of thin client against fat client that run the web browser locally on a pda our result show that thin client can provide better web browsing performance compared to fat client both in term of speed and ability to correctly display web content surprisingly thin client are faster even when having to send more data over the network we characterize and analyze different design choice in various thin client system and explain why these approach can yield superior web browsing performance on mobile wireless pda 
query answering over commonsense knowledge base typically employ a first order logic theorem prover while first order inference is intractable in general provers can often be hand tuned to answer query with reasonable performance in practice appealing to previous theoretical work on partition based reasoning we propose resolution based theorem proving strategy that exploit the structure of a kb to improve the efficiency of reasoning we analyze and experimentally evaluate these strategy with a testbed based on the snark theorem prover exploiting graph based partitioning algorithm we show how to compute a partition derived ordering for ordered resolution with good experimental result offering an automatic alternative to hand crafted ordering we further propose a new resolution strategy mf resolution that combine partition based message passing with focused sublanguage resolution our experiment show a significant reduction in the number of resolution step when this strategy is used finally we augment partition based message passing partition derived ordering and mf by combining them with the set of support restriction while these combination are incomplete they often produce dramatic improvement in practice this work present promising practical technique for query answering with large and potentially distributed commonsense kb 
both ranking function and user query are very important factor affecting a search engine s performance prior research ha looked at how to improve ad hoc retrieval performance for existing query while tuning the ranking function or modify and expand user query using a fixed ranking scheme using blind feedback however almost no research ha looked at how to combine ranking function tuning and blind feedback together to improve ad hoc retrieval performance in this paper we look at the performance improvement for ad hoc retrieval from a more integrated point of view by combining the merit of both technique in particular we argue that the ranking function should be tuned first using user provided query before applying the blind feedback technique the intuition is that highly tuned ranking offer more high quality document at the top of the hit list thus offer a stronger baseline for blind feedback we verify this integrated model in a large scale heterogeneous collection and the experimental result show that combining ranking function tuning and blind feedback can improve search performance by almost over the baseline okapi system 
the is a new layer of the internet that enables semantic representation of the content of existing web page using common ontology human user sketch out the most important fact in model that act a intelligent whiteboards once model are broadcasted to the internet new and intelligent search engine ambient intelligent device and agent would be able to exploit this knowledge network the main idea of semtalk is to empower end user to contribute to the semantic web by offering an easy to use based graphical editor to create rdf like schema and workflow since the modeled data is found by microsoft s smarttags user can benefit from these semantic web a part of their daily work with other microsoft office product such a word excel or outlook semtalk s graphically configurable meta model also extends the functionality of the visio modeling tool because it make it easy to configure visio to different modeling world such a business engineering and case methodology but also to these feature can be applied to any other visio drawing ontology project department wide information modeling at the credit suisse bank main emphasis wa on linguistic standardization of term based on a common central glossary local knowledge management team were able to develop specialized model for their decentralized department a part of the knowledge management process local glossary were continually carried over into a common shared model business process management project distributed process modeling of the bausparkasse deutscher ring a german financial institution several group of student from the technical university fh brandenburg explored how to develop and apply an industry specific semantic web to business process modeling 
the web graph meaning the graph induced by web page a node and their hyperlink a directed edge ha become a fascinating object of study for many people physicist sociologist mathematician computer scientist and information retrieval specialist recent result range from theoretical e g model for the graph semi external algorithm to experimental e g new insight regarding the rate of change of page new data on the distribution of degree to practical e g improvement in crawling technology recent result range from theoretical e g model for the graph semi external algorithm to experimental e g new insight regarding the rate of change of page new data on the distribution of degree to practical e g improvement in crawling technology the goal of this talk is to convey an introduction to the state of the art in this area and to sketch the current issue in collecting representing analyzing and modeling this graph although graph analytic method are essential tool in the web ir arsenal they are well known to the sigir community and will not be discussed here in any detail instead we will explore some challenge and opportunity for using ir method and technique in the exploration of the web graph in particular in dealing with legitimate and spam perturbation of the natural process of birth and death of node and link and conversely the challenge and opportunity of using graph method in support of ir on the web and in the enterprise 
because sequential auction have permeated society more than ever it is desirable for participant to have the optimal strategy beforehand however finding closed form solution to various sequential auction game is challenging current literature provides some answer for specific case but not for general case a decision support system that can automate optimal bid for player in different sequential auction game will be useful in solving these complex economic problem which requires not only economic but also computational efficiency this thesis contributes in several direction first this dissertation derives result related to the multiplicity of equilibrium in first price sealed bid fpsb auction and sequential fpsb auction with discrete bid under complete information it also provides theoretical result for fpsb auction with discrete bid under incomplete information these result are applicable to both two person and multi person case second this thesis develops a technique to compute strategy in sequential auction it applies monte carlo simulation to approximate perfect bayesian equilibrium for sequential auction with discrete bid and incomplete information it also utilizes the leveraged substructure of the game tree which can dramatically reduce the memory and computation time required to solve the game this approach is applicable to sequence of a wide variety of auction finally this thesis analyzes the impact of information in sequential auction with continuous bid and incomplete information when bid are revealed it provides theoretical result especially the non existence of pure strategy symmetric equilibrium in both the symmetric sequential fpsb and the symmetric sequential vickrey auction 
the information bottleneck ib method is an information theoretic formulation for clustering problem given a joint distribution this method construct a new variable that defines partition over the value of that are informative about maximum likelihood ml of mixture model is a standard statistical approach to clustering problem in this paper we ask how are the two method related we define a simple mapping between the ib problem and the ml problem for the multinomial mixture model we show that under this mapping the problem are strongly related in fact for uniform input distribution over or for large sample size the problem are mathematically equivalent specifically in these case every fixed point of the ib functional defines a fixed point of the log likelihood and vice versa moreover the value of the functionals at the fixed point are equal under simple transformation a a result in these case every algorithm that solves one of the problem induces a solution for the other 
the paper present an approach to ellipsisresolution in a framework of scope underspecification underspecified discourserepresentation theory it is argued thatthe approach improves on previous proposalsto integrate ellipsis resolution andscope underspecification crouch egg et al in that application processeslike anaphora resolution do not requirefull disambiguation but can workdirectly on the underspecified representation 
we examine the marriage of recent probabilistic generative model for social network with classical framework from mathematical economics we are particularly interested in how the statistical structure of such network influence global economic quantity such a price variation our finding are a mixture of formal analysis simulation and experiment on an international trade data set from the united nation 
nonlinear ltering can solve very complex problem but typically involve very time consuming calculation here we show that for lters that are constructed a a rbf network with gaussian basis function a decomposition into linear lters exists which can be computed ecien tly in the frequency domain yielding dramatic improvement in speed we present an application of this idea to image processing in electron micrograph image of photoreceptor terminal of the fruit y drosophila synaptic vesicle containing neurotransmitter should be detected and labeled automatically we use hand label provided by human expert to learn a rbf lter using support vector regression with gaussian kernel we will show that the resulting nonlinear lter solves the task to a degree of accuracy which is close to what can be achieved by human expert this allows the very time consuming task of data evaluation to be done ecien tly 
positive definite kernel between labeled graph have recently been proposed they enable the application of kernel method such a support vector machine to the analysis and classification of graph for example chemical compound these graph kernel are obtained by marginalizing a kernel between path with respect to a random walk model on the graph vertex along the edge we propose two extension of these graph kernel with the double goal to reduce their computation time and increase their relevance a measure of similarity between graph first we propose to modify the label of each vertex by automatically adding information about it environment with the use of the morgan algorithm second we suggest a modification of the random walk model to prevent the walk from coming back to a vertex that wa just visited these extension are then tested on benchmark experiment of chemical compound classification with promising result 
the problem of identifying approximately duplicate record in database is an essential step for data cleaning and data integration process most existing approach have relied on generic or manually tuned distance metric for estimating the similarity of potential duplicate in this paper we present a framework for improving duplicate detection using trainable measure of textual similarity we propose to employ learnable text distance function for each database field and show that such measure are capable of adapting to the specific notion of similarity that is appropriate for the field s domain we present two learnable text similarity measure suitable for this task an extended variant of learnable string edit distance and a novel vector space based measure that employ a support vector machine svm for training experimental result on a range of datasets show that our framework can improve duplicate detection accuracy over traditional technique 
the task of object identification occurs when integrating information from multiple website the same data object can exist in inconsistent text format across site making it difficult to identify matching object using exact text match previous method of object identification have required manual construction of domain specific string transformation or manual setting of general transformation parameter weight for recognizing format inconsistency this manual process can be time consuming and error prone we have developed an object identification system called active atlas which applies a set of domain independent string transformation to compare the object shared attribute in order to identify matching object in this paper we discus extension to the active atlas system which allow it to learn to tailor the weight of a set of general transformation to a specific application domain through limited user input the experimental result demonstrate that this approach achieves higher accuracy and requires le user involvement than previous method across various application domain 
log concavity is an important property in the context of optimization laplace approximation and sampling bayesian method based on gaussian process prior have become quite popular recently for classification regression density estimation and point process intensi ty estimation here we prove that the predictive density corresponding to each of these application are log concave given any observed data we also prove that the likelihood is log concave in the hyperparameters controlling the mean function of the gaussian prior in the density and point process intensity estimation case and the mean covariance and observation noise parameter in the classification and regression case this result lead to a useful parameterization of these hyperparameters indicating a suitably large class of prior for which the corresponding maximum a posteriori problem is log concave 
recent eye tracking study in natural task suggest that there is a tight link between eye movement and goal directed motor action however most existing model of human eye movement provide a bottom up account that relates visual attention to attribute of the visual scene the purpose of this paper is to introduce a new model of human eye movement that directly tie eye movement to the ongoing demand of behavior the basic idea is that eye movement serve to reduce uncertainty about environmental variable that are task relevant a value is assigned to an eye movement by estimating the expected cost of the uncertainty that will result if the movement is not made if there are several candidate eye movement the one with the highest expected value is chosen the model is illustrated using a humanoid graphic figure that navigates on a sidewalk in a virtual urban environment simulation show our protocol is superior to a simple round robin scheduling mechanism this paper introduces a new framework for understanding the scheduling of human eye movement the human eye is characterized by a small high resolution fovea the importance of foveal vision mean that fast ballistic eye movement called saccade are made at a rate of approximately three per second to direct gaze to relevant area of the visual field since the location of the fovea provides a powerful clue to what information the visual system is processing understanding the scheduling and targeting of eye movement is key to understanding the organization of human vision the recent advent of portable eye tracker ha made it possible to study eye movement in everyday behavior these study show that behavior such a driving or navigating a city sidewalk show rapid alternating saccade to different target indicative of competing perceptual demand this paper introduces a model of how human select visual target in term of the value of the information obtained previous work ha modeled the direction of the eye to target primarily in term of visual saliency such model fail to incorporate the role of task demand and do not address the problem of resource contention in contrast our underlying premise is that much of routine human behavior can be understood in the framework of reward maximization in other word human choose action by trading off the cost of the 
we propose a novel method for new view generation from a rectified sequence of image our new image correspond to a new camera model which we call a bi centric camera in this model the center of horizontal and vertical projection lie in different location on the camera s optical axis this model reduces to the regular pinhole camera when the two projection center coincide and the pushbroom camera when one projection center lie at infinity we first analyze the property of this camera model we then show how to generate new bi centric view from vertical cut in the epipolar volume of a rectified sequence every vertical cut generates a new bi centric view where the specific parameter of the cut determine the location of the projection center we discus and demonstrate application including the generation of image where the virtual camera lie behind occluding surface e g behind the back wall of a room and in unreachable position e g in front of a glass window our final application is the generation of movie taken by a simulated forward moving camera using a input a movie taken by a sideways moving camera 
we present analog neuromorphic circuit for implementing bistable synapsis with spike timing dependent plasticity stdp property in these type of synapsis the short term dynamic of the synaptic efficacy are governed by the relative timing of the preand post synaptic spike while on long time scale the efficacy tend asymptotically to either a potentiated state or to a depressed one we fabricated a prototype vlsi chip containing a network of integrate and fire neuron interconnected via bistable stdp synapsis test result from this chip demonstrate the synapse s stdp learning property and it long term bistable characteristic 
information gain is a well known and empirically proven method for high dimensional feature selection we found that it and other existing method failed to produce good result on an industrial text classification problem on investigating the root cause we find that a large class of feature scoring method suffers a pitfall they can be blinded by a surplus of strongly predictive feature for some class while largely ignoring feature needed to discriminate difficult class in this paper we demonstrate this pitfall hurt performance even for a relatively uniform text classification task based on this understanding we present solution inspired by round robin scheduling that avoid this pitfall without resorting to costly wrapper method empirical evaluation on datasets show substantial improvement 
we consider a dynamic market place of self interested agent with differing capability a task to be completed is proposed to the agent population an agent attempt to form a coalition of agent to perform the task before proposing a coalition the agent must determine the optimal set of agent with whom to enter into a coalition for this task we refer to this activity a coalition calculation to determine the optimal coalition the agent must have a mean of calculating the value of any given coalition multiple metric cost time quality etc determine the true value of a coalition however because of conflicting metric differing metric importance and the tendency of metric importance to vary over time it is difficult to obtain a true valuation of a given coalition previous work ha not addressed these issue we present a solution based on the adaptation of a multi objective optimization evolutionary algorithm in order to obtain a true valuation of any coalition we use the concept of pareto dominance coupled with a distance weighting algorithm we determine the pareto optimal set of coalition and then use an instance based learning algorithm to select the optimal coalition we show through empirical evaluation that the proposed technique is capable of eliciting metric importance and adapting to metric variation over time 
we present metric a provably near optimal algorithm for reinforcement learning in markov decision process in which there is a natural metric on the state space that allows the construction of accurate local model the algorithm is a generalization of the algorithm of kearns and singh and assumes a black box for approximate planning unlike the original metricfinds a near optimal policy in an amount of time that doe not directly depend on the size of the state space but instead depends on the covering number of the state space informally the covering number is the number of neighborhood required for accurate local modeling 
although information retrieval research ha always been concerned with improving the effectiveness of search in some application such a information analysis a more specific requirement exists for high accuracy retrieval this mean that achieving high precision in the top document rank is paramount in this paper we present work aimed at achieving high accuracy in ad hoc document retrieval by incorporating approach from question answering qa we focus on getting the first relevant result a high a possible in the ranked list and argue that traditional precision and recall are not appropriate measure for evaluatin this task we instead use the mean reciprocal rank mrr of the first relevant result we evaluate three different method for modifying query to achieve high accuracy the experiment done on trec data provide support for the approach of using mrr and incorporating qa technique for getting high accuracy in ad hoc retrieval task 
this paper proposes a dynamic model supporting multimodal state space probability distribution and present the application of the model in dealing with visual occlusion when tracking multiple object jointly for a set of hypothesis multiple measurement are acquired at each time instant the model switch among a set of hypothesized measurement during the propagation two computationally efficient filtering algorithm are derived for online joint tracking both the occlusion relationship and state of the object are recursively estimated from the history of measurement data the switching hypothesized measurement shm model is generally applicable to describe various dynamic process with multiple alternative measurement method 
detecting low level image feature such a edge and ridge with spatial filter is improved if the scale of the feature are known a priori scale space representation and wavelet pyramid address the problem by using filter over multiple scale however the scale of the filter are still fixed beforehand and the number of scale is limited by computational power the filtering operation are thus not adapted to detect image structure at their optimal or intrinsic scale we adopt the steering approach to obtain filter response at arbitrary scale from a small set of filter at scale chosen to accurately sample the scale space within a given range in particular we use the moore penrose inverse to learn the steering coefficient which we then regress by polynomial function fitting to the scale parameter in order to steer the filter response continuously across scale we show that the extremum of the polynomial steering function can be easily computed to detect interesting feature such a phase independent energy maximum such point of energy maximum in our scale space correspond to the intrinsic scale of the filtered image structure we apply the technique to several well known image to segment image structure which are mostly characterised by their intrinsic scale 
we investigate global index grammar gig a grammar formalism that us a stack of index associated with production and ha restricted context sensitive power we discus some of the structural description that gig can generate compared with those generated by ligs we show also how gig can represent structural description corresponding to hpsgs pollard and sag schema 
traditional clustering is a descriptive task that seek to identify homogeneous group of object based on the value of their attribute while domain knowledge is always the best way to justify clustering few clustering algorithm have ever take domain knowledge into consideration in this paper the domain knowledge is represented by hierarchical ontology we develop a framework by directly incorporating domain knowledge into clustering process yielding a set of cluster with strong ontology implication during the clustering process ontology information is utilized to efficiently prune the exponential search space of the subspace clustering algorithm meanwhile the algorithm generates automatical interpretation of the clustering result by mapping the natural hierarchical organized subspace cluster with significant categorical enrichment onto the ontology hierarchy our experiment on a set of gene expression data using gene ontology demonstrate that our pruning technique driven by ontology significantly improve the clustering performance with minimal degradation of the cluster quality meanwhile many hierarchical organization of gene cluster corresponding to a sub hierarchy in gene ontology were also successfully captured 
collaborative filtering identifies information interest of a particular user based on the information provided by other similar user the memory based approach for collaborative filtering e g pearson correlation coefficient approach identify the similarity between two user by comparing their rating on a set of item in these approach different item are weighted either equally or by some predefined function the impact of rating discrepancy among different user ha not been taken into consideration for example an item that is highly favored by most user should have a smaller impact on the user similarity than an item for which different type of user tend to give different rating even though simple weighting method such a variance weighting try to address this problem empirical study have shown that they are ineffective in improving the performance of collaborative filtering in this paper we present an optimization algorithm to automatically compute the weight for different item based on their rating from training user more specifically the new weighting scheme will create a clustered distribution for user vector in the item space by bringing user of similar interest closer and separating user of different interest more distant empirical study over two datasets have shown that our new weighting scheme substantially improves the performance of the pearson correlation coefficient method for collaborative filtering 
decision tree are surprisingly adaptive in three importan t respect they automatically adapt to favorable condition near the bayes decision boundary focus on data distributed on lower dimensional manifold reject irrelevant feature in this paper we examine a decision tre e based on dyadic split that adapts to each of these condition to achieve minimax optimal rate of convergence the proposed classifi er is the first known to achieve these optimal rate while being practi cal and implementable each of the above property can be formalized and translated into a class of distribution with known minimax rate of convergence adaptivity is a highly desirable quality of classifier since in practice the precise characteristic o f the distribution are unknown we show that dyadic decision tree achieve the minimax optimal rate to within a log factor without needing to know the specific parameter defin ing the class such tree are constructed by minimizing a complexity penalized empirical risk over an appropriate family of dyadic partition the complexity term is derived from a new generalization error bound for tree inspired by this bound in turn lead to an oracle inequality from which the optimal rate are derived full proof of all result are given in 
we present a method for object categorization in real world scene following a common consensus in the field we do not assume that a figureground segmentation is available prior to recognition however in contrast to most standard approach for object class recognition our approach automatically segment the object a a result of the categorization this combination of recognition and segmentation into one process is made possible by our use of an implicit shape model which integrates both into a common probabilistic framework in addition to the recognition and segmentation result it also generates a per pixel confidence measure specifying the area that support a hypothesis and how much it can be trusted we use this confidence to derive a natural extension of the approach to handle multiple object in a scene and resolve ambiguity between overlapping hypothesis with a novel mdl based criterion in addition we present an extensive evaluation of our method on a standard dataset for car detection and compare it performance to existing method from the literature our result show that the proposed method significantly outperforms previously published method while needing one order of magnitude le training example finally we present result for articulated object which show that the proposed method can categorize and segment unfamiliar object in different articulation and with widely varying texture pattern even under significant partial occlusion 
the ability to accurately identify the network traffic associated with different p p application is important to a broad range of network operation including application specific traffic engineering capacity planning provisioning service differentiation etc however traditional traffic to higher level application mapping technique such a default server tcp or udp network port baseddisambiguation is highly inaccurate for some p p application in this paper we provide an efficient approach for identifying the p p application traffic through application level signature we firstidentify the application level signature by examining some available documentation and packet level trace we then utilize the identified signature to develop online filter that can efficiently and accurately track the p p traffic even on high speed network link we examine the performance of our application level identification approach using five popular p p protocol our measurement show thatour technique achieves le than false positive and false negative ratio in most case we also show that our approach only requires the examination of the very first few packet le than packet to identify a p p connection which make our approach highly scalable our technique can significantly improve the p p traffic volume estimate over what pure network port based approach provide for instance we were able to identify time a much traffic for the popular kazaa p p protocol compared to the traditional port based approach 
approximate linear programming alp ha emerged recently a one of the most promising method for solving complex factored mdps with nite state space in this work we show that alp solution are not limited only to mdps with nite state space but that they can also be applied successfully to factored continuous state mdps cmdps we show how one can build an alp based approximation for such a model and contrast it to existing solution method we argue that this approach offer a robust alternative for solving high dimensional continuous state space problem the point is supported by experiment on three cmdp problem with continuous state factor 
this paper extends our earlier analysis on approximate linear programming a an approach to approximating the cost to go function in a discounted cost dynamic program in this paper we consider the average cost criterion and a version of approximate linear programming that generates approximation to the optimal average cost and differential cost function we demonstrate that a naive version of approximate linear programming prioritizes approximation of the optimal average cost and that this may not be well aligned with the objective of deriving a policy with low average cost for that the algorithm should aim at producing a good approximation of the differential cost function we propose a twophase variant of approximate linear programming that allows for external control of the relative accuracy of the approximation of the differential cost function over different portion of the state space via state relevance weight performance bound suggest that the new algorithm is compatible with the objective of optimizing performance and provide guidance on appropriate choice for state relevance weight 
in this paper we present an abstract framework for learning a finite domain constraint solver modeled by a set of operator enforcing a consistency the behavior of the consistency to be learned is taken a the set of example on which the learning process is applied the best possible expression of this operator in a given language is then searched we instantiate this framework to the learning of bound consistency in the indexical language of gnu prolog 
we define a connection subgraph a a small subgraph of a large graph that best capture the relationship between two node the primary motivation for this work is to provide a paradigm for exploration and knowledge discovery in large social network graph we present a formal definition of this problem and an ideal solution based on electricity analogue we then show how to accelerate the computation to produce approximate but high quality connection subgraphs in real time on very large disk resident graph we describe our operational prototype and we demonstrate result on a social network graph derived from the world wide web our graph contains million node and million edge and our system still produce quality response within second 
we present a connectionist architecture that can learn a model of the relation between perception and action and use this model for behavior planning state representation are learned with a growing selforganizing layer which is directly coupled to a perception and a motor layer knowledge about possible state transition is encoded in the lateral connectivity motor signal modulate this lateral connectivity and a dynamic field on the layer organizes a planning process all mechanism are local and adaptation is based on hebbian idea the model is continuous in the action perception and time domain 
we explore the phenomenon of subjective randomness a a case study in understanding how people discover structure embedded in noise we present a rational account of randomness perception based on the statistical problem of model selection given a stimulus inferring whether the process that generated it wa random or regular inspired by the mathematical definition of randomness given by kolmogorov complexity we characterize regularity in term of a hierarchy of automaton that augment a finite controller with different form of memory we find that the regularity detected in binary sequence depend upon presentation format and that the kind of automaton that can identify these regularity are informative about the cognitive process engaged by different format 
network and server centric computing paradigm are quickly returning to being the dominant method by which we use computer web application are so prevalent that the role of a pc today ha been largely reduced to a terminal for running a client or viewer such a a web browser implementers of network centric application typically rely on the limited capability of html employing proprietary plug in or transmitting the binary image of an entire application that will be executed on the client alternatively implementers can develop without regard for remote use requiring user who wish to run such application on a remote server to rely on a system that creates a virtual frame buffer on the server and transmits a copy of it raster image to the local client we review some of the problem that these current approach pose and show how they can be solved by developing a distributed user interface toolkit a distributed user interface toolkit applies technique to the high level component of a toolkit that are similar to those used at a low level in the x window system a an example of this approach we present remotejfc a working distributed user interface toolkit that make it possible to develop thin client application using a distributed version of the java foundation class 
there exist many portal server that support the construction of my portal that is portal that allow the user to have one or more personal page composed of a number of personalizable service the main drawback of current portal server is their lack of generality and adaptability this paper present the design of mypersonalizer a j ee based framework for engineering my portal the framework is structured according to the model view controller and layer architectural pattern providing generic adaptable model and controller layer that implement the typical use case of a my portal mypersonalizer allows for a good separation of role in the development team graphical designer without programming skill develop the portal view by writing jsp page while software engineer implement service plugins and specify framework configuration 
the rapid growth of the web ha been noted and tracked extensively recent study have however documented the dual phenomenon web page have small half life and thus the web exhibit rapid death a well consequently page creator are faced with an increasingly burdensome task of keeping link up to date and many are falling behind in addition to just individual page collection of page or even entire neighborhood of the web exhibit significant decay rendering them le effective a information resource such neighborhood are identified only by frustrated searcher seeking a way out of these stale neighborhood back to more up to date section of the web measuring the decay of a page purely on the basis of dead link on the page is too naive to reflect this frustration in this paper we formalize a strong notion of a decay measure and present algorithm for computing it efficiently we explore this measure by presenting a number of validation and use it to identify interesting artifact on today s web we then describe a number of application of such a measure to search engine web page maintainer ontologists and individual user 
data quality is a central issue for many information oriented organization recent advance in the data quality field reflect the view that a database is the product of a manufacturing process while routine error such a non existent zip code can be detected and corrected using traditional data cleansing tool many error systemic to the manufacturing process cannot be addressed therefore the product of the data manufacturing process is an imprecise recording of information about the entity of interest i e customer transaction or asset in this way the database is only one flawed version of the entity it is supposed to represent quality assurance system such a motorola s six sigma and other continuous improvement method document the data manufacturing process s shortcoming a widespread method of documentation is quality matrix in this paper we explore the use of the readily available data quality matrix for the data mining classification task we first illustrate that if we do not factor in these quality matrix then our result for prediction are sub optimal we then suggest a general purpose ensemble approach that perturbs the data according to these quality matrix to improve the predictive accuracy and show the improvement is due to a reduction in variance 
although originally designed for large scale electronic publishing xml play an increasingly important role in the exchange of data on the web in fact it is expected that xml will become the lingua franca of the web eventually replacing html not surprisingly there ha been a great deal of interest on xml both in industry and in academia nevertheless to date no comprehensive study on the xml web i e the subset of the web made of xml document only nor on it content ha been made this paper is the first attempt at describing the xml web and the document contained in it our result are drawn from a sample of a repository of the publicly available xml document on the web consisting of about document our result show that despite it short history xml already permeates the web both in term of generic domain and geographically also our result about the content of the xml web provide valuable input for the design of algorithm tool and system that use xml in one form or another 
the evaluation of incomplete satisabilit y solver depends critically on the availability of hard satisable instance a plausible source of such instance consists of random ksat formula whose clause are chosen uniformly from among all clause satisfying some randomly chosen truth assignment a unfortunately instance generated in this manner tend to be relatively easy and can be solved ecien tly by practical heuristic roughly speaking for a number of dieren t algorithm a act a a stronger and stronger attractor a the formula s density increase motivated by recent result on the geometry of the space of satisfying truth assignment of random k sat and nae k sat formula we introduce a simple twist on this basic model which appears to dramatically increase it hardness namely in addition to forbidding the clause violated by the hidden assignment a we also forbid the clause violated by it complement so that both a and a are satisfying it appears that under this symmetrization the eects of the two attractor largely cancel out making it much harder for algorithm to nd any truth assignment we give theoretical and experimental evidence supporting this assertion 
this paper provides a logical framework for negotiation between agent that are assumed to be rational cooperative and truthful we present a characterisation of the permissible outcome of a process of negotiation in term of a set of rationality postulate a well a a method for constructing exactly the rational outcome the framework is extended by describing two mode of negotiation from which an outcome can be reached in the concessionary mode agent are required to weaken their demand in order to accommodate the demand of others in the adaptationist mode agent are required to adapt to the demand of others in some appropriate fashion both concession and adaptation are characterised in term of rationality postulate we also provide method for constructing exactly the rational concession a well a the rational adaptation the central result of the paper is the observation that the outcome obtained from the concessionary and adaptationist mode both correspond to the rational outcome we conclude by pointing out the link between negotiation and agm belief change and providing a glimpse of how this may be used to define a notion of preference based negotiation 
the focus of this paper is to present a methodology for validating the relevance of autonomy technology to current and future space mission in this paper we will discus the objective of nasa space exploration mission and explain the requirement needed for autonomy technology to achieve mission goal by focusing on the underlying purpose of the mission that of maximizing scientific yield we will analyze how autonomy technology address achievement of mission objective we will discus how technology such a reasoning planning and autonomous control have a direct influence on mission success the methodology proposed break down mission component into operational function and discus how technology based on performance metric enable achievement of these function and increase in science return specific example of validating autonomy technology applied to surface exploration mission will be provided 
we propose a multiclass mc classification approach to text categorization tc to fully take advantage of both positive and negative training example a maximal figure of merit mfom learning algorithm is introduced to train high performance mc classifier in contrast to conventional binary classification the proposed mc scheme assigns a uniform score function to each category for each given test sample and thus the classical bayes decision rule can now be applied since all the mc mfom classifier are simultaneously trained we expect them to be more robust and work better than the binary mfom classifier which are trained separately and are known to give the best tc performance experimental result on the reuters tc task indicate that the mc mfom classifier achieve a micro averaging f value of which is significantly better than obtained with the binary mfom classifier for the category with le than training sample furthermore for all category most with large training size the mc mfom classifier give a micro averaging f value of better than obtained with the binary mfom classifier 
in order to investigate the deep structure of gaussian scale space image one need to understand the behaviour of spatial critical point under the influence of blurring we show how the mathematical framework of catastrophe theory can be used to describe the behaviour of critical point trajectory when various different type of generic event viz annihilation and creation of pair of spatial critical point almost coincide although such event are nongeneric in mathematical sense they are not unlikely to be encountered in practice furthermore the behaviour lead to the observation that fine to coarse tracking of critical point doesn t suffice we apply the theory to an artificial image and a simulated mr image and show the occurrence of the described behaviour 
in order to investigate the deep structure of gaussian scale space image one need to understand the behaviour of spatial critical point under the influence of blurring we show how the mathematical framework of catastrophe theory can be used to describe and model the behaviour of critical point trajectory when various different type of generic event viz annihilation and creation of pair of spatial critical point almost coincide although such event are non generic in mathematical sense they are not unlikely to be encountered in practice due to numerical limitation furthermore the behaviour of these trajectory lead to the observation that fine to coarse tracking of critical point doesn t suffice since they can form closed loop in scale space the modelling of the trajectory include these loop we apply the theory to an artificial image and a simulated mr image and show the occurrence of the described behaviour 
we formulate the regression problem a one of maximizing the minimum probability symbolized by that future predicted output of the regression model will be within some bound of the true regression function our formulation is unique in that we obtain a direct estimate of this lower probability bound the proposed framework minimax probability machine regression mpmr is based on the recently described minimax probability machine classification algorithm lanckriet et al and us mercer kernel to obtain nonlinear regression model mpmr is tested on both toy and real world data verifying the accuracy of the bound and the efficacy of the regression model 
we introduce our work on the backdoor key a concept that show promise for characterizing problem hardness in backtracking search algorithm the general notion of backdoor wa recently introduced to explain the source of heavy tailed behavior in backtracking algorithm williams gomes selman a b we describe empirical study that show that the key faction i e the ratio of the key size to the corresponding backdoor size is a good predictor of problem hardness of ensemble and individual instance within an ensemble for structure domain with large key fraction 
we describe a new algorithm for computing a nash equilibrium ingraphical game a compact representation for multi agent systemsthat we introduced in previous work the algorithm is the rstto compute equilibrium both eciently and exactly for a non trivialclass of graphical game 
we present a method for learning feature descriptor using multiple image motivated by the problem of mobile robot navigation and localization the technique us the relative simplicity of small baseline tracking in image sequence to develop descriptor suitable for the more challenging task of wide baseline matching across significant viewpoint change the variation in the appearance of each feature are learned using kernel principal component analysis kpca over the course of image sequence an approximate version of kpca is applied to reduce the computational complexity of the algorithm and yield a compact representation our experiment demonstrate robustness to wide appearance variation on non planar surface including change in illumination viewpoint scale and geometry of the scene 
similarity measure of time series is an important subroutine in many kdd application previous similarity model mainly focus on the prominent series behavior by considering the whole information of time series in this paper we address the problem which portion of information is more suitable for similarity measure for the data collected from a certain field we propose a model for the retrieval and representation of the partial information in time series data and a methodology for evaluating the similarity measurement based on partial information the methodology is to retrieve various portion of information from the raw data and represent it in a concise form then cluster the time series using the partial information and evaluate the similarity measurement through comparing the result with a standard classification experiment on data set from stock market give some interesting observation and justify the usefulness of our approach 
in this paper we describe research which could lead to a novel approach to gathering an overview of a document in a foreign language the research explores how much of the meaning of a document could be represented using image by researching the ability of subject to derive the search term that might have been used to return a set of image from an image library the google image search engine wa used to retrieve the image for this experiment which us english throughout the result were analysed with respect to a previous paper exploring ability to recognise concrete object in hierarchy it wa found that there is a tendency to use one particular level of categorization 
how do cortical neuron represent the acoustic environment this question is often addressed by probing with simple stimulus such a click or tone pip such stimulus have the advantage of yielding easily interpreted answer but have the disadvantage that they may fail to uncover complex or higher order neuronal response property here we adopt an alternative approach probing neuronal response with complex acoustic stimulus including animal vocalization and music we have used in vivo whole cell method in the rat auditory cortex to record subthreshold membrane potential fluctuation elicited by these stimulus whole cell recording reveals the total synaptic input to a neuron from all the other neuron in the circuit instead of just it output a sparse binary spike train a in conventional single unit physiological recording whole cell recording thus provides a much richer source of information about the neuron s response many neuron responded robustly and reliably to the complex stimulus in our ensemble here we analyze the linear component the spectrotemporal receptive field strf of the transformation from the sound a represented by it time varying spectrogram to the neuron s membrane potential we find that the strf ha a rich dynamical structure including excitatory region positioned in general accord with the prediction of the simple tuning curve we also find that in many case much of the neuron s response although deterministically related to the stimulus cannot be predicted by the linear component indicating the presence of a yet uncharacterized nonlinear response property 
we interpret several well known algorithm for dimensionality reduction of manifold a kernel method isomap graph laplacian eigenmap and locally linear embedding lle all utilize local neighborhood information to construct a global embedding of the manifold we show how all three algorithm can be described a kernel pca on specially constructed gram matrix and illustrate the similarity and difference between the algorithm with representative example 
abstract this paper summarizes the author s experience implementing somethingresembling abstract type in acl it assumes some experiencewith the acl theorem prover 
we propose modeling image and related visual object a bag ofpixels or set of vector for instance gray scale image aremodeled a a collection or bag of x y i pixel vector thisrepresentation implies a permutational invariance over the bag ofpixels which is naturally handled by endowing each image with apermutation matrix each matrix permit the image to span amanifold of multiple configuration capturing the vector set sinvariance to ordering or permutation transformation permutationconfigurations are optimized while jointly modeling many image viamaximum likelihood the solution is a uniquely solvable convexprogram which computes correspondence simultaneously for all image a opposed to traditional pairwise correspondence solution maximum likelihood performs a nonlinear dimensionality reduction choosing permutation that compact the permuted image vector intoa volumetrically minimal subspace this is highly suitable forprincipal component analysis which when applied to thepermutationally invariant bag of pixel representation outperformspca on appearance based vectorization by order ofmagnitude furthermore the bag of pixel subspace benefit fromautomatic correspondence estimation giving rise to meaningfullinear variation such a morphings translation and jointlyspatio textural image transformation result are shown forseveral datasets 
deduplication is a key operation in integrating data from multiple source the main challenge in this task is designing a function that can resolve when a pair of record refer to the same entity in spite of various data inconsistency most existing system use hand coded function one way to overcome the tedium of hand coding is to train a classifier to distinguish between duplicate and non duplicate the success of this method critically hinge on being able to provide a covering and challenging set of training pair that bring out the subtlety of deduplication function this is non trivial because it requires manually searching for various data inconsistency between any two record spread apart in large list we present our design of a learning based deduplication system that us a novel method of interactively discovering challenging training pair using active learning our experiment on real life datasets show that active learning significantly reduces the number of instance needed to achieve high accuracy we investigate various design issue that arise in building a system to provide interactive response fast convergence and interpretable output 
there is controversy a to whether explicit support for pddl like axiom and derived predicate is needed for planner to handle real world domain effectively many researcher have deplored the lack of precise semantics for such axiom while others have argued that it might be best to compile them away we propose an adequate semantics for pddl axiom and show that they are an essential feature by proving that it is impossible to compile them away if we restrict the growth of plan and domain description to be polynomial these result suggest that adding a reasonable implementation to handle axiom inside the planner is beneficial for the performance our experiment confirm this suggestion 
increasing dialogue efficiency in case based reasoning cbr must be balanced against the risk of commitment to a sub optimal solution focusing on incremental query elicitation in recommender system we examine the limitation of naive strategy such a terminating the dialogue when the similarity of any case reach a predefined threshold we also identify necessary and sufficient condition for recommendation dialogue to be terminated without loss of solution quality finally we evaluate a number of attribute selection strategy in term of dialogue efficiency given the requirement that there must be no loss of solution quality 
simulation based training is increasingly being used within the military to practice and develop the skill of successful soldier for the skill associated with successful military leadership our inability to model human behavior to the necessary degree of fidelity in constructive simulation requires that new interactive design be developed the ict leader project support leadership development through the use of branching storyline realized within a virtual reality environment trainee assume a role in a fictional scenario where the decision that they make in this environment ultimately affect the success of a mission all trainee decision are made in the context of natural language conversation with virtual character the ict leader project advance a new form of interactive training by incorporating a suite of artificial intelligence technology including control architecture agent of mixed autonomy and natural language processing algorithm 
abstract knowledge about local invariance with respect to given pattern transformation can greatly improve the accuracy of classification previous approach are either based on regularisation or on the generation of virtual transformed example we develop a new framework for learning linear classifier under known transformation based on semidefinite programming we present a new learning algorithm the semidefinite programming machine sdpm which is able to find a maximum margin hyperplane when the training example are polynomial trajectory instead of single point the solution is found to be sparse in dual variable and allows to identify those point on the trajectory with minimal real valued output a virtual support vector extension to segment of trajectory to more than one transformation parameter and to learning with kernel are discussed in experiment we use a taylor expansion to locally approximate rotational invariance in pixel image from usps and find improvement over known method 
a population of neuron typically exhibit a broad diversity of response to sensory input the intuitive notion of functional classification is that cell can be clustered so that most of the diversity is captured by the identity of the cluster rather than by individual within cluster we show how this intuition can be made precise using information theory without any need to introduce a metric on the space of stimulus or response applied to the retinal ganglion cell of the salamander this approach recovers classical result but also provides clear evidence for subclass beyond those identified previously further we find that each of the ganglion cell is functionally unique and that even within the same subclass only a few spike are needed to reliably distinguish between cell 
weprovideamethodformassmeta analysisinaneuroinformatics database containing stereotaxic talairach coordinate from neuroimaging experiment database label are used to group the individualexperiments e g accordingtocognitivefunction andthe consistent pattern of the experiment within the group are determined themethodvoxelizeseachgroupofexperimentviaakernel densityestimationformingprobabilitydensityvolumes thevalues intheprobabilitydensityvolumesarecomparedtonull hypothesis distributionsgeneratedfromresamplingintheentireunlabeledset of experiment and the distance to the null hypothesis are used to sort the voxel across group of experiment this allows for mass meta analysis with the construction of a list with the most prominent association between brain area and group label furthermore themethodcanbeusedforfunctionallabelingofvoxels 
recently the isomap algorithm ha been proposed for learning a nonlinear manifold from a set of unorganized high dimensional data point it is based on extending the classical multidimensional scaling method for dimension reduction in this paper we present a continuous version of isomap which we call continuum isomap and show that manifold learning in the continuous framework is reduced to an eigenvalue problem of an integral operator we also show that the continuum isomap can perfectly recover the underlying natural parametrization if the nonlinear manifold can be isometrically embedded onto an euclidean space several numerical example are given to illustrate the algorithm 
various problem in machine learning database and statistic involve pairwise distance among a set of object it is often desirable for these distance to satisfy the property of a metric especially the triangle inequality application where metric data is useful include clustering classification metric based indexing and approximation algorithm for various graph problem this paper present the metric nearness problem given a dissimilarity matrix find the nearest matrix of distance that satisfy the triangle inequality for p nearness measure this paper develops efficient triangle fixing algorithm that compute globally optimal solution by exploiting the inherent structure of the problem empirically the algorithm have time and storage cost that are linear in the number of triangle constraint the method can also be easily parallelized for additional speed 
abstract particle filtering is a very popular technique for sequential state estimation problem however it convergence greatly depends on the balance between the number of particle hypothesis and the fitness of the dynamic model in particular in case where the dynamic are complex or poorly modeled thousand of particle are usually required for real application this paper present a hybrid sampling solution that combine the sampling in the image feature space and in the state space via ransac and particle filtering respectively we show that the number of particle can be reduced to dozen for a full d tracking problem which contains considerable noise of different type for unexpected motion a specific set of dynamic may not exist but it is avoided in our algorithm the theoretical convergence proof for particle filtering when integrating ransac is difficult but we address this problem by analyzing the likelihood distribution of particle from a real tracking example the sampling efficiency on the more likely area is much higher by the use of ransac we also discus the tracking quality measurement in the sense of entropy or statistical testing the algorithm ha been applied to the problem of d face pose tracking with changing moderate or intense expression we demonstrate the validity of our approach with several video sequence acquired in an unstructured environment key word random projection ransac particle filtering robust d face tracking 
active and semi supervised learning are important technique when labeled data are scarce we combine the two under a gaussian random field model labeled and unlabeled data are represented a vertex in a weighted graph with edge weight encoding the similarity between instance the semi supervised learning problem is then formulated in term of a gaussian random field on this graph the mean of which is characterized in term of harmonic function active learning is performed on top of the semisupervised learning scheme by greedily selecting query from the unlabeled data to minimize the estimated expected classification error risk in the case of gaussian field the risk is efficiently computed using matrix method we present experimental result on synthetic data handwritten digit recognition and text classification task the active learning scheme requires a much smaller number of query to achieve high accuracy compared with random query selection 
standard ir system can process query such a web not internet enabling user who are interested in arachnid to avoid document about computing the document retrieved for such a query should be irrelevant to the negated query term most system implement this by reprocessing result after retrieval to remove document containing the unwanted string of letter this paper describes and evaluates a theoretically motivated method for removing unwanted meaning directly from the original query in vector model with the same vector negation operator a used in quantum logic irrelevance in vector space is modelled using orthogonality so query vector are made orthogonal to the negated term or term a well a removing unwanted term this form of vector negation reduces the occurrence of synonym and neighbour of the negated term by a much a compared with standard boolean method by altering the query vector itself vector negation remove not only unwanted string but unwanted meaning 
existing algorithm for discrete partially observable markov decision process can at best solve problem of a few thousand state due to two important source of intractability the curse of dimensionality and the policy space complexity this paper describes a new algorithm vdcbpi that mitigates both source of intractability by combining the value directed compression vdc technique with bounded policy iteration bpi the scalability of vdcbpi is demonstrated on synthetic network management problem with up to million state 
we consider the problem of updating nonmonotonic knowledge base represented by epistemic logic program where disjunctive information and notion of knowledge and belief can be explicitly expressed we propose a formulation for epistemic logic program update based on a principle called minimal change and maximal coherence the central feature of our approach is that during an update procedure contradictory information is removed on a basis of minimal change under the semantics of epistemic logic program and then coherent information is maximally retained in the update result by using our approach we can characterize an update result in both semantic and syntactic form we show that our approach handle update sequence and satisfies the consistency requirement we also investigate important semantic property of our update approach such a reduction persistence and preservation 
bayesian regularization and nonnegative deconvolution brand is proposed for estimating time delay of acoustic signal in reverberant environment sparsity of the nonnegative filter coefficient is enforced using an l norm regularization a probabilistic generative model is used to simultaneously estimate the regularization parameter and filter coefficient from the signal data iterative update rule are derived under a bayesian framework using the expectation maximization procedure the resulting time delay estimation algorithm is demonstrated on noisy acoustic data 
two important architectural choice underlie the success of the web numerous independently operated server speak a common protocol and a single type of client the web browser provides point and click access to the content and service on these decentralized server however because html marries content and presentation into a single representation end user are often stuck with inappropriate choice made by the web site designer of how to work with and view the content rdf metadata on the semantic web doe not have this limitation user can gain direct access to information and control over how it is presented this principle form the basis for our semantic web browser an end user application that automatically locates metadata and assembles point and click interface from a combination of relevant information ontological specification and presentation knowledge all described in rdf and retrieved dynamically from the semantic web because data and service are accessed directly through a standalone client and not through a central point of access e g a portal new content and service can be consumed a soon a they become available in this way we take advantage of an important sociological force that encourages the production of new semantic web content while remaining faithful to the decentralized nature of the web 
we extend recent work on the connection between loopy belief propagation and the bethe free energy constrained minimization of the bethe free energy can be turned into an unconstrained saddle point problem both converging double loop algorithm and standard loopy belief propagation can be interpreted a attempt to solve this saddle point problem stability analysis then lead u to conclude that stable xed point of loopy belief propagation must be local minimum of the bethe free energy perhaps surprisingly the converse need not be the case minimum can be unstable xed point we illustrate this with an example and discus implication 
background modeling and subtraction is a core componentin motion analysis the central idea behind such moduleis to create a probabilistic representation of the staticscene that is compared with the current input to performsubtraction such approach is efficient when the scene to bemodeled refers to a static structure with limited perturbation in this paper we address the problem of modeling dynamicscenes where the assumption of a static backgroundis not valid waving tree beach escalator naturalscenes with rain or snow are example inspired by the workproposed in we propose an on line auto regressivemodel to capture and predict the behavior of such scene towards detection of event we introduce a new metric thatis based on a state driven comparison between the predictionand the actual frame promising result demonstratethe potential of the proposed framework 
we analyze the convergence property of three spike triggered data analysis technique all of our result are obtained in the setting of a possibly multidimensional linear nonlinear ln cascade model for stimulus driven neural activity we start by giving exact rate of convergence result for the common spike triggered average sta technique next we analyze a spike triggered covariance method variant of which have been recently exploited successfully by bialek simoncelli and colleague these rst two method suffer from extraneous condition on their convergence therefore we introduce an estimator for the ln model parameter which is designed to be consistent under general condition we provide an algorithm for the computation of this estimator and derive it rate of convergence we close with a brief discussion of the eciency of these estimator and an application to data recorded from the primary motor cortex of awake behaving primate 
learning kernel parameter is important for kernel based method because these parameter have significant impact on the generalization ability of these method besides the method of cross validation and leave one out minimizing some upper bound on the generalization error such a the radius margin bound wa also proposed to more efficiently learn the optimal kernel parameter in this paper a class separability criterion is proposed for learning kernel parameter the optimal kernel parameter are regarded a those that can maximize the class separability in the induced feature space with this criterion learning the kernel parameter in svm can avoid solving the quadratic programming problem the relationship between this criterion and the radius margin bound is also explored both theoretical analysis and experimental result show that the class separability criterion is effective in learning kernel parameter for svm 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we describe a procedure which find a hierarchical clustering by hillclimbing the cost function we use is a hierarchical extension of the mean cost our local move are tree restructurings and node reordering we show these can be accomplished efficiently by exploiting special property of squared euclidean distance and by using technique from scheduling and vlsi algorithm 
we describe a modification to the adaboostalgorithm that permit the incorporation ofprior human knowledge a a mean of compensatingfor a shortage of training data wegive a convergence result for the algorithm 
existing source location and recovery algorithm used in magnetoencephalographic imaging generally assume that the source activity at different brain location is independent or that the correlation structure is known however electrophysiological recording of local field potential show strong correlation in aggregate activity over significant distance indeed it seems very likely that stimulus evoked activity would follow strongly correlated time course in different brain area here we present and validate through simulation a new approach to source reconstruction in which the correlation between source is modelled and estimated explicitly by variational bayesian method facilitating accurate recovery of source location and the time course of their activation 
many sensing technique and image processing applicationsare characterized by noisy or corrupted image data anisotropic diffusion is a popular and theoretically wellunderstood technique for denoising such image diffusionapproaches however require the selection of an edgestopping function the definition of which is typically adhoc we exploit and extend recent work on the statisticsof natural image to define principled edge stopping functionsfor different type of imagery we consider a varietyof anisotropic diffusion scheme and note that they computespatial derivative at fixed scale from which we estimatethe appropriate algorithm specific image statistic goingbeyond traditional work on image statistic we also modelthe statistic of the eigenvalue of the local structure tensor novel edge stopping function are derived from these imagestatistics giving a principled way of formulating anisotropicdiffusion problem in which all edge stopping parametersare learned from training data 
we address the problem of finding a set of contour curve in a d or d image we consider the problem of perceptual grouping and contour completion where the data is an unstructured set of region in the image a new method to find complete curve from a set of edge point is presented contour are found a minimal path between connected component using the fast marching algorithm we find the minimal path between each of these component until the complete set of these region is connected the path are obtained using backpropagation from the saddle point to both component we then extend this technique to d the data is a set of connected component in a d image we find d minimal path that link together these component using a potential based on vessel detection we illustrate the capability of our approach to reconstruct tree structure in a d medical image dataset 
this paper proposes the hierarchical directed acyclic graph hdag kernel for structured natural language data the hdag kernel directly accepts several level of both chunk and their relation and then efficiently computes the weighed sum of the number of common attribute sequence of the hdags we applied the proposed method to question classification and sentence alignment task to evaluate it performance a a similarity measure and a kernel function the result of the experiment demonstrate that the hdag kernel is superior to other kernel function and baseline method 
identity uncertainty is a pervasive problem in real world data analysis it arises whenever object are not labeled with unique identifier or when those identifier may not be perceived perfectly in such case two observation may or may not correspond to the same object in this paper we consider the problem in the context of citation matching the problem of deciding which citation correspond to the same publication our approach is based on the use of a relational probability model to define a generative model for the domain including model of author and title corruption and a probabilistic citation grammar identity uncertainty is handled by extending standard model to incorporate probability over the possible mapping between term in the language and object in the domain inference is based on markov chain monte carlo augmented with specific method for generating efficient proposal when the domain contains many object result on several citation data set show that the method outperforms current algorithm for citation matching the declarative relational nature of the model also mean that our algorithm can determine object characteristic such a author name by combining multiple citation of multiple paper 
we describe how simple commonly understood statistical model such a statistical dependency parser probabilistic context free grammar and word to word translation model can be effectively combined into a unified bilingual parser that jointly search for the best english parse korean parse and word alignment where these hidden structure all constrain each other the model used for parsing is completely factored into the two parser and the tm allowing separate parameter estimation we evaluate our bilingual parser on the penn korean treebank and against several baseline system and show improvement parsing korean with very limited labeled data 
preference elicitation is a serious bottleneck in many decision support application and agent spec ification task cp net were designed to make the preference elicitation process simpler and more in tuitive for lay user by graphically structuring a set of ceteris paribus cp preference statement preference statement most people find natural and intuitive in various context cp net with an un derlying cyclic structure emerge naturally often they are inconsistent according to the current se mantics and the user is required to revise them in this paper we show how optimization query can be meaningfully answered in many inconsistent network without troubling the user with request for revision we also describe a method for focus ing user revision process when revision are truly needed in the process we provide a formal seman tic that justifies our approach and we introduce new technique for computing optimal outcome 
over constrained problem can have an exponential number of conflict which explain the failure and an exponential number of relaxation which restore the consistency a user of an interactive application however desire explanation and relaxation containing the most important constraint to address this need we define preferred explanation and relaxation based on user preference between constraint and we compute them by a generic method which work for arbitrary cp sat or dl solver we significantly accelerate the basic method by a divide and conquer strategy and thus provide the technological basis for the explanation facility of a principal industrial constraint programming tool which is for example used in numerous configuration application 
we describe a method that can make a scanned handwritten mediaeval latin manuscript accessible to full text search a generalized hmm is tted using transcribed latin to obtain a transition model and one example each of letter to obtain an emission model we show result for unigram bigram and trigram model our method transcribes page of a manuscript of terence with fair accuracy of letter correctly transcribed search result are very strong we use example of variant spelling to demonstrate that the search respect the ink of the document furthermore our model produce fair search on a document from which we obtained no training data intoduction there are many large corpus of handwritten scanned document and their number is growing rapidly collection range from the complete work of mark twain to thousand of page of zoological note spanning two century large scale analysis of such corpus is currently very dif cult because handwriting recognition work poorly recently rath and manmatha have demonstrated that one can use small body of aligned material a supervised data to train a word spotting mechanism the result can make scanned handwritten document searchable current technique assume a closed vocabulary one can search only for word in the training set and search for instance of whole word this approach is particularly unattractive for an in ected language because individual word can take so many form that one is unlikely to see all in the training set furthermore one would like the method used to require very little aligned training data so that it is possible to process document written by different scribe with little overhead mediaeval latin manuscript are a natural rst corpus for studying this problem because there are many scanned manuscript and because the handwriting is relatively regular we expect the primary user need to be search over a large body of document to allow comparison between document rather than transcription of a particular document which is usually relatively easy to do by hand desirable feature for a system are first that it use little or no aligned training data an 
a formal framework for specifying and developing agent robot must handle not only knowledge and sensing action but also time and concurrency researcher have extended the situation calculus to handle knowledge and sensing action other researcher have addressed the issue of adding time and concurrent action here both of these feature are combined into a united logical theory of knowledge sensing time and concurrency the result preserve the solution to the frame problem of previous work maintains the distinction between indexical and objective knowledge of time and is capable of representing the various way in which concurrency interacts with time and knowledge furthermore a method based on regression is developed for solving the projection problem for theory specified in this version of the situation calculus 
in this paper we provide a fast data driven solution to the failing query problem given a query that return an empty answer how can one relax the query s constraint so that it return a non empty set of tuples we introduce a novel algorithm loqr which is designed to relax query that are in the disjunctive normal form and contain a mixture of discrete and continuous attribute loqr discovers the implicit relationship that exist among the various domain attribute and then us this knowledge to relax the constraint from the failing query in a first step loqr us a small randomly chosen subset of the target database to learn a set of decision rule that predict whether an attribute s value satisfies the constraint in the failing query this query driven operation is performed online for each failing query in the second step loqr us nearest neighbor technique to find the learned rule that is the most similar to the failing query then it us the attribute value from this rule to relax the failing query s constraint our experiment on six application domain show that loqr is both robust and fast it successfully relaxes more than of the failing query and it take under a second for processing query that consist of up to attribute larger query of up to attribute are processed in several second 
for a robot the ability to get from one place to another is one of the most basic skill however locomotion on legged robot is a challenging multidimensional control problem this paper present a machine learning approach to legged locomotion with all training done on the physical robot the main contribution are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithm for learning quadrupedal locomotion the resulting learned walk is considerably faster than all previously reported hand coded walk for the same robot platform 
we describe ineats an interactive multi document summarization system that integrates a state of the art summarization engine with an advanced user interface three main goal of the system are provide a user with control over the summarization process support exploration of the document set with the summary a the staring point and combine text summary with alternative presentation such a a map based visualization of document 
subspace learning approach have attracted much attention in academia recently however the classical batch algorithm no longer satisfy the application on streaming data or large scale data to meet this desirability incremental principal component analysis ipca algorithm ha been well established but it is an unsupervised subspace learning approach and is not optimal for general classification task such a face recognition and web document categorization in this paper we propose an incremental supervised subspace learning algorithm called incremental maximum margin criterion immc to infer an adaptive subspace by optimizing the maximum margin criterion we also present the proof for convergence of the proposed algorithm experimental result on both synthetic dataset and real world datasets show that immc converges to the similar subspace a that of batch approach 
a fast and accurate unsupervised clustering algorithm ha been developed for clustering very large datasets though designed for very large volume of geospatial data the algorithm is general enough to be used in a wide variety of domain application the number of computation the algorithm requires is o n and thus faster than hierarchical algorithm unlike the popular k mean heuristic this algorithm doe not require a series of iteration to converge to a solution in addition this method doe not depend on initialization of a given number of cluster representative and so is insensitive to initial condition being unsupervised the algorithm can also rank each cluster based on density the method relies on weighting a dataset to grid point on a mesh and using a small number of rule based agent to find the high density cluster this method effectively reduces large datasets to the size of the grid which is usually many order of magnitude smaller numerical experiment are shown that demonstrate the advantage of this algorithm over other technique 
most existing tracking algorithm construct a representation of a target object prior to the tracking task start and utilize invariant feature to handle appearance variation of the target caused by lighting pose and view angle change in this paper we present an efficient and effective online algorithm that incrementally learns and adapts a low dimensional eigenspace representation to reflect appearance change of the target thereby facilitating the tracking task furthermore our incremental method correctly update the sample mean and the eigenbasis whereas existing incremental subspace update method ignore the fact the sample mean varies over time the tracking problem is formulated a a state inference problem within a markov chain monte carlo framework and a particle filter is incorporated for propagating sample distribution over time numerous experiment demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environment where the target object undergo large pose and lighting change 
recommender system have emerged in the past several year a an effective way to help people cope with the problem of information overload one application in which they have become particularly common is in e commerce where recommendation of item can often help a customer find what she is interested in and therefore can help drive sale unscrupulous producer in the never ending quest for market penetration may find it profitable to shill recommender system by lying to the system in order to have their product recommended more often than those of their competitor this paper explores four open question that may affect the effectiveness of such shilling attack which recommender algorithm is being used whether the application is producing recommendation or prediction how detectable the attack are by the operator of the system and what the property are of the item being attacked the question are explored experimentally on a large data set of movie rating taken together the result of the paper suggest that new way must be used to evaluate and detect shilling attack on recommender system 
we are entering an era of more intelligent cognitive vision system such system can analyse activity in dynamic scene to compute conceptual description from motion trajectory of moving people and the object they interact with here we review progress in the development of flexible generative model that can explain visual input a a combination of hidden variable and can adapt to new type of input such model are particularly appropriate for the task posed by cognitive vision a 
we propose a new method for estimating intrinsic dimension of a dataset derived by applying the principle of maximum likelihood to the distance between close neighbor we derive the estimator by a poisson process approximation ass it bias and variance theoretically and by simulation and apply it to a number of simulated and real datasets we also show it ha the best overall performance compared with two other intrinsic dimension estimator 
using sql ha not been considered an efficient and feasible way to implement data mining algorithm although this is true for many data mining machine learning and statistical algorithm this work show it is feasible to get an efficient sql implementation of the well known k mean clustering algorithm that can work on top of a relational dbms the article emphasizes both correctness and performance from a correctness point of view the article explains how to compute euclidean distance nearest cluster query and updating clustering result in sql from a performance point of view it is explained how to cluster large data set defining and indexing table to store and retrieve intermediate and final result optimizing and avoiding join optimizing and simplifying clustering aggregation and taking advantage of sufficient statistic experiment evaluate scalability with synthetic data set varying size and dimensionality the proposed k mean implementation can cluster large data set and exhibit linear scalability 
automated image interpretation is an important task with numerous application until recently designing such system required extensive subject matter and computer vision expertise resulting in poor cross domain portability and expensive maintenance recently a machine learned system adore wa successfully applied in an aerial image interpretation domain subsequently it wa re trained for another man made object recognition task in this paper we propose and implement several extension of adore addressing it primary limitation these extension enable the first successful application of this emerging ai technology to a natural image interpretation domain the resulting system is shown to be robust with respect to noise in the training data illumination and camera angle variation a well a competitively adaptive with respect to novel image 
this paper describes and evaluates log linear parsing model for combinatory categorial grammar ccg a parallel implementation of the l bfgs optimisation algorithm is described which run on a beowulf cluster allowing the complete penn treebank to be used for estimation we also develop a new efficient parsing algorithm for ccg which maximises expected recall of dependency we compare model which use all ccg derivation including non standard derivation with normal form model the performance of the two model are comparable and the result are competitive with existing wide coverage ccg parser 
the constraint classification framework capture many flavor of multiclass classification including winner take all multiclass classification multilabel classification and ranking we present a meta algorithm for learning in this framework that learns via a single linear classifier in high dimension we discus distribution independent a well a margin based generalization bound and present empirical and theoretical evidence showing that constraint classification benefit over existing method of multiclass classification 
unlike conventional data or text web page typically contain a large amount of information that is not part of the main content of the page e g banner ad navigation bar and copyright notice such irrelevant information which we call web page noise in web page can seriously harm web mining e g clustering and classification in this paper we propose a novel feature weighting technique to deal with web page noise to enhance web mining this method first build a compressed structure tree to capture the common structure and comparable block in a set of web page it then us an information based measure to evaluate the importance of each node in the compressed structure tree based on the tree and it node importance value our method assigns a weight to each word feature in it content block the resulting weight are used in web mining we evaluated the proposed technique with two web mining task web page clustering and web page classification experimental result show that our weighting method is able to dramatically improve the mining result 
using neural network to represent value function in reinforcement learning algorithm often involves a lot of work in hand crafting the network structure and tuning the learning parameter in this paper we explore the potential of using constructive neural network in reinforcement learning constructive neural network method are appealing because they can build the network structure based on the data that need to be represented to our knowledge such algorithm have not been used in reinforcement learning a major issue is that constructive algorithm often work in batch mode while many reinforcement learning algorithm work on line we use a cache to accumulate data then use a variant of cascade correlation to update the value function preliminary result on the game of tic tac toe show the potential of this new algorithm compared to using static feed forward neural network trained with backpropagation 
a lexical signature of a web page is often sufficient for finding the page even if it url ha changed we conduct a large scale empirical study of eight method for generating lexical signature including phelps and wilensky s original proposal pw and seven of our own variation we examine their performance on the web and on a trec data set evaluating their ability both to uniquely identify the original document and to locate other relevant document if the original is lost lexical signature chosen to minimize document frequency df are good at unique identification but poor at finding relevant document pw work well on the relatively small trec data set but act almost identically to df on the web which contains billion of document term frequency based lexical signature tf are very easy to compute and often perform well but are highly dependent on the ranking system of the search engine used in general tfidf based method and hybrid method which combine df with tf or tfidf seem to be the most promising candidate for generating effective lexical signature 
the mean algorithm is by far the most widely used method for discovering cluster in data we show how to accelerate it dramatically while still always computing exactly the same result a the standard algorithm the accelerated algorithm avoids unnecessary distance calculation by applying the triangle inequality in two different way and by keeping track of lower and upper bound for distance between point and center experiment show that the new algorithm is effective for datasets with up to dimension and becomes more and more effective a the number of cluster increase for it is many time faster than the best previously known accelerated mean method 
many real world graph have been shown to be scale free vertex degree follow power law distribution vertex tend to cluster and the average length of all shortest path is small we present a new model for understanding scale free network based on multilevel geodesic approximation using a new data structure called a multilevel mesh using this multilevel framework we propose a new kind of graph clustering for data reduction of very large graph system such a social biological or electronic network finally we apply our algorithm to real world social network and protein interaction graph to show that they can reveal knowledge embedded in underlying graph structure we also demonstrate how our data structure can be used to quickly answer approximate distance and shortest path query on scale free network 
this paper describes two method for detecting word segment and their morphological information in a japanese spontaneous speech corpus and describes how to tag a large spontaneous speech corpus accurately by using the two method the first method is used to detect any type of word segment the second method is used when there are several definition for word segment and their po category and when one type of word segment includes another type of word segment in this paper we show that by using semi automatic analysis we achieve a precision of better than for detecting and tagging short word and for long word the two type of word that comprise the corpus we also show that better accuracy is achieved by using both method than by using only the first 
in many application good ranking is a highly desirable performance for a classifier the criterion commonly used to measure the rank ing quality of a classification algorithm is the area under the roc curve auc to report it properly it is crucial to determine an interval of confidence for it value this paper provides confidence interval for the a uc based on a statistical and combinatorial analysis using only simple parameter such a the error rate and the number of positive and negative example the analysis is distribution independent it make no assumption about the distribution of the score of negative or positive example the result are of practical use and can be viewed a the equivalent for auc of the standard confidence interval given in the case of the error r ate they are compared with previous approach in several standard classification task demonstrating the benefit of our analysis 
this paper describes the process of constructing a markup language for maritime information from the starting point of ontology building ontology construction from source material in the maritime information domain is outlined the structure of the markup language is described in term of xml schema and dtds a prototype application that us the markup language is also described 
super solution are solution in which if a small number of variable lose their value we are guaranteed to be able to repair the solution with only a few change in this paper we stress the need to extend the super solution framework along several dimension to make it more useful practically we demonstrate the usefulness of those extension on an example from jobshop scheduling an optimization problem solved through constraint satisfaction in such a case there is indeed a trade off between optimality and robustness however robustness may be increased without sacrificing optimality 
this paper describes method for service selection and service access for mobile sensor enhanced web client such a wireless camera or wireless pda with sensor device attached the client announce their data creating capability in produce header sent to server server respond with form that match these capability client fill in these form with sensor data a well a text or file data the resultant system enables client to access dynamically discovered service spontaneously a their user engage in everyday nomadic activity 
with the proliferation of heterogeneous device desktop computer personal digital assistant phone multimedia document must be played under various constraint small screen low bandwidth taking these constraint into account with current document model is impossible hence generic source document must be transformed into document compatible with the target context currently the design of transformation is left to programmer we propose here a semantic framework which account for multimedia document adaptation in very general term a model of a multimedia document is a potential execution of this document and a context defines a particular class of model the adaptation should then retain the source document model that belong to the class defined by the context if such model exist otherwise the adaptation should produce a document whose model belong to this class and are close to those of the source document we focus on the temporal dimension of multimedia document and show how adaptation can take advantage of temporal reasoning technique several metric are given for assessing the proximity of model 
abstract this paper introduces an algorithm for the automatic relevance determinationof input variable in kernelized support vector machine relevanceis measured by scale factor defining the input space metric andfeature selection is performed by assigning zero weight to irrelevantvariables the metric is automatically tuned by the minimization of thestandard svm empirical risk where scale factor are added to the usualset of parameter defining the classifier feature selection is 
the goal of this paper is to show that generalizing the notion of support can be useful in extending association analysis to non traditional type of pattern and non binary data to that end we describe a framework for generalizing support that is based on the simple but useful observation that support can be viewed a the composition of two function a function that evaluates the strength or presence of a pattern in each object transaction and a function that summarizes these evaluation with a single number a key goal of any framework is to allow people to more easily express explore and communicate idea and hence we illustrate how our support framework can be used to describe support for a variety of commonly used association pattern such a frequent itemsets general boolean pattern and error tolerant itemsets we also present two example of the practical usefulness of generalized support one example show the usefulness of support function for continuous data another example show how the hyperclique pattern an association pattern originally defined for binary data can be extended to continuous data by generalizing a support function 
this paper address agent intention a building block of imitation learning that abstract local situation of the agent and proposes a hierarchical hidden markov model hmm to represent cooperative behavior of teamwork the key of the proposed model is introduction of gate probability that restrict transition among agent intention according to others intention using these probability the framework can control transition flexibly among basic behavior in a cooperative behavior 
due to the increasing availability and use of digital video data on the web video caching will be an important performance factor in the future www we propose an architecture of a video proxy cache that integrates modern multimedia and communication standard especially we describe feature of the mpeg and mpeg multimedia standard that can be helpful for a video proxy cache qbix support real time adaptation in the compressed and in the decompressed domain it us adaptation to improve the cache replacement strategy in the proxy but also to realize medium gateway functionality driven by the client terminal capability 
we describe the ongoing construction of a large semantically annotated corpus resource a reliable basis for the large scale acquisition of word semantic information e g the construction of domain independent lexica the backbone of the annotation are semantic role in the frame semantics paradigm we report experience and evaluate the annotated data from the first project stage on this basis we discus the problem of vagueness and ambiguity in semantic annotation 
this demonstration utilizes a geographic information system interface to display multilingual news document in time and space by extracting place name from text and matching them to a multilingual multi script gazetteer which identifies the latitude and longitude of the location 
this paper proposes an example based phrase translation method in a chinese to english cross language information retrieval clir system the method can generate much more accurate query translation than dictionary based and common mt based method and then improves the retrieval performance of our clir system 
researcher in web engineering have regularly noted that existing web application development environment provide little support for managing the evolution of web application key limitation of web development environment include line oriented change model that inadequately represent web document semantics and in ability to model change to link structure or the set of object making up the webapplication developer may find it difficult to grasp how theoverall structure of the web application ha changed over time and may respond by using ad hoc solution that lead to problem of maintain ability quality and reliability web application are software artifact and a such can benefit from advanced version control and software configuration management scm technology from software engineering we have modified an integrated development environment to manage the evolution and maintenance of web application the resulting environment is distinguished by itsfine grained version control framework fine grained web contentchange management and product versioning configuration management in which a web project can be organized at the logical level and itsstructure and component are versioned in a fine grained manner aswell this paper describes the motivation for this environment a well a it user interface feature and implementation 
classification is a well established operation in text mining given a set of label a and a set da of training document tagged with these label a classifier learns to assign label to unlabeled test document suppose we also had available a different set of label b together with a set of document db marked with label from b if a and b have some semantic overlap can the availability of db help u build a better classifier for a and vice versa we answer this question in the affirmative by proposing cross training a new approach to semi supervised learning in presence of multiple label set we give distributional and discriminative algorithm for cross training and show through extensive experiment that cross training can discover and exploit probabilistic relation between two taxonomy for more accurate classification 
in this paper we consider the solution of scheduling problem that are inherently over subscribed in such problem there are always more task to execute within a given time frame than available resource capacity will allow and hence decision must be made about which task should be included in the schedule and which should be excluded we adopt a controlled iterative repair search approach and focus on improving the result of an initial priority driven solution generation procedure central to our approach is a new retraction heuristic termed max flexibility which is responsible for identifying which task to temporarily retract from the schedule for reassignment in an effort to incorporate additional task into the schedule the max flexibility heuristic chooses those task that have maximum flexibility for assignment within their feasible window we empirically evaluate the performance of max flexibility using problem data and the basic scheduling procedure from a fielded airlift mission scheduling application we show that it produce better improvement result than two contention based retraction heuristic including a variant of min conflict l minton et al with significantly le search and computational cost 
two new technique based on nonparametric estimation of probability density are introduced which improve on the performance of equivalent robust method currently employed in computer vision the first technique draw from the projection pursuit paradigm in statistic and carry out regression m estimation with a weak dependence on the accuracy of the scale estimate the second technique exploit the property of the multivariate adaptive mean shift and accomplishes the fusion of uncertain measurement arising from an unknown number of source a an example the two technique are extensively used in an algorithm for the recovery of multiple structure from heavily corrupted data 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
previous work in pruning algorithm for maxn multi player game tree ha produced shallow pruning and alpha beta branch and bound pruning the effectiveness of these algorithm is dependant a much on the range of terminal value found in the game tree a on the ordering of node we introduce last branch and speculative pruning technique which can prune any constantsum multi player game tree their effectiveness depends only on node ordering within the game tree a b grows large these algorithm will in the best case reduce the branching factor of a nplayer game from b to b n n in chinese checker these method reduce average expansion at depth from million to k node and in heart and spade they increase the average search depth by ply 
the goal of clustering is to identify distinct group in a dataset compared to non parametric clustering method like complete linkage hierarchical model based clustering ha the advantage of offering a way to estimate the number of group present in the data however it computational cost is quadratic in the number of item to be clustered and it is therefore not applicable to large problem we review an idea called fractionation originally conceived by cutting karger pedersen and tukey for non parametric hierarchical clustering of large datasets and describe an adaptation of fractionation to model based clustering a further extension called refractionation lead to a procedure that can be successful even in the difficult situation where there are large number of small group 
aliasing occurs in web transaction when request containing different url elicit reply containing identical data payload conventional cache associate stored data with url and can therefore suffer redundant payload transfer due to aliasing and other cause existing research literature however say little about the prevalence of aliasing in user initiated transaction or about redundant payload transfer in conventional web cache hierarchy this paper quantifies the extent of aliasing and the performance impact of url indexed cache management using a large client trace from webtv network fewer than of reply payload are aliased referenced via multiple url but over of successful transaction involve aliased payload aliased payload account for under of the trace s working set size sum of payload size but over of byte transferred for the webtv workload roughly of payload transfer to browser cache and of payload transfer to a shared proxy are redundant assuming infinite capacity conventional cache our analysis of a large proxy trace from compaq corporation yield similar result url indexed caching doe not entirely explain the large number of redundant proxy to browser payload transfer previously reported in the webtv system we consider other possible cause of redundant transfer e g reply metadata and browser cache management policy and discus a simple hop by hop protocol extension that completely eliminates all redundant transfer regardless of cause 
an interpretation system find the likely mapping from portion of an image to real world object an interpretation policy specifies when to apply which imaging operator to which portion of the image during every stage of interpretation earlier result compared a number of policy and demonstrated that policy that select operator which maximize the information gain per cost worked most effectively however those policy are myopic they rank the operator based only on their immediate reward this can lead to inferior overall result it may be better to use a relatively expensive operator first if that operator provides information that will significantly reduce the cost of the subsequent operator this suggests using some lookahead process to compute the quality for operator non myopically unfortunately this is prohibitively expensive for most domain especially for domain that have a large number of complex state we therefore use idea from reinforcement learning to compute the utility of each operator sequence in particular our system first us dynamic programming over abstract simplification of interpretation state to precompute the utility of each relevant sequence it doe this off line over a training sample of image at run time our interpretation system us these estimate to decide when to use which imaging operator our empirical result in the challenging real world domain of face recognition demonstrate that this approach work more effectively than myopic approach 
we show how to interoperate semantically and inferentially between the leading semantic web approach to rule ruleml logic program and ontology owl daml oil description logic via analyzing their expressive intersection to do so we define a new intermediate knowledge representation kr contained within this intersection description logic program dlp and the closely related description horn logic dhl which is an expressive fragment of first order logic fol dlp provides a significant degree of expressiveness substantially greater than the rdf schema fragment of description logic we show how to perform dlp fusion the bidirectional translation of premise and inference including typical kind of query from the dlp fragment of dl to lp and vice versa from the dlp fragment of lp to dl in particular this translation enables one to build rule on top of ontology it enables the rule kr to have access to dl ontological definition for vocabulary primitive e g predicate and individual constant used by the rule conversely the dlp fusion technique likewise enables one to build ontology on top of rule it enables ontological definition to be supplemented by rule or imported into dl from rule it also enables available efficient lp inferencing algorithm implementation to be exploited for reasoning over large scale dl ontology 
a requirement common to most dynamic vision applicationsis the ability to track object in a sequence of frame thisproblem ha been extensively studied in the past few year leading to several technique such a unscented particlefilter based tracker that exploit a combination of the assumed target dynamic empirically learned noise distributionsand past position observation while successful inmany scenario these tracker remain fragile to occlusionand model uncertainty in the target dynamic a we showin this paper these difficulty can be addressed by modelingthe dynamic of the target a an unknown operator thatsatisfies certain interpolation condition result from interpolationtheory can then be used to find this operator bysolving a convex optimization problem a illustrated withseveral example combining this operator with kalman andupf technique lead to both robustness improvement andcomputational complexity reduction 
we apply statistical machine translation smt tool to generate novel paraphrase of input sentence in the same language the system is trained on large volume of sentence pair automatically extracted from clustered news article available on the world wide web alignment error rate aer is measured to gauge the quality of the resulting corpus a monotone phrasal decoder generates contextual replacement human evaluation show that this system outperforms baseline paraphrase generation technique and in a departure from previous work offer better coverage and scalability than the current best of breed paraphrasing approach 
this paper present a novel discriminative learning technique for label sequence based on a combination of the two most successful learning algorithm support vector machine and hidden markov model which we call hidden markov support vector machine the proposed architecture handle dependency between neighboring label using viterbi decoding in contrast to standard hmm training the learning procedure is discriminative and is based on a maximum soft margin criterion compared to previous method like conditional random field maximum entropy markov model and label sequence boosting hm svms have a number of advantage most notably it is possible to learn non linear discriminant function via kernel function at the same time hm svms share the key advantage with other discriminative method in particular the capability to deal with overlapping feature we report experimental evaluation on two task named entity recognition and part of speech tagging that demonstrate the competitiveness of the proposed approach 
abstract this paper quantifies the information gained in integrating local measurement using spectral graph partitioning we employ a large dataset of manually segmented image in order to learn an optimal affinity function between nearby pair of pixel region cue are computed a the similarity in brightness color and texture between image patch boundary cue are incorporated by looking for the presence of an intervening contour a large gradient along a straight line connecting two pixel we then use spectral clustering to find an approximate minimizer of the normalized cut partitioning the image into coherent segment we evaluate the power of local measurement and global segmentation in predicting the location of image boundary by computing the precision and recall with respect to the human groundtruth data the result show that spectral clustering is successful in suppressing noise and boosting weak signal over a wide variety of natural image 
we consider the problem of illusory or artefactual structure from the visualisation of high dimensional structureless data in particular we examine the role of the distance metric in the use of topographic mapping based on the statistical field of multidimensional scaling we show that the use of a squared euclidean metric i e the sstress measure give rise to an annular structure when the input data is drawn from a highdimensional isotropic distribution and we provide a theoretical justification for this observation 
kernel based learning algorithm work by embedding the data into a euclidean space and then searching for linear relation among the embedded data point the embedding is performed implicitly by specifying the inner product between each pair of point in the embedding space this information is contained in the so called kernel matrix a symmetric and positive definite matrix that encodes the relative position of all point specifying this matrix amount to specifying the geometry of the embedding space and inducing a notion of similarity in the input space classical model selection problem in machine learning in this paper we show how the kernel matrix can be learned from data via semi definite programming sdp technique when applied to a kernel matrix associated with both training and test data this give a powerful transductive algorithm using the labelled part of the data one can learn an embedding also for the unlabelled part the similarity between test point is inferred from training point and their label importantly these learning problem are convex so we obtain a method for learning both the model class and the function without local minimum furthermore this approach lead directly to a convex method to learn the norm soft margin parameter in support vector machine solving another important open problem finally the novel approach presented in the paper is supported by positive empirical result 
in word sense disambiguation wsd the heuristic of choosing the most common sense is extremely powerful because the distribution of the sens of a word is often skewed the problem with using the predominant or first sense heuristic aside from the fact that it doe not take surrounding context into account is that it assumes some quantity of hand tagged data whilst there are a few hand tagged corpus available for some language one would expect the frequency distribution of the sens of word particularly topical word to depend on the genre and domain of the text under consideration we present work on the use of a thesaurus acquired from raw textual corpus and the wordnet similarity package to find predominant noun sens automatically the acquired predominant sens give a precision of on the noun of the senseval english all word task this is a very promising result given that our method doe not require any hand tagged text such a semcor furthermore we demonstrate that our method discovers appropriate predominant sens for word from two domain specific corpus 
this paper describes the patent retrieval task in the fourth ntcir workshop and the test collection produced in this task we perform the invalidity search task in which each participant group search a patent collection for the patent that can invalidate the demand in an existing claim we also perform the automatic patent map generation task in which the patent associated with a specific topic are organized in a multi dimensional matrix 
in recent year particle filter have become a tremendouslypopular tool to perform tracking for non linearand or non gaussian model this is due to their simplicity generality and success over a wide range of challengingapplications particle filter and monte carlo methodsin general are however poor at consistently maintainingthe multi modality of the target distribution that may arisedue to ambiguity or the presence of multiple object toaddress this shortcoming this paper proposes to model thetarget distribution a a non parametric mixture model andpresents the general tracking recursion in this case it isshown how a monte carlo implementation of the generalrecursion lead to a mixture of particle filter that interactonly in the computation of the mixture weight thus leadingto an efficient numerical algorithm where all the resultspertaining to standard particle filter apply the ability ofthe new method to maintain posterior multi modality is illustratedon a synthetic example and a real world trackingproblem involving the tracking of football player in a videosequence 
we analyze visibility from static sensor in a dynamic scene with moving obstacle people such analysis is considered in a probabilistic sense in the context of multiple sensor so that visibility from e ven one sensor might be sufficient additionally we analyze worst case scenari o for high security area where target are non cooperative such visibility analysis provides important performance characterization of multi camera system furthermore maximization of visibility in a given region of interest yield the op timum number and placement of camera in the scene our analysis ha application in surveillance manual or automated and can be utilized for sensor planning in place like museum shopping mall subway station and parking lot we present several example scene simulated and real for which interesting camera configuration were obtained using the formal analysis developed in the paper 
some document genre contain a large number of figure this position paper outline approach to diagram summarization that can augment the many well developed technique of text summarization we discus figure a surrogate for entire document thumbnail extraction the relation between text and figure a well a how automation might be achieved the focus is on diagram line drawing because they allow parsing technique to be used in contrast to the difficulty of general image understanding we describe the advance in raster image vectorization and parsing needed to produce corpus for diagram summarization 
we discus an identification framework for noisy speech mixture a block based generative model is formulated that explicitly incorporates the time varying harmonic plus noise h n model for a number of latent source observed through noisy convolutive mixture all parameter including the pitch of the source signal the amplitude and phase of the source the mixing filter and the noise statistic are estimated by maximum likelihood using an em algorithm exact averaging over the hidden source is obtained using the kalman smoother we show that pitch estimation and source separation can be performed simultaneously the pitch estimate are compared to laryngograph egg measurement artificial and real room mixture are used to demonstrate the viability of the approach intelligible speech signal are re synthesized from the estimated h n model 
we describe how we used a data set of chorale harmonisation composed by johann sebastian bach to train hidden markov model using a probabilistic framework allows u to create a harmonisation system which learns from example and which can compose new harmonisation we make a quantitative comparison of our system s harmonisati on performance against simpler model and provide example harmonisation 
we describe two probabilistic model for unsupervised word sense disambiguation using parallel corpus the first model which we call the sense model build on the work of diab and resnik that us both parallel text and a sense inventory for the target language and recasts their approach in a probabilistic framework the second model which we call the concept model is a hierarchical model that us a concept latent variable to relate different language specific sense label we show that both model improve performance on the word sense disambiguation task over previous unsupervised approach with the concept model showing the largest improvement furthermore in learning the concept model a a by product we learn a sense inventory for the parallel language 
the performance of graph based clustering method critically depends on the quality of the distance function used to compute similarity between pair of neighboring node in this paper we learn distance function by training binary classifier with margin the classifier are defined over the product space of pair of point and are trained to distinguish whether two point come from the same class or not the signed margin is used a the distance value our main contribution is a distance learning method distboost which combine boosting hypothesis over the product space with a weak learner based on partitioning the original feature space each weak hypothesis is a gaussian mixture model computed using a semi supervised constrained em algorithm which is trained using both unlabeled and labeled data we also consider svm and decision tree boosting a margin based classifier in the product space we experimentally compare the margin based distance function with other existing metric learning method and with existing technique for the direct incorporation of constraint into various clustering algorithm clustering performance is measured on some benchmark database from the uci repository a sample from the mnist database and a data set of color image of animal in most case the distboost algorithm significantly and robustly outperformed it competitor 
in constraint programming one model a problem by stating constraint on acceptable solution the constraint model is then usually solved by interleaving backtracking search and constraint propagation previous study have demonstrated that designing special purpose constraint propagator for commonly occurring constraint can significantly improve the efficiency of a constraintprogramming approach in this paper we present a fast simple algorithm for bound consistency propagation of the alldifferent constraint the algorithm ha the same worst case behavior a the previous best algorithm but is much faster in practice using a variety of benchmark and random problem we show that our algorithm outperforms existing bound consistency algorithm and also outperforms on problem with an easily identifiable property state ofthe art commercial implementationsof propagator for stronger form of local consistency 
in this paper we study market share rule rule that have a certain market share statistic associated with them such rule are particularly relevant for decision making from a business perspective motivated by market share rule in this paper we consider statistical quantitative rule sq rule that are quantitative rule in which the rh can be any statistic that is computed for the segment satisfying the lh of the rule building on prior work we present a statistical approach for learning all significant sq rule i e sq rule for which a desired statistic lie outside a confidence interval computed for this rule in particular we show how resampling technique can be effectively used to learn significant rule since our method considers the significance of a large number of rule in parallel it is susceptible to learning a certain number of false rule to address this we present a technique that can determine the number of significant sq rule that can be expected by chance alone and suggest that this number can be used to determine a false discovery rate for the learning procedure we apply our method to online consumer purchase data and report the result 
recent increase in the number of search engine on the web and the availability of meta search engine that can query multiple search engine make it important to find effective method for combining result coming from different source in this paper we introduce novel method for reranking in a meta search environment based on expert agreement and content of the snippet we also introduce an objective way of evaluating different method for ranking search result that is based upon implicit user judgement we incorporated our method and two variation of commonly used merging method in our meta search engine mearf and carried out an experimental study using log accumulated over a period of twelve month our experiment show that the choice of the method used for merging the output produced by different search engine play a significant role in the overall quality of the search result in almost all case examined result produced by some of the new method introduced were consistently better than the one produced by traditional method commonly used in various meta search engine these observation suggest that the proposed method can offer a relatively inexpensive way of improving the meta search experience over existing method 
we present the status of an on going work aiming at introducingsymbolic simulation and theorem proving in a design flow that usesconventional description and simulation language this paper focuseson the formalization of the cycle simulation semantics of a synchronoussubset of vhdl in the acl logic the model is executable and theresults of it symbolic simulation can be proven equal to a specified expression 
in this paper we propose a novel data mining technique for the efficient damage detection within the large scale complex mechanical structure every mechanical structure is defined by the set of finite element that are called structure element large scale complex structure may have extremely large number of structure element and predicting the failure in every single element using the original set of natural frequency a feature is exceptionally time consuming task traditional data mining technique simply predict failure in each structure element individually using global prediction model that are built considering all data record in order to reduce the time complexity of these model we propose a localized clustering regression based approach that consists of two phase building a local cluster around a data record of interest and predicting an intensity of damage only in those structure element that correspond to data record from the built cluster for each test data record we first build a cluster of data record from training data around it then for each data record that belongs to discovered cluster we identify corresponding structure element and we build a localized regression model for each of these structure element these regression model for specific structure element are constructed using only a specific set of relevant natural frequency and merely those data record that correspond to the failure of that structure element experiment performed on the problem of damage prediction in a large electric transmission tower frame indicate that the proposed localized clustering regression based approach is significantly more accurate and more computationally efficient than our previous hierarchical clustering approach a well a global prediction model 
we develop and test new machine learning method for the prediction of topological representation of protein structure in the form of coarseor ne grained contact or distance map that are translation and rotation invariant the method are based on generalized input output hidden markov model giohmms and generalized recursive neural network grnns the method are used to predict topology directly in the ne grained case and in the coarsegrained case indirectly by rst learning how to score candidate graph and then using the scoring function to search the space of possible congurations computer simulation show that the predictor achieve state of the art performance 
in this paper we study the problem of finding most topical named entity among all entity in a document which we refer to a focused named entity recognition we show that these focused named entity are useful for many natural language processing application such a document summarization search result ranking and entity detection and tracking we propose a statistical model for focused named entity recognition by converting it into a classification problem we then study the impact of various linguistic feature and compare a number of classification algorithm from experiment on an annotated chinese news corpus we demonstrate that the proposed method can achieve near human level accuracy 
we investigate the statistic of local geometric structure in natural image previous study of high contrast natural image patch have shown that in the state space of these patch we have a concentration of data point along a low dimensional non linear manifold that corresponds to edge structure in this paper we extend our analysis to a filter based multiscale image representation namely the local jet of gaussian scale space representation a new picture of natural image statistic seems to emerge where primitive such a edge blob and bar generate low dimensional non linear structure in the state space of image data 
neuron are often assumed to operate in a highly unreliable manner a neuron can signal the same stimulus with a variable number of action potential however much of the experimental evidence supporting this view wa obtained in the visual cortex we have therefore assessed trial to trial variability in the auditory cortex of the rat to ensure single unit isolation we used cell attached recording tone evoked response were usually transient often consisting of on average only a single spike per stimulus surprisingly the majority of response were not just transient but were also binary consisting of or action potential but not more in response to each stimulus several dramatic example consisted of exactly one spike on of trial with no trial to trial variability in spike count the variability of such binary response differs from comparably transient response recorded in visual cortical area such a area mt and represent the lowest trial to trial variability mathematically possible for response of a given firing rate our study thus establishes for the first time that transient response in auditory cortex can be described a a binary process rather than a a highly variable poisson process these result demonstrate that cortical architecture can support a more precise control of spike number than wa previously recognized and they suggest a re evaluation of model of cortical processing that assume noisiness to be an inevitable feature of cortical code 
this paper present a new method of detecting andpredicting motion tracking failure with application inhuman motion and gait analysis we define a trackingfailure a an event and describe it temporal characteristicsusing a hidden markov model hmm thisstochastic model is trained using previous example oftracking failure we derive vector observation for thehmm using the noise covariance matrix characterizinga tracked d structural model of the human body weshow a causal relationship between the conditional outputprobability of the hmm a transformed using alogarithmic mapping function and impending trackingfailures result are illustrated on several multi viewsequences of complex human motion 
we provide a worst case analysis of selective sampling algorithm for learning linear threshold function the algorithm considered in this paper are perceptron like algorithm i e algorithm which can be efficiently run in any reproducing kernel hilbert space our algorithm exploit a simple margin based randomized rule to decide whether to query the current label we obtain selective sampling algorithm achieving on average the same bound a those proven for their deterministic counterpart but using much fewer label we complement our theoretical finding with an empirical comparison on two text categorization task the outcome of these experiment is largely predicted by our theoretical result our selective sampling algorithm tend to perform a good a the algorithm receiving the true label after each classification while observing in practice substantially fewer label 
relativized option combine model minimization method and a hierarchical reinforcement learning framework to derive compact reduced representation of a related family of task relativized option are dened without an absolute frame of reference and an option s policy is transformed suitably based on the circumstance under which the option is invoked in earlier work we addressed the issue of learning the option policy online in this article we develop an algorithm for choosing from among a set of candidate transformation the right transformation for each member of the family of task 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we propose a new interpretation of spiking neuron a bayesian integrator accumulating evidence over time about event in the external world or the body and communicating to other neuron their certainty about these event in this model spike signal the occurrence of new information i e what cannot be predicted from the past activity a a result firing statistic are close to poisson albeit providing a deterministic representation of probability we proceed to develop a theory of bayesian inference in spiking neural network recurrent interaction implementing a variant of belief propagation 
in this paper we present a novel algorithm opportune project for mining complete set of frequent item set by projecting database to grow a frequent item set tree our algorithm is fundamentally different from those proposed in the past in that it opportunistically chooses between two different structure array based or tree based to represent projected transaction subset and heuristically decides to build unfiltered pseudo projection or to make a filtered copy according to feature of the subset more importantly we propose novel method to build tree based pseudo projection and array based unfiltered projection for projected transaction subset which make our algorithm both cpu time efficient and memory saving basically the algorithm grows the frequent item set tree by depth first search whereas breadth first search is used to build the upper portion of the tree if necessary we test our algorithm versus several other algorithm on real world datasets such a bm po and on ibm artificial datasets the empirical result show that our algorithm is not only the most efficient on both sparse and dense database at all level of support threshold but also highly scalable to very large database 
the emerging edge service architecture promise to improve the availability and performance of web service by replicating server at geographically distributed site a key challenge in such system is data replication and consistency so that edge server code can manipulate shared data without incurring the availability and performance penalty that would be incurred by accessing a traditional centralized database this paper explores using a distributed object architecture to build an edge service system for an e commerce application an online bookstore represented by the tpc w benchmark we take advantage of application specific semantics to design distributed object to manage a specific subset of shared information using simple and effective consistency model our experimental result show that by slightly relaxing consistency within individual distributed object we can build an edge service system that is highly available and efficient for example in one experiment we find that our object based edge server system provides a factor of five improvement in response time over a traditional centralized cluster architecture and a factor of nine improvement over an edge service system that distributes code but retains a centralized database 
it is a common practice that merchant selling product on the web ask their customer to review the product and associated service a e commerce is becoming more and more popular the number of customer review that a product receives grows rapidly for a popular product the number of review can be in hundred this make it difficult for a potential customer to read them in order to make a decision on whether to buy the product in this project we aim to summarize all the customer review of a product this summarization task is different from traditional text summarization because we are only interested in the specific feature of the product that customer have opinion on and also whether the opinion are positive or negative we do not summarize the review by selecting or rewriting a subset of the original sentence from the review to capture their main point a in the classic text summarization in this paper we only focus on mining opinion product feature that the reviewer have commented on a number of technique are presented to mine such feature our experimental result show that these technique are highly effective 
the instability of the medial axis of a shape under deformation have long been recognized a a major obstacle to it use in recognition and other application these instability or transition occur when the structure of the medial axis graph change abruptly under deformation of shape the recent classification of these transition in d for the medial axis and for the shock graph wa a key factor both in the development of an object recognition system and an approach to perceptual organization this paper classifies generic transition of the d medial axis by examining the order of contact of sphere with the surface leading to an enumeration of possible transition which are then examined on a case by case basis some case are ruled out a never occurring in any family of deformation while others are shown to be non generic in a one parameter family of deformation finally the remaining case are shown to be viable by developing a specific example for each we relate these transition to a classification by bogaevsky of singularity of the viscosity solution of the hamilton jacobi equation we believe that the classification of these transition is vital to the successful regularization of the medial axis and it use in real application 
the computation and memory required for kernel machine with n training sample is at least o n such a complexity is significant even for moderate size problem and is prohibitive for large datasets we present an approximation technique based on the improved fast gauss transform to reduce the computation to o n we also give an error bound for the approximation and provide experimental result on the uci datasets 
automatic unloading of piled box like object is undoubtedlyof great importance to the industry in this contributiona system addressing this problem is described weemploy a laser range finder for data acquisition and globallydeformable superquadrics for object modeling our technique is based on a hypothesis generation andrefinement scheme the vertex of the piled object are extractedand superquadric seed are aligned at these vertex the model parameter recovery task is decomposedinto two subproblems each dealing with a subset of themodel s parameter set both region and boundary based informationsources are used for parameter estimation comparedto a widespread strategy for superquadric recovery our method show advantage in term of robustnessand computational efficiency in addition our system exhibitsversatility with regard to existing industrial system since it can effectively deal with both neatly placed and jumbledconfigurations of object 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
in this paper we present a generative latent variable model for rating based collaborative ltering called the user rating prole model urp the generative process which underlies urp is designed to produce complete user rating prole an assignment of one rating to each item for each user our model represents each user a a mixture of user attitude and the mixing proportion are distributed according to a dirichlet random variable the rating for each item is generated by selecting a user attitude for the item and then selecting a rating according to the preference pattern associated with that attitude urp is related to several model including a multinomial mixture model the aspect model and lda but ha clear advantage over each 
digital medium audio video can be difficult to search and share in a personal way souvenir is a software system that offer user a flexible and comprehensive way to use their handwritten or text note to retrieve and share specific medium moment user can take note on a variety of device such a the paper based crosspad the palm pilot and standard keyboard device souvenir segment handwritten note into an effective medium index without the need for handwriting recognition user can use their note to create hyperlink to random access medium stored in a digital library souvenir also ha web publishing and email capability to enable anyone to access or email medium moment directly from a web page souvenir annotation capture information that can not be easily inferred by automatic medium indexing tool 
a linear discriminative supervised technique for reducing feature vector extracted from image data to a lower dimensional representation is proposed it is derived from classical fisher linear discriminant analysis lda and useful for example in supervised segmentation task in which high dimensional feature vector describes the local structure of the image in general the main idea of the technique is applicable in discriminative and statistical modelling that involves contextual data lda is a basic well known and useful technique in many application our contribution is that we extend the use of lda to case where there is dependency between the output variable i e the class label and not only between the input variable the latter can be dealt with in standard lda the principal idea is that where standard lda merely take into account a single class label for every feature vector the new technique incorporates class label of it neighborhood in it analysis a well in this way the spatial class label configuration in the vicinity of every feature vector is accounted for resulting in a technique suitable for e g image data this spatial lda is derived from a formulation of standard lda in term of canonical correlation analysis the linearly dimension reduction transformation thus obtained is called the canonical contextual correlation projection an additional drawback of lda is that it cannot extract more feature than the number of class minus one in the two class case this mean that only a reduction to one dimension is possible our contextual lda approach can avoid such extreme deterioration of the classification space and retain more than one dimension the technique is exemplified on a pixel based segmentation problem an illustrative experiment on a medical image segmentation task show the performance improvement possible employing the canonical contextual correlation projection 
there ha been significant recent progress in reasoning and constraint processing method in area such a planning and finite model checking current solution technique can handle combinatorial problem with up to a million variable and five million constraint the good scaling behavior of these method appears to defy what one would expect based on a worst case complexity analysis in order to bridge this gap between theory and practice we propose a new framework for studying the complexity of these technique on practical problem instance in particular our approach incorporates general structural property observed in practical problem instance into the formal complexity analysis we introduce a notion of backdoor which are small set of variable that capture the overall combinatorics of the problem instance we provide empirical result showing the existence of such backdoor in real world problem we then present a series of complexity result that explain the good scaling behavior of current reasoning and constraint method observed on practical problem instance 
an agent with limited consumable execution resource need policy that attempt to achieve good performance while respecting these limitation otherwise an agent such a a plane might fail catastrophically crash when it run out of resource fuel at the wrong time in midair we present a new approach to constructing policy for agent with limited execution resource that build on principle of real time ai a well a research in constrained markov decision process specifically we formulate solve and analyze the policy optimization problem where constraint are imposed on the probability of exceeding the resource limit we describe and empirically evaluate our solution technique to show that it is computationally reasonable and that it generates policy that sacrifice some potential reward in order to make the kind of precise guarantee about the probability of resource overutilization that are crucial for mission critical application 
the seldon model combine concept from agent based modeling and social network analysis to create a computation model of social dynamic for terrorist recruitment the underlying recruitment model is based on a unique hybrid agent based architecture that contains simple agent individual such a expatriate and abstract agent conceptual entity such a society and mosque interaction between agent are determined by multiple social network which form and dissipate according to the action of the individual we have implemented a java based toolkit to evaluate the dynamic of social behavior and the specific dynamic associated with terrorist recruitment described by expert social scientist creating an architecture for simple adaptation to other group phenomenon 
this short paper present a light weight technique to merge result list obtained from querying different database the motivation for such a technique is a general purpose search engine for palm o based pda 
the broadcast news navigator bnn is a fully implemented system that incorporates image speech and language processing together with visualization and user preference modeling to support intelligent personalized access to broadcast news video the demonstration will illustrate the use of the system s underlying machine learning enabled story segmentation and processing called the broadcast news editor bne a live scenario based demonstration will illustrate the use of named entity search temporal visualization of entity story clustering and geospatial story visualization discovery of entity relation and personalized multimedia summary generation by transforming access from sequential to direct search and providing hierarchical hyperlinked summary we will demonstrate how user can access topic and entity specific news cluster nearly three time a fast a direct search of digital video in short we will demonstrate intelligent news on demand enabled by a suite of ai technology 
we argue that human inductive generalization is best explained in a bayesian framework rather than by traditional model based on similarity computation we go beyond previous work on bayesian concept learning by introducing an unsupervised method for constructing flexible hypothesis space and we propose a version of the bayesian occam s razor that trade off prior and likelihood to prevent underor over generalization in these flexible space we analyze two published data set on inductive reasoning a well a the result of a new behavioral study that we have carried out 
in this paper we will show that a restricted class of constrained minimum divergence problem named generalized inference problem can be solved by approximating the kl divergence with a bethe free energy the algorithm we derive is closely related to both loopy belief propagation and iterative scaling this unified propagation and scaling algorithm reduces to a convergent alternative to loopy belief propagation when no constraint are present experiment show the viability of our algorithm 
query have specific property and may need individualized method and parameter to optimize retrieval length is one property we look at how two word query may attain higher precision by re ranking using word co occurrence evidence in retrieved document co occurrence within document context is not sufficient but window context including sentence context evidence can provide precision improvement at low recall region of to using initial retrieval result and positively affect pseudo relevance feedback 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
given is a problem sequence and a probability distribution the bias onprograms computing solution candidate we present an optimally fastway of incrementally solving each task in the sequence bias shift arecomputed by program prefix that modify the distribution on their suffixesby reusing successful code for previous task stored in non modifiablememory no tested program get more runtime than it probabilitytimes the total search time in illustrative experiment ours 
adaboost ha proved to be an eective method to improve the performance of base classifier both theoretically and empirically however previous study have shown that adaboost might suer from the overfitting problem especially for noisy data in addition most current work on boosting assumes that the combination weight are fixed constant and therefore doe not take particular input pattern into consideration in this paper we present a new boosting algorithm weightboost which try to solve these two problem by introducing an inputdependent regularization factor to the combination weight similarly to adaboost we derive a learning procedure for weightboost which is guaranteed to minimize training error empirical study on eight dierent uci data set and one text categorization data set show that weightboost almost always achieves a considerably better classification accuracy than adaboost furthermore experiment on data with artificially controlled noise indicate that the weightboost is more robust to noise than adaboost 
what happens to the optimal interpretation of noisy data when there exists more than one equally plausible interpretation of the data in a bayesian model learning framework the answer depends on the prior ex pectations of the dynamic of the model parameter that is to be inferred from the data local time constraint on the prior are insufficient to pick one interpretation over another on the other hand nonlocal time constraint induced by a f noise spectrum of the prior is shown to permit learning of a specific model parameter even when there are in finitely many equally plausible interpretation of the data this transition is inferred by a remarkable mapping of the model estimation problem to a dissipative physical system allowing the use of powerful statisti cal mechanical method to uncover the transition from indeterminate to determinate model learning 
many museum and library archive are digitizing their large collection of handwritten historical manuscript to enable public access to them these collection are only available in image format and require expensive manual annotation work for access to them current handwriting recognizers have word error rate in excess of and therefore cannot be used for such material we describe two statistical model for retrieval in large collection of handwritten manuscript given a text query both use a set of transcribed page image to learn a joint probability distribution between feature computed from word image and their transcription the model can then be used to retrieve unlabeled image of handwritten document given a text query we show experiment with a training set of transcribed page and a test set of handwritten page image from the george washington collection experiment show that the precision at document is about to depending on the model to the best of our knowledge this is the first automatic retrieval system for historical manuscript using text query without manual transcription of the original corpus 
in this paper we present hearsay a system for browsing hypertext web document via audio the hearsay system is based on our novel approach to automatically creating audio browsable content from hypertext web document it combine two key technology automatic partitioning of web document through tightly coupled structural and semantic analysis which transforms raw html document into semantic structure so a to facilitate audio browsing and voicexml an already standardized technology which we adopt to represent voice dialog automatically created from the xml output of partitioning this paper describes the software component of hearsay and present an initial system evaluation 
we present a computational framework capable of labelingthe effort of an action corresponding to the perceivedlevel of exertion by the performer low high the approachinitially factorizes example at different effort ofan action into it three mode principal component to reducethe dimensionality then a learning phase is introducedto compute expressive feature weight to adjust themodel s estimation of effort to conform to given perceptuallabels for the example experiment are demonstrated recognizingthe effort of a person carrying bag of differentweight and for multiple people walking at different pace 
table is a commonly used presentation scheme especially for describing relational information however table understanding remains an open problem in this paper we consider the problem of table detection in web document it potential application include web mining knowledge management and web content summarization and delivery to narrow bandwidth device we describe a machine learning based approach to classify each given table entity a either genuine or non genuine various feature reflecting the layout a well a content characteristic of table are studied in order to facilitate the training and evaluation of our table classifier we designed a novel web document table ground truthing protocol and used it to build a large table ground truth database the database consists of html file collected from hundred of different web site and contains leaf table element out of which are genuine table experiment were conducted using the cross validation method and an f measure of wa achieved 
unexpected rule are interesting because they are either previously unknown or deviate from what prior user knowledge would suggest in this paper we study three important issue that have been previously ignored in mining unexpected rule first the unexpectedness of a rule depends on how the user prefers to apply the prior knowledge to a given scenario in addition to the knowledge itself second the prior knowledge should be considered right from the start to focus the search on unexpected rule third the unexpectedness of a rule depends on what other rule the user ha seen so far thus only rule that remain unexpected given what the user ha seen should be considered interesting we develop an approach that address all three problem above and evaluate it by mean of experiment focusing on finding interesting rule 
from the web with their enclosing web page or captioned news image we demonstrate that it is possible to automatically identify and often fix inaccuracy and resolve ambiguity in a large pool of inaccurately and ambiguously labelled face image rich and complex datasets tend to be inaccurately labelled cleaning up the labelling automatically make it possible to perform recognition experiment on interesting real world datasets we show quite good face clustering is possible for such a dataset our dataset is face image obtained by apply ing a face finder to approximately half a million captioned news image this dataset is more realistic than usual face recognition datasets because it contains face captured in the wild in a variety of configuration with respect to the camera taking a variety of expression and under illumi nation of widely varying color each face image is associ ated with a set of name automatically extracted from the associated caption many but not all such set contain the correct name we cluster face image in appropriate discriminant co ordinate we use a clustering procedure to break ambigu ities in labelling and identify incorrectly labelled face a merging procedure then identifies variant of name that re fer to the same individual the resulting representation can be used to label face in news image or to organize news picture by individual present an alternative view of our procedure is a a process that clean up noisy supervised data we demonstrate how to use entropy measure to evaluate such procedure and compare a variety of method for cleaning up our face data set 
existing method for incorporating subspace model constraint in contour tracking use only partial information from the measurement and model distribution we propose a complete fusion formulation for robust contour tracking optimally resolving uncertainty from heteroscedastic measurement noise system dynamic and a subspace model the resulting non orthogonal subspace projection is a natural extension of the traditional model constraint using orthogonal projection we build model for coupled double contour and exploit information from the ground truth initialization through a strong model adaptation our framework is applied for tracking in echocardiogram where the noise is heteroscedastic each heart ha distinct shape and the relative motion of epiand endocardial border reveal crucial diagnostic feature the proposed method significantly outperforms the traditional shape space constrained tracking algorithm due to the joint fusion of heteroscedastic uncertainty the strong model adaptation and the coupled tracking of double contour robust performance is observed even on the most challenging case 
we introduce a probabilistic noisy channel model for question answering and we show how it can be exploited in the context of an end to end qa system our noisy channel system outperforms a state of the art rule based qa system that us similar resource we also show that the model we propose is flexible enough to accommodate within one mathematical framework many qa specific resource and technique which range from the exploitation of wordnet structured and semi structured database to reasoning and paraphrasing 
finding informative gene from microarray data is an important research problem in bioinformatics research and application most of the existing method rank feature according to their discriminative capability and then find a subset of discriminative gene usually top k gene in particular t statistic criterion and it variant have been adopted extensively this kind of method rely on the statistic principle of t test which requires that the data follows a normal distribution however according to our investigation the normality condition often cannot be met in real data set to avoid the assumption of the normality condition in this paper we propose a rank sum test method for informative gene discovery the method us a rank sum statistic a the ranking criterion moreover we propose using the significance level threshold instead of the number of informative gene a the parameter the significance level threshold a a parameter carry the quality specification in statistic we follow the pitman efficiency theory to show that the rank sum method is more accurate and more robust than the t statistic method in theory to verify the effectiveness of the rank sum method we use support vector machine svm to construct classifier based on the identified informative gene on two well known data set namely colon data and leukemia data the prediction accuracy reach on the colon data and on the leukemia data the result are clearly better than those from the previous feature ranking method by experiment we also verify that using significance level threshold is more effective than directly specifying an arbitrary k 
the openvxi is a portable open source based toolkit that interprets the voicexml dialog markup language it is designed to serve a a framework for system integrator and platform vendor who want to incorporate voicexml into their platform a first version of the toolkit wa released in the winter of with a second version released in september of a number of company and individual have adopted the toolkit for their platform in this paper we discus the architecture of the toolkit the architectural issue involved with implementing a framework for voicexml performance result with the openvxi and future direction for the toolkit 
abstract one of the central problem in machine learning and pattern recognitionis to develop appropriate representation for complex data weconsider the problem of constructing a representation for data lyingon a low dimensional manifold embedded in a high dimensional space 
we propose an unsupervised methodology using independent component analysis ica to cluster gene from dna microarray data based on an ica mixture model of genomic expression pattern linear and nonlinear ica find component that are specific to certain biological process gene that exhibit significant up regulation or down regulation within each component are grouped into cluster we test the statistical significance of enrichment of gene annotation within each cluster ica based clustering outperformed other leading method in constructing functionally coherent cluster on various datasets this result support our model of genomic expression data a composite effect of independent biological process comparison of clustering performance among various ica algorithm including a kernel based nonlinear ica algorithm show that nonlinear ica performed the best for small datasets and natural gradient maximization likelihood worked well for all the datasets 
protein secondary structure prediction is an important step towards understanding the relation between protein sequence and structure however most current prediction method use feature difficult for biologist to interpret in this paper we present a new method that applies information retrieval technique to solve the problem we extract a context sensitive biological vocabulary for protein sequence and apply text classification method to predict protein secondary structure experimental result show that our method performs comparably to the state of art method furthermore the context sensitive vocabulary can serve a a useful tool to discover meaningful regular expression pattern for protein structure 
combinatorial auction ca are important mechanism for allocating interrelated item unfortunately winner determination is np complete unless there is special structure we study the setting where there is a graph with some desired property with the item a vertex and every bid bid on a connected set of item two computational problem arise clearing the auction when given the item graph and constructing an item graph if one exists with the desired property wa previously solved for the case of a tree or a cycle and for the case of a line graph or a cycle we generalize the first result by showing that given an item graph with bounded treewidth the clearing problem can be solved in polynomial time and every ca instance ha some treewidth the complexity is exponential in only that parameter we then give an algorithm for constructing an item tree treewidth if such a tree exists thus closing a recognized open problem we show why this algorithm doe not work for treewidth greater than but leave open whether item graph of say treewidth can be constructed in polynomial time we show that finding the item graph with the fewest edge is np complete even when a graph of treewidth exists finally we study how the result change if a bid is allowed to have more than one connected component even for line graph we show that clearing is hard even with component and constructing the line graph is hard even with 
learning easily un derstandable decision rule from example is one of the c lassic problem in machine learning most l earning system for this problem employ some variation of a greedy separate and conquer algorithm which make the rule order dependent and hence difficult t o understand in this paper we describe a system called lerils that l earns highly accurate and comprehensible rule from example using a randomized iterative local search we c ompare it performance to c ripper cn g net smog and brutedl and show that it compare favorably in accuracy and simplicity of hypothesis in a number of domain 
question answering qa evaluation effort have largely been tailored to open domain system the trec qa test collection contain newswire article and the accompanying query cover a wide variety of topic while some apprehension about the limitation of restricteddomain system is no doubt justified the strict promotion of unlimited domain qa evaluation may have some unintended consequence simply applying the open domain qa evaluation paradigm to a restricted domain system pose problem in the area of test question development answer key creation and test collection construction this paper examines the evaluation requirement of restricted domain system it incorporates evaluation criterion identified by user of an operational qa system in the aerospace engineering domain while the paper demonstrates that user centered task based evaluation are required for restricted domain system these evaluation are found to be equally applicable to open domain system 
while classical kernel based classifier are based on a single kernel in practice it is often desirable to base classifier on combination of multiple kernel lanckriet et al considered conic combination of kernel matrix for the support vector machine svm and showed that the optimization of the coefficient of such a combination reduces to a convex optimization problem known a a quadratically constrained quadratic program qcqp unfortunately current convex optimization toolbox can solve this problem only for a small number of kernel and a small number of data point moreover the sequential minimal optimization smo technique that are essential in large scale implementation of the svm cannot be applied because the cost function is non differentiable we propose a novel dual formulation of the qcqp a a second order cone programming problem and show how to exploit the technique of moreau yosida regularization to yield a formulation to which smo technique can be applied we present experimental result that show that our smo based algorithm is significantly more efficient than the general purpose interior point method available in current optimization toolbox 
recent study show that a majority of web page access are referred by search engine in this paper we study the widespread use of web search engine and it impact on the ecology of the web in particular we study how much impact search engine have on the popularity evolution of web page for example given that search engine return currently popular page at the top of search result are we somehow penalizing newly created page that are not very well known yet are popular page getting even more popular and new page completely ignored we first show that this unfortunate trend indeed exists on the web through an experimental study based on real web data we then analytically estimate how much longer it take for a new page to attract a large number of web user when search engine return only popular page at the top of search result our result show that search engine can have an immensely worrisome impact on the discovery of new web page 
we discus existing approach to train lr parser which have been used for statistical resolution of structural ambiguity these approach are nonoptimal in the sense that a collection of probability distribution cannot be obtained in particular some probability distribution expressible in term of a context free grammar cannot be expressed in term of the lr parser constructed from that grammar under the restriction of the existing approach to training of lr parser we present an alternative way of training that is provably optimal and that allows all probability distribution expressible in the context free grammar to be carried over to the lr parser we also demonstrate empirically that this kind of training can be effectively applied on a large treebank 
this paper present a multimodal learning system that can ground spoken name of object in their physical referent and learn to recognize those object simultaneously from naturally co occurring multisensory input there are two technical problem involved the correspondence problem in symbol grounding how to associate word symbol with their perceptually grounded meaning from multiple cooccurrences between word and object in the physical environment object learning how to recognize and categorize visual object we argue that those two problem can be fundamentally simplified by considering them in a general system and incorporating the spatio temporal and cross modal constraint of multimodal data the system collect egocentric data including image sequence a well a speech while user perform natural task it is able to automatically infer the meaning of object name from vision and categorize object based on teaching signal potentially encoded in speech the experimental result reported in this paper reveal the effectiveness of using multimodal data and integrating heterogeneous technique in machine learning natural language processing and computer vision 
we formulate the shape localization problem in the bayesian framework in the learning stage we propose the constrained rank boost approach to model the likelihood of local feature associated with the key point of an object like face while preserve the prior ranking order between the ground truth position of a key point and it neighbor in the inferring stage a simple efficient iterative algorithm is proposed to uncover the map shape by locally modeling the likelihood distribution around each key point via our proposed variational locally weighted learning vlwl method our proposed framework ha the following benefit compared to the classical pca model the likelihood presented by the ranking prior likelihood model ha more discriminating power a to the optimal position and it neighbor especially in the problem with ambiguity between the optimal position and their neighbor the vlwl method guarantee that the posterior probability of the derived shape increase monotonously and the above two method are both based on accurate probability formulation which spontaneously lead to a robust confidence measure for the discovered shape moreover we present a theoretical analysis for the convergence of the constrained rank boost extensive experiment compared with the active shape model demonstrate the accuracy robustness and stability of our proposed framework 
support vector machine svms excel at two class discriminative learning problem they often outperform generative classifier especially those that use inaccurate generative model such a the na ve bayes nb classifier on the other hand generative classifier have no trouble in handling an arbitrary number of class efficiently and nb classifier train much faster than svms owing to their extreme simplicity in contrast svms handle multi class problem by learning redundant yes no one v others classifier for each class further worsening the performance gap we propose a new technique for multi way classification which exploit the accuracy of svms and the speed of nb classifier we first use a nb classifier to quickly compute a confusion matrix which is used to reduce the number and complexity of the two class svms that are built in the second stage during testing we first get the prediction of a nb classifier and use that to selectively apply only a subset of the two class svms on standard benchmark our algorithm is to time faster than svms and yet match or even exceeds their accuracy 
a major issue in evaluating speech enhancement and hearing compensation algorithm is to come up with a suitable metric that predicts intelligibility a judged by a human listener previous method such a the widely used speech transmission index sti fail to account for masking effect that arise from the highly nonlinear cochlear transfer function we therefore propose a neural articulation index nai that estimate speech intelligibility from the instantaneous neural spike rate over time produced when a signal is processed by an auditory neural model by using a well developed model of the auditory periphery and detection theory we show that human perceptual discrimination closely match the modeled distortion in the instantaneous spike rate of the auditory nerve in highly rippled frequency transfer condition the nai s prediction error is versus the sti s prediction error of 
single class classification scc seek to distinguish one class of data from the universal set of multiple class we present a new scc algorithm that efficiently computes an accurate boundary of the target class from positive and unlabeled data without labeled negative data 
online mechanism design omd address the problem of sequential decision making in a stochastic environment with multiple self interested agent the goal in omd is to make value maximizing decision despite this self interest in previous work we presented a markov decision process mdp based approach to omd in large scale problem domain in practice the underlying mdp needed to solve omd is too large and hence the mechanism must consider approximation this raise the possibility that agent may be able to exploit the approximation for selfish gain we adopt sparse sampling based mdp algorithm to implement efficient policy and retain truth revelation a an approximate bayesiannash equilibrium our approach is empirically illustrated in the context of the dynamic allocation of wifi connectivity to user in a coffeehouse 
inspired by event ranging from to the collapse of the accounting firm arthur ander sen economist kunreuther and heal recently introduced an interesting game theoretic model for problem of interdependent security id in which a large number of player must make individual investment decision related to security whether physical finan cial medical or some other type but in which the ultimate safety of each participant may depend in a complex way on the action of the entire population a simple example is the choice of whether to install a fire sprinkler system in an individual condominium in a large building while such a system might greatly reduce the chance of the owner s prop erty being destroyed by a fire originating within their own unit it might do little or nothing to reduce the chance of damage caused by fire originating in other unit since sprinkler can usually only douse small fire early if enough other unit owner have not made the investment in sprinkler it may be not cost effective for any individual to do so 
the quality of translation resource is arguably the most important factor affecting the performance of a cross language information retrieval system while many investigation have explored the use of query expansion technique to combat error induced by translation no study ha yet examined the effectiveness of these technique across resource of varying quality this paper present result using parallel corpus and bilingual wordlists that have been deliberately degraded prior to query translation across different language translingual resource and degree of resource degradation pre translation query expansion is tremendously effective in several instance pre translation expansion result in better performance when no translation are available than when an uncompromised resource is used without pre translation expansion we also demonstrate that post translation expansion using relevance feedback can confer modest performance gain measuring the efficacy of these technique with resource of different quality suggests an explanation for the conflicting report that have appeared in the literature 
learning algorithm from the field of artificial neural network and machine learning typically do not take any cost into account or allow only cost depending on the class of the example that are used for learning a an extension of class dependent cost we consider cost that are example i e feature and class dependent we derive a costsensitive perceptron learning rule for nonseparable class that can be extended to multi modal class dipol we also derive aa approach for including example dependent cost into an arbitrary cost insensitive learning algorithm by sampling according to roodified probability distribution 
the web service world consists of loosely coupled distributed system which adapt to ad hoc change by the use of service description that enable opportunistic service discovery at present these service description are semantically impoverished being concerned with describing the functional signature of the service rather than characterising their meaning in the semantic web community the daml service effort attempt to rectify this by providing a more expressive way of describing web service using ontology however this approach doe not separate the domain neutral communicative intent of a message considered in term of speech act from it domain specific content unlike similar development from the multi agent system community in this paper we describe our experience of designing and building an ontologically motivated web service system for situational awareness and information triage in a simulated humanitarian aid scenario in particular we discus the merit of using technique from the multi agent system community for separating the intentional force of message from their content and the implementation of these technique within the daml service model 
we propose a framework for simple causal theory of action and study the computational complexity in it of various reasoning task such a determinism progression and regression under various assumption a it turned out even the simplest one among them one step temporal projection with complete initial state is intractable we also briefly consider an extension of the framework to allow truly indeterministic action and find that this extension doe not increase the complexity of any of the task considered here 
current search technology work in a one size fit all fashion therefore the answer to a query is independent of specific user information need in this paper we describe a novel ranking technique for personalized search servicesthat combine content based and community based evidence the community based information is used in order to provide context for query andis influenced by the current interaction of the user with the service ouralgorithm is evaluated using data derived from an actual service available on the web an online bookstore we show that the quality of content based ranking strategy can be improved by the use of communityinformation a another evidential source of relevance in our experiment the improvement reach up to in term of average precision 
in this paper we present the autocat system for product classification autocat us a vector space model modified to consider product attribute unavailable in traditional document classification we present key feature of our user interface developed to assist user with evaluating and editing the output of the classification algorithm finally we present observation about the use of this technology in the field 
security remains a major roadblock to universal acceptance of the web for many kind of transaction especially since the recent sharp increase in remotely exploitable vulnerability have been attributed to web application bug many verification tool are discovering previously unknown vulnerability in legacy c program raising hope that the same success can be achieved with web application in this paper we describe a sound and holistic approach to ensuring web application security viewing web application vulnerability a a secure information flow problem we created a lattice based static analysis algorithm derived from type system and typestate and addressed it soundness during the analysis section of code considered vulnerable are instrumented with runtime guard thus securing web application in the absence of user intervention with sufficient annotation runtime overhead can be reduced to zero we also created a tool named webssari web application security by static analysis and runtime inspection to test our algorithm and used it to verify open source web application project on sourceforge net which were selected to represent project of different maturity popularity and scale contained vulnerability after notifying the developer acknowledged our finding and stated their plan to provide patch our statistic also show that static analysis reduced potential runtime overhead by 
the ability of the web to share data regardless of geographical location raise a new issue called remote authoring with the internet and web browser being independent of hardware it becomes possible to build web enabled database application many approach are provided to integrate database into the web environment which use the web s protocol i e http to transfer the data between client and server however those method are affected by the http shortfall with regard to remote authoring this paper introduces and discus a new methodology for remote authoring of database which is based on the webdav protocol it is a seamless and effective methodology for accessing and authoring database particularly in that it naturally benefit from the webdav advantage such a metadata and access control these feature establish a standard way of accessing database metadata and increase the database security while speeding up the database connection 
cortical neuron in vivo show fluctuation in their membrane potential of the order of several milli volt using simple and biophysically realistic model of a single neuron we demonstrate that noise induced fluctuation can be used to adaptively optimize the sensitivity of the neuron s output to ensemble of subthreshold input of different average strength optimal information transfer is achieved by changing the strength of the noise such that the neuron s average firing rate remains constant adaptation is fast because only crude estimate of the output rate are required at any time 
attribute interaction are the irreducible dependency between attribute interaction underlie feature relevance and selection the structure of joint probability and classification model if and only if the attribute interact they should be connected while the issue of way interaction especially of those between an attribute and the label ha already been addressed we introduce an operational definition of a generalized n way interaction by highlighting two model the reductionistic part to whole approximation where the model of the whole is reconstructed from model of the part and the holistic reference model where the whole is modelled directly an interaction is deemed significant if these two model are significantly different in this paper we propose the kirkwood superposition approximation for constructing part to whole approximation to model data we do not assume a particular structure of interaction but instead construct the model by testing for the presence of interaction the resulting map of significant interaction is a graphical model learned from the data we confirm that the p value computed with the assumption of the asymptotic x distribution closely match those obtained with the boot strap 
we present an automatic alignment procedure which map the disparate internal representation learned by several local dimensionality reduction expert into a single coherent global coordinate system for the original data space our algorithm can be applied to any set of expert each of which produce a low dimensional local representation of a high dimensional input unlike recent effort to coordinate such model by modifying their objective function our algorithm is invoked after training and applies an efficient eigensolver to post process the trained model the post processing ha no local optimum and the size of the sys tem it must solve scale with the number of local model rather than the number of original data point making it more efficient than model free algorithm such a isomap or lle 
it is increasingly common for user to interact with the web using a number of different alias this trend is a double edged sword on one hand it is a fundamental building block in approach to online privacy on the other hand there are economic and social consequence to allowing each user an arbitrary number of free alias thus there is great interest in understanding the fundamental issue in obscuring the identity behind alias however most work in the area ha focused on linking alias through analysis of lower level property of interaction such a network route we show that alias that actively post text on the web can be linked together through analysis of that text we study a large number of user posting on bulletin board and develop algorithm to anti alias those user we can with a high degree of success identify when two alias belong to the same individual our result show that such technique are surprisingly effective leading u to conclude that guaranteeing privacy among alias that post actively requires mechanism that do not yet exist 
this paper describes a system that can annotate a video sequence with a description of the appearance of each actor when the actor is in view and a representation of the actor s activity while in view the system doe not require a fixed background and is automatic the system work by tracking people in d and then using an annotated motion capture dataset synthesizing an annotated d motion sequence matching the d track the d motion capture data is manually annotated off line using a class structure that describes everyday motion and allows motion annotation to be composed one may jump while running for example description computed from video of real motion show that the method is accurate 
application level web security refers to vulnerability inherent in the code of a web application itself irrespective of the technology in which it is implemented or the security of the web server back end database on which it is built in the last few month application level vulnerability have been exploited with serious consequence hacker have tricked e commerce site into shipping good for no charge user name and password have been harvested and condential information such a address and credit card number ha been leaked in this paper we investigate new tool and technique which address the problem of application level web security we i describe a scalable structuring mechanism facilitating the abstraction of security policy from large web application developed in heterogenous multi platform environment ii present a tool which assist programmer develop secure application which are resilient to a wide range of common attack and iii report result and experience arising from our implementation of these technique 
we present a product representation of belief space for planning under partial observability in earlier work we investigated backward plan construction based on a combination operation for belief state the main problem in explicit construction of belief state is their high number to remedy this problem we refrain from representing individual belief state explicitly and instead represent part of the belief space in a factored form the factorization is induced by the division of the state space to observational class each consisting of observationally indistinguishable state finally we show that the representation lead to a simple planning algorithm that is competitive with other algorithm for planning under partial observability 
industry professional and everyday user of the internet have long accepted that due to both the size and growth of this ubiquitous repository new tool are needed to assist with the finding and extraction of very specific resource relevant to a user s task previously this definition of relevance ha been based on the extremely generic matching between resource and query term but recently the emphasis is shifting towards a more personalised model based on the relevance of a particular resource for one specific user we introduce a prototype tt fetch which adopts this concept within an information seeking environment specifically designed to provide user with the mean to better describe a problem s he doesn t understand 
we present a new approach to d scene modeling basedon geometric constraint contrary to the existing method we can quickly obtain d scene model that respectthe given constraint exactly our system can describe alarge variety of linear and non linear constraint in a flexibleway to deal with the constraint we decided to exploit theproperties of the gpdof algorithm developed in the constraintprogramming community the approach isbased on a dictionary of so called r method based on theoremsof geometry which can solve a subset of geometricconstraints in a very efficient way gpdof is used to find in polynomial time a reduced parameterization of a scene and to decompose the equation system induced by constraint into a sequence of r method we have validatedour approach in reconstructing from image d modelsof building based on linear and quadratic geometric constraint 
many of standard practical technique of solving constraint satisfaction problem use various decomposition method to represent a problem a a combination of smaller one we study a general method of decomposing constraint satisfaction problem in which every constraint is represented a a disjunction of two or more simpler constraint defined possibly on smaller set of value we call a problem an amalgam if it can be decomposed in this way some particular case of this construction have been considered in cohen et al b a including amalgam of problem with disjoint set of value and amalgam of independent problem in this paper we concentrate on constraint class determined by relational clone and study amalgam of such class in the general case of arbitrary finite set of value we completely characterise amalgam of this form solvable in polynomial time and provide efficient algorithm 
we study a class of overrelaxed bound optimization algorithm and their relationship to standard bound optimizers such a expectationmaximization iterative scaling cccp and non negative matrix factorization we provide a theoretical analysis of the convergence property of these optimizers and identify analytic condition under which they are expected to outperform the standard version based on this analysis we propose a novel simple adaptive overrelaxed scheme for practical optimization and report empirical result on several synthetic and real world data set showing that these new adaptive method exhibit superior performance in certain case by several time speedup compared to their traditional counterpart our extension are simple to implement apply to a wide variety of algorithm almost always give a substantial speedup and do not require any theoretical analysis of the underlying algorithm 
this paper present an approach to build high resolution digital elevation map from a sequence of unregistered low altitude stereovision image pair the approach first us a visual motion estimation algorithm that determines the d motion of the camera between consecutive acquisition on the basis of visually detected and matched environment feature an extended kalman filter then estimate both the position parameter and the d position of the memorized feature a image are acquired detail are given on the filter implementation and on the estimation of the uncertainty on the feature observation and motion estimation experimental result show that the precision of the method enables to build spatially consistent very large map 
predictive state representation psrs use prediction of a set of test to represent the state of controlled dynamical system one reason why this representation is exciting a an alternative to partially observable markov decision process pomdps is that psr model of dynamical system may be much more compact than pomdp model empirical work on psrs to date ha focused on linear psrs which have not allowed for compression relative to pomdps we introduce a new notion of test which allows u to define a new type of psr that is nonlinear in general and allows for exponential compression in some deterministic dynamical system these new test called e test are related to the test used by rivest and schapire in their work with the diversity representation but our psr avoids some of the pitfall of their representation in particular it potential to be exponentially larger than the equivalent pomdp 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we present a generalization of the nonnegative matrix factorization nmf where a multilayer generative network with nonnegative weight is used to approximate the observed nonnegative data the multilayer generative network with nonnegativity constraint is learned by a multiplicative uppropagation algorithm where the weight in each layer are updated in a multiplicative fashion while the mismatch ratio is propagated from the bottom to the top layer the monotonic convergence of the multiplicative up propagation algorithm is shown in contrast to nmf the multiplicative uppropagation is an algorithm that can learn hierarchical representation where complex higher level representation are defined in term of le complex lower level representation the interesting behavior of our algorithm is demonstrated with face image data 
the paper is concerned with two class active learning while the common approach for collecting data in active learning is to select sample close to the classification boundary better performance can be achieved by taking into account the prior data distribution the main contribution of the paper is a formal framework that incorporates clustering into active learning the algorithm first construct a classifier on the set of the cluster representative and then propagates the classification decision to the other sample via a local noise model the proposed model allows to select the most representative sample a well a to avoid repeatedly labeling sample in the same cluster during the active learning process the clustering is adjusted using the coarse to fine strategy in order to balance between the advantage of large cluster and the accuracy of the data representation the result of experiment in image database show a better performance of our algorithm compared to the current method 
we introduce a logical formalism of irreflexivc causal production relation that posse both a standard monotonic semantics and a natural non monotonic semantics the formalism is shown to provide a complete characterization for the causal reasoning behind causal theory from mccain and turner it is shown also that any causal re lation is reducible to it horn sub relation with re spect to the nonmonotonic semantics we describe also a general correspondence between causal re lations and abductive system which show in ef fect that causal relation allow to express abductive reasoning the result of the study seem to sug gest causal production relation a a viable general framework for nonmonotonic reasoning 
model induction from relational data requires aggregation of the value of attribute of related entity this paper make three contribution to the study of relational learning it present a hierarchy of relational concept of increasing complexity using relational schema characteristic such a cardinality and derives class of aggregation operator that are needed to learn these concept expanding one level of the hierarchy it introduces new aggregation operator that model the distribution of the value to be aggregated and for classification problem the difference in these distribution by class it demonstrates empirically on a noisy business domain that more complex aggregation method can increase generalization performance constructing feature using target dependent aggregation can transform relational prediction task so that well understood feature vector based modeling algorithm can be applied successfully 
two notion of optimality have been explored in previous work on hierarchical reinforcement learning hrl hierarchical optimality or the optimal policy in the space dened by a task hierarchy and a weaker local model called recursive optimality in this paper we introduce two new average reward hrl algorithm for nding hierarchically optimal policy we compare them to our previously reported algorithm for computing recursively optimal policy using a grid world taxi problem and a more real world agv scheduling problem the new algorithm are based on a three part value function decomposition proposed recently by andre and russell which generalizes dietterich s maxq value function decomposition a key dierence between the algorithm proposed in this paper and our previous work is that there is only a single global gain average reward instead of a gain for each subtask our result show the new average reward algorithm have better performance than both the previous recursively optimal counterpart a well a the corresponding discounted hierarchical optimal algorithm 
we present a system for identifying the semantic relationship or semantic role filled by constituent of a sentence within a semantic frame given an input sentence and a target word and frame the system label constituent with either abstract semantic role such a agent or patient or more domain specific semantic role such a speaker message and topic the system is based on statistical classifier trained on roughly sentence that were hand annotated with semantic role by the framenet semantic labeling project we then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic feature including the phrase type of each constituent it grammatical function and it position in the sentence these feature were combined with knowledge of the predicate verb noun or adjective a well a information such a the prior probability of various combination of semantic role we used various lexical clustering algorithm to generalize across possible filler of role test sentence were parsed were annotated with these feature and were then passed through the classifier our system achieves accuracy in identifying the semantic role of presegmented constituent at the more difficult task of simultaneously segmenting constituent and identifying their semantic role the system achieved precision and recall our study also allowed u to compare the usefulness of different feature and feature combination method in the semantic role labeling task we also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicate unseen in the training data 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we apply the message from monte carlo mmc algorithm to inference of univariate polynomial mmc is an algorithm for point estimation from a bayesian posterior sample it partition the posterior sample into set of region that contain similar model each region ha an associated message length given by dowe s mmld approximation and a point estimate that is representative of model in the region the region and point estimate are chosen so that the kullbackleibler distance between model in the region and the associated point estimate is small using wallace s fsmml boundary rule we compare the mmc algorithm s point estimation performance with minimum message length and structural risk minimisation on a set of ten polynomial and nonpolynomial function with gaussian noise the orthonormal polynomial parameter are sampled using reversible jump markov chain monte carlo method 
in this paper we show how the generation of document can be thought of a a k stage markov process which lead to a fisher kernel from which the n gram and string kernel can be re constructed the fisher kernel view give a more flexible insight into the string kernel and suggests how it can be parametrised in a way that reflects the statistic of the training corpus furthermore the probabilistic modelling approach suggests extending the markov process to consider sub sequence of varying length rather than the standard fixed length approach used in the string kernel we give a procedure for determining which sub sequence are informative feature and hence generate a finite state machine model which can again be used to obtain a fisher kernel by adjusting the parametrisation we can also influence the weighting received by the feature in this way we are able to obtain a logarithmic weighting in a fisher kernel finally experiment are reported comparing the dierent kernel using the standard bag of word kernel a a baseline 
although the study of clustering is centered around an intuitively compelling goal it ha been very dicult to develop a unied framework for reasoning about it at a technical level and profoundly diverse approach to clustering abound in the research community here we suggest a formal perspective on the dicult y in nding such a unication in the form of an impossibility theorem for a set of three simple property we show that there is no clustering function satisfying all three relaxation of these property expose some of the interesting and unavoidable trade o at work in well studied clustering technique such a single linkage sum of pair k mean and k median 
side chainpredictionisanimportantsubtaskintheprotein folding problem we show that flnding a minimal energy side chain conflguration is equivalent to performing inference in an undirected graphical model the graphical model is relatively sparse yet ha manycycles weusedthisequivalencetoassesstheperformanceof approximate inference algorithm in a real world setting speciflcallywecomparedbeliefpropagation bp generalizedbp gbp and naive mean fleld mf in case where exact inference wa possible max product bp always found the global minimum of the energy except in few case where it failed to converge while other approximation algorithm of similar complexity did not in the full protein data set maxproduct bp always found a lower energy conflguration than the other algorithm including a widely used protein folding software scwrl 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
named entity tagging comprises the sub task of identifying a text span and classifying it but this view ignores the relationship between the entity and the world spatial and temporal entity ground event in space time and this relationship is vital for application such a question answering and event tracking there is much recent work regarding the temporal dimension setzer and gaizauskas mani and wilson but no detailed study of the spatial dimension i propose to investigate how spatial named entity which are often referentially ambiguous can be automatically resolved with respect to an extensional coordinate model toponym resolution to this end various information source including linguistic cue pattern co occurrence information discourse positional information world knowledge such a size and population a well a minimality heuristic leidner et al will be combined in a supervised machine learning regime the major contribution of this research project will be a corpus of text manually annotated for spatial named entity with their model correlate a a training and evaluation resource a novel method to spatially ground toponym in text and a component based evaluation based on this new reference corpus 
two popular link based webpage ranking algorithm are i pagerank and ii hit hypertext induced topic selection hit make the crucial distinction of hub and authority and computes them in a mutually reinforcing way pagerank considers the hyperlink weight normalization and the equilibrium distribution of random surfer a the citation score we generalize and combine these key concept into a unified framework in which we prove that ranking produced by pagerank and hit are both highly correlated with the ranking by in degree and out degree 
in the past few year a number of constraint language for xml document ha been proposed they are cumulatively called schema language or validation language and they comprise among others dtd xml schema relax ng schematron dsd xlinkit one major point of discrimination among schema language is the support of co constraint or co occurrence constraint e g requiring that attribute a is present if and only if attribute b is or is not presentin the same element although there is no way in xml schema to express these requirement they are in fact frequently used in many xml document type usually only expressed in plain human readable text and validated by mean of special code module by the relevant application in this paper we propose schemapath a light extension of xml schema to handle conditional constraint on xml document two new construct have been added to xml schema condition based on xpath pattern on type assignment for element and attribute and a new simple type xsd error for the direct expression of negative constraint e g it is prohibited for attribute a to be present if attribute b is also present a proof of concept implementation is provided a web interface is publicly accessible for experiment and assessment of the real expressiveness of the proposed extension 
abstract repeated spike pattern have often been taken a evidence for the synfire chain a phenomenon that a stable spike synchrony propagates through a feedforward network inter spike interval which represent a repeated spike pattern are influenced by the propagation speed of a spike packet however the relation between the propagation speed and network structure is not well understood while it is apparent that the propagation speed depends on the excitatory synapse strength it might also be related to spike pattern we analyze a feedforward network with mexican hattype connectivity fmh using the fokker planck equation we show that both a uniform and a localized spike packet are stable in the fmh in a certain parameter region we also demonstrate that the propagation speed depends on the distinct firing pattern in the same network 
aligning word from sentence which are mutual translation is an important problem in different setting such a bilingual terminology extraction machine translation or projection of linguistic feature here we view word alignment a matrix factorisation in order to produce proper alignment we show that factor must satisfy a number of constraint such a orthogonality we then propose an algorithm for orthogonal non negative matrix factorisation based on a probabilistic model of the alignment data and apply it to word alignment this is illustrated on a french english alignment task from the hansard 
this paper describes a logical extension to microsoft business framework mbf called analytical view av av consists of three component model service for design time business intelligence entity bie for programming model and intelldrill for runtime navigation between oltp and olap data source av feature set fulfills enterprise application requirement for analysis and decision support complementing the transactional feature set currently provided by mbf model service automatically transforms an object oriented model transactional view to a multi dimensional model analytical view without the traditional extraction transformation loading etl overhead and complexity it infers dimensionality from the object layer where richer metadata is stored eliminating the guesswork that a traditional data warehousing process requires when going through physical database schema bi entity are class code generated by model service a an intrinsic part of the framework bi entity enable a consistent object oriented way of programming model with strong type and rich semantics for olap similar to what mbf object persistence technology doe for oltp data more importantly data contained in bi entity have a higher degree of application awareness such a the integrated application level security and customizability intellidrill link together all the information island in mbf using metadata because of the automatic transformation from transactional view to analytical view enabled by model service we have the ability to understand natively what kind of drill ability an object would have thus making information navigation in mbf fully discover able with built in ontology 
recent multi agent extension of q learning require knowledge of other agent payoff and q function and assume game theoretic play at all time by all other agent this paper proposes a fundamentally different approach dubbed hyper q learning in which value of mixed strategy rather than base action are learned and in which other agent strategy are estimated from observed action via bayesian inference hyper q may be effective against many different type of adaptive agent even if they are persistently dynamic against certain broad category of adaptation it is argued that hyper q may converge to exact optimal time varying policy in test using rock paper scissors hyper q learns to significantly exploit an infinitesimal gradient ascent iga player a well a a policy hill climber phc player preliminary analysis of hyper q against itself is also presented 
shaping can be an effective method for improving the learning rate in reinforcement system previously shaping ha been heuristically motivated and implemented we provide a formal structure with which to interpret the improvement afforded by shaping reward central to our model is the idea of a reward horizon which focus exploration on an mdp s critical region a subset of state with the property that any policy that performs well on the critical region also performs well on the mdp we provide a simple algorithm and prove that it learning time is polynomial in the size of the critical region and crucially independent of the size of the mdp this identifies low reward horizon with easy to learn mdps shaping reward which encode our prior knowledge about the relative merit of decision can be seen a artificially reducing the mdp s natural reward horizon we demonstrate empirically the effect of using shaping to reduce the reward horizon 
imaging of object under variable lighting direction is animportant and frequent practice in computer vision andimage based rendering we introduce an approach that significantlyimproves the quality of such image traditionalmethods for acquiring image under variable illuminationdirections use only a single light source per acquired image in contrast our approach is based on a multiplexing principle in which multiple light source illuminate the objectsimultaneously from different direction thus the objectirradiance is much higher the acquired image are thencomputationally demultiplexed the number of image acquisitionsis the same a in the single source method theapproach is useful for imaging dim object area we givethe optimal code by which the illumination should be multiplexedto obtain the highest quality output for n imagescorresponding to n light source the noise is reduced by sqrt n relative to the signal this noise reduction translatesto a faster acquisition time or an increase in density of illuminationdirection sample it also enables one to use lightingwith high directional resolution using practical setup a we demonstrate in our experiment 
in this poster we describe a novel session based search engine which put the search in context the search engine ha a number of session based feature including expansion of the current query with user query history and clickthrough data title and summary of clicked web page in the same search session and the session boundary recognition through temporal closeness and probabilistic similarity between query term in addition the search engine visualizes the rank change of web page a different query are submitted in the same search session to help the user reformulate the query 
we present a technique that improves the efficiency of word lattice parsing a used in speech recognition language modeling our technique applies a probabilistic parser iteratively where on each iteration it focus on a different subset of the wordlattice the parser s attention is shifted towards word lattice subset for which there are few or no syntactic analysis posited this attention shifting technique provides a six time increase in speed measured a the number of parser analysis evaluated while performing equivalently when used a the first stage of a multi stage parsing based language model 
a machine learning methodology for the disambiguation of acronym sens is presented which start from an acronym sense dictionary training data is automatically extracted from downloaded document identified from the result of search engine query leave one out cross validation on document with acronym form achieves accuracy and f 
we seek to gain improved insight into how web search engine shouldcope with the evolving web in an attempt to provide user with themost up to date result possible for this purpose we collectedweekly snapshot of some web site over the course of one year and measured the evolution of content and link structure our measurement focus on aspect of potential interest to search engine designer the evolution of link structure over time the rate ofcreation of new page and new distinct content on the web and the rate of change of the content of existing page under search centric measure of degree of change our finding indicate a rapid turnover rate of web page i e high rate of birth and death coupled with an even higher rate ofturnover in the hyperlink that connect them for page that persistover time we found that perhaps surprisingly the degree of contentshift a measured using tf idf cosine distance doe not appear to beconsistently correlated with the frequency of contentupdating despite this apparent non correlation the rate of content shift of a given page is likely to remain consistent over time that is page that change a great deal in one week will likely change by a similarly large degree in the following week conversely page that experience little change will continue to experience little change we conclude the paper with a discussion of the potential implication ofour result for the design of effective web search engine 
while there is a lot of empirical evidence showing that traditional rule learning approach work well in practice it is nearly impossible to derive analytical result about their predictive accuracy in this paper we investigate rule learning from a theoretical perspective we show that the application of mcallester s pac bayesian bound to rule learning yield a practical learning algorithm which is based on ensemble of weighted rule set experiment with the resulting learning algorithm show not only that it is competitive with state of the art rule learner but also that it error rate can often be bounded tightly in fact the bound turn out to be tighter than one of the best bound for a practical learning scheme known so far the set covering machine finally we prove that the bound can be further improved by allowing the learner to abstain from uncertain prediction 
in this paper we define a function r p which is defined for any polygon p and which map a given polygon p into a number from the interval the number r p can be used a an estimate of the rectilinearity of p the mapping r p ha the following desirable property any polygon p ha the estimated rectilinearity r p which is a number from r p if and only if p is a rectilinear polygon i e all interior angle of p belong to the set inf p r p where denotes the set of all polygon a polygon s rectilinearity measure is invariant under similarity transformation a simple procedure for computing r p for a given polygon p is described a well 
in this paper we consider the possibility of altering the pagerank of web page from an administrator s point of view through the modification of the pagerank equation it is shown that this problem can be solved using the traditional quadratic programming technique in addition it is shown that the number of parameter can be reduced by clustering web page together through simple clustering technique this problem can be formulated and solved using quadratic programming technique it is demonstrated experimentally on a relatively large web data set viz the wt g that it is possible to modify the pageranks of the web page through the proposed method using a set of linear constraint it is also shown that the pagerank of other page may be affected and that the quality of the result depends on the clustering technique used it is shown that our result compared well with those obtained by a hit based method 
information retrieval using meta data can be traced back to the early age of ir where document are represented by the controlled vocabulary in this paper we explore the usage of meta data information under the framework of language model we present a new language model that is able to take advantage of the category information for document to improve the retrieval accuracy we compare the new language model with the traditional language model over the trec dataset where the collection information for document is obtained using the k mean clustering method the new language model outperforms the traditional language model which verifies our statement 
best first search is limited by the memory needed to store node in order to detect duplicate disk can greatly expand the amount of storage available but randomly accessing a disk is impractical rather than checking newly generated node a soon a they are generated we append them to a disk file then sort the file and finally scan the sorted file in one pas to detect and remove duplicate node this also speed up such search that fit entirely in memory by improving cache performance we implement this idea for breadth first search performing the first complete search of the sliding tile puzzle and the disk peg tower of hanoi puzzle 
predicting the next request of a user a she visit web page ha gained importance a web based activity increase markov model and their variation or model based on sequence mining have been found well suited for this problem however higher order markov model are extremely complicated due to their large number of state whereas lower order markov model do not capture the entire behavior of a user in a session the model that are based on sequential pattern mining only consider the frequent sequence in the data set making it difficult to predict the next request following a page that is not in the sequential pattern furthermore it is hard to find model for mining two different kind of information of a user session we propose a new model that considers both the order information of page in a session and the time spent on them we cluster user session based on their pair wise similarity and represent the resulting cluster by a click stream tree the new user session is then assigned to a cluster based on a similarity measure the click stream tree of that cluster is used to generate the recommendation set the model can be used a part of a cache prefetching system a well a a recommendation model 
query by committee is an effective approach to selective sampling in which disagreement amongst an ensemble of hypothesis is used to select data for labeling query by bagging and query by boosting are two practical implementation of this approach that use bagging and boosting respectively to build the committee for effective active learning it is critical that the committee be made up of consistent hypothesis that are very different from each other decorate is a recently developed method that directly construct such diverse committee using artificial training data this paper introduces active decorate which us decorate committee to select good training example extensive experimental result demonstrate that in general active decorate outperforms both query by bagging and query by boosting 
we propose new feature and algorithm for automating web page classification task such a content recommendation and ad blocking we show that the automated classification of web page can be much improved if instead of looking at their textual content we consider each link s url and the visual placement of those link on a referring page these feature are unusual rather than being scalar measurement like word count they are tree structured describing the position of the item in a tree we develop a model and algorithm for machine learning using such tree structured feature we apply our method in automated tool for recognizing and blocking web advertisement and for recommending interesting news story to a reader experiment show that our algorithm are both faster and more accurate than those based on the text content of web document 
we have designed and tested a single chip analog vlsi sensor that detects imminent collision by measuring radially expansive optic flow the design of the chip is based on a model proposed to explain leg extension behavior in fly during landing approach a new elementary motion detector emd circuit wa developed to measure optic flow this emd circuit model the bandpass nature of large monopolar cell lmcs immediately postsynaptic to photoreceptors in the fly visual system a array of d motion detector wa fabricated on a mm mm die in a standard m cmos process the chip consumes w of power from a v supply with the addition of wide angle optic the sensor is able to detect collision around m before impact in complex real world scene 
in a wide range of business area dealing with text data stream including crm knowledge management and web monitoring service it is an important issue to discover topic trend and analyze their dynamic in real time specifically we consider the following three task in topic trend analysis topic structure identification identifying what kind of main topic exist and how important they are topic emergence detection detecting the emergence of a new topic and recognizing how it grows topic characterization identifying the characteristic for each of main topic for real topic analysis system we may require that these three task be performed in an on line fashion rather than in a retrospective way and be dealt with in a single framework this paper proposes a new topic analysis framework which satisfies this requirement from a unifying viewpoint that a topic structure is modeled using a finite mixture model and that any change of a topic trend is tracked by learning the finite mixture model dynamically in this framework we propose the usage of a time stamp based discounting learning algorithm in order to realize real time topic structure identification this enables tracking the topic structure adaptively by forgetting out of date statistic further we apply the theory of dynamic model selection to detecting change of main component in the finite mixture model in order to realize topic emergence detection we demonstrate the effectiveness of our framework using real data collected at a help desk to show that we are able to track dynamic of topic trend in a timely fashion 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
regularization play a central role in the analysis of modern data where non regularized fitting is likely to lead to over fitted model useless for both prediction and interpretation we consider the design of incremental algorithm which follow path of regularized solution a the regularization varies these approach often result in method which are both efficient and highly flexible we suggest a general path following algorithm based on second order approximation prove that under mild condition it remains very close to the path of optimal solution and illustrate it with example 
we derive a new class of photometric invariant that can beused for a variety of vision task including lighting invariantmaterial segmentation change detection and tracking aswell a material invariant shape recognition the key ideais the formulation of a scene radiance model for the class of separable brdfs that can be decomposed into materialrelated term and object shape and lighting related term all the proposed invariant are simple rational function ofthe appearance parameter say material or shape and lighting the invariant in this class differ from one another in thenumber and type of image measurement they require mostof the invariant in this class need change in illumination orobject position between image acquisition the invariantscan handle large change in lighting which pose problem formost existing vision algorithm we demonstrate the power ofthese invariant using scene with complex shape material texture shadow and specularities 
building data integration system today is largely done by hand in a very labor intensive and error prone process in this paper we describe a conceptually new solution to this problem that of mass collaboration the basic idea is to think about a data integration system a having a finite set of parameter whose value must be set to build such a system the system administrator can construct and deploy a system shell then ask the user to help the system automatically converge to the correct parameter value this way the enourmous burden of system development is lifted from the administrator and spread thinly over a multitude of user we discus the challenge to this approach and propose solution we then describe our current effort in applying this approach to the problem of schema matching in the context of data integration 
one of the main goal of the rialist group at ames is to support human exploration and development of space heds by applying spoken dialogue technology to training and task performance on the ground and in space we are moving to address this goal by developing a system aimed at taking technical documentation from the international space station and producing spoken dialogue geared towards astronaut training and task support in this demo abstract we will introduce the scope and nature of task performed on board the international space station describe our system architecture and discus the range of functionality afforded by the current system in the demonstration itself conference participant will be provided a hand on experience with our system which convert a technical procedure written a a set of 
abstract we present a syntax based statistical translation model our model transforms a source language parse tree into a target language string by applying stochastic operation at each node these operation capture linguistic difference such a word order and case marking model parameter are estimated in polynomial time using an em algorithm the model produce word alignment that are better than those produced by ibm model 
in this paper we present a general technique for taking forward chaining planner for deterministic domain e g hsp tlplan talplanner and shop and adapting them to work in nondeterministic domain our result suggest that our technique preserve many of the desirable property of these planner such a the ability to use heuristic technique to achieve highly ecien t planning in our experimental study on two problem domain the well known mbp algorithm took exponential time conrming prior result by others a nondeterminized version of shop took only polynomial time the polynomial time gures are conrmed by a complexity analysis and a similar complexity analysis show that a nondeterminized version of tlplan would perform similarly 
we provide a principle for semi supervised learning based on optimizing the rate of communicating label for unlabeled point with side information the side information is expressed in term of identity of set of point or region with the purpose of biasing the label in each region to be the same the resulting regularization objective is convex ha a unique solution and the solution can be found with a pair of local propagation operation on graph induced by the region we analyze the property of the algorithm and demonstrate it performance on document classification task 
the primary goal of web usage mining is the discovery of pattern in the navigational behavior of web user standard approach such a clustering of user session and discovering association rule or frequent navigational path do not generally provide the ability to automatically characterize or quantify the unobservable factor that lead to common navigational pattern it is therefore necessary to develop technique that can automatically discover hidden semantic relationship among user a well a between user and web object probabilistic latent semantic analysis plsa is particularly useful in this context since it can uncover latent semantic association among user and page based on the co occurrence pattern of these page in user session in this paper we develop a unified framework for the discovery and analysis of web navigational pattern based on plsa we show the flexibility of this framework in characterizing various relationship among user and web object since these relationship are measured in term of probability we are able to use probabilistic inference to perform a variety of analysis task such a user segmentation page classification a well a predictive task such a collaborative recommendation we demonstrate the effectiveness of our approach through experiment performed on real world data set 
for many year i have wanted to give a talk like this look backon our subject identify the high and perhaps low point consider what worked what did not work and speculate a littleabout the future now that i at last have the opportunity to givesuch a talk the realisation ha dawned just how difficult it is todo justice to the topic the only way out of this difficulty for mei to emphasise that this is a personal account based on myinvolvement with the field since and that error of omissionand commission are not deliberate but simply due to lack ofknowledge and time on my part to talk of landmark is easy but to say what they are in ir isnot they come in various shape and size event publication experiment idea etc in the course of this presentation i shallbe judiciously mixing all of these however the emphasis will beon idea and their subsequent modelling and testing throughexperimentation the interaction between theory and experiment willbe a recurring theme i will try and associate these developmentswith key individual thereby running the risk of ignoring some iapologise for this in advance the pre history of our subject can be traced back to the work inthe th century perhaps even further but i will pick it up atthe middle of the last century th starting with the work ofrobert fairthorne and vannevar bush this early work emphasised thepossibility of using mechanical device to store and retrieveinformation of course the foundation of modern informationretrieval were properly laid after with the pioneering work ofcleverdon salton sparck jones and others this work gave rise toa strong experimental methodology for the evaluation of theoreticalideas which ha been sustained to this day it ha been a hallmarkof ir research that theory is developed in the context ofexperimentation there is no doubt that many discipline arejealous of the success of trec ir research ha thrown up a number of successful model thesemodels have been based on some often unstated assumption orhypotheses i will attempt to identify some of the underlyingideas giving credit where is due that led to the fruitfulexploration of retrieval model this will include system orientedas well a user oriented idea especially those concerned with themeasurement of retrieval performance ir ha been fortunate in that the subject ha grown through theactive collaboration between computer scientist and informationscientists this ha meant that traditional approach to thestorage and retrieval of information emanating from the libraryworld for example have always strongly influenced newdevelopments this tension between manual human process andautomatic computer based process in ir ha always been fruitful even now with the evolution of idea about meta data and ontologiesneeded to enhance web retrieval the debate about controlledvocabularies versus automatic indexing is relevant issue ofscalability are particularly important here one of the strength that have emerged in our subject is thatmany of our model can be deployed independently of medium ormodality for example retrieving image or audio sequence can behandled in way similar to those used to retrieve text data thishas proved to be great boon to ir the development of web retrievalthrough the deployment of various kind of search engine ha beenbased on the considerable early work in ir although detailing thespecific influence is not easy it is clear that the underlyingmathematical and statistical model in ir have been ubiquitous inapplication the extreme difficulty encountered in making nlp workfor ir forced researcher to develop powerful statistical probabilistic geometrical and logical technique to complementlinguistic one this is now paying off because of the similardifficulties encountered in other medium having given some account of how we got here i will spend alittle time talking about where we go from here how do we extractthe message from the bottle 
we show that anomaly detection can be interpreted a a binary classification problem using this interpretation we propose a support vector machine svm for anomaly detection we then present some theoretical result which include consistency and learning rate finally we experimentally compare our svm with the standard one class svm 
discriminative model have been preferred over generative model in many machine learning problem in the recent past owing to some of their attractive theoretical property in this paper we explore the applicability of discriminative classifier for ir we have compared the performance of two popular discriminative model namely the maximum entropy model and support vector machine with that of language modeling the state of the art generative model for ir our experiment on ad hoc retrieval indicate that although maximum entropy is significantly worse than language model support vector machine are on par with language model we argue that the main reason to prefer svms over language model is their ability to learn arbitrary feature automatically a demonstrated by our experiment on the home page finding task of trec 
we use machine learner trained on a combination of acoustic confidence and pragmatic plausibility feature computed from dialogue context to predict the accuracy of incoming n best recognition hypothesis to a spoken dialogue system our best result show a weighted f score improvement over a baseline system that implement a grammar switching approach to context sensitive speech recognition 
early disease outbreak detection system typically monitor health care data for irregularity by comparing the distribution of recent data against a baseline distribution determining the baseline is difficult due to the presence of different trend in health care data such a trend caused by the day of week and by seasonal variation in temperature and weather creating the baseline distribution without taking these trend into account can lead to unacceptably high false positive count and slow detection time this paper replaces the baseline method of wong et al with a bayesian network which produce the baseline distribution by taking the joint distribution of the data and conditioning on attribute that are responsible for the trend we show that our algorithm called wsare is able to detect outbreak in simulated data with almost the earliest possible detection time while keeping a low false positive count we also include the result of running wsare on real emergency department data 
we develop a protocol for optimizing dynamic behavior of a network of simple electronic component such a a sensor network an ad hoc network of mobile device or a network of communication switch this protocol requires only local communication and simple computation which are distributed among device the protocol is scalable to large network a a motivating example we discus a problem involving optimization of power consumption delay and buffer overflow in a sensor network our approach build on policy gradient method for optimization of markov decision process the protocol can be viewed a an extension of policy gradient method to a context involving a team of agent optimizing aggregate performance through asynchronous distributed communication and computation we establish that the dynamic of the protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective this paper is motivated by the potential of policy gradient method a a general approach to designing simple scalable distributed optimization protocol for network of electronic device we offer a general framework for such protocol that build on idea from the policy gradient literature we also explore a specific example involving a network of sensor that aggregate data in this context we propose a distributed optimization protocol that minimizes power consumption delay and buffer overflow the proposed approach for designing protocol based on policy gradient method comprises one contribution of this paper in addition this paper offer fundamental contribution to the policy gradient literature in particular the kind of protocol we propose can be viewed a extending policy gradient method to a context involving a team of agent optimizing system behavior through asynchronous distributed computation and parsimonious local communication our main theoretical contribution is to show that the dynamic of our protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective 
this paper describes a noisy channel model of speech repair which can identify and correct repair in speech transcript a syntactic parser is used a the source model and a novel type of tag based transducer is the channel model the use of tag is motivated by the intuition that the reparandum is a rough copy of the repair the model is trained and tested on the switchboard disfluency annotated corpus 
patient scheduling in hospital is a highly complex task hospital have a distributed organisational structure being divided into several autonomous ward and ancillary unit moreover the treat ment process is dynamic information about the patient disease often varies during treatment causing change in the treatment process current approach are insufficient because they either fo cu only on the single ancillary unit and therefore do not consider the entire treatment process of the patient or they do not account for the distribution and dynamic of the patient scheduling problem therefore we propose an agent based approach in which the patient and hospital resource are mod elled a autonomous agent with their own goal reflecting the decentralised structure in hospital in this multi agent system the patient agent com pete over the scarce hospital resource moreover to improve the overall solution the agent then ne gotiate with one another to this end a market mechanism is described in which each self inter ested agent try to improve it own situation in particular we focus on how the agent can calculate demand and supply price based upon their current schedule further an evaluation of first result of the proposed method is given 
traditionally data quality program have acted a a preprocessing stage to make data suitable for a data mining or analysis operation recently data quality concept have been applied to database that support business operation such a provisioning and billing incorporating business rule that drive operation and their associated data process is critically important to the success of such project however there are many practical complication for example documentation on business rule is often meager rule change frequently domain knowledge is often fragmented across expert and those expert do not always agree typically rule have to be gathered from subject matter expert iteratively and are discovered out of logical or procedural sequence like a jigsaw puzzle our approach is to impement business rule a constraint on data in a classical expert system formalism sometimes called production rule our system work by allowing good data to pas through a system of constraint unchecked bad data violate constraint and are flagged and then fed back after correction constraint are added incrementally a better understanding of the business rule is gained we include a real life case study 
we present a novel approach called the one circle algorithm for measuring the eye gaze using a monocular image that zoom in on only one eye of a person observing that the iris contour is a circle we estimate the normal direction of this iris circle considered a the eye gaze from it elliptical image from basic projective geometry an ellipse can be back projected into space onto two circle of different orientation however by using an anthropometric property of the eyeball the correct solution can be disambiguated this allows u to obtain a higher resolution image of the iris with a zoom in camera and thereby achieving higher accuracy in the estimation the robustness of our gaze determination approach wa verified statistically by the extensive experiment on synthetic and real image data the two key contribution are that we show the possibility of finding the unique eye gaze direction from a single image of one eye and that one can obtain better accuracy a a consequence of this 
dominant set are a new graph theoretic concept that ha proven tobe relevant in partitional flat clustering a well a imagesegmentation problem however in many computer visionapplications such a the organization of an image database it isimportant to provide the data to be clustered with a hierarchicalorganization and it is not clear how to do this within thedominant set framework in this paper we address precisely thisproblem and present a simple and elegant solution to it to thisend we consider a family of continuous quadratic program whichcontain a parameterized regularization term that control theglobal shape of the energy landscape when the regularizationparameter is zero the local solution are known to be in one to onecorrespondence with dominant set but when it is positive aninteresting picture emerges we determine bound for theregularization parameter that allow u to exclude from the set oflocal solution those inducing cluster of size smaller than aprescribed threshold this suggests a new divisive hierarchicalapproach to clustering which is based on the idea of properlyvarying the regularization parameter during the clustering process straight forward dynamic from evolutionary game theory are used tolocate the solution of the quadratic program at each level of thehierarchy we apply the proposed framework to the problem oforganizing a shape database experiment with three differentsimilarity matrix and database reported in the literature havebeen conducted and the result confirm the effectiveness of ourapproach 
assume a uniform multidimensional grid of bivariate data where each cell of the grid ha a count ci and a baseline bi our goal is to find spatial region d dimensional rectangle where the ci are significantly higher than expected given bi we focus on two application detection of cluster of disease case from epidemiological data emergency department visit over the counter drug sale and discovery of region of increased brain activity corresponding to given cognitive task from fmri data each of these problem can be solved using a spatial scan statistic kulldorff where we compute the maximum of a likelihood ratio statistic over all spatial region and find the significance of this region by randomization however computing the scan statistic for all spatial region is generally computationally infeasible so we introduce a novel fast spatial scan algorithm generalizing the d scan algorithm of neill and moore to arbitrary dimension our new multidimensional multiresolution algorithm allows u to find spatial cluster up to x faster than the naive spatial scan without any loss of accuracy 
abstract auc area under the curve of roc receiveroperating characteristic ha beenrecently used a a measure for ranking performanceof learning algorithm in this paper we present a novel probability estimationalgorithm that improves the auc valueof decision tree instead of estimating theprobability at the single leaf where the examplefalls into our method average probabilityestimates from all leaf of the tree 
we propose a simple novel and yet effective method for building and testing decision tree that minimizes the sum of the misclassification and test cost more specifically we first put forward an original and simple splitting criterion for attribute selection in tree building our tree building algorithm ha many desirable property for a cost sensitive learning system that must account for both type of cost then assuming that the test case may have a large number of missing value we design several intelligent test strategy that can suggest way of obtaining the missing value at a cost in order to minimize the total cost we experimentally compare these strategy and c and demonstrate that our new algorithm significantly outperform c and it variation in addition our algorithm s complexity is similar to that of c and is much lower than that of previous work our work is useful for many diagnostic task which must factor in the misclassification and test cost for obtaining missing information 
we present a general framework for studying heuristic for planning in the belief space earlier work ha focused on giving implementation of heuristic that work well on benchmark without studying them at a more analytical level existing heuristic have evaluated belief state in term of their cardinality or have used distance heuristic directly based on the distance in the underlying state space neither of these type of heuristic is very widely applicable often goal belief state is not approached through a sequence of belief state with a decreasing cardinality and distance in the state space ignore the main implication of partial observability to remedy these problem we present a family of admissible increasingly accurate distance heuristic for planning in the belief space parameterized by an integer n we show that the family of heuristic is theoretically robust it includes the simplest heuristic based on the state space a a special case and a a limit the exact distance in the belief space 
inventory of manually compiled dictionary usually serve a a source for word sens however they often include many rare sens while missing corpus domain specific sens we present a clustering algorithm called cbc clustering by committee that automatically discovers word sens from text it initially discovers a set of tight cluster called committee that are well scattered in the similarity space the centroid of the member of a committee is used a the feature vector of the cluster we proceed by assigning word to their most similar cluster after assigning an element to a cluster we remove their overlapping feature from the element this allows cbc to discover the le frequent sens of a word and to avoid discovering duplicate sens each cluster that a word belongs to represents one of it sens we also present an evaluation methodology for automatically measuring the precision and recall of discovered sens 
we present a novel sequential clustering algorithm which is motivated by the information bottleneck ib method in contrast to the agglomerative ib algorithm the new sequential sib approach is guaranteed to converge to a local maximum of the information with time and space complexity typically linear in the data size information a required by the original ib principle moreover the time and space complexity are significantly improved we apply this algorithm to unsupervised document classification in our evaluation on small and medium size corpus the sib is found to be consistently superior to all the other clustering method we examine typically by a significant margin moreover the sib result are comparable to those obtained by a supervised naive bayes classifier finally we propose a simple procedure for trading cluster s recall to gain higher precision and show how this approach can extract cluster which match the existing topic of the corpus almost perfectly 
this paper proposes a method for computing fast approximation to support vector decision function in the field of object detection in the present approach we are building on an existing algorithm where the set of support vector is replaced by a smaller so called reduced set of synthesized input space point in contrast to the existing method that find the reduced set via unconstrained optimization we impose a structural constraint on the synthetic point such that the resulting approximation can be evaluated via separable filter for application that require scanning large image this decrease the computational complexity by a significant amount experimental result show that in face detection rank deficient approximation are to time faster than unconstrained reduced set system 
knowing the reputation of your own and or competitor product is important for marketing and customer relationship management it is however very costly to collect and analyze survey data manually this paper present a new framework for mining product reputation on the internet it automatically collect people s opinion about target product from web page and it us text mining technique to obtain the reputation of those product on the basis of human test sample we generate in advance syntactic and linguistic rule to determine whether any given statement is an opinion or not a well a whether such any opinion is positive or negative in nature we first collect statement regarding target product using a general search engine and then using the rule extract opinion from among them and attach three label to each opinion label indicating the positive negative determination the product name itself and an numerical value expressing the degree of system confidence that the statement is in fact an opinion the labeled opinion are then input into an opinion database the mining of reputation i e the finding of statistically meaningful information included in the database is then conducted we specify target category using label value such a positive opinion of product a and perform four type of text mining extraction of characteristic word co occurrence word typical sentence for individual target category and correspondence analysis among multiple target category actual marketing data is used to demonstrate the validity and effectiveness of the framework which offer a drastic reduction in the overall cost of reputation analysis over that of conventional survey approach and support the discovery of knowledge from the pool of opinion on the web 
the structural rigidity property a generalization of laman s theorem which characterizes rigid bar framework in d is generally considered a good approximation of rigidity in geometric constraint satisfaction problem gcsps however it may fail even on simple gcsps because it doe not take geometric property into account in this paper we question the flow based algorithm used by hoffmann et al to identify rigid subgcsps we show that this algorithm may fail because of the structural rigidity but also by design we introduce a new flow based algorithm which us jermann et al s characterization of rigidity we show that this algorithm is correct in d and d and can be used to tackle the major issue related to rigidity deciding whether a gcsp is rigid or not and identifying rigid or over rigid subgcsps 
naive bayes classifier ha long been used for text categorization task it sibling from the unsupervised world the probabilistic mixture of multinomial model ha likewise been successfully applied to text clustering problem despite the strong independence assumption that these model make their attractiveness come from low computational cost relatively low memory consumption ability to handle heterogeneous feature and multiple class and often competitiveness with the top of the line model recently there ha been several attempt to alleviate the problem of naive bayes by performing heuristic feature transformation such a idf normalization by the length of the document and taking the logarithm of the count we justify the use of these technique and apply them to two problem classification of product in yahoo shopping and clustering the vector of collocated term in user query to yahoo search the experimental evaluation allows u to draw conclusion about the promise that these transformation carry with regard to alleviating the strong assumption of the multinomial model 
viewing knowledge discovery a a user centered process that requires an effective collaboration between the user and the discovery system our work aim to support an active role of the user in that process by developing synergistic visualization tool integrated in our discovery system d m these tool provide an ability of visualizing the entire process of knowledge discovery in order to help the user with data preprocessing selecting mining algorithm and parameter evaluating and comparing discovered model and taking control of the whole discover process our case study with two medical datasets on meningitis and stomach cancer show that with visualization tool in d m the user gain better insight in each step of the knowledge discovery process a well the relationship between data and discovered knowledge 
in this work we present discriminative random field drfs a discriminative framework for the classification ofimage region by incorporating neighborhood interactionsin the label a well a the observed data the discriminativerandom field offer several advantage over the conventionalmarkov random field mrf framework first the drfs allow to relax the strong assumption of conditionalindependence of the observed data generally used inthe mrf framework for tractability this assumption is toorestrictive for a large number of application in vision second the drfs derive their classification power by exploitingthe probabilistic discriminative model instead of thegenerative model used in the mrf framework finally allthe parameter in the drf model are estimated simultaneouslyfrom the training data unlike the mrf frameworkwhere likelihood parameter are usually learned separatelyfrom the field parameter we illustrate the advantage ofthe drfs over the mrf framework in an application ofman made structure detection in natural image taken fromthe corel database 
a novel active contour method is presented and applied to pose refinement and tracking the main innovation is that no feature are detected at any stage contour are simply assumed to remove statistical dependency between pixel on opposite side of the contour this assumption together with a simple model of shape variability of the geometric model lead to the application of an em method for maximizing the likelihood of pose parameter in addition a dynamical model of the system lead to the application of a kalman filter the method is demonstrated by tracking motor vehicle with d model 
this paper describes a prototype that predicts the shopping list for customer in a retail store the shopping list prediction is one aspect of a larger system we have developed for retailer to provide individual and personalized interaction with customer a they navigate through the retail store instead of using traditional personalization approach such a clustering or segmentation we learn separate classifier for each customer from historical transactional data this allows u to make very fine grained and accurate prediction about what item a particular individual customer will buy on a given shopping trip we formally frame the shopping list prediction a a classification problem describe the algorithm and methodology behind our system it impact on the business case in which we frame it and explore some of the property of the data source that make it an interesting testbed for kdd algorithm our result show that we can predict a shopper s shopping list with high level of accuracy precision and recall we believe that this work impact both the data mining and the retail business community the formulation of shopping list prediction a a machine learning problem result in algorithm that should be useful beyond retail shopping list prediction for retailer the result is not only a practical system that increase revenue by up to but also enhances customer experience and loyalty by giving them the tool to individually interact with customer and anticipate their need 
mining frequent closed itemsets provides complete and non redundant result for frequent pattern analysis extensive study have proposed various strategy for efficient frequent closed itemset mining such a depth first search v breadthfirst search vertical format v horizontal format tree structure v other data structure top down v bottom up traversal pseudo projection v physical projection of conditional database etc it is the right time to ask what are the pro and con of the strategy and what and how can we pick and integrate the best strategy to achieve higher performance in general case in this study we answer the above question by a systematic study of the search strategy and develop a winning algorithm closet closet integrates the advantage of the previously proposed effective strategy a well a some one newly developed here a thorough performance study on synthetic and real data set ha shown the advantage of the strategy and the improvement of closet over existing mining algorithm including closet charm and op in term of runtime memory usage and scalability 
abstract this paper is about bound on futureerror rate we present a theorem forcombining an arbitrary test set basedbound with an arbitrary training setbased bound appropriate use of thistheorem result in a combined boundwith two property the combinedbound is never much worse than eitherthe training set based bound or thetest set based bound and the combinedbound is sometimes better thaneither bound individually empiricalvalidation is presented showing the 
this paper discus the application of particle filtering algorithm to fault diagnosis in complex industrial process we consider two ubiquitous process an industrial dryer and a level tank for these application we compared three particle filtering variant standard particle filtering rao blackwellised particle filtering and a version of raoblackwellised particle filtering that doe one step look ahead to select good sampling region we show that the overhead of the extra processing per particle of the more sophisticated method is more than compensated by the decrease in error and variance 
we describe a single document text summarizer using the text engineering framework gate the summariser extract sentence using a combination of simple bayes classifier resolve anaphora using gate s annie module simplifies word using the mrc psycho linguistic database and wordnet and supply background information to named person and place using internet resource 
we understand and reconstruct special surface from d data with line geometry method based on estimated surface normal we use approximation technique in line space to recognize and reconstruct rotational helical developable and other surface which are characterized by the configuration of locally intersecting surface normal for the computational solution we use a modified version of the klein model of line space obvious application of these method lie in reverse engineering we have tested our algorithm on real world data obtained from object a antique pottery gear wheel and a surface of the ankle joint 
in this paper we present a methodology for extracting subcategorisation frame based on an automatic lfg f structure annotation algorithm for the penn ii treebank we extract abstract syntactic function based subcategorisation frame lfg semantic form traditional cfg category based subcategorisation frame a well a mixed function category based frame with or without preposition information for oblique and particle information for particle verb our approach doe not predefine frame associate probability with frame conditional on the lemma distinguishes between active and passive frame and fully reflects the effect of long distance dependency in the source data structure we extract verb lemma semantic form type an average of per lemma with frame type we present a large scale evaluation of the complete set of form extracted against the full comlex resource 
we consider the problem of image segmentation using active contoursthrough the minimization of an energy criterion involving bothregion and boundary functionals these functionals are derivedthrough a shape derivative approach instead of classical calculusof variation the equation can be elegantly derived withoutconverting the region integral into boundary integral from thederivative we deduce the evolution equation of an active contourthat make it evolve towards a minimum of the criterion we focusmore particularly on statistical feature globally attached to theregion and especially to the probability density function of imagefeatures such a the color histogram of a region a theoreticalframework is set for the minimization of the distance between twohistograms for matching or tracking purpose an application ofthis framework to the segmentation of color histogram in videosequences is then proposed we briefly describe our numericalscheme and show some experimental result 
despite the popularity of connectionist model in cognitive science their performance can often be difficult to evaluate inspired by the geometric approach to statistical model selection we introduce a conceptually similar method to examine the global behavior of a connectionist model by counting the number and type of response pattern it can simulate the markov chain monte carlo based algorithm that we constructed nd these pattern efficiently we demonstrate the approach using two localist network model of speech perception 
this paper is concerned with computing graph edit distance one ofthe criticism that can be leveled at existing method forcomputing graph edit distance is that it lack the formality andrigour of the computation of string edit distance hence our aimis to convert graph to string sequence so that standard stringedit distance technique can be used to do this we use graphspectral seriation method to convert the adjacency matrix into astring or sequence order we pose the problem of graph matching asmaximum aposteriori probability alignment of the seriationsequences for pair of graph this treatment lead to anexpression for the edit cost we compute the edit distance byfinding the sequence of string edit operation which minimise thecost of the path traversing the edit lattice the edit cost aredefined in term of the a posteriori probability of visiting a siteon the lattice we demonstrate the method with result on adata set of delaunay graph 
data mining technique are routinely used by fundraiser to select those prospect from a large pool of candidate who are most likely to make a financial contribution these technique often rely on statistical model based on trial performance data this trial performance data is typically obtained by soliciting a smaller sample of the possible prospect pool collecting this trial data involves a cost therefore the fundraiser is interested in keeping the trial size small while still collecting enough data to build a reliable statistical model that will be used to evaluate the remainder of the prospect we describe an experimental design approach to optimally choose the trial prospect from an existing large pool of prospect prospect are clustered to render the problem practically tractable we modify the standard d optimality algorithm to prevent repeated selection of the same prospect cluster since each prospect can only be solicited at most once we ass the benefit of this approach on the kdd data set by comparing the performance of the model based on the optimal trial data set with that of a model based on a randomly selected trial data set of equal size 
a variety of mining and analysis problem ranging from association rule discovery to contingency table analysis to materialization of certain approximate datacubes involve the extraction of knowledge from a set of categorical count data such data can be viewed a a collection of transaction where a transaction is a fixed length vector of count classical algorithm for solving count data problem require one or more computationally intensive pass over the entire database and can be prohibitively slow one effective method for dealing with this ever worsening scalability problem is to run the algorithm on a small sample of the data we present a new data reduction algorithm called ease for producing such a sample like the fast algorithm introduced by chen et al ease is especially designed for count data application both ease and fast take a relatively large initial random sample and then deterministically produce a subsample whose distance appropriately defined from the complete database is minimal unlike fast which obtains the final subsample by quasi greedy descent ease us epsilon approximation method to obtain the final subsample by a process of repeated halving experiment both in the context of association rule mining and classical contingency table analysis show that ease outperforms both fast and simple random sampling sometimes dramatically 
we consider learning to classify cognitive state of human subject based on their brain activity observed via functional magnetic resonance imaging fmri this problem is important because such classifier constitute virtual sensor of hidden cognitive state which may be useful in cognitive science research and clinical application in recent work mitchell et al have demonstrated the feasibility of training such classifier for individual human subject e g to distinguish whether the subject is reading an ambiguous or unambiguous sentence or whether they are reading a noun or a verb here we extend that line of research exploring how to train classifier that can be applied across multiple human subject including subject who were not involved in training the classifier we describe the design of several machine learning approach to training multiple subject classifier and report experimental result demonstrating the success of these method in learning cross subject classifier for two different fmri data set 
with application in biology the world wide web and several other area mining of graph structured object ha received significant interest recently one of the major research direction in this field is concerned with predictive data mining in graph database where each instance is represented by a graph some of the proposed approach for this task rely on the excellent classification performance of support vector machine to control the computational cost of these approach the underlying kernel function are based on frequent pattern in contrast to these approach we propose a kernel function based on a natural set of cyclic and tree pattern independent of their frequency and discus it computational aspect to practically demonstrate the effectiveness of our approach we use the popular nci hiv molecule dataset our experimental result show that cyclic pattern kernel can be computed quickly and offer predictive performance superior to recent graph kernel based on frequent pattern 
peer to peer file sharing network are currently receiving much attention a a mean of sharing and distributing information however a recent experience show the anonymous open nature of these network offer an almost ideal environment for the spread of self replicating inauthentic file we describe an algorithm to decrease the number of downloads of inauthentic file in a peer to peer file sharing network that assigns each peer a unique global trust value based on the peer s history of uploads we present a distributed and secure method to compute global trust value based on power iteration by having peer use these global trust value to choose the peer from whom they download the network effectively identifies malicious peer and isolates them from the network in simulation this reputation system called eigentrust ha been shown to significantly decrease the number of inauthentic file on the network even under a variety of condition where malicious peer cooperate in an attempt to deliberately subvert the system 
goal recognition for dialogue system need to be fast make early prediction and be portable we present initial work which show that using statistical corpus based method to build goal recognizers may be a viable way to meet those need our goal recognizer is trained on data from apian corpus and then used to determine the agent s most likely goal based on that data the algorithm is linear in the number of goal and performs very well in term of accuracy and early prediction in addition it is more easily portable to new domain a doe not require a hand crafted plan library 
in this poster we will present the result of effort we have undertaken to conduct evaluation of a qa system in a real world environment and to understand the nature of the dimension on which user evaluate qa system when given full reign to comment on whatever dimension they deem important 
studying web graph is often difcult due to their large size recently several proposal have been published about various technique that allow to store a web graph in memory in a limited space exploiting the inner redundancy of the web the webgraph framework is a suite of code algorithm and tool that aim at making it easy to manipulate large web graph this paper present the compression technique used in webgraph which are centred around referentiation and intervalisation which in turn are dual to each other webgraph can compress the webbase graph mnodes glinks in a little a bit per link and it transposed version in a little a bit per link 
we address the task of problem determination in a distributed system using probe or test transaction which gather information about system component effective probing requires minimizing the cost of probing while maximizing the diagnostic accuracy of the probe set we show that pre planning an optimal probe set is np hard and present polynomial time approximation algorithm that perform well we then implement an active probing strategy which selects probe dynamically and show that it yield a significant reduction in probe set size in both simulation and a real system environment 
we address the problem of using external rotation information with uncalibrated video sequence the main problem addressed is what is the benefit of the orientation information for camera calibration it is shown that in case of a rotating camera the camera calibration problem is linear even in the case that all intrinsic parameter vary for arbitrarily moving camera the calibration problem is also linear but underdetermined for the general case of varying all intrinsic parameter however if certain constraint are applied to the intrinsic parameter the camera calibration can be computed linearily it is analyzed which constraint are needed for camera calibration of freely moving camera furthermore we address the problem of aligning the camera data with the rotation sensor data in time we give an approach to align these data in case of a rotating camera 
the fluid document project ha developed various research prototype that show that powerful annotation technique based on animated typographical change can help reader utilize annotation more effectively our recently developed fluid open hypermedia prototype support the authoring and browsing of fluid annotation on third party web page this prototype is an extension of the arakne environment an open hypermedia application that can augment web page with externally stored hypermedia structure this paper describes how various web standard including dom cs xlink xpointer and rdf can be used and extended to support fluid annotation 
in this paper we present a mathematical theory for marr sprimal sketch we first conduct a theoretical study ofthe descriptive markov random field model and the generativewavelet sparse coding model from the perspectiveof entropy and complexity the competition between thetwo type of model defines the concept of sketchability which divide image into texture and geometry we then proposea primal sketch model that integrates the two modelsand in addition a gestalt field model for spatial organization we also propose a sketching pursuit process that coordinatesthe competition between two pursuit algorithm the matching pursuit and the filter pursuit that seekto explain the image by base and filter respectively themodel can be used to learn a dictionary of image primitive or textons in julesz s language for natural image the primal sketch model is not only parsimonious for imagerepresentation but produce meaningful sketch overa large number of generic image 
recognizing the e xpressive power of graph representation and the a bility of certain graph grammar to generalize we attempt to use graph grammar learning for concept formation in this paper we describe our initial progress toward that goal and focus on ho w certain graph grammar can be learned from example we also establish ground for using graph grammar in machine learning task several example are presented to highlight the validity of the approach 
registration of a preoperative ct d image to one or more x rayprojection d image a special case of the pose estimationproblem ha been attempted in a variety of way with varyingdegrees of success recently there ha been a great deal ofinterest in intensity based method one of the drawback to suchmethods is the need to create digitally reconstructed radiograph drrs at each step of the optimization process drrs are typicallygenerated by ray casting an operation that requires o n time where we assume that n is approximately the size in voxels of oneside of the drr a well a one side of the ct volume we addressthis issue by extending light field rendering technique from thecomputer graphic community to generate drrs instead ofconventional rendered image using light field allows most of thecomputation to be performed in a preprocessing step after thisprecomputation very accurate drrs can be generated in o n time another important issue for d d registration algorithm isvalidation previously reported d d registration algorithm werevalidated using synthetic data or phantom but not clinical data we present an intensity based d d registration system thatgenerates drrs using light field we validate it performanceusing clinical data with a known gold standard transformation 
although most time series data mining research ha concentrated on providing solution for a single distance function in this work we motivate the need for a single index structure that can support multiple distance measure our specific area of interest is the efficient retrieval and analysis of trajectory similarity trajectory datasets are very common in environmental application mobility experiment video surveillance and are especially important for the discovery of certain biological pattern our primary similarity measure is based on the longest common subsequence lcss model that offer enhanced robustness particularly for noisy data which are encountered very often in real world application however our index is able to accommodate other distance measure a well including the ubiquitous euclidean distance and the increasingly popular dynamic time warping dtw while other researcher have advocated one or other of these similarity measure a major contribution of our work is the ability to support all these measure without the need to restructure the index our framework guarantee no false dismissal and can also be tailored to provide much faster response time at the expense of slightly reduced precision recall the experimental result demonstrate that our index can help speed up the computation of expensive similarity measure such a the lcss and the dtw 
ensemble method like bagging and boosting that combine the decision of multiple hypothesis are some of the strongest existing machine learning method the diversity of the member of an ensemble is known to be an important factor in determining it generalization error this paper present a new method for generating ensemble that directly construct diverse hypothesis using additional artificially constructed training example the technique is a simple general metalearner that can use any strong learner a a base classifier to build diverse committee experimental result using decision tree induction a a base learner demonstrate that this approach consistently achieves higher predictive accuracy than both the base classifier and bagging whereas boosting can occasionally decrease accuracy and also obtains higher accuracy than boosting early in the learning curve when training data is limited 
multi view algorithm such a co training and co em utilize unlabeled data when the available attribute can be split into independent and compatible subset co em outperforms co training for many problem but it requires the underlying learner to estimate class probability and to learn from probabilistically labeled data therefore co em ha so far only been studied with naive bayesian learner we cast linear classifier into a probabilistic framework and develop a co em version of the support vector machine we conduct experiment on text classification problem and compare the family of semi supervised support vector algorithm under different condition including violation of the assumption underlying multi view learning for some problem such a course web page classification we observe the most accurate result reported so far 
recently a d face recognition approach based on geometric invariant signature ha been proposed the key idea of the algorithm is a representation of the facial surface invariant to isometric deformation such a those resulting from facial expression one of the crucial stage in the construction of the geometric invariant is the measurement of geodesic distance on triangulated surface carried out by fast marching on triangulated domain fmtd proposed here is a method which us only the metric tensor of the surface for geodesic distance computation when combined with photometric stereo used for facial surface acquisition it allows constructing a bendinginvariant representation of the face without reconstructing the d surface 
abstract we describe the application of kernel method to natural language processing nlp problem in many nlp task the object being modeled are string tree graph or other discrete structure which require some mechanism to convert them into feature vector we describe kernel for various natural language structure allowing rich high dimensional representation of these structure we show how a kernel over tree can be applied to parsing using the voted perceptron algorithm and we give experimental result on the atis corpus of parse tree 
we develop a maximum entropy maxent approach to generating recommendation in the context of a user s current navigation stream suitable for environment where data is sparse highdimensional and dynamic condition typical of many recommendation application we address sparsity and dimensionality reduction by rst clustering item based on user access pattern so a to attempt to minimize the apriori probability that recommendation will cross cluster boundary and then recommending only within cluster we address the inherent dynamic nature of the problem by explicitly modeling the data a a time series we show how this representational expressivity t naturally into a maxent framework we conduct experiment on data from researchindex a popular online repository of over computer science document we show that our maxent formulation outperforms several competing algorithm in oine test simulating the recommendation of document to researchindex user 
this paper give an overview of the evaluation method used for the web retrieval task in the third ntcir workshop which is currently in progress in the web retrieval task we try to ass the retrieval effectiveness of each web search engine system using a common data set and attempt to build a re usable test collection suitable for evaluating web search engine system with these objective we have built gigabyte and gigabyte document set mainly gathered from the jp domain relevance judgment is performed on the retrieved document which are written in japanese or english 
clustering and prediction of set of curve is an important problem in many area of science and engineering it is often the case that curve tend to be misaligned from each other in a continuous manner either in space across the measurement or in time we develop a probabilistic framework that allows for joint clustering and continuous alignment of set of curve in curve space a opposed to a fixed dimensional featurevector space the proposed methodology integrates new probabilistic alignment model with model based curve clustering algorithm the probabilistic approach allows for the derivation of consistent em learning algorithm for the joint clustering alignment problem experimental result are shown for alignment of human growth data and joint clustering and alignment of gene expression time course data 
this paper compare alternative approach to pose estimation using visual cue from the environment we examine approach that derive pose estimate from global image property such a principal component analysis pca versus from local image property commonly referred to a landmark we also consider the failure mode of the different method our work is validated with experimental result 
we investigate an approach for simultaneously committing to multiple activity each modeled a a temporally extended action in a semi markov decision process smdp for each activity we dene a set of admissible solution consisting of the redundant set of optimal policy and those policy that ascend the optimal statevalue function associated with them a plan is then generated by merging them in such a way that the solution to the subordinate activity are realized in the set of admissible solution satisfying the superior activity we present our theoretical result and empirically evaluate our approach in a simulated domain 
this article proposes a solution of the lambertian shapefrom shading sfs problem in the case of a pinhole cameramodel performing a perspective projection our approachis based upon the notion of viscosity solution of hamilton jacobiequations this approach allows u to naturallydeal with nonsmooth solution and provides a mathematicalframework for proving correctness of our algorithm our work extends previous work in the area in three aspect first it model the camera a a pinhole whereasmost author assume an orthographic projection see for a panorama of the sfs problem up to and for a recent survey thereby extending the applicability ofshape from shading method to more realistic image inparticular it extends the work of and second byadapting the brightness equation to the perspective problem we obtain a new partial differential equation pde result about the existence and uniqueness of it solutionare also obtained third it allows u to come up with a newapproximation scheme and a new algorithm for computingnumerical approximation of the continuous solution aswell a a proof of their convergence toward that solution 
this article present a high level discussion of some problem in information retrieval that are unique to web search engine the goal is to raise awareness and stimulate research in these area 
arabic a highly inflected language requires good stemming for effective information retrieval yet no standard approach to stem ming ha emerged we developed several light stemmer based on heuristic and a statistical stemmer based on co occurrence for arabic retrieval we compared the retrieval effectiveness of our stemmer and of a morphological analyzer on the trec data the best light stemmer wa more effective for cross lan guage retrieval than a morphological stemmer which tried to find the root for each word a repartitioning process consisting of vowel removal followed by clustering using co occurrence analy si pro duced stem class which were better than no stemming or very light stemming but still inferior to good light stemming or mor phological analysis 
the world wide web contains a number of source code archive program are usually classified into various category within the archive by hand we report on experiment for automatic classification of source code into these category we examined a number of factor that affect classification accuracy weighting feature by expected entropy loss make a significant improvement in classification accuracy we show a support vector machine can be trained to classify source code with a high degree of accuracy we feel these result show promise for software reuse 
we present a single image highlight removal method thatincorporates illumination based constraint into image inpainting unlike occluded image region filled by traditional inpainting highlight pixel contain some useful information for guiding theinpainting process constraint provided by observed pixel color highlight color analysis and illumination color uniformity areemployed in our method to improve estimation of the underlyingdiffuse color the inclusion of these illumination constraintsallows for better recovery of shading and texture by inpainting experimental result are given to demonstrate the performance ofour method 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
new event detection is a challenging task that still offer scope for great improvement after year of effort in this paper we show how performance on new event detection ned can be improved by the use of text classification technique a well a by using named entity in a new way we explore modification to the document representation in a vector space based ned system we also show that addressing named entity preferentially is useful only in certain situation a combination of all the above result in a multi stage ned system that performs much better than baseline single stage ned system 
medical imaging often involves the injection of contrast agent andthe subsequent analysis of tissue enhancement pattern manyimportant type of tissue have characteristic enhancement pattern for example in magnetic resonance mr mammography malignanciesexhibit a characteristic wash out temporal pattern while in mrangiography artery vein and parenchyma each have their owndistinctive temporal signature in such image sequence there aresubstantial change in intensity however this change is dueprimarily to the contrast agent rather than the motion of sceneelements a a result the task of segmenting contrast enhancedimages pose interesting new challenge for computer vision inthis paper we propose a new image segmentation algorithm for imagesequences with contrast enhancement using a model based timeseries analysis of individual pixel we use energy minimizationvia graph cut to efficiently ensure spatial coherence the energyis minimized in an expectation maximization fashion that alternatesbetween segmenting the image into a number of non overlappingregions and finding the temporal profile parameter which bestdescribe the behavior of each region preliminary experiment on mrmammography and mr angiography study show the algorithm s abilityto find an accurate segmentation 
automated image interpretation is an important task in numerous application ranging from security system to natural resource inventorization based on remote sensing recently a second generation of adaptive machine learned image interpretation system have shown expertlevel performance in several challenging domain while demonstrating an unprecedented improvement over hand engineered or first generation machine learned system in term of cross domain portability design cycle time and robustness such system are still severely limited in this paper we inspect the anatomy of a state of the art adaptive image interpretation system and discus the range of the corresponding machine learning problem we then report on the novel machine learning approach engaged and the resulting improvement 
traditional information retrieval typically represents data using a bag of word data mining typically us a highly structured database representation this paper explores the middle ground using a representation which we term entity model in which question about structured data may be posed and answered but the complexity and task specific restriction of ontology are avoided an entity model is a language model or word distribution associated with an entity such a a person place or organization using these perentity language model entity may be clustered link may be detected or described with a short summary entity may be collectively classified and question answering may be performed on a corpus of entity extracted from newswire and the web we group entity by profession with accuracy improve accuracy further on the task of classifying politician a liberal or conservative using collective classification and conditional random field and answer question about who a person is with mean reciprocal rank mrr of 
we introduce a class of string kernel called mismatch kernel for use with support vector machine svms in a discriminative approach to the protein classification problem these kernel measure sequence similarity based on shared occurrence of length subsequence counted with up to mismatch and do not rely on any generative model for the positive training sequence we compute the kernel efficiently using a mismatch tree data structure and report experiment on a benchmark scop dataset where we show that the mismatch kernel used with an svm classifier performs a well a the fisher kernel the most successful method for remote homology detection while achieving considerable computational saving 
abstract we introduce a class of string kernel called mismatch kernel for usewith support vector machine svms in a discriminative approach tothe protein classification problem these kernel measure sequence similaritybased on shared occurrence of k length subsequence countedwith up to m mismatch and do not rely on any generative model forthe positive training sequence we compute the kernel efficiently usinga mismatch tree data structure and report experiment on a benchmark 
in this paper we present a novel class based segmentation method which is guided by a stored representation of the shape of object within a general class such a horse image the approach is different from bottom up segmentation method that primarily use the continuity of grey level texture and bounding contour we show that the method lead to markedly improved segmentation result and can deal with significant variation in shape and varying background we discus the relative merit of class specific and general image based segmentation method and suggest how they can be usefully combined 
the simplicity of http wa a major factor in the success of the web however a both the protocol and it us have evolved http ha grown complex this complexity result in numerous problem including confused implementors interoperability failure difficulty in extending the protocol and a long specification without much documented rationale many of the problem with http can be traced to unfortunate choice about fundamental definition and model this paper analyzes the current http protocol design showing how it fails in certain case and how to improve these fundamental some problem with http can be fixed simply by adopting new model and terminology allowing u to think more clearly about implementation and extension other problem require explicit but compatible protocol change 
metabolomics is the omics science of biochemistry the associated data include the quantitative measurement of all small molecule metabolite in a biological sample these datasets provide a window into dynamic biochemical network and conjointly with other omic data gene and protein have great potential to unravel complex human disease the dataset used in this study ha individual normal and diseased and the diseased are drug treated or not so there are three class the goal is to classify these individual using the observed metabolite level for measured metabolite there are a number of statistical challenge non normal data the number of sample is le than the number of metabolite there are missing data and the fact that data are missing is informative assay value below detection limit can point to a specific class also there are high correlation among the metabolite we investigate support vector machine svm and random forest rf for outlier detection variable selection and classification we use the variable selected with rf in svm and visa versa the benefit of this study is insight into interplay of variable selection and classification method we link our selected predictor to the biochemistry of the disease 
we present policyblocks an algorithm by which a reinforcement learning agent can extract useful macro action from a set of related task the agent creates macroactions by finding commonality in solution to previous task using these macro action learning to do future related task is accelerated 
text categorization or classification is the automated assigning of text document to pre defined class based on their content this problem ha been studied in information retrieval machine learning and data mining so far many effective technique have been proposed however most technique are based on some underlying model and or assumption when the data fit the model well the classification accuracy will be high however when the data doe not fit the model well the classification accuracy can be very low in this paper we propose a refinement approach to dealing with this problem of model misfit we show that we do not need to change the classification technique itself or it underlying model to make it more flexible instead we propose to use successive refinement of classification on the training data to correct the model misfit we apply the proposed technique to improve the classification performance of two simple and efficient text classifier the rocchio classifier and the na ve bayesian classifier these technique are suitable for very large text collection because they allow the data to reside on disk and need only one scan of the data to build a text classifier extensive experiment on two benchmark document corpus show that the proposed technique is able to improve text categorization accuracy of the two technique dramatically in particular our refined model is able to improve the na ve bayesian or rocchio classifier s prediction performance by on average 
compression reduces both the size of index and the time needed to evaluate query in this paper we revisit the compression of inverted list of document posting that store the position and frequency of indexed term considering two approach to improving retrieval efficiency better implementation and better choice of integer compression scheme first we propose several simple optimisation to well known integer compression scheme and show experimentally that these lead to significant reduction in time second we explore the impact of choice of compression scheme on retrieval efficiency in experiment on large collection of data we show two surprising result use of simple byte aligned code half the query evaluation time compared to the most compact golomb rice bitwise compression scheme and even when an index fit entirely in memory byte aligned code result in faster query evaluation than doe an uncompressed index emphasising that the cost of transferring data from memory to the cpu cache is le for an appropriately compressed index than for an uncompressed index moreover byte aligned scheme have only a modest space overhead the most compact scheme result in index that are around of the size of the collection while a byte aligned scheme is around we conclude that fast byte aligned code should be used to store integer in inverted list 
clustering aim at extracting hidden structure in dataset while the problem of finding compact cluster ha been widely studied in the literature extracting arbitrarily formed elongated structure is considered a much harder problem in this paper we present a novel clustering algorithm which tackle the problem by a two step procedure first the data are transformed in such a way that elongated structure become compact one in a second step these new object are clustered by optimizing a compactness based criterion the advantage of the method over related approach are threefold i robustness property of compactness based criterion naturally transfer to the problem of extracting elongated structure leading to a model which is highly robust against outlier object ii the transformed distance induce a mercer kernel which allows u to formulate a polynomial approximation scheme to the generally nphard clustering problem iii the new method doe not contain free kernel parameter in contrast to method like spectral clustering or mean shift clustering 
abstract recently the fisher score or the fisher kernel is increasingly used a a feature extractor for classification problem the fisher score is a vector of parameter derivative of loglikelihood of a probabilistic model this paper give a theoretical analysis about how class information is pre served in the space of the fisher score which turn out that the fisher score consists of a few important dimension with class information and many nuisance dimension when we perform clustering with the fisher score k mean type method are obviously inappropriate because they make use of all dimension so we will develop a novel but simple clus tering algorithm specialized for the fisher score which can exploit im portant dimension this algorithm is successfully tested in experiment with artificial data and real data amino acid sequence 
this paper introduces a scalable method for feature extraction and navigation of large data set by mean of local clustering where cluster are modeled a overlapping neighborhood under the model intra cluster association and external differentiation are both assessed in term of a natural confidence measure minor cluster can be identified even when they appear in the intersection of larger cluster scalability of local clustering derives from recent generic technique for efficient approximate similarity search the cluster overlap structure give rise to a hierarchy that can be navigated and queried by user experimental result are provided for two large text database 
linear prediction method such a least square for regression logistic regression and support vector machine for classification have been extensively used in statistic and machine learning in this paper we study stochastic gradient descent sgd algorithm on regularized form of linear prediction method this class of method related to online algorithm such a perceptron are both efficient and very simple to implement we obtain numerical rate of convergence for such algorithm and discus it implication experiment on text data will be provided to demonstrate numerical and statistical consequence of our theoretical finding 
in this paper we broaden the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database specifically the causality rule explored in this paper consists of a sequence of triggering event and a set of consequential event and is designed with the capability of mining non sequential inter transaction information hence the causality rule mining provides a very general framework for rule derivation note however that the procedure of causality rule mining is very costly particularly in the presence of a huge number of candidate set and a distributed database and in our opinion cannot be dealt with by direct extension from existing rule mining method consequently we devise in this paper a series of level matching algorithm including level matching abbreviatedly a lm level matching with selective scan abbreviatedly a lm and distributed level matching abbreviatedly a distibuted lm to minimize the computing cost needed for the distributed data mining of causality rule in addition the phenomenon of time window constraint are also taken into consideration for the development of our algorithm a a result of properly employing the technology of level matching and selective scan the proposed algorithm present good efficiency and scalability in the mining of local and global causality rule scale up experiment show that the proposed algorithm scale well with the number of site and the number of customer transaction index term knowledge discovery distributed data mining causality rule triggering event consequential event 
in this paper we propose a new data clustering method called concept factorization that model each concept a a linear combination of the data point and each data point a a linear combination of the concept with this model the data clustering task is accomplished by computing the two set of linear coefficient and this linear coefficient computation is carried out by finding the non negative solution that minimizes the reconstruction error of the data point the cluster label of each data point can be easily derived from the obtained linear coefficient this method differs from the method of clustering based on non negative matrix factorization nmf citexu in that it can be applied to data containing negative value and the method can be implemented in the kernel space our experimental result show that the proposed data clustering method and it variation performs best among algorithm and their variation that we have evaluated on both tdt and reuters corpus in addition to it good performance the new method also ha the merit in it easy and reliable derivation of the clustering result 
we investigate the verbal and nonverbal mean for grounding and propose a design for embodied conversational agent that relies on both kind of signal to establish common ground in human computer interaction we analyzed eye gaze head nod and attentional focus in the context of a direction giving task the distribution of nonverbal behavior differed depending on the type of dialogue move being grounded and the overall pattern reflected a monitoring of lack of negative feedback based on these result we present an eca that us verbal and nonverbal grounding act to update dialogue state 
previous manifold learning algorithm mainly focus on uncovering the low dimensional geometry structure from a set of sample that lie on or nearly on a manifold in an unsupervised manner however the representation from unsupervised learning are not always optimal in discriminating capability in this paper a novel algorithm is introduced to conduct discriminant analysis in term of the embedded manifold structure we propose a novel clustering algorithm called intra cluster balanced k mean icbkm which ensures that there are balanced sample for the class in a cluster and the local discriminative feature for all cluster are simultaneously calculated by following the global fisher criterion compared to the traditional linear kernel discriminant analysis algorithm ours ha the following characteristic it is approximately a locally linear yet globally nonlinear discriminant analyzer it can be considered a special kernel da with geometry adaptive kernel in contrast to traditional kda whose kernel is independent to the sample and it computation and memory cost are reduced a great deal compared to traditional kda especially for the case with large number of sample it doe not need to store the original sample for computing the low dimensional representation for new data the evaluation on toy problem show that it is effective in deriving discriminative representation for the problem with nonlinear classification hyperplane when applied to the face recognition problem it is shown that compared with lda and traditional kda on yale and pie database the proposed algorithm significantly outperforms lda and 
term based representation of document have found wide spread use in information retrieval however one of the main shortcoming of such method is that they largely disregard lexical semantics and a a consequence are not sufficiently robust with respect to variation in word usage in this paper we investigate the use of concept based document representation to supplement wordor phrase based feature the utilized concept are automatically extracted from document via probabilistic latent semantic analysis we propose to use adaboost to optimally combine weak hypothesis based on both type of feature experimental result on standard benchmark confirm the validity of our approach showing that adaboost achieves consistent improvement by including additional semantic feature in the learned ensemble 
part of the process of data integration is determining which set of identifier refer to the same real world entity in integrating database found on the web or obtained by using information extraction method it is often possible to solve this problem by exploiting similarity in the textual name used for object in different database in this paper we describe technique for clustering and matching identifier name that are both scalable and adaptive in the sense that they can be trained to obtain better performance in a particular domain an experimental evaluation on a number of sample datasets show that the adaptive method sometimes performs much better than either of two non adaptive baseline system and is nearly always competitive with the best baseline system 
recent advance in information retrieval over hyperlinked corpus have convincingly demonstrated that link carry le noisy information than text we investigate the feasibility of applying link based method in new application domain the specific application we consider is to partition author into opposite camp within a given topic in the context of newsgroups a typical newsgroup posting consists of one or more quoted line from another posting followed by the opinion of the author this social behavior give rise to a network in which the vertex are individual and the link represent responded to relationship an interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree this behavior is in sharp contrast to the www link graph where linkage is an indicator of agreement or common interest by analyzing the graph structure of the response we are able to effectively classify people into opposite camp in contrast method based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two side tends to be largely identical and many newsgroup posting consist of relatively few word of text 
this paper follows a formal approach to information retrieval based on statistical language model by introducing some simple reformulations of the basic language modeling approach we introduce the notion of importance of a query term the importance of a query term is an unknown parameter that explicitly model which of the query term are generated from the relevant document the important term and which are not the unimportant term the new language modeling approach is shown to explain a number of practical fact of today s information retrieval system that are not very well explained by the current state of information retrieval theory including stop word mandatory term coordination level ranking and retrieval using phrase 
coreference resolution system usually attempt to find a suitable antecedent for almost every noun phrase recent study however show that many definite np are not anaphoric the same claim obviously hold for the indefinites a well in this study we try to learn automatically two classification relevant for this problem we use a small training corpus muc but also acquire some data from the internet combining our classifier sequentially we achieve precision and recall for discourse new entity we expect our classifier to provide a good prefiltering for coreference resolution system improving both their speed and performance 
refinement operator for theory avoid the problem related to the myopia of many relational learning algorithm based on the operator that refine single clause however the non existence of ideal refinement operator ha been proven for the standard clausal search space based on subsumption or logical implication which scale up to the space of theory by adopting different generalization model constrained by the assumption of object identity we extend the theoretical result on the existence of ideal refinement operator for space of clause to the case of space of theory 
topic tracking is complicated when the story in the stream occur in multiple language typically researcher have trained only english topic model because the training story have been provided in english in tracking non english test story are then machine translated into english to compare them with the topic model we propose a native language hypothesis stating that comparison would be more effective in the original language of the story we first test and support the hypothesis for story link detection for topic tracking the hypothesis implies that it should be preferable to build separate language specific topic model for each language in the stream we compare different method of incrementally building such native language topic model 
this paper concern approximate nearest neighbor searching algorithm which have become increasingly important especially in high dimensional perception area such a computer vision with dozen of publication in recent year much of this enthusiasm is due to a successful new approximate nearest neighbor approach called locality sensitive hashing lsh in this paper we ask the question can earlier spatial data structure approach to exact nearest neighbor such a metric tree be altered to provide approximate answer to proximity query and if so how we introduce a new kind of metric tree that allows overlap certain datapoints may appear in both the child of a parent we also introduce new approximate k nn search algorithm on this structure we show why these structure should be able to exploit the same randomprojection based approximation that lsh enjoys but with a simpler algorithm and perhaps with greater efficiency we then provide a detailed empirical evaluation on five large high dimensional datasets which show up to fold acceleration over lsh this result hold true throughout the spectrum of approximation level 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we investigate method for planning in a markov decision process where the cost function is chosen by an adversary after we x our policy a a running example we consider a robot path planning problem where cost are inuenced by sensor that an adversary place in the environment we formulate the problem a a zero sum matrix game where row correspond to deterministic policy for the planning player and column correspond to cost vector the adversary can select for a xed cost vector fast algorithm such a value iteration are available for solving mdps we develop ecien t algorithm for matrix game where such best response oracle exist we show that for our path planning problem these algorithm are at least an order of magnitude faster than direct solution of the linear programming formulation 
html document composed of frame can be difficult to write correctly we demonstrate a technique that can be used by author manually creating html document or by document editor to verify that complex frame construction exhibit the intended behavior when browsed the method is based on model checking an automated program verification technique and on temporal logic specification of expected frame behavior we show how to model the html frame source a a cobweb protocol related to the trellis model of hypermedia document we show how to convert the cobweb protocol to input for a model checker and discus several way for author to create the necessary behavior specification our solution allows web document to be built containing a large number of frame and content page interacting in complex way we expect such web structure to be more useful in literary hypermedia than for web site used a interface to organizational information or database 
this paper quantitatively analyzes indicator of agent policy seller adjuster indemnity claim adjuster producer policy purchaser holder indemnity behavior suggestive of collusion in the united state department of agriculture usda risk management agency rma national crop insurance program according to guidance from the federal law and using six indicator variable of indemnity behavior those entity equal to or exceeding of the county mean computed using a simple jackknife procedure on all entity relevant indicator were flagged a anomalous log linear analysis wa used to test i hierarchical node node arrangement and a non recursive model of node information sharing chi square distributed deviance statistic identified the optimal log linear model the result of the applied data mining technique used here suggest that the non recursive triplet and agent producer doublet collusion probabilistically account for the greatest proportion of waste fraud and abuse in the federal crop insurance program triplet and agent producer doublet need detailed investigation for possible collusion hence this data mining technique provided a high level of confidence when million record were quantitatively analyzed for possible fraud waste or other abuse of the crop insurance program administered by the usda rma and suspect entity reported to usda this data mining technique can be applied where vast amount of data are available to detect pattern of collusion or conspiracy a may be of interest to the criminal justice or intelligence agency 
digital information is increasingly more and more important to enable interaction and transaction on the internet on the other hand leakage of sensitive information can have harmful effect for people enterprise and government this paper focus on the problem of dealing with timed release of confidential information and simplifying it access once public it is a common issue in the industry government and day to day life we introduce the hp time vault service based on the emerging identifier based encryption ibe cryptography schema ibe public encryption key specify the disclosure time these key are used to encrypt confidential information an independent time server generates and publishes ibe decryption key correspondent to the current time at predefined interval we discus the advantage of this approach against current approach based on traditional cryptography a web service based prototype is described a a proof of concept 
we propose the hierarchical dirichlet process hdp a nonparametric bayesian model for clustering problem involving multiple group of data each group of data is modeled with a mixture with the number of component being open ended and inferred automatically by the model further component can be shared across group allowing dependency across group to be modeled effectively a well a conferring generalization to new group such grouped clustering problem occur often in practice e g in the problem of topic discovery in document corpus we report experimental result on three text corpus showing the effective and superior performance of the hdp over previous model 
reconstruction based super resolution from motion video ha been an active area of study in computer vision and video analysis image alignment is a key component of super resolution algorithm almost all previous super resolution algorithm have assumed that standard method of image alignment can provide accurate enough alignment for creating super resolution image however a systematic study of the demand on accuracy of multi image alignment and it effect on super resolution ha been lacking furthermore implicitly or explicitly most algorithm have assumed that the multiple video frame or specific region of interest are related through global parametric transformation from previous work it is not at all clear how super resolution performs under alignment with piecewise parametric or local optical flow based method this paper is an attempt at understanding the influence of image alignment and warping error on super resolution requirement on the consistency of optical flow across multiple image are studied and it is shown that error resulting from traditional flow algorithm may render super resolution infeasible 
in this paper we present a new algorithm suitable for matching discrete object such a string and tree in linear time thus obviat ing dynamic programming with quadratic time complexity furthermore prediction cost in many case can be reduced to linear cost in the length of the sequence to be classified regardless of the number of support v ectors this improvement on the currently available algorithm make string kernel a viable alternative for the practitioner 
the robust regression technique in the ransac family are popular today in computer vision but their performance depends on a user supplied threshold we eliminate this drawback of ransac by reformulating another robust method the m estimator a a projection pursuit optimization problem the projection based pbm estimator automatically derives the threshold from univariate kernel density estimate nevertheless the performance of the pbm estimator equal or exceeds that of ransac technique tuned to the optimal threshold a value which is never available in practice experiment were performed both with synthetic and real data in the affine motion and fundamental matrix estimation task 
there ha historically been very little concern with extrapolation in machine learning yet extrapolation can be critical to diagnose predictor function are almost always learned on a set of highly correlated data comprising a very small segment of predictor space moreover flexible predictor by their very nature are not controlled at point of extrapolation this becomes a problem for diagnostic tool that require evaluation on a product distribution it is also an issue when we are trying to optimize a response over some variable in the input space finally it can be a problem in non static system in which the underlying predictor distribution gradually drift with time or when typographical error misrecord the value of some predictor we present a diagnosis for extrapolation a a statistical test for a point originating from the data distribution a opposed to a null hypothesis uniform distribution this allows u to employ general classification method for estimating such a test statistic further we observe that cart can be modified to accept an exact distribution a an argument providing a better classification tool which becomes our extrapolation detection procedure we explore some of the advantage of this approach and present example of it practical application 
push technology i e the ability of sending relevant information to client in reaction to new event is a fundamental aspect of modern information system xml is rapidly emerging a the widely adopted standard for information exchange and representation and hence several xml based protocol have been defined and are the object of investigation at w c and throughout commercial organization in this paper we propose the new concept of active xml rule for pushing reactive service to xml enabled repository rule operate on xml document and deliver information to interested remote user in reaction to update event occurring at the repository site the proposed mechanism assumes the availability of xml repository supporting a standard xml query language such a xquery that is being developed by the w c for the implementation of the reactive component it capitalizes on the use of standard dom event and of the soap interchange standard to enable the remote installation of active rule a simple protocol is proposed for subscribing and unsubscribing remote rule 
information extraction can be defined a the task of automatically extracting instance of specified class or relation from text we consider the case of using machine learning method to induce model for extracting relation instance from biomedical article we propose and evaluate an approach that is based on using hierarchical hidden markov model to represent the grammatical structure of the sentence being processed our approach first us a shallow parser to construct a multi level representation of each sentence being processed then we train hierarchical hmms to capture the regularity of the par for both positive and negative sentence we evaluate our method by inducing model to extract binary relation in three biomedical domain our experiment indicate that our approach result in more accurate model than several baseline hmm approach 
to understand the brain mechanism involved in reward prediction on different time scale we developed a markov decision task that requires prediction of both immediate and future reward and analyzed subject brain activity using functional mri we estimated the time course of reward prediction and reward prediction error on different time scale from subject performance data and used them a the explanatory variable for spm analysis we found topographic map of different time scale in medial frontal cortex and striatum the result suggests that different cortico basal ganglion loop are specialized for reward prediction on different time scale 
a person reading a book need to build an understanding based on the available textual material a a result of a survey of user reading behaviour and of existing e book user interface we found that most of these interface provide poor support for the actual process of reading and comprehension in particular there is generally minimal support for understanding the overall structure or contextual structure and the narrative structure of a book we propose adapting topic tracking and detection technique to discover the narrative thread within a book and hence it narrative structure the contextual and narrative structure will be presented to the user through purpose designed visualisation which will be integrated and linked within a newly developed e book browser we have chosen to use the bible a our test corpus a it ha a rich narrative structure and relatively complex contextual structure evaluation of the interface and it component will be done through field study involving actual reader of the bible to ass the effectiveness of the user interface in enhancing a user s experience 
we consider the policy search approach to reinforcement learning we show that if a baseline distribution is given indicating roughly how often we expect a good policy to visit each state then we can derive a policy search algorithm that terminates in a finite number of step and for which we can provide non trivial performance guarantee we also demonstrate this algorithm on several grid world pomdps a planar biped walking robot and a double pole balancing problem 
we present an xml programming language designed for the implementation of web service xl is portable and fully compliant with w c standard such a xquery xml protocol and xml schema one of the key feature of xl is that it allows programmer to concentrate on the logic of their application xl provides high level and declarative construct for action which are typically carried out in the implementation of a web service e g logging error handling retry of action workload management event etc issue such a performance tuning e g caching horizontal partitioning etc should be carried out automatically by an implementation of the language this way the productivity of the programmer the ability of evolution of the program and the chance to achieve good performance are substantially enhanced 
the intelligence analysis task of anticipating crisis and providing decision maker with reasonable supportable explainable possible future is extremely difficult to perform this task a team of analyst must consider the political economic and military aspect of national government and non governmental organization this paper describes an agent based simulation framework the advanced global intelligence and leadership experiment agile for building executable model for conducting regional analysis agile is a full featured simulation framework that enables the specification of simulation parameter model and agent let the user define and run simulation under varying condition and enables post run analysis of the result in this paper we describe the system implementation and some example of it use in a pseudo operational setting 
searching for parallel solution in state space planner is a challenging problem because it would require the planner to branch on all possible subset of parallel action exponentially increasing their branching factor we introduce a variant of our heuristic state search planner altalt which generates parallel plan by using greedy online parallelization of partial plan empirical result show that our online approach outperforms post processing offline technique in term of the quality of the solution returned 
topic detection and tracking tdt research explores the development of algorithm to detect novel event and track their development over time for online report development of these method requires careful evaluation and analysis traditional reductive method of evaluation only represent some of the available information of algorithm behaviour we describe a visualisation tool for topic tracking which make it easy to analysis and compare the temporal behaviour of tracking algorithm 
active object tracking for example in surveillance task becomesmore and more important these day besides the tracking algorithmsthemselves methodology have to be developed for reasonable activecontrol of the degree of freedom of all involved camera in thispaper we present an information theoretic approach that allows theoptimal selection of the focal length of two camera during active d object tracking the selection is based on the uncertainty inthe d estimation this allows u to resolve the trade off betweensmall and large focal length in the former case the chance isincreased to keep the object in the field of view of the camera in the latter one d estimation becomes more reliable also moredetails are provided for example for recognizing theobjects beyond a rigorous mathematical framework we presentreal time experiment demonstrating that we gain an improvementin d trajectory estimation by up to in comparison with trackingusing a fixed focal length 
dimensionality reduction via random projection ha attracted considerable attention in recent year the approach ha interesting theoretical underpinnings and offer computational advantage in this paper we report a number of experiment to evaluate random projection in the context of inductive supervised learning in particular we compare random projection and pca on a number of different datasets and using different machine learning method while we find that the random projection approach predictively underperforms pca it computational advantage may make it attractive for certain application 
even under perfect fixation the human eye is under steady motion tremor microsaccades slow drift the dynamic theory of vision state that eye movement can improve hyperacuity according to this theory eye movement are thought to create variable spatial excitation pattern on the photoreceptor grid which will allow for better spatiotemporal summation at later stage we reexamine this theory using a realistic model of the vertebrate retina by comparing response of a resting and a moving eye the performance of simulated ganglion cell in a hyperacuity task is evaluated by ideal observer analysis we find that in the central retina eye micromovements have no effect on the performance here optical blurring limit vernier acuity in the retinal periphery however eye micromovements clearly improve performance based on roc analysis our prediction are quantitatively testable in electrophysiological and psychophysical experiment 
the process driven composition of web service is emerging a a promising approach to integrate business application within and across organizational boundary in this approach individual web service are federated into composite web service whose business logic is expressed a a process model the task of this process model are essentially invocation to functionality offered by the underlying component service usually several component service are able to execute a given task although with different level of pricing and quality in this paper we advocate that the selection of component service should be carried out during the execution of a composite service rather than at design time in addition this selection should consider multiple criterion e g price duration reliability and it should take into account global constraint and preference set by the user e g budget constraint accordingly the paper proposes a global planning approach to optimally select component service during the execution of a composite service service selection is formulated a an optimization problem which can be solved using efficient linear programming method experimental result show that this global planning approach outperforms approach in which the component service are selected individually for each task in a composite service 
feature point for image correspondence are often selectedaccording to subjective criterion e g edge density nostril in this paper we present a general non subjectivecriterion for selecting informative feature point based onthe correspondence model itself we describe the approachwithin the framework of the bayesian markov random field mrf model where the degree of feature point informationis encoded by the entropy of the likelihood term we proposethat feature selection according to minimum entropy of likelihood eol is le likely to lead to correspondence ambiguity thus improving the optimization process in termsof speed and quality of solution experimental resultsdemonstrate the criterion s ability to select optimal featurespoints in a wide variety of image context e g object face comparison with the automatic kanade lucas tomasifeature selection criterion show correspondence tobe significantly faster with feature point selected accordingto minimum eol in difficult correspondence problem 
two neural network that are trained on their mutual output synchronize to an identical time dependant weight vector this novel phenomenon can be used for creation of a secure cryptographic secret key using a public channel several model for this cryptographic system have been suggested and have been tested for their security under different sophisticated attack strategy the most promising model are network that involve chaos synchronization the synchronization process of mutual learning is described analytically using statistical physic method 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we consider the problem of improving named entity recognition ner system by using external dictionary more specifically the problem of extending state of the art ner system by incorporating information about the similarity of extracted entity to entity in an external dictionary this is difficult because most high performance named entity recognition system operate by sequentially classifying word a to whether or not they participate in an entity name however the most useful similarity measure score entire candidate name to correct this mismatch we formalize a semi markov extraction process which is based on sequentially classifying segment of several adjacent word rather than single word in addition to allowing a natural way of coupling high performance ner method and high performance similarity function this formalism also allows the direct use of other useful entity level feature and provides a more natural formulation of the ner problem than sequential word classification experiment in multiple domain show that the new model can substantially improve extraction performance over previous method for using external dictionary in ner 
behavioral goal are achieved reliably and repeatedly with movement rarely reproducible in their detail here we offer an explanation we show that not only are variability and goal achievement compatible but indeed that allowing variability in redundant dimension is the optimal control strategy in the face of uncertainty the optimal feedback control law for typical motor task obey a minimal intervention principle deviation from the average trajectory are only corrected when they interfere with the task goal the resulting behavior exhibit task constrained variability a well a synergetic coupling among actuator which is another unexplained empirical phenomenon 
an iterative method for reconstructing a d polygonalmesh and color texture map from multiple view of an objectis presented in each iteration the method first estimate atexture map given the current shape estimate the texturemap and it associated residual error image are obtainedvia maximum a posteriori estimation and reprojection of themultiple view into texture space next the surface shape isadjusted to minimize residual error in texture space thesurface is deformed towards a photometrically consistentsolution via a series of d epipolar search at randomlyselected surface point the texture space formulation hasimproved computational complexity over standard image basederror aproaches and allows computation of the reprojectionerror and uncertainty for any point on the surface moreover shape adjustment can be constrained suchthat the recovered model s silhouette match those of theinput image experiment with real world imagery demonstratethe validity of the approach 
a novel procedure is presented to construct image domainfilters receptive field that directly recover localmotion and shape parameter these receptive field arederived from training on image deformation that best discriminatebetween different shape and motion parameter beginning with the construction of d receptive fieldsthat detect local surface shape and motion parameterswithin cross section we show how the recovered shape andmotion model parameter are sufficient to produce local estimatesof time to collision in general filter pair receptive field can be synthesizedto perform or detect specific image deformation atthe heart of the method is the use of a matrix to representimage deformation correspondence between individual pixelsof two view of a surface the image correspondencematrix can be decomposed using singular value decompositionto yield a pair of corresponding receptive field thatdetect image change due to the deformation of interest 
existing association rule mining algorithm suffer from many problem when mining massive transactional datasets one major problem is the high memory dependency either the gigantic data structure built is assumed to fit in main memory or the recursive mining process is too voracious in memory resource another major impediment is the repetitive and interactive nature of any knowledge discovery process to tune parameter many run of the same algorithm are necessary leading to the building of these huge data structure time and again this paper proposes a new disk based association rule mining algorithm called inverted matrix which achieves it efficiency by applying three new idea first transactional data is converted into a new database layout called inverted matrix that prevents multiple scanning of the database during the mining phase in which finding frequent pattern could be achieved in le than a full scan with random access second for each frequent item a relatively small independent tree is built summarizing co occurrence finally a simple and non recursive mining process reduces the memory requirement a minimum candidacy generation and counting is needed experimental study reveal that our inverted matrix approach outperform fp tree especially in mining very large transactional database with a very large number of unique item our random access disk based approach is particularly advantageous in a repetitive and interactive setting 
capturing dependency in image in an unsupervised manner is important for many image processing application we propose a new method for capturing nonlinear dependency in image of natural scene this method is an extension of the linear independent component analysis ica method by building a hierarchical model based on ica and mixture of laplacian distribution the model parameter are learned via an em algorithm and it can accurately capture variance correlation and other high order structure in a simple manner we visualize the learned variance structure and demonstrate application to image segmentation and denoising 
previous research in novelty detection ha focused on the task of finding novel material given a set or stream of document on a certain topic this study investigates the more difficult two part task defined by the trec novelty track given a topic and a group of document relevant to that topic find the relevant sentence from the document and find the novel sentence from the collection of relevant sentence our research show that the former step appears to be the more difficult part of this task and that the performance of novelty measure is very sensitive to the presence of non relevant sentence 
traditional technique of dense optical flow estimation don t generally yield symmetrical solution the result will differ if they are applied between image i and i or between image i and i in this work we present a method to recover a dense optical flow field map from two image while explicitely taking into account the symmetry across the image a well a possible occlusion and discontinuity in the flow field the idea is to consider both displacement vector from i to i and i to i and to minimise an energy functional that explicitely encodes all those property this variational problem is then solved using the gradient flow defined by the euler lagrange equation associated to the energy in order to reduce the risk to be trapped within some irrelevant minimum a focusing strategy based on a multi resolution technique is used to converge toward the solution promising experimental result on both synthetic and real image are presented to illustrate the capability of this symmetrical variational approach to recover accurate optical flow 
in this paper we present a new framework for online novelty detection on temporal sequence this framework include a mechanism for associating each detection result with a confidence value based on this framework we develop a concrete online detection algorithm by modeling the temporal sequence using an online support vector regression algorithm experiment on both synthetic and real world data are performed to demonstrate the promising performance of our proposed detection algorithm 
dynamic web content provides u with time sensitive and continuously changing data to glean up to date information user need to regularly browse collect and analyze this web content without proper tool support this information management task is tedious time consuming and error prone especially when the quantity of the dynamic web content is large when many information management service are needed to analyze it and when underlying service network are not completely reliable this paper describes a multi level lifecycle design time and run time coordination mechanism that enables rapid efficient development and execution of information management application that are especially useful for processing dynamic web content such a coordination mechanism brings dynamism to coordinating independent distributed information management service dynamic parallelism spawn merges multiple execution service branch based on available data and dynamic run time reconfiguration coordinate service execution to overcome faulty service and bottleneck these feature enable information management application to be more efficient in handling content and format change in web resource and enable the application to be evolved and adapted to process dynamic web content 
algorithm for r elational l earning and p ropositional l earning face different statistical challenge in contrast to propositional learner relational l earner often make statistical i nferences about data that exhibit linkage and autocorrelation recent work h a s hown that t hese c haracteristics of relational data ca n bias inference made by relational l earner in this paper we develop a novel variant of a known statistical procedure a randomization test that produce accurate hypothesis test for r elational data we show that our procedure produce unbiased inference in situation where more obvious adaptation of existing randomization test fail 
abstract 
we have produced an ontology specifying a model of computer attack our ontology is based upon an analysis of over class of computer intrusion and their corresponding attack strategy and is categorized according to system component targeted mean of attack consequence of attack and location of attacker we argue that any taxonomic characteristic used to define a computer attack be limited in scope to those feature that are observable and measurable at the target of the attack we present our model a a target centric ontology that is to be refined and expanded over time we state the benefit of forgoing dependence upon taxonomy in favor of ontology for the classification of computer attack and intrusion we have specified our ontology using daml oil and have prototyped it using damljesskb we present our model a a target centric ontology and illustrate the benefit of utilizing an ontology in lieu of a taxonomy by presenting a use case scenario of a distributed intrusion detection system 
in this paper we present a framework for using multi layer perceptron mlp network in nonlinear generative model trained by variational bayesian learning the nonlinearity is handled by linearizing it using a gauss hermite quadrature at the hidden neuron this yield an accurate approximation for case of large posterior variance the method can be used to derive nonlinear counterpart for linear algorithm such a factor analysis independent component factor analysis and state space model this is demonstrated with a nonlinear factor analysis experiment in which even source can be estimated from a real world speech data set 
a common characteristic of relational data set lead many relational learning algorithm to discover misleading correlation this characteristic degree disparity occurs when entity of one class participate in systematically higher number of relation than entity of another class in such case the aggregation function that are used in many relational learning algorithm e g avg mode sum exists count max min will result in misleading correlation and added complexity in model we examine this problem through a combination of simulation and experiment we show how two novel significance testing procedure can adjust for the effect of using aggregation function in the presence of degree disparity 
in this paper we propose a lattice based approach intended for extracting semantics from datacubes border of version space for supervised classification closed cube lattice to summarize the semantics of datacubes w r t count sum and covering graph of the quotient cube a a visualization tool of minimal multidimensional association with this intention we introduce two novel concept the cube transversals and the cube closure over the cube lattice of a categorical database relation we propose a levelwise merging algorithm for mining minimal cube transversals with a single database scan we introduce the cube connection show that it is a galois connection and derive a closure operator over the cube lattice using cube transversals and closure we define a new characterization of boundary set which provide a condensed representation of version space used to enhance supervised classification the algorithm designed for computing such border improves the complexity of previous proposal we also introduce the concept of closed cube lattice and show that it is isomorph to on one hand the galois lattice and on the other hand the quotient cube w r t count sum proposed in the quotient cube is a succinct summary of a datacube preserving the rollup drilldown semantics we show that the quotient cube w r t count sum and the closed cube lattice have a similar expression power but the latter ha the smallest possible size finally we focus on the multidimensional association issue and introduce the covering graph of the quotient cube which provides the user with a visualization tool of minimal multidimensional association 
boosting is a strong ensemble based learning algorithm with the promise of iteratively improving the classification accuracy using any base learner a long a it satisfies the condition of yielding weighted accuracy in this paper we analyze boosting with respect to this basic condition on the base learner to see if boosting ensures prediction of rarely occurring event with high recall and precision first we show that a base learner can satisfy the required condition even for poor recall or precision level especially for very rare class furthermore we show that the intelligent weight updating mechanism in boosting even in it strong cost sensitive form doe not prevent case where the base learner always achieves high precision but poor recall or high recall but poor precision when mapped to the original distribution in either of these case we show that the voting mechanism of boosting fall to achieve good overall recall and precision for the ensemble in effect our analysis indicates that one cannot be blind to the base learner performance and just rely on the boosting mechanism to take care of it weakness we validate our argument empirically on variety of real and synthetic rare class problem in particular using adacost a the boosting algorithm and variation of pnrule and ripper a the base learner we show that if algorithm a achieves better recall precision balance than algorithm b then using a a the base learner in adacost yield significantly better performance than using b a the base learner 
the parameter of statistical translation model are typically estimated from sentence aligned parallel corpus we show that significant improvement in the alignment and translation quality of such model can be achieved by additionally including word aligned data during training incorporating word level alignment into the parameter estimation of the ibm model reduces alignment error rate and increase the bleu score when compared to training the same model only on sentence aligned data on the verbmobil data set we attain a reduction in the alignment error rate and a higher bleu score with half a many training example we discus how varying the ratio of word aligned to sentence aligned data affect the expected performance gain 
in large multiagent game partial observability coordination and credit assignment persistently plague attempt to design good learning algorithm we provide a simple and efficient algorithm that in part us a linear system to model the world from a single agent s limited perspective and take advantage of kalman filtering to allow an agent to construct a good training signal and learn an effective policy 
this paper present a new bottom up chart parsing algorithm for prolog along with a compilation procedure that reduces the amount of copying at run time to a constant number per edge it ha application to unification based grammar with very large partially ordered category in which copying is expensive and can facilitate the use of more sophisticated indexing strategy for retrieving such category that may otherwise be overwhelmed by the cost of such copying it also provides a new perspective on quick checking and related heuristic which seems to confirm that forcing an early failure a opposed to seeking an early guarantee of success is in fact the best approach to use a preliminary empirical evaluation of it performance is also provided 
collaborative filtering is a popular technique for recommending item to people several method for collaborative filtering have been proposed in the literature and the quality of their prediction compared in empirical study in this paper we argue that the measure of quality used in these study are based on rather simple assumption we propose and apply additional measure for comparing the effectiveness of collaborative filtering method which are grounded in decision theory 
we propose a switching hypothesized measurement shm model supporting multimodal probability distribution and present the application of the model in handling potential variability in visual environment when tracking multiple object jointly for a set of occlusion hypothesis a frame is measured once under each hypothesis resulting in a set of measurement at each time instant a computationally efficient shm filter is derived for online joint region tracking both occlusion relationship and state of the object are recursively estimated from the history of hypothesized measurement the reference image is updated adaptively to deal with appearance change of the object the shm model is generally applicable to various dynamic process with multiple alternative measurement method 
textual communication in message board is analyzed for classifying web community we present a communication content based generalization of an existing business oriented classification of web community using keygraph a method for visualizing the co occurrence relation between word and word cluster in text here the text in a message board is analyzed with keygraph and the structure obtained is shown to reflect the essence of the content flow the relation of this content flow with participant interest is then formalized three structure feature of relation between participant and word determining the type of the community are shown to be computed and visualized centralization context coherence and creative decision this help in surveying the essence of a community e g whether the community creates useful knowledge how easy it is to join the community and whether why the community is good for making commercial advertisement 
this research concern the comparison of three different artificial evolution approach to the design of cooperative behavior in a group of simulated mobile robot the first and second approach termed single pool and plasticity are characterized by robot that share a single genotype though the plasticity approach includes a learning mechanism the third approach termed multiple pool is characterized by robot that use different genotype the application domain implement a pursuit evasion game in which team of robot of various size termed predator collectively work to capture either one or two others termed prey these artificial evolution approach are also compared with a static rule based cooperative pursuit strategy specified a priori result indicate that the multiple pool approach is superior comparative to the other approach in term of measure defined for prey capture strategy performance that is this approach facilitated specialization of behavioral role allowing it to be effective for all predator team size tested 
we present a unified framework for simultaneously solving both the pooling problem the construction of efficient document pool for the evaluation of retrieval system and metasearch the fusion of ranked list returned by retrieval system in order to increase performance the implementation is based on the hedge algorithm for online learning which ha the advantage of convergence to bounded error rate approaching the performance of the best linear combination of the underlying system the choice of a loss function closely related to the average precision measure of system performance ensures that the judged document set performs well both in constructing a metasearch list and a a pool for the accurate evaluation of retrieval system our experimental result on trec data demonstrate excellent performance in all measure evaluation of system retrieval of relevant document and generation of metasearch list 
a web site are now ordinary product it is necessary to explicit the notion of quality of a web site the quality of a site may belinked to the easiness of accessibility and also to other criterion such a the fact that the site is up to date and coherent this last quality is difficult to insure because site may be updated very frequently may have many author may be partially generated and inthis context proof reading is very difficult the same piece of information may be found in different occurrence but also in data ormeta data leading to the need for consistency checking in this paper we make a parallel between program and web site we present some example of semantic constraint that one would like to specify constraint between the meaning of category and sub category in a thematic directory consistency between the organization chart and the rest of the site in an academic site we present quickly the natural semantics a way to specify the semantics of programming language that inspires ourworks natural semantics itself come from both an operational semantics and from logic programming and it implementation us prolog then we propose a specification language for semantic constraint in web site that in conjunction with the well known make program permit to generate some site verification tool by compiling the specification into prolog code we apply our method to alarge xml document which is the scientific part of our instituteactivity report tracking error or inconsistency and alsoconstructing some indicator that can be used by the management of theinstitute 
machine learning ha reached a point where most probabilistic meth od can be understood a variation extension and combination of a much smaller set of abstract theme e g a different instance of the em algorithm this enables the systematic derivation of algorithm cu tomized for different model here we demonstrate the autobayes system which take a high level statistical model specification us pow erful symbolic technique based on schema based program synthesis and computer algebra to derive an efficient specialized algorithm for learning that model and generates executable code implementing that algorithm this capability is far beyond that of code collection such a matlab tool box or even tool for model independent optimization such a bug for gibbs sampling complex new algorithm can be generated with out new programming algorithm can be highly specialized and tightly crafted for the exact structure of the model and data and efficient and commented code can be generated for different language or system we present automatically derived algorithm ranging from closed form solution of bayesian textbook problem to recently proposed em algo rithms for clustering regression and a multinomial form of pca 
we present a biophysically constrained cerebellar model of classical conditioning implemented using a neuromorphic analog vlsi avlsi chip like it biological counterpart our cerebellar model is able to control adaptive behavior by predicting the precise timing of event here we describe the functionality of the chip and present it learning performance a evaluated in simulated conditioning experiment at the circuit level and in behavioral experiment using a mobile robot we show that this avlsi model support the acquisition and extinction of adaptively timed conditioned response under real world condition with ultra low power consumption 
we introduce mcp net an extension of the cp net formalism to model and handle the qualitative and conditional preference of multiple agent we give a number of different semantics for reasoning with mcp net the semantics are all based on the idea of individual agent voting we describe how to test optimality and preference ordering within a mcp net and we give complexity result for such task we also discus whether the voting scheme fairly combine together the preference s of the individual agent 
abstract this paper give distribution free concentration inequality for the missing mass and the error rate of histogram rule negative association method can be used to reduce these concentration problem to concentration question about independent sum although the sum are independent they are highly heterogeneous such highly heterogeneous independent sum cannot be analyzed using standard concentration inequality such a hoeffding s inequality the angluin valiant bound bernstein s inequality bennett s inequality or mcdiarmid s theorem the concentration inequality for histogram rule error is motivated by the desire to construct a new class of bound on the generalization error of decision tree 
to address the problem of algorithm selection for the classication task we equip a case based system with new similarity measure that are able to cope with multirelational representation the proposed approach build on notion from clustering and is closely related to idea developed in distance based relational learning the idea presented are pertinent not only for metalearning representational issue but for all domain with similar representation requirement 
edge detection depends not only upon the assumed model of what an edge is but also on how this model is represented the problem of how to represent the edge model is typically neglected despite the fact that the representation is a bottleneck for both computational cost and accuracy we propose to represent edge model by a partition of the edge manifold corresponding to the edge model where each local element of the partition is described by it principal component we describe the construction of this representation and demonstrate it benefit for various edge model 
the relative depth of object cause small shift in the left and right retinal position of these object called binocular disparity here we describe a neuromorphic implementation of a disparity selective complex cell using the binocular energy model which ha been proposed to model the response of disparity selective cell in the visual cortex our system consists of two silicon chip containing spiking neuron with monocular gabor type spatial receptive field rf and circuit that combine the spike output to compute a disparity selective complex cell response the disparity selectivity of the cell can be adjusted by both position and phase shift between the monocular rf profile which are both used in biology our neuromorphic system performs better with phase encoding because the relative response of neuron tuned to different disparity by phase shift are better matched than the response of neuron tuned by position shift the accurate perception of the relative depth of object enables both biological organism and artificial autonomous system to interact successfully with their environment binocular disparity the positional shift between corresponding point in two eye or camera caused by the difference in their vantage point is one important cue that can be used to infer depth in the mammalian visual system neuron in the visual cortex combine signal from the left and right eye to generate response selective for a particular disparity ohzawa et al proposed the binocular energy model to explain the response of binocular complex cell in the cat visual cortex and found that the prediction of this model are in good agreement with measured data this model also match data from the macaque in the energy model a neuron achieves it particular disparity tuning by either a position or a phase shift between it monocular receptive field rf profile for the left and right eye based on an analysis of a population of binocular cell anzai et al suggest that the cat primarily encodes disparity via a phase shift although position shift may play a larger role at higher spatial frequency computational study show that it is possible to estimate disparity from the relative response of model complex cell tuned to different disparity 
we examine the set covering machine when it us data dependent half spacesfor it set of feature and bound it generalization error in term of the number of training error and the number of half space it achieves on the training data we show that it provides a favorable alternative to data dependent ball on some natural data set compared to the support vector machine the set covering machine with data dependent halfspaces produce substantially sparser classifier with comparable and sometimes better generalization furthermore we show that our bound on the generalization error provides an effective guide for model selection 
in this paper we explore the use of random forest rf amit and geman breiman in language modeling the problem of predicting the next word based on word already seen before the goal in this work is to develop a new language modeling approach based on randomly grown decision tree dts and apply it to automatic speech recognition we study our rf approach in the context of gram type language modeling unlike regular gram language model rf language model have the potential to generalize well to unseen data even when a complicated history is used we show that our rf language model are superior to regular gram language model in reducing both the perplexity ppl and word error rate wer in a large vocabulary speech recognition system 
this paper examines whether the cranfield evaluation methodology is robust to gross violation of the completeness assumption i e the assumption that all relevant document within a test collection have been identified and are present in the collection we show that current evaluation measure are not robust to substantially incomplete relevance judgment a new measure is introduced that is both highly correlated with existing measure when complete judgment are available and more robust to incomplete judgment set this finding suggests that substantially larger or dynamic test collection built using current pooling practice should be viable laboratory tool despite the fact that the relevance information will be incomplete and imperfect 
traditionally when one want to learn about a particular topic one read a book or a survey paper with the rapid expansion of the web learning in depth knowledge about a topic from the web is becoming increasingly important and popular this is also due to the web s convenience and it richness of information in many case learning from the web may even be essential because in our fast changing world emerging topic appear constantly and rapidly there is often not enough time for someone to write a book on such topic to learn such emerging topic one can resort to research paper however research paper are often hard to understand by non researcher and few research paper cover every aspect of the topic in contrast many web page often contain intuitive description of the topic to find such web page one typically us a search engine however current search technique are not designed for in depth learning top ranking page from a search engine may not contain any description of the topic even if they do the description is usually incomplete since it is unlikely that the owner of the page ha good knowledge of every aspect of the topic in this paper we attempt a novel and challenging task mining topic specific knowledge on the web our goal is to help people learn in depth knowledge of a topic systematically on the web the proposed technique first identify those sub topic or salient concept of the topic and then find and organize those informative page containing definition and description of the topic and sub topic just like those in a book experimental result using topic show that the proposed technique are highly effective 
vinci is a local area service oriented architecture soa designed for rapid development and management of robust web application based on xml document exchange vinci is designed to complement and interoperate with wide area soas such a soap uddi and net this paper present the vinci architecture the rationale behind it design and an evaluation of it performance specifically we show how system architected with vinci are developed quickly scaled effortlessly and easily moved from prototype to production 
market research ha shown that consumer exhibit a variety of different purchasing behavior specifically some tend to purchase product earlier than other consumer identifying such early buyer can help personalize marketing strategy potentially improving their effectiveness in this paper we present a non parametric approach to the problem of identifying early buyer from purchase data our formulation take a input the detailed purchase information of each consumer with which we construct a weighted directed graph whose node correspond to consumer and whose edge correspond to purchase consumer have in common the edge weight indicate how frequently consumer purchase product earlier than other consumer identifying early buyer corresponds to the problem of finding a subset of node in the graph with maximum difference between the weight of the outgoing and incoming edge this problem is a variation of the maximum cut problem in a directed graph we provide an approximation algorithm based on semidefinite programming sdp relaxation pioneered by goemans and williamson and analyze it performance we apply the algorithm to real purchase data from amazon com providing new insight into consumer behavior 
the purpose of this paper is to re examine the balance between clarity and efficiency in hpsg design with particular reference to the design decision made in the english resource grammar lingo erg it is argued that a simple generalization of the conventional delay statement used in logic programming is sufficient to restore much of the functionality and concomitant benefit that the erg elected to forego with an acceptable although still perceptible computational cost 
in this paper we present a novel method for learning complex concept hypothesis directly from raw training data the task addressed here concern data driven synthesis of recognition procedure for real world object recognition task the method us linear genetic programming to encode potential solution expressed in term of elementary operation and handle the complexity of the learning task by applying cooperative coevolution to decompose the problem automatically the training consists in coevolving feature extraction procedure each being a sequence of elementary image processing and feature extraction operation extensive experimental result show that the approach attains competitive performance for d object recognition in real synthetic aperture radar sar imagery 
wepresentaprobabilisticframeworkforrecognizingobjects in image of cluttered scene hundred of object may be considered and searched in parallel each object is learned from a single training image and modeled by the visual appearance of a set of feature and theirpositionwithrespecttoacommonreferenceframe therecognition processcomputesidentityandpositionofobjectsinthescenebyflnding thebestinterpretationofthesceneintermsoflearnedobjects feature detected in an input image are either paired with database feature or marked a clutter each hypothesis is scored using a generative model of the image which is deflned using the learned object and a model for clutter while the space of possible hypothesis is enormously large one mayflndthebesthypothesise ciently weexploresomeheuristicstodo so our algorithm compare favorably with state of the art recognition system 
human activity can be described a a sequence of d body posture the traditional approach to recognition and d reconstruction of human activity ha been to track motion in d mainly using advanced geometric and dynamic model in this paper we reverse this process view based activity recognition serf a an input to a human body location tracker with the ultimate goal of d reanimation in mind we demonstrate that specific human action can be detected from single frame posture in a video sequence by recognizing the image of a person s posture a corresponding to a particular key frame from a set of stored key frame it is possible to map body location from the key frame to actual frame this is achieved using a shape matching algorithm based on qualitative similarity that computes point to point correspondence between shape together with information about appearance a the mapping is from fixed key frame our tracking doe not suffer from the problem of having to reinitialise when it get lost it is effectively a closed loop we present experimental result both for recognition and tracking for a sequence of a tennis player 
we present a graph theoretic approach to discover storyline from search result storyline are window that offer glimpse into interesting theme latent among the top search result for a query they are different from and complementary to cluster obtained through traditional approach our framework is axiomatically developed and combinatorial in nature based on generalization of the maximum induced matching problem on bipartite graph the core algorithmic task involved is to mine for signature structure in a robust graph representation of the search result we present a very fast algorithm for this task based on local search experiment show that the collection of storyline extracted through our algorithm offer a concise organization of the wealth of information hidden beyond the first page of search result 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
our goal is to recognize human action at a distance at resolution where a whole person may be say pixelstall we introduce a novel motion descriptor based onoptical flow measurement in a spatio temporal volume foreach stabilized human figure and an associated similaritymeasure to be used in a nearest neighbor framework makinguse of noisy optical flow measurement is the key challenge which is addressed by treating optical flow not asprecise pixel displacement but rather a a spatial patternof noisy measurement which are carefully smoothed andaggregated to form our spatio temporal motion descriptor to classify the action being performed by a human figurein a query sequence we retrieve nearest neighbor s from adatabase of stored annotated video sequence we can alsouse these retrieved exemplar to transfer d d skeletonsonto the figure in the query sequence a well a two formsof data based action synthesis do a i do and do a isay result are demonstrated on ballet tennis a well asfootball datasets 
the web provides a global platform for knowledge sharing however several shortcoming still arise from the absence of personalization and collaboration in web search more effective retrieval technique could be provided by mean of transforming explicit knowledge into implicit knowledge the approach presented in this paper is based on a peer to peer architecture and aim at complementing classical web search in term of personalized ranking list these local ranking can be accumulated and evaluated in order to supplement the process of knowledge generation by building virtual knowledge community furthermore the aggregation of ranking list can be used to identify topic a well a community of interest together with social aspect for community support a framework for congenial web search is defined 
the paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human bias in summarization in particular we explore the use of probabilistic decision tree within the clustering framework to account for the variation a well a regularity in human created summary 
document clustering ha long been an important problem in information retrieval in this paper we present a new clustering algorithm asi which us explicitly modeling of the subspace structure associated with each cluster asi simultaneously performs data reduction and subspace identification via an iterative alternating optimization procedure motivated from the optimization procedure we then provide a novel method to determine the number of cluster we also discus the connection of asi with various existential clustering approach finally extensive experimental result on real data set show the effectiveness of asi algorithm 
use of semantic content is one of the major issue which need to be addressed for improving image retrieval effectiveness we present a new approach to classify image based on the combination of image processing technique and hybrid neural network multiple keywords are assigned to an image to represent it main content i e semantic content image are divided into a number of region and colour and texture feature are extracted the first classifier a self organising map som cluster similar image based on the extracted feature then region of the representative image of these cluster were labeled and used to train the second classifier composed of several support vector machine svms initial experiment on the accuracy of keyword assignment for a small vocabulary are reported 
there are various source code archive on the world wide web these archive are usually organized by application category and programming language however manually organizing source code repository is not a trivial task since they grow rapidly and are very large on the order of terabyte we demonstrate machine learning method for automatic classification of archived source code into eleven application topic and ten programming language for topical classification we concentrate on c and c program from the ibiblio and the sourceforge archive support vector machine svm classifier are trained on example of a given programming language or program in a specified category we show that source code can be accurately and automatically classified into topical category and can be identified to be in a specific programming language class 
pro active agent typically have multiple simultaneous goal these may interact with each other both positively and negatively in this paper we provide a mechanism allowing agent to detect and avoid a particular kind of negative interaction where the effect of one goal undo condition that must be protected for successful pursuit of another goal in order to detect such interaction we maintain summary information about the definite and potential conditional requirement and resulting effect of goal and their associated plan we use these summary to guard protected condition by scheduling the execution of goal and plan step the algorithm and data structure developed allow agent to act rationally instead of blindly pursuing goal that will conflict 
we extend the constellation model to include heterogeneous part which may represent either the appearance or the geometry of a region of the object the part and their spatial conguration are learnt simultaneously and automatically without supervision from cluttered image we describe how this model can be employed for ranking the output of an image search engine when searching for object category it is shown that visual consistency in the output image can be identied and then used to rank the image according to their closeness to the visual object category although the proportion of good image may be small the algorithm is designed to be robust and is capable of learning in either a totally unsupervised manner or with a very limited amount of supervision we demonstrate the method on image set returned by google s image search for a number of object category including bottle camel car horse tiger and zebra 
we present a competitive analysis of bayesian learning algorithm in the online learning setting and show that many simple bayesian algorithm such a gaussian linear regression and bayesian logistic regression perform favorably when compared in retrospect to the single best model in the model class the analysis doe not assume that the bayesian algorithm modeling assumption are correct and our bound hold even if the data is adversarially chosen for gaussian linear regression using logloss our error bound are comparable to the best bound in the online learning literature and we also provide a lower bound showing that gaussian linear regression is optimal in a certain worst case sense we also give bound for some widely used maximum a posteriori map estimation algorithm including regularized logistic regression 
online algorithm for classification often require vast amount of memory and computation time when employed in conjunction with kernel function in this paper we describe and analyze a simple approach for an on the fly reduction of the number of past example used for prediction experiment performed with real datasets show that using the proposed algorithmic approach with a single epoch is competitive with the support vector machine svm although the latter being a batch algorithm access each training example multiple time 
detection of space time cluster is an important function in various domain e g epidemiology and public health the pioneering work on the spatial scan statistic is often used a the basis to detect and evaluate such cluster state of the art system based on this approach detect cluster with restrictive shape that cannot model growth and shift in location over time we extend these method significantly by using the flexible square pyramid shape to model such effect a heuristic search method is developed to detect the most likely cluster using a randomized algorithm in combination with geometric shape processing the use of monte carlo method in the original scan statistic formulation is continued in our work to address the multiple hypothesis testing issue our method is applied to a real data set on brain cancer occurrence over a year period the cluster detected by our method show both growth and movement which could not have been modeled with the simpler cylindrical shape used earlier our general framework can be extended quite easily to handle other flexible shape for the space time cluster 
default logic is used to describe regular behavior and normal property we suggest to exploit the framework of default logic for detecting outlier individual who behave in an unexpected way or feature abnormal property the ability to locate outlier can help to maintain knowledge base integrity and to single out irregular individual we first formally define the notion of an outlier and an outlier witness we then show that finding outlier is quite complex indeed we show that several version of the outlier detection problem lie over the second level of the polynomial hierarchy for example the question of establishing if at least one outlier can be detected in a given propositional default theory is p complete although outlier detection involves heavy computation the query involved can frequently be executed offline thus somewhat alleviating the difficulty of the problem in addition we show that outlier detection can be done in polynomial time for both the class of acyclic normal unary default and the class of acyclic dual normal unary default 
understanding the difference between contrasting group is a fundamental task in data analysis this realization ha led to the development of a new special purpose data mining technique contrast set mining we undertook a study with a retail collaborator to compare contrast set mining with existing rule discovery technique to our surprise we observed that straightforward application of an existing commercial rule discovery system magnum opus could successfully perform the contrast set mining task this led to the realization that contrast set mining is a special case of the more general rule discovery task we present the result of our study together with a proof of this conclusion 
p an efficient method using bayesian and linear classifier is presented for analyzing the dynamic of information in high dimensional circuit state and applied to investigate emergent computation in generic cortical microcircuit model it is shown that such recurrent circuit of spiking neuron have an inherent capability to carry out rapid computation on complex spike pattern merging information contained in the order of spike arrival with previously acquired context information p 
we study a method of optimal data driven aggregation of classifier in a convex combination and establish tight upper bound on it excess risk with respect to a convex loss function under the assumption that the solution of optimal aggregation problem is sparse we use a boosting type algorithm of optimal aggregation to develop aggregate classifier of activation pattern in fmri based on locally trained svm classifier the aggregation coefficient are then used to design a boosting map of the brain needed to identify the region with most significant im pact on classification 
finding desired information on the internet is becoming increasingly difficult internet directory such a yahoo which organize web page into hierarchical category provide one solution to this problem however such directory are of limited use because some bias is applied both in the collection and categorization of page we propose a method for integrating multiple internet directory by instance based learning our method provides the mapping of category in order to transfer document from one directory to another instead of simply merging two directory into one we present herein an effective algorithm for determining similar category between two directory via a statistical method called the k statistic in order to evaluate the proposed method we conducted experiment using two actual internet directory yahoo and google the result show that the proposed method achieves extensive improvement relative to both the naive bayes and enhanced naive bayes approach without any text analysis on document 
in the standard feature selection problem we are given a fixed set of candidate feature for use in a learning problem and must select a subset that will be used to train a model that is a good a possible according to some criterion in this paper we present an interesting and useful variant the online feature selection problem in which instead of all feature being available from the start feature arrive one at a time the learner s task is to select a subset of feature and return a corresponding model at each time step which is a good a possible given the feature seen so far we argue that existing feature selection method do not perform well in this scenario and describe a promising alternative method based on a stagewise gradient descent technique which we call grafting 
answer pattern have been shown to improve the perfor mance of open domain factoid qa system their use however requires either constructing the pattern manually or developing algorithm for learning them automatically we present here a simpler approach that extends the technique of language modeling to create answer model these are language model trained on the correct answer to training question we show how they fit naturally into a probabilis tic model for answer passage retrieval and demonstrate their effectiveness on the trec qa corpus 
fages showed that if a program is tight then every propositional model of it completion is also it stable model recently babovich erdem and lifschitz generalized fages result and showed that this is also true if the program is tight on the given model of the completion a it turned out this is quite a general result among the commonly known benchmark domain only niemelii s normal logic program encoding of the hamiltonian circuit hc problem doe not have this property in this paper we propose a new normal logic program for solving the hc problem and show that the program is tight on every model of it completion experimental result showed that for many graph this new encoding improves the performance of both smodels and assat chaff especially of the latter system which is based on the sat solver chaff we also propose a notion of inherently tight logic program and show that for any program it is inherently tight iff all it completion model are stable model we then propose a polynomial transformation from a logic program to one that is inherently tight thus providing a reduction of stable model semantics to program completion semantics and sat 
we consider the situation where a number of agent are distributed and each of them collect a data sequence generated according to an unknown probability distribution here each of the distribution is specified by common parameter and individual parameter e g a normal distribution with an identical mean and a different variance here we introduce a notion of an information consortium which is a framework where the agent cannot show raw data to one another but they like to enjoy significant information gain for estimating the respective distribution such an information consortium ha recently received much interest in a broad range of area including financial risk management ubiquitous network mining etc in this paper we are concerned with the following three issue how to design a collaborative strategy for agent to estimate the respective distribution in the information consortium characterizing when each agent ha a benefit in term of information gain for estimating it distribution or information loss for predicting future data and charracterizing how much benefit each agent obtains in this paper we yield a statistical formulation of information consortium and solve all of the above three problem for a general form of probability distribution specifically we propose a basic strategy for cooperative estimation and derive a necessary and sufficient condition for each agent to have a significant benefit 
given an n n grid of square where each square ha a count and an underlying population our goal is to find the square region with the highest density and to calculate it significance by randomization any density measure d dependent on the total count and total population of a region can be used for example if each count represents the number of disease case occurring in that square we can use kulldorff s spatial scan statistic dk to find the most significant spatial disease cluster a naive approach to finding the maximum density region requires o n time and is generally computationally infeasible we present a novel algorithm which partition the grid into overlapping region bound the maximum score of subregions contained in each region and prune region which cannot contain the maximum density region for sufficiently dense region this method find the maximum density region in optimal o n time in practice resulting in significant x speedup 
learning visual model of object category notoriously requiresthousands of training example this is due to thediversity and richness of object appearance which requiresmodels containing hundred of parameter we present amethod for learning object category from just a few image it is based on incorporating generic knowledge which may be obtained from previously learntmodels of unrelated category we operate in a variationalbayesian framework object category are represented byprobabilistic model and prior knowledge is representedas a probability density function on the parameter of thesemodels the posterior model for an object category is obtainedby updating the prior in the light of one or more observation our idea are demonstrated on four diverse category human face airplane motorcycle spotted cat initially three category are learnt from hundred of trainingexamples and a prior is estimated from these thenthe model of the fourth category is learnt from to trainingexamples and is used for detecting new exemplar a setof test image 
the simultaneous localization and mapping problem is a fundamental problem in mobile robotics while a robot navigates in an unknown environment it must incrementally build a map of it surroundings and localize itself within that map traditional approach to the problem are based upon kalman filter but suffer from complexity issue first the belief state grows quadratically in the size of the map and second the filtering operation can take time quadratic in the size of the map i present a linear space filter that maintains a tractable approximation of the belief state a a thin junction tree the junction tree grows under measurement and motion update and is periodically thinned to remain tractable the time complexity of the filter operation is linear in the size of the map i also present simple enhancement that permit constant time approximate filtering 
the goal of feature induction is to automatically create nonlinear combination of existing feature a additional input feature to improve classification accuracy typically nonlinear feature are introduced into a support vector machine svm through a nonlinear kernel function one disadvantage of such an approach is that the feature space induced by a kernel function is usually of high dimension and therefore will substantially increase the chance of over fitting the training data another disadvantage is that nonlinear feature are induced implicitly and therefore are difficult for people to understand which induced feature are critical to the classification performance in this paper we propose a boosting style algorithm that can explicitly induces important nonlinear feature for svms we present empirical study with discussion to show that this approach is effective in improving classification accuracy for svms the comparison with an svm model using nonlinear kernel also indicates that this approach is effective and robust particularly when the number of training data is small 
spoken dialogue system promise ef cient and natural access to information service from any phone recently spo ken dialogue system for widely used ap plication such a email travel informa tion and customer care have moved from research lab into commercial use these application can receive million of call a month this huge amount of spoken dialogue data ha led to a need for fully automatic method for selecting a subset of caller dialogue that are most likely to be useful for further system improve ment to be stored transcribed and further analyzed this paper report result on automatically training a problematic di alogue identi er to classify problematic human computer dialogue using a corpus of darpa communicator dialogue in the travel planning domain we show that using fully automatic feature we can identify class of problematic dialogue with accuracy from to 
we claim and present argument to the effect that a large class of manifold learning algorithm that are essentially local and can be framed a kernel learning algorithm will suffer from the curse of dimensionality at the dimension of the true underlying manifold this observation suggests to explore non local manifold learning algorithm which attempt to discover shared structure in the tangent plane at different position a criterion for such an algorithm is proposed and experiment estimating a tangent plane prediction function are presented showing it advantage with respect to local manifold learning algorithm it is able to generalize very far from training data on learning handwritten character image rotation where a local non parametric method fails 
the introduction of the joint image manifold allows to treatthe problem of recovering camera motion and epipolar geometryas the problem of fitting a manifold to the data measuredin a stereo pair the manifold ha a singularity andboundary therefore care must be taken when fitting it this paper review the notion of joint image manifold and how previous motion recovery method can be viewedin it context and then offer a new fitting method whichimproves upon previous result especially when the extentof the data and or the motion are small 
according to a widely held view neuron in lateral geniculate nucleus lgn operate on visual stimulus in a linear fashion there is ample evidence however that lgn response are not entirely linear to account for nonlinearities we propose a model that synthesizes more than year of research in the field model neuron have a linear receptive field and a nonlinear divisive suppressive field the suppressive field computes local root meansquare contrast to test this model we recorded response from lgn of anesthetized paralyzed cat we estimate model parameter from a basic set of measurement and show that the model can accurately predict response to novel stimulus the model might serve a the new standard model of lgn response it specifies how visual processing in lgn involves both linear filtering and divisive gain control 
abstract this paper introduces an approach to sentiment analysis which us support vector machine svms to bring together diverse source of potentially pertinent information including several favorability measure for phrase and adjective and where available knowledge of the topic of the text model using the feature introduced are further combined with unigram model which have been shown to be effective in the past pang et al and lemmatized version of the unigram model experiment on movie review data from epinions com demonstrate that hybrid svms which combine unigram style feature based svms with those based on real valued favorability measure obtain superior performance producing the best result yet published using this data further experiment using a feature set enriched with topic information on a smaller dataset of music review handannotated for topic are also reported the result of which suggest that incorporating topic information into such model may also yield improvement 
bayesian filtering provides a principled approach for a variety of problem in machine perception and robotics current filtering method work with analog hypothesis space and find approximate solution to the resulting non linear filtering problem using monte carlo approximation i e particle filter or linear approximation e g extended kalman filter instead in this paper we propose digitizing the hypothesis space into a large number n of discrete hypothesis thus the approach becomes equivalent to standard hidden markov model hmm except for the fact that we use a very large number of state one reason this approach ha not been tried in the past is that the standard forward filtering equation for discrete hmms require order n operation per time step and thus rapidly become prohibitive in our model however the state are arranged in two dimensional topology with locationindependent dynamic with this arrangement predictive distribution can be computed via convolution in addition the computation of log likelihood ratio can also be performed via convolution we describe algorithm that solve the filtering equation performing this convolution for a special class of transition kernel in order n operation per time step this allows exact solution of filtering problem in real time with hundred of thousand of discrete hypothesis we found this number of hypothesis sucient for object tracking problem we also propose principled method to adapt the model parameter in non stationary environment and to detect and recover from tracking error 
abstract applicability of ilp to real world problemsis constrained by the high dimensionality ofilp task this paper proposes to reducethe dimensionality of the ilp example spaceby bringing feature subset selection to therealm of ilp seen a a black box the methodreduces ilp example in the form of nonrecursivedatalog clause by removing literalsjudged irrelevant for the ilp task athand the approach exploit existing avlfeature selection f algorithm by changingthe 
most of the work on xml query and search ha stemmed from the publishing and database community mostly for the need of business application recently the information retrieval community began investigating the xml search issue to answer information discovery need following this trend we present here an approach where information need can be expressed in an approximate manner a piece of xml document or xml fragment of the same nature a the document that are being searched we present an extension of the vector space model for searching xml collection via xml fragment and ranking result by relevance we describe how we have extended a full text search engine to comply with this model the value of the proposed method is demonstrated by the relative high precision of our system which wa among the top performer in the recent inex workshop our result indicate that certain query are more appropriate than others for the extended vector space model specifically query with relatively specific context but vague information need are best situated to reap the benefit of this model finally our result show that one method may not fit all type of query and that it could be worthwhile to use different solution for different application 
in this paper we explore effect of various feature selection algorithm on document classification performance we propose to use two possibly distinct linear classifier one used exclusively for feature selection in order to obtain the feature space for training the second classifier using possibly a different training set the resulting classifier is used to classify new document experiment show that feature selection based on the linear svm algorithm combine well with different type of classifier based on the experimental result we make a conjecture that it is the level of sophis tication at which the scoring method take into account information about feature rather than it compatibility with the classifier in term of it design that make the feature selection method more or le successful 
to enable information integration schema matching is a critical step for discovering semantic correspondence of attribute across heterogeneous source while complex matchings are common because of their far more complex search space most existing technique focus on simple matchings to tackle this challenge this paper take a conceptually novel approach by viewing schema matching a correlation mining for our task of matching web query interface to integrate the myriad database on the internet on this deep web query interface generally form complex matchings between attribute group e g author corresponds to first name last name in the book domain we observe that the co occurrence pattern across query interface often reveal such complex semantic relationship grouping attribute e g first name last name tend to be co present in query interface and thus positively correlated in contrast synonym attribute are negatively correlated because they rarely co occur this insight enables u to discover complex matchings by a correlation mining approach in particular we develop the dcm framework which consists of data preparation dual mining of positive and negative correlation and finally matching selection unlike previous correlation mining algorithm which mainly focus on finding strong positive correlation our algorithm care both positive and negative correlation especially the subtlety of negative correlation due to it special importance in schema matching this lead to the introduction of a new correlation measure h measure distinct from those proposed in previous work we evaluate our approach extensively and the result show good accuracy for discovering complex matchings 
in this paper we introduce the fractal summarization model based on the fractal theory in fractal summarization the important information is captured from the source text by exploring the hierarchical structure and salient feature of the document a condensed version of the document that is informatively close to the original is produced iteratively using the contractive transformation in the fractal theory user evaluation ha shown that fractal summarization outperforms traditional summarization 
supervised learning technique for text classification often require a large number of labeled example to learn accurately one way to reduce the amount of labeled data required is to develop algorithm that can learn effectively from a small number of labeled example augmented with a large number of unlabeled example current text learning technique for combining labeled and unlabeled such a em and co training are mostly applicable for classification task with a small number of class and do not scale up well for large multiclass problem in this paper we develop a framework to incorporate unlabeled data in the error correcting output coding ecoc setup by first decomposing multiclass problem into multiple binary problem and then using co training to learn the individual binary classification problem we show that our method is especially useful for text classification task involving a large number of category and outperforms other semi supervised learning technique such a em and co training in addition to being highly accurate this method utilizes the hamming distance from ecoc to provide high precision result we also present result with algorithm other than co training in this framework and show that co training is uniquely suited to work well within ecoc 
we consider how much influence a center can exert on a game if it only power is to propose contract to the agent before the original game and enforce the contract after the game if all agent sign it modelling the situation a an extensive form game we note that the outcome that are enforceable are precisely those in which the payoff to each agent is higher than it payoff in at least one of the nash equilibrium of the original game we then show that these outcome can still be achieved without any effort actually expended by the center we propose a mechanism in which the center doe not monitor the game and the contract are written so that in equilibrium all agent sign and obey the contract with no need for center intervention 
we present test result from spike timing correlation learning experiment carried out with silicon neuron with stdp spike timing dependent plasticity synapsis the weight change scheme of the stdp synapsis can be set to either weight independent or weight dependent mode we present result that characterise the learning window implemented for both mode of operation when presented with spike train with dieren t type of synchronisation the neuron develop bimodal weight distribution we also show that a layered network of silicon spiking neuron with stdp synapsis can perform hierarchical synchrony detection 
natural image are highly structured in their spatialconfiguration where one would expect a different spatialdistribution for every image a each image ha a differentspatiallay out we show that the spatial statistic of recordedimages can be explained by a single process of sequentialfragmentation the observation by a resolution limited sensorysystem turn out to have a profound influence on the observedstatistics of natural image the power law and normal distributionrepresent the extreme case of sequential fragmentation betweenthese two extreme spatial detail statistic deform from power lawto normal through the weibull type distribution a receptive fieldsize increase relative to image detail size 
abstract we prove the soundness of a compositional model checking algorithm using acl the algorithm usesconjunctive and cone of inuence reduction to reduce a large model checking problem into a collectionof smaller problem and we prove the soundness of the composition of these reduction the algorithmchecks property specied in linear temporal logic ltl but the acl logic doe not allow u toexpress either the classical semantics of ltl or the classical soundness proof for these 
in previous work levesque proposed an extension to classical database that would allow for a certain form of incomplete first order knowledge since this extension wa sufficient to make full logical deduction undecidable he also proposed an alternative reasoning scheme with desirable logical property he also claimed without proof that this reasoning could be implemented efficiently using database technique such a projection and join in this paper we substantiate this claim and show how to adapt a bottom up database query evaluation algorithm for this purpose thus obtaining a tractability result comparable to those that exist for database 
we report on an automated runtime anomaly detection method at the application layer of multi node computer system although several network management system are available in the market none of them have sufficient capability to detect fault in multi tier web based system with redundancy we model a web based system a a weighted graph where each node represents a service and each edge represents a dependency between service since the edge weight vary greatly over time the problem we address is that of anomaly detection from a time sequence of graph in our method we first extract a feature vector from the adjacency matrix that represents the activity of all of the service the heart of our method is to use the principal eigenvector of the eigenclusters of the graph then we derive a probability distribution for an anomaly measure defined for a time series of directional data derived from the graph sequence given a critical probability the threshold value is adaptively updated using a novel online algorithm we demonstrate that a fault in a web application can be automatically detected and the faulty service are identified without using detailed knowledge of the behavior of the system 
we present a framework for calculating low dimensional base torepresent image irradiance from surface with isotropic reflectanceunder arbitrary illumination by representing the illumination andthe bidirectional reflectance distribution function brdf infrequency space a model for the image irradiance is derived thismodel is then reduced in dimensionality by analyticallyconstructing the principal component basis for all image given thevariations in both the illumination and the surface material theprincipal component basis are constructed in such a way that allthe symmetry helmholtz reciprocity and isotropy of the brdf arepreserved in the basis function using the framework we calculatea basis using a database of natural illumination and the curetdatabase containing brdfs of real world surface material 
minimal recursion semantics mr is the standard formalism used in large scale hpsg grammar to model underspecified semantics we present the first provably efficient algorithm to enumerate the reading of mr structure by translating them into normal dominance constraint 
extensive study have shown that mining microarray data set is important in bioinformatics research and biomedical application in this paper we explore a novel type of gene sample time microarray data set which record the expression level of various gene under a set of sample during a series of time point in particular we propose the mining of coherent gene cluster from such data set each cluster contains a subset of gene and a subset of sample such that the gene are coherent on the sample along the time series the coherent gene cluster may identify the sample corresponding to some phenotype e g disease and suggest the candidate gene correlated to the phenotype we present two efficient algorithm namely the sample gene search and the gene sample search to mine the complete set of coherent gene cluster we empirically evaluate the performance of our approach on both a real microarray data set and synthetic data set the test result have shown that our approach are both efficient and effective to find meaningful coherent gene cluster 
the formation of disulphide bridge among cysteine is an important feature of protein structure here we develop new method for the prediction of disulphide bond connectivity we first build a large curated data set of protein containing disulphide bridge and then use dimensional recursive neural network to predict bonding probability between cysteine pair these probability in turn lead to a weighted graph matching problem that can be addressed efficiently we show how the method consistently achieves better result than previous approach on the same validation data in addition the method can easily cope with chain with arbitrary number of bonded cysteine therefore it overcomes one of the major limitation of previous approach restricting prediction to chain containing no more than oxidized cysteine the method can be applied both to situation where the bonded state of each cysteine is known or unknown in which case bonded state can be predicted with precision and recall the method also yield an estimate for the total number of disulphide bridge in each chain 
this paper proposes a novel texture representation suitablefor recognizing image of textured surface under a widerange of transformation including viewpoint change andnon rigid deformation unlike many existing feature extractionmethods which treat the neighborhood of everypixel a a candidate texture element the proposed algorithmworks by selecting a sparse set of affine invariant localpatches this spatial selection process besides providinggreater computational efficiency 
paracatadioptric sensor combine a parabolic shaped mirrorand a camera inducing an orthographic projection such a configuration provides a wide field of view whilekeeping a single effective viewpoint previous work in centralcatadioptric sensor proved that a line project into aconic curve and that three line image are enough to calibratethe system however the estimation of the coniccurves where line are mapped is hard to accomplish ingeneral only a small arc of the conic is visible in the imageand conventional conic fitting technique are unable to correctlyestimate the curve the present work show that a setof conic curve corresponds to paracatadioptric line imagesif and only if certain property are verified these propertiesare used to constraint the search space and correctlyestimate the curve the accurate estimation of a minimumof three line image allows the complete calibration of theparacatadioptric camera if the camera is skewless and theaspect ratio is known then the conic fitting problem is solvednaturally by an eigensystem for the general situation theconic curve are estimated using non linear optimization 
significant progress in clustering ha been achieved by algorithm that are based on pairwise affinity between the datapoints in particular spectral clustering method have the advantage of being able to divide arbitrarily shaped cluster and are based on efficient eigenvector calculation however spectral method lack a straightforward probabilistic interpretation which make it difficult to automatically set parameter using training data in this paper we use the previously proposed typical cut framework for pairwise clustering we show an equivalence between calculating the typical cut and inference in an undirected graphical model we show that for clustering problem with hundred of datapoints exact inference may still be possible for more complicated datasets we show that loopy belief propagation bp and generalized belief propagation gbp can give excellent result on challenging clustering problem we also use graphical model to derive a learning algorithm for affinity matrix based on labeled data 
a animal interact with their environment they must constantly update estimate about their state bayesian model combine prior probability a dynamical model and sensory evidence to update estimate optimally these model are consistent with the result of many diverse psychophysical study however little is known about the neural representation and manipulation of such bayesian information particularly in population of spiking neuron we consider this issue suggesting a model based on standard neural architecture and activation we illustrate the approach on a simple random walk example and apply it to a sensorimotor integration task that provides a particularly compelling example of dynamic probabilistic computation bayesian model have been used to explain a gamut of experimental result in task which require estimate to be derived from multiple sensory cue these include a wide range of psychophysical study of perception motor action and decision making central to bayesian inference is that computation are sensitive to uncertainty about afferent and efferent quantity arising from ignorance noise or inherent ambiguity e g the aperture problem and that these uncertainty change over time a information accumulates and dissipates understanding how neuron represent and manipulate uncertain quantity is therefore key to understanding the neural instantiation of these bayesian inference 
we present a description of three different algorithm that use background knowledge to improve text classifier one us the background knowledge a an index into the set of training example the second method us background knowledge to reexpress the training example the last method treat piece of background knowledge a unlabeled example and actually classifies them the choice of background knowledge affect each method s performance and we discus which type of background knowledge is most useful for each specific method 
the world wide web is evolving into a medium that will soon make it possible for conceiving and implementing situation aware service a situation aware or situated web application is one that render the user with an experience content interaction and presentation that is so tailored to his her current situation this requires the fact and opinion regarding the context to be communicated to the server by mean of a profile which is then applied against the description of the application object at the server in order to generate the required experience this paper discus a profile view of the situated web architecture and analyzes the key technology and capability that enable them we conclude that trusted framework wherein rich vocabulary describing user and their context application and document along with rule for processing them are critical element of such architecture 
graphplan planning graph are structure widely used in modern planner the exclusion relation calculated in the planning graph extension provide very useful information especially in temporal planning where action have different duration however graphplan backward search ha some inefficiency that impose limitation when dealing with large temporal problem this paper present a new search process for temporal planning to avoid these inefficiency this search us the information of a planning graph and show beneficial in the scalability of the planner moreover our experiment show that a planner with this new search is competitive with other state of the art planner w r t the plan quality 
kernel method have gained a great deal of popularity in the machine learning community a a method to learn indirectly in highdimensional feature space those interested in relational learning have recently begun to cast learning from structured and relational data in term of kernel operation we describe a general family of kernel function built up from a description language of limited expressivity and use it to study the benet and drawback of kernel learning in relational domain learning with kernel in this family directly model learning over an expanded feature space constructed using the same description language this allows u to examine issue of time complexity in term of learning with these and other relational kernel and how these relate to generalization ability the tradeos between using kernel in a very high dimensional implicit space versus a restricted feature space is highlighted through two experiment in bioinformatics and in natural language processing 
it is widely known that for the affine camera model both shapeand motion can be factorized directly from the so called imagemeasurement matrix constructed from image point coordinate theability to extract both shape and motion from this matrix by asingle svd operation make this shape from motion approachattractive however it cannot deal with missing feature pointsand in the presence of outlier a direct svd to the matrix wouldyield highly unreliable shape and motion component in this paper we present an outlier correction scheme that iteratively updatesthe element of the image measurement matrix the magnitude andsign of the update to each element is dependent upon the residualrobustly estimated in each iteration the result is that outliersare corrected and retained giving improved reconstruction andsmaller reprojection error our iterative outlier correctionscheme ha been applied to both synthesized and real videosequences the result obtained are remarkably good 
we propose a meta grammatical framework for dependency grammar accommodating any number of dimension and restricting their licensed model through the application of parametric principle we rst instantiate this framework to obtain the version of topological dependency grammar tdg proposed by duchier and debusmann we then describe an extended instantiation which add support for semantic dependency thus also providing an account of control and raising construction and meaning assembly 
the marine corp project albert seek to model complex phenomenon by observing the behavior of relatively simple simulation over thousand of run a rich data base is developed by running the simulation thousand of time varying the agent and scenario input parameter a well a the random seed exploring this result space may provide significant insight into nonlinear surprising and emergent behavior capturing these result can provide a path for making the result usable for decision support to a military commander this paper present two data mining approach rule discovery and bayesian network for analyzing the albert simulation data the first approach generates rule from the data and then us them to create descriptive model the second generates bayesian network which provide a quantitative belief model for decision support both of these approach a well a the project albert simulation are framed in the context of a system architecture for decision support 
this paper provides evidence for genzel and charniak s entropy rate principle which predicts that the entropy of a sentence increase with it position in the text we show that this principle hold for individual sentence not just for average but we also find that the entropy rate effect is partly an artifact of sentence length which also correlate with sentence position secondly we evaluate a set of prediction that the entropy rate principle make for human language processing using a corpus of eye tracking data we show that entropy and processing effort are correlated and that processing effort is constant throughout a text 
contextual retrieval is a critical technique for facilitating many important application such a mobile search personalized search pc troubleshooting etc despite of it importance there is no comprehensive retrieval model to describe the contextual retrieval process we observed that incompatible context noisy context and incomplete query are several important issue commonly existing in contextual retrieval application however these issue have not been previously explored and discussed in this paper we propose probabilistic model to address these problem our study clearly show that query log is the key to build effective contextual retrieval model we also conduct a case study in the pc troubleshooting domain to testify the performance of the proposed model and experimental result show that the model can achieve very good retrieval precision 
we describe a class of causal discrete latent variable model called multiple multiplicative factor model mmfs a data vector is represented in the latent space a a vector of factor that have discrete non negative expression level each factor proposes a distribution over the data vector the distinguishing feature of mmfs is that they combine the factor proposed distribution multiplicatively taking into account factor expression level the product formulation of mmfs allow factor to specialize to a subset of the item while the causal generative semantics mean mmfs can readily accommodate missing data this make mmfs distinct from both directed model with mixture semantics and undirected product model in this paper we present empirical result from the collaborative filtering domain showing that a binary multinomial mmf model match the performance of the best existing model while learning an interesting latent space description of the user 
we propose to selectively remove example from the training set using probabilistic estimate related to editing algorithm devijver and kittler this heuristic procedure aim at creating a separable distribution of training example with minimal impact on the position of the decision boundary it break the linear dependency between the number of svs and the number of training example and sharply reduces the complexity of svms during both the training and prediction stage 
a form of advertisement which is becoming very popular on the web is based on electronic coupon e coupon distribution e coupon are the digital analogue of paper coupon which are used to provide customer with discount or gift in order to incentive the purchase of some product nowadays the potential of digital coupon ha not been fully exploited on the web this is mostly due to the lack of efficient technique to handle the generation and distribution of e coupon in this paper we discus model and protocol for e coupon satisfying a number of security requirement our protocol is lightweight and preserve the privacy of the user since it doe not require any registration phase 
go is an ancient oriental game whose complexity ha defeated attempt to automate it we suggest using probability in a bayesian sense to model the uncertainty arising from the vast complexity of the game tree we present a simple conditional markov random eld model for predicting the pointwise territory outcome of a game the topology of the model reects the spatial structure of the go board we describe a version of the swendsen wang process for sampling from the model during learning and apply loopy belief propagation for rapid inference and prediction the model is trained on several hundred record of professional game our experimental result indicate that the model successfully learns to predict territory despite it simplicity 
reminder system support people with impaired prospective memory and or executive function by providing them with reminder of their functional daily activity we integrate temporal constraint reasoning with reinforcement learning rl to build an adaptive reminder system and in a simulated environment demonstrate that it can personalize to a user and adapt to both shortand long term change in addition to advancing the application domain our integrated algorithm contributes to research on temporal constraint reasoning by showing how rl can select an optimal policy from amongst a set of temporally consistent one and it contributes to the work on rl by showing how temporal constraint reasoning can be used to dramatically reduce the space of action from which an rl agent need to learn 
in this paper we tackle a real world problem the search of a function to evaluate the merit of beef cattle a meat producer the independent variable represent a set of live animal measurement while the output cannot be captured with a single number since the available expert tend to ass each animal in a relative way comparing animal with the other partner in the same batch therefore this problem can not be solved by mean of regression method our approach is to learn the preference of the expert when they order small group of animal thus the problem can be reduced to a binary classification and can be dealt with a support vector machine svm improved with the use of a feature subset selection f method we develop a method based on recursive feature elimination rfe that employ an adaptation of a metric based method devised for model selection adj finally we discus the extension of the resulting method to more general setting and provide a comparison with other possible alternative 
we address visual correspondence problem without assuming that scene point have similar intensity in different view this situation is common usually due to nonlambertian scene or to difference between camera we use maximization of mutual information a powerful technique for registering image that requires no a priori model of the relationship between scene intensity in different view however it ha proven difficult to use mutual information to compute dense visual correspondence comparing fixed size window via mutual information suffers from the well known problem of fixed window namely poor performance at discontinuity and in low texture region in this paper we show how to compute visual correspondence using mutual information without suffering from these problem using a simple approximation mutual information can be incorporated into the standard energy minimization framework used in early vision the energy can then be efficiently minimized using graph cut which preserve discontinuity and handle low texture region the resulting algorithm combine the accurate disparity map that come from graph cut with the tolerance for intensity change that come from mutual information 
semantic taxonomy such a wordnet provide a rich source of knowledge for natural language processing application but are expensive to build maintain and extend motivated by the problem of automatically constructing and extending such taxonomy in this paper we present a new algorithm for automatically learning hypernym is a relation from text our method generalizes earlier work that had relied on using small number of hand crafted regular expression pattern to identify hypernym pair using dependency path feature extracted from parse tree we introduce a general purpose formalization and generalization of these pattern given a training set of text containing known hypernym pair our algorithm automatically extract useful dependency path and applies them to new corpus to identify novel pair on our evaluation task determining whether two noun in a news article participate in a hypernym relationship our automatically extracted database of hypernym attains both higher precision and higher recall than wordnet 
it is fairly common that different people are associated with the same name in tracking person entity in a large document pool it is important to determine whether multiple mention of the same name across document refer to the same entity or not previous approach to this problem involves measuring context similarity only based on co occurring word this paper present a new algorithm using information extraction support in addition to co occurring word a learning scheme with minimal supervision is developed within the bayesian framework maximum entropy modeling is then used to represent the probability distribution of context similarity based on heterogeneous feature statistical annealing is applied to derive the final entity coreference chain by globally fitting the pairwise context similarity benchmarking show that our new approach significantly outperforms the existing algorithm by percentage point in overall f measure 
random decision tree is an ensemble of decision tree the feature at any node of a tree in the ensemble is chosen randomly from remaining feature a chosen discrete feature on a decision path cannot be chosen again continuous feature can be chosen multiple time however with a different splitting value each time during classification each tree output raw posterior probability the probability from each tree in the ensemble are averaged a the final posterior probability estimate although remarkably simple and somehow counter intuitive random decision tree ha been shown to be highly accurate under loss and cost sensitive loss function preliminary explanation of it high accuracy is due to the error tolerance property of probabilistic decision making our study ha shown that the actual reason for random tree s superior performance is due to it optimal approximation to each example s true probability to be a member of a given class 
recently relevance vector machine rvm have been fashioned from a sparse bayesian learning sbl framework to perform supervised learning using a weight prior that encourages sparsity of representation the methodology incorporates an additional set of hyperparameters governing the prior one for each weight and then adopts a specific a pproximation to the full marginalization over all weight and hyperparameters despite it empirical success however no rigorous motivation for this particular approximation is currently available to addre s this issue we demonstrate that sbl can be recast a the application of a rigorous variational approximation to the full model by expressing the prior in a dual form this formulation obviates the necessity of assuming any hyperpriors and lead to natural intuitive explanation of why spar sity is achieved in practice 
a directed network of people connected by rating or trust score and a model for propagating those trust score is a fundamental building block in many of today s most successful e commerce and recommendation system we develop a framework of trust propagation scheme each of which may be appropriate in certain circumstance and evaluate the scheme on a large trust network consisting of k trust score expressed among k people we show that a small number of expressed trust distrust per individual allows u to predict trust between any two people in the system with high accuracy our work appears to be the first to incorporate distrust in a computational trust propagation setting 
geodesic active contour and graph cut are two standard imagesegmentation technique we introduce a new segmentation methodcombining some of their benefit our main intuition is that anycut on a graph embedded in some continuous space can be interpretedas a contour in d or a surface in d we show how to build agrid graph and set it edge weight so that the cost of cut isarbitrarily close to the length area of the correspondingcontours surface for any anisotropic riemannian metric there aretwo interesting consequence of this technical result first graphcut algorithm can be used to find globally minimum geodesiccontours minimal surface in d under arbitrary riemannian metricfor a given set of boundary condition second we show how tominimize metrication artifact in existing graph cut based methodsin vision theoretically speaking our work provides an interestinglink between several branch of mathematics differentialgeometry integral geometry and combinatorial optimization themain technical problem is solved using cauchy crofton formula fromintegral geometry 
a new approach to ensemble learning is introduced that take ranking rather than classification a fundamental leading to model on the symmetric group and it cosets the approach us a generalization of the mallow model on permutation to combine multiple input ranking application include the task of combining the output of multiple search engine and multiclass or multilabel classification where a set of input classifier is viewed a generating a ranking of class label experiment for both type of application are presented 
a longstanding goal in planning research is the ability to generalize plan developed for some set of environment to a new but similar environment with minimal or no replanning such generalization can both reduce planning time and allow u to tackle larger domain than the one tractable for direct planning in this paper we present an approach to the generalization problem based on a new framework of relational markov decision process rmdps an rmdp can model a set of similar environment by representing object a instance of different class in order to generalize plan to multiple environment we define an approximate value function specified in term of class of object and in a multiagent setting by class of agent this class based approximate value function is optimized relative to a sampled subset of environment and computed using an efficient linear programming method we prove that a polynomial number of sampled environment suffices to achieve performance close to the performance achievable when optimizing over the entire space our experimental result show that our method generalizes plan successfully to new significantly larger environment with minimal loss of performance relative to environment specific planning we demonstrate our approach on a real strategic computer war game 
problem in which abnormal or novel situation should be detected can be approached by describing the domain of the class of typical example these application come from the area of machine diagnostics fault detection illness identification or in principle refer to any problem where little knowledge is available outside the typical class in this paper we explain why proximity are natural representation for domain descriptor and we propose a simple one class classifier for dissimilarity representation by the use of linear programming an efficient one class description can be found based on a small number of prototype object this classifier can be made more robust by transforming the dissimilarity and cheaper to compute by using a reduced representation set finally a comparison to a comparable one class classifier by campbell and bennett is given 
there ha been a recent swell of interest in the automatic identification and extraction of opinion and emotion in text in this paper we present the first experimental result classifying the strength of opinion and other type of subjectivity and classifying the subjectivity of deeply nested clause we use a wide range of feature including new syntactic feature developed for opinion recognition in fold cross validation experiment using support vector regression we achieve improvement in mean squared error over baseline ranging from to 
the paper describes a user study examining method for improving user query specifically interactive and automatic query expansion and advanced search option the user study includes subjective and objective evaluation of the effect of the above method and a comparison between the real and perceived effect 
we propose a model for natural image in which the probability of an image is proportional to the product of the probability of some filter output we encourage the system to find sparse feature by using a studentt distribution to model each filter output if the t distribution is used to model the combined output of set of neurally adjacent filter the system learns a topographic map in which the orientation spatial frequency and location of the filter change smoothly across the map even though maximum likelihood learning is intractable in our model the product form allows a relatively efficient learning procedure that work well even for highly overcomplete set of filter once the model ha been learned it can be used a a prior to derive the iterated wiener filter for the purpose of denoising image 
we present a non traditional retrieval problem we call subtopic retrieval the subtopic retrieval problem is concerned with finding document that cover many different subtopics of a query topic in such a problem the utility of a document in a ranking is dependent on other document in the ranking violating the assumption of independent relevance which is assumed in most traditional retrieval method subtopic retrieval pose challenge for evaluating performance a well a for developing effective algorithm we propose a framework for evaluating subtopic retrieval which generalizes the traditional precision and recall metric by accounting for intrinsic topic difficulty a well a redundancy in document we propose and systematically evaluate several method for performing subtopic retrieval using statistical language model and a maximal marginal relevance mmr ranking strategy a mixture model combined with query likelihood relevance ranking is shown to modestly outperform a baseline relevance ranking on a data set used in the trec interactive track 
applying belief revision logic to model adaptive information retrieval is appealing since it provides a rigorous theoretical foundation to model partiality and uncertainty inherent in any information retrieval ir process in particular a retrieval context can be formalised a a belief set and the formalised context is used to disambiguate vague user query belief revision logic also provides a robust computational mechanism to revise an ir system s belief about the user changing information need in addition information flow is proposed a a text mining method to automatically acquire the initial ir context the advantage of a belief based irsystem is that it ir behaviour is more predictable and explanatory however computational efficiency is often a concern when the belief revision formalism are applied to large real life application this paper describes our belief based adaptive ir system which is underpinned by an efficient belief revision mechanism our initial experiment show that the belief based symbolic ir model is more effective than a classical quantitative ir model to our best knowledge this is the first successful empirical evaluation of a logic based ir model based on large ir benchmark collection 
negotiation event in industrial procurement involving multiple highly customisable good pose serious challenge to buying agent when trying to determine the best set of providing agent offer typically a buying agent s decision involves a large variety of constraint that may involve attribute of a very same item a well a attribute of multiple item in this paper we describe ibundler an agentaware negotiation service to solve the winner determination problem considering buyer and provider constraint and preference 
annotea is a web based shared annotation system based on a general purpose open resource description framework rdf infrastructure where annotation are modeled a a class of metadata annotation are viewed a statement made by an author about a web document annotation are external to the document and can be stored in one or more annotation server one of the goal of this project ha been to re use a much existing w c technology a possible we have reached it mostly by combining rdf with xpointer xlink and http we have also implemented an instance of our system using the amaya editor browser and a generic rdf database accessible through an apache http server in this implementation the merging of annotation with document take place within the client the paper present the overall design of annotea and describes some of the issue we have faced and how we have solved them 
we give an unified convergence analysis of ensemble learning method including e g adaboost logistic regression and the least squareboost algorithm for regression these method have in common that they iteratively call a base learning algorithm which return hypothesis that are then linearly combined we show that these method are related to the gauss southwell method known from numerical optimization and state non asymptotical convergence result for all these method our analysis includes norm regularized cost function leading to a clean and general way to regularize ensemble learning 
we propose a framework for classifier design based on discriminative density for representation of the difference of the class conditional distribution in a way that is optimal for classification the density are selected from a parametrized set by constrained maximization of some objective function which measure the average bounded difference i e the contrast between discriminative density we show that maximization of the contrast is equivalent to minimization of an approximation of the bayes risk therefore using suitable class of probability density function the resulting maximum contrast classifier mccs can approximate the bayes rule for the general multiclass case in particular for a certain parametrization of the density function we obtain mccs which have the same functional form a the well known support vector machine svms we show that mcc training in general requires some nonlinear optimization but under certain condition the problem is concave and can be tackled by a single linear program we indicate the close relation between svmand mcc training and in particular we show that linear programming machine can be viewed a an approximate realization of mccs in the experiment on benchmark data set the mcc show a competitive classification performance 
in order to understand adaboost s dynamic especially it ability to maximize margin we derive an associated simplified nonlinear iterated map and analyze it behavior in low dimensional case we find stable cycle for these case which can explicitly be used to solve for adaboost s output by considering adaboost a a dynamical system we are able to prove r atsch and warmuth s conjecture that adaboost may fail to converge to a maximal margin combined classifier when given a nonoptimal weak learning algorithm adaboost is known to be a coordinate descent method but other known algorithm that explicitly aim to maximize the margin such a adaboost and arc gv are not we consider a differentiable function for which coordinate ascent will yield a maximum margin solution we then make a simple approximation to derive a new boosting algorithm whose update are slightly more aggressive than those of arc gv 
while navigating in an environment a vision system ha to be ableto recognize where it is and what the main object in the sceneare in this paper we present a context based vision system forplace and object recognition the goal is to identify familiarlocations e g office conference room main street tocategorize new environment office corridor street and to usethat information to provide contextual prior for objectrecognition e g table are more likely in an office than astreet we present a low dimensional global image representationthat provides relevant information for place recognition andcategorization and show how such contextual information introducesstrong prior that simplify object recognition we have trained thesystem to recognize over location indoors and outdoors and tosuggest the presence and location of more than different objecttypes the algorithm ha been integrated into a mobile system thatprovides real time feedback to the user 
in this paper we present an evaluation of technique that are designed to encourage web searcher to interact more with the result of a web search two specific technique are examined the presentation of sentence that highly match the searcher s query and the use of implicit evidence implicit evidence is evidence captured from the searcher s interaction with the retrieval result and is used to automatically update the display our evaluation concentrate on the effectiveness and subject perception of these technique the result show with statistical significance that the technique are effective and efficient for information seeking 
we consider an online learning scenario in which the learner can make prediction on the basis of a fixed set of expert we derive up per and lower relative loss bound for a class of universal learning algorithm involving a switching dynamic over the choice of the expert on the basis of the performance bound we provide the optimal a priori discretization of the switching rate parameter that governs the switching dynamic we demonstrate the algorithm in the context of wireless network 
abstract an efficient algorithmic solution to the classical five point relative pose problem is presented the problem is to find the possible solution for relative camera motion between two calibrated view given five corresponding point the algorithm consists of computing the coefficient of a tenth degree polynomial and subsequently finding it root it is the first algorithm well suited for numerical implementation that also corresponds to the inherent complexity of the problem the algorithm is used in a robust hypothesise and test framework to estimate structure and motion in real time 
we present a novel representation of parse tree a list of path leaf projection path from leaf to the top level of the tree this representation allows u to achieve significantly higher accuracy in the task of hpsg parse selection than standard model and make the application of string kernel natural we define tree kernel via string kernel on projection path and explore their performance in the context of parse disambiguation we apply svm ranking model and achieve an exact sentence accuracy of on the redwood corpus 
word sense ambiguity is recognized a having a detrimental effect on the precision of information retrieval system in general and web search system in particular due to the sparse nature of the query involved despite continued research into the application of automated word sense disambiguation the question remains a to whether le than accurate automated word sense disambiguation can lead to improvement in retrieval effectiveness in this study we explore the development and subsequent evaluation of a statistical word sense disambiguation system which demonstrates increased precision from a sense based vector space retrieval model over traditional tf idf technique 
reiter s variant of the situation calculus is tightly related to relational database when complete information on the initial situation is available in particular the information on the initial situation can be seen a a relational database and action a specified by the precondition and successor state axiom can be seen a operation that change the state of the database in this paper we show how to exploit such a correspondence to build system for reasoning about action based on standard rdational database technology indeed by exploiting standlrd relational dbms service a system may be able to perform both projection exploiting dbms querying service and progression exploiting dbms update service in very large action theory a key result towards such a realization is that under very natural condition reiter s basic action theory turn out to be made of safe formula where basically negation is used a a form of difference between predicate only and that regression and progression preserve such a safeness this is a fundamental property to efficiently exploit relational database technology for reasoning we then show that even when action theory are not safe they can be made so while trying to retain efficiency a much a possible finally we briefly discus how such result can be extended to certain form of incomplete information 
abstract current computational approach to learning vi sual object category require thousand of training image are slow cannot learn in an incremental manner and cannot incorporate prior information into the learning process in addition no algorithm presented in the literature ha been tested on more than a handful of object category we present an method for learning object category from just a few training image it is quick and it us prior information in a principled way we test it on a dataset composed of image of object belonging to widely varied category our proposed method is based on making use of prior information assembled from unrelated object category which were previously learnt a generative probabilistic model is used which represents the shape and appearance of a constellation of feature belonging to the object the parameter of the model are learnt incrementally in a bayesian manner our incremental algorithm is compared experimentally to an earlier batch bayesian algorithm a well a to one based on maximum likelihood the incremental and batch version have comparable classification performance on small training set but incremental learning is significantly faster making real time learning feasible both bayesian method outperform maximum likelihood on small training set 
in this paper we present a probabilistic tracking framework that combine sound and vision to achieve more robust and accurate tracking of multiple object in a cluttered or noisy scene our measurement have a non gaussian multimodal distribution we apply a particle filter to track multiple people using combined audio and video observation we have applied our algorithm to the domain of tracking people with a stereo based visual foreground detection algorithm and audio localization using a beamforming technique our model also accurately reflects the number of people present we test the efficacy of our system on a sequence of multiple people moving and speaking in an indoor environment 
the world wide web wa originally developed a a shared writable hypertext medium a facility that is still widely needed we have recently developed a web based management reporting system for a legal firm in an attempt to improve the efficiency and management of their overall business process this paper share our experience in relating the firm s specific writing and issue tracking task to existing web open hypermedia and semantic web research and describes why we chose to develop a new solution a set of open hypermedia component collectively called the management reporting system rather than employ an existing system 
in low level vision the representation of scene property such a shape albedo etc are very high dimensional a they haveto describe complicated structure the approach proposed here is to let the image itself bear a much of the representationalburden a possible in many situation scene and image are closely related and it is possible to find a functional relationshipbetween them the scene information can be represented in reference to the image where the functional specifies how 
we describe a formal framework for diagnosis and repair problem that share element of the well known partially observable mop and cost sensitive classification model our cost sensitive fault remediation model is amenable to implementation a a reinforcement learning system and we describe an instance based state representation that is compatible with learning and planning in this framework we demonstrate a system that us these idea to learn to efficiently restore network connectivity after a failure 
synchronous reinforcement learning rl algorithm with linear function approximation are representable a inhomogeneous matrix iteration of a special form schoknecht merke in this paper we state condition of convergence for general inhomogeneous matrix iteration and prove that they are both necessary and sufficient this result extends the work presented in schoknecht merke where only a sufficient condition of convergence wa proved a the condition of convergence is necessary and sufficient the new result is suitable to prove convergence and divergence of rl algorithm with function approximation we use the theorem to deduce a new concise proof of convergence for the synchronous residual gradient algorithm baird moreover we derive a counterexample for which the uniform rl algorithm merke schoknecht diverges this yield a negative answer to the open question if the uniform rl algorithm converges for arbitrary multiple transition 
mining microarray gene expression data is an important research topic in bioinformatics with broad application while most of the previous study focus on clustering either gene or sample it is interesting to ask whether we can partition the complete set of sample into exclusive group called phenotype and find a set of informative gene that can manifest the phenotype structure in this paper we propose a new problem of simultaneously mining phenotype and informative gene from gene expression data some statistic based metric are proposed to measure the quality of the mining result two interesting algorithm are developed the heuristic search and the mutual reinforcing adjustment method we present an extensive performance study on both real world data set and synthetic data set the mining result from the two proposed method are clearly better than those from the previous method they are ready for the real world application between the two method the mutual reinforcing adjustment method is in general more scalable more effective and with better quality of the mining result 
the optimal distance measure for a given discrimination task underthe nearest neighbor framework ha been shown to be the likelihoodthat a pair of measurement have different class label forimplementation and efficiency consideration the optimal distancemeasure wa approximated by combining more elementary distancemeasures defined on simple feature space in this paper weaddress two important issue that arise in practice for such anapproach a what form should the elementary distance measure ineach feature space take we motivate the need to use the optimaldistance measure in simple feature space a the elementarydistance measure such distance measure have the desirableproperty that they are invariant to distance respectingtransformations b how do we combine the elementary distancemeasures we present the precise statistical assumption underwhich a linear logistic model hold exactly we benchmark our modelwith three other method on a challenging face discrimination taskand show that our approach is competitive with the state of theart 
the response of cortical sensory neuron are notoriously variable with the number of spike evoked by identical stimulus varying significantly from trial to trial this variability is most often interpreted a noise purely detrimental to the sensory system in this paper we propose an alternative view in which the variability is related to the uncertainty about world parameter which is inherent in the sensory stimulus specifically the response of a population of neuron are interpreted a stochastic sample from the posterior distribution in a latent variable model in addition to giving theoretical argument supporting such a representational scheme we provide simulation suggesting how some aspect of response variability might be understood in this framework 
recent theoretical result have shown that improved bound on generalization error of classiers can be obtained by explicitly taking the observed margin distribution of the training data into account currently algorithm used in practice do not make use of the margin distribution and are driven by optimization with respect to the point that are closest to the hyperplane this paper enhances earlier theoretical result and derives a practical data dependent complexity measure for learning the new complexity measure is a function of the observed margin distribution of the data and can be used a we show a a model selection criterion we then present the margin distribution optimization mdo learning algorithm that directly optimizes this complexity measure empirical evaluation of mdo demonstrates that it consistently outperforms svm 
this paper summarizes a probabilistic approach for localizing people through the signal strength of a wireless ieee lb network our approach us data labeled by ground truth position to learn a probabilistic mapping from location to wireless signal represented by piecewise linear gaussians it then us sequence of wireless signal data without position label to acquire motion model of individual people which further improves the localization accuracy the approach ha been implemented and evaluated in an office environment 
a method is proposed to track the full hand motion from d point reconstructed using a stereoscopic set of camera this approach combine the advantage of method that use d motion e g optical flow and those that use a d reconstruction at each time frame to capture the hand motion matching either contour or a d reconstruction against a d hand model is usually very difficult due to self occlusion and the locally cylindrical structure of each phalanx in the model but our use of d point trajectory constrains the motion and overcomes these problem our tracking procedure us both the d point match between two time frame and a smooth surface model of the hand build with implicit surface we used animation technique to represent faithfully the skin motion especially near joint robustness is obtained by using an em version of the icp algorithm for matching point between consecutive frame and the tracked point are then registered to the surface of the hand model result are presented on a stereoscopic sequence of a moving hand and are evaluated using a side view of the sequence 
mutual boosting is a method aimed at incorporating contextual information to augment object detection when multiple detector of object and part are trained in parallel using adaboost object detector might use the remaining intermediate detector to enrich the weak learner set this method generalizes the efficient feature suggested by viola and jones thus enabling information inference between part and object in a compositional hierarchy in our experiment eye nose mouthand face detector are trained using the mutual boosting framework result show that the method outperforms application overlooking contextual information we suggest that achieving contextual integration is a step toward human like detection capability 
recent proposal to apply data mining system to problem in law enforcement national security and fraud detection have attracted both medium attention and technical critique of their expected accuracy and impact on privacy unfortunately the majority of technical critique have been based on simplistic assumption about data classifier inference procedure and the overall architecture of such system we consider these critique in detail and we construct a simulation model that more closely match realistic system we show how both the accuracy and privacy impact of a hypothetical system could be substantially improved and we discus the necessary and sufficient condition for this improvement to be achieved this analysis is neither a defense nor a critique of any particular system concept rather our model suggests alternative technical design that could mitigate some concern but also raise more specific condition that must be met for such system to be both accurate and socially desirable 
many classification algorithm including the support vector machine boosting and logistic regression can be viewed a minimum contrast method that minimize a convex surrogate of the loss function we characterize the statistical consequence of using such a surrogate by providing a general quantitative relationship between the risk a assessed using the loss and the risk a assessed using any nonnegative surrogate loss function we show that this relationship give nontrivial bound under the weakest possible condition on the loss function that it satisfy a pointwise form of fisher consistency for classification the relationship is based on a variational transformation of the loss function that is easy to compute in many application we also present a refined version of this result in the case of low noise finally we present application of our result to the estimation of convergence rate in the general setting of function class that are scaled hull of a finite dimensional base class 
we are designing and developing a mobile clinical decision support system known a met mobile emergency triage for supporting emergency triage of different type of acute pain presentation met need to interact with an existing hospital information system run on handheld computing device and be suitable for operation in weak connectivity condition with unstable connection between mobile client and a server the met system capture necessary hospital data allows for patient data entry and provides triage support by operating on handheld computer it fit to the regular clinical workflow without introducing any hindrance and disruption it support triage anytime and anywhere directly at the point of care and can be used a an electronic patient chart that facilitates structured data collection 
multi document person name resolution focus on the problem of determining if two instance with the same name and from different document refer to the same individual we present a two step approach in which a maximum entropy model is trained to give the probability that two name refer to the same individual we then apply a modified agglomerative clustering technique to partition the instance according to their referent 
many difficult visual perception problem like d human motion estimation can be formulated in term of inference using complex generative model defined over high dimensional state space despite progress optimizing such model is difficult because prior knowledge cannot be flexibly integrated in order to reshape an initially designed representation space nonlinearities inherent sparsity of high dimensional training set and lack of global continuity make dimensionality reduction challenging and low dimensional search inefficient to address these problem we present a learning and inference algorithm that restricts visual tracking to automatically extracted non linearly embedded low dimensional space this formulation produce a layered generative model with reduced state representation that can be estimated using efficient continuous optimization method our prior flattening method allows a simple analytic treatment of low dimensional intrinsic curvature constraint and allows consistent interpolation operation we analyze reduced manifold for human interaction activity and demonstrate that the algorithm learns continuous generative model that are useful for tracking and for the reconstruction of d human motion in monocular video 
we will demonstrate the secure wireless agent testbed swat a unique facility developed at drexel university to study integration networking and information assurance for next generation wireless mobile agent system swat is an implemented system that fully integrates mobile agent wireless ad hoc multi hop network and security the demonstration will show the functionality of a number of decentralized agent based application including application for authentication collaboration messaging and remote sensor monitoring the demonstration will take place on a live mobile ad hoc network consisting of approximately a dozen node pda tablet pc and laptop and hundred of mobile software agent 
traditional single agent search algorithm usually make simplifying assumption single search agent stationary target complete knowledge of the state and sufficient time there are algorithm for relaxing one or two of these constraint in this paper we want to relax all four the application domain is to have multiple search agent cooperate to pursue and capture a moving target agent are allowed to communicate with each other for solving multiple agent moving target mamt application we present a framework for specifying a family of suitable search algorithm this paper investigates several effective approach for solving problem instance in this domain 
merchant selling product on the web often ask their customer to review the product that they have purchased and the associated service a e commerce is becoming more and more popular the number of customer review that a product receives grows rapidly for a popular product the number of review can be in hundred or even thousand this make it difficult for a potential customer to read them to make an informed decision on whether to purchase the product it also make it difficult for the manufacturer of the product to keep track and to manage customer opinion for the manufacturer there are additional difficulty because many merchant site may sell the same product and the manufacturer normally produce many kind of product in this research we aim to mine and to summarize all the customer review of a product this summarization task is different from traditional text summarization because we only mine the feature of the product on which the customer have expressed their opinion and whether the opinion are positive or negative we do not summarize the review by selecting a subset or rewrite some of the original sentence from the review to capture the main point a in the classic text summarization our task is performed in three step mining product feature that have been commented on by customer identifying opinion sentence in each review and deciding whether each opinion sentence is positive or negative summarizing the result this paper proposes several novel technique to perform these task our experimental result using review of a number of product sold online demonstrate the effectiveness of the technique 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
there are two kind of omnidirectional camera often used in computer vision central catadioptric camera and fisheye camera previous literature use different imaging model to describe them separately a unified imaging model is however presented in this paper the unified model in this paper can be considered a an extension of the unified imaging model for central catadioptric camera proposed by geyer and daniilidis we show that our unified model can cover some existing model for fisheye camera and fit well for many actual fisheye camera used in previous literature under our unified model central catadioptric camera and fisheye camera can be classified by the model s characteristic parameter and a fisheye image can be transformed into a central catadioptric one vice versa an important merit of our new unified model is that existing calibration method for central catadioptric camera can be directly applied to fisheye camera furthermore the metric calibration from single fisheye image only using projection of line becomes possible via our unified model but the existing method for fisheye camera in the literature till now are all non metric under the same condition experimental result of calibration from some central catadioptric and fisheye image confirm the validity and usefulness of our new unified model 
this article describes preliminary work on a research environment called virtual synergy to represent a shared virtual map of an area for multiple autonomous robot by modifying the gamebots d multi agent system the use of gamebots will allow multiple user to interact with robot and agent at different level of adjustable or dynamic autonomy by interacting with the robot a another team member the user take on different role to suit the situation supervisor peer mechanic teleoperator and spectator 
we consider multi agent system whose agent compete for resource by striving to be in the minority group the agent adapt to the environment by reinforcement learning of the preference of the policy they hold diversity of preference of policy is introduced by adding random bias to the initial cumulative payoff of their policy we explain and provide evidence that agent cooperation becomes increasingly important when diversity increase analysis of these mechanism yield excellent agreement with simulation over nine decade of data 
the problem of finding a specified pattern in a time series database i e query by content ha received much attention and is now a relatively mature field in contrast the important problem of enumerating all surprising or interesting pattern ha received far le attention this problem requires a meaningful definition of surprise and an efficient search technique all previous attempt at finding surprising pattern in time series use a very limited notion of surprise and or do not scale to massive datasets to overcome these limitation we introduce a novel technique that defines a pattern surprising if the frequency of it occurrence differs substantially from that expected by chance given some previously seen data 
given a set of number and a set of bin of fixed capacity the np complete problem of bin packing is to find the minimum number of bin needed to contain the number such that the sum of the number assigned to each bin doe not exceed the bin capacity we present two improvement to our previous bin completion algorithm the first speed up the constant factor per node generation and the second prune redundant part of the search tree the resulting algorithm appears to be asymptotically faster than our original algorithm on problem with element it run over time faster furthermore the ratio of node generation and running time both increase with increasing problem size 
we argue that k mean and deterministic annealing algorithm for geometric clustering can be derived from the more general information bottleneck approach if we cluster the identity of data point to preserve information about their location the set of optimal solution is massively degenerate but if we treat the equation that define the optimal solution a an iterative algorithm then a set of smooth initial condition selects solution with the desired geometrical property in addition to conceptual unification we argue that this approach can be more efficient and robust than classic algorithm 
most learning method assume that the training set is drawn randomly from the population to which the learned model is to be applied however in many application this assumption is invalid for example lending institution create model of who is likely to repay a loan from training set consisting of people in their record to whom loan were given in the past however the institution approved loan application previously based on who wa thought unlikely to default learning from only approved loan yield an incorrect model because the training set is a biased sample of the general population of applicant the issue of including rejected sample in the learning process or alternatively using rejected sample to adjust a model learned from accepted sample only is called reject inference the main contribution of this paper is a systematic analysis of different case that arise in reject inference with explanation of which case arise in various real world situation we use bayesian network to formalize each case a a set of conditional independence relationship and identify eight case including the familiar missing completely at random mcar missing at random mar and missing not at random mnar case for each case we present an overview of available learning algorithm these algorithm have been published in separate field of research including epidemiology econometrics clinical trial evaluation sociology and credit scoring our second major contribution is to describe these algorithm in a common framework 
this article address the issue of colour classification and collision detection a they occur in the legged league robot soccer environment of robocup we show how the method of one class classification with support vector machine svms can be applied to solve these task satisfactorily using the limited hardware capacity of the prescribed sony aibo quadruped robot the experimental evaluation show an improvement over our previous method of ellipse fitting for colour classification and the statistical approach used for collision detection 
tabu search algorithm are amongst the most successful local search based method for the maximum satisfiability problem the practical superiority of tabu search over the local search alone lias been already shown experimentally several time a natural question addressed here is to understand if this superiority hold also from the worst case point of view moreover it is well known that the main critical parameter of tabu technique is the tabu list length focussing on max sat problem the main contribution of this paper is a worst case analysis of tabu search a a function of the tabu list length we give a first theoretical evidence of the advantage of a tabu search strategy over the basic local search alone that critically depends on the tabu list length 
we present a new web middleware architecture that allows user to customize their view of the web for optimal interaction and system operation when using non traditional resource limited client machine such a wireless pda personal digital assistant web stream customizers wsc are dynamically deployable software module and can be strategically located between client and server to achieve improvement in performance reliability or security an important design feature is that customizers provide two point of control in the communication path between client and server supporting adaptive system based and content based customization our architecture exploit http s proxy capability allowing customizers to be seamlessly integrated with the basic web transaction model we describe the wsc architecture and implementation and illustrate it use with three non trivial adaptive customizer application that we have built we show that the overhead in our implementation is small and tolerable and is outweighed by the benefit that customizers provide 
we have recently proposed augmenting clause in a boolean database with group of permutation the augmented clause then standing for the set of all clause constructed by acting on the original clause with a permutation in the group this approach ha many attractive theoretical property including representational generality and reduction from exponential to polynomial proof length in a variety of setting in this paper we discus the issue that arise in implementing a group based generalization of resolution and give preliminary result describing this procedure s effectiveness 
in this paper we present a graphical model for protein secondary structure prediction this model extends segmental semi markov model ssmm to exploit multiple sequence alignment profile which contain information from evolutionarily related sequence a novel parameterized model is proposed a the likelihood function for the ssmm to capture the segmental conformation by incorporating the information from long range interaction in sheet this model is capable of carrying out inference on contact map the numerical result on benchmark data set show that incorporating the profile result in substantial improvement and the generalization performance is promising 
we study the recognized open problem of designing revenue maximizing combinatorial auction it is unsolved even for two bidder and two item for sale rather than pursuing the pure economic approach of attempting to characterize the optimal auction we explore technique for automatically modifying existing mechanism in a way that increase expected revenue we introduce a general family of auction based on bidder weighting and allocation boosting which we call virtual valuation combinatorial auction vvca all auction in the family are based on the vickrey clarke grove vcg mechanism executed on virtual valuation that are linear transformation of the bidder real valuation the restriction to linear transformation is motivated by incentive compatibility the auction family is parameterized by the coefficient in the linear transformation the problem of designing a high revenue mechanism is therefore reduced to search in the parameter space of vvca we analyze the complexity of the search for the optimal such mechanism and conclude that the search problem is computationally hard despite that optimal parameter for vvca can be found at least in setting with few item and bidder the experiment show that vvca yield a substantial increase in revenue over the traditionally used vcg in larger auction locally optimal parameter which still yield an improvement over vcg can be found 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we consider the problem of reconstructing pattern from a feature map learning algorithm using kernel to operate in a reproducing kernel hilbert space rkhs express their solution in term of input point mapped into the rkhs we introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding pattern in the input space aka pre image and review it performance in several application requiring the construction of pre im age the introduced technique avoids difficult and or unstable numerical optimization is easy to implement and unlike previous method permit the computation of pre image in discrete input space 
we investigate a general semi markov decision process smdp framework for modeling concurrent decision making where agent learn optimal plan over concurrent temporally extended action we introduce three type of parallel termination scheme all any and continue and theoretically and experimentally compare them 
this paper describes a multi site project to annotate six sizable bilingual parallel corpus for interlingual content after presenting the background and objective of the effort we will go on to describe the data set that is being annotated the interlingua representation language used an interface environment that support the annotation task and the annotation process itself we will then present a preliminary version of our evaluation methodology and conclude with a summary of the current status of the project along with a number of issue which have arisen 
this paper address the problem of computing the trajectoryof a camera from sparse positional measurementsthat have been obtained from visual localisation and densedifferential measurement from odometry or inertial sensor a fast method is presented for fusing these two sourcesof information to obtain the maximum a posteriori estimateof the trajectory a formalism is introduced for representingprobability density function over euclidean transformation and it is shown how these density function can bepropagated along the data sequence and how multiple estimatesof a transformation can be combined a three passalgorithm is described which make use of these result toyield the trajectory of the camera simulation result are presented which are validatedagainst a physical analogue of the vision problem and resultsare then shown from sequence of approximately frame captured from a video camera mounted on a go kart several of these frame are processed using computer visionto obtain estimate of the position of the go kart the algorithmfuses these estimate with odometry from the entiresequence in m to obtain the trajectory of the kart 
we use machine learning technique to find the best combination of local focus and lexical distance feature for identifying the anchor of mereological bridging reference we find that using first mention utterance distance and lexical distance computed using either google or wordnet result in an accuracy significantly higher than obtained in previous experiment 
boosting is a popular approach for building accurate classifier despite the initial popular belief boosting algorithm do exhibit overfitting and are sensitive to label noise part of the sensitivity of boosting algorithm to outlier and noise can be attributed to the unboundedness of the margin based loss function that they employ in this paper we describe two leveraging algorithm that build on boosting technique and employ a bounded loss function of the margin the first algorithm interleaf the expectation maximization em algorithm with boosting step the second algorithm decomposes a non convex loss into a difference of two convex loss we prove that both algorithm converge to a stationary point we also analyze the generalization property of the algorithm using the rademacher complexity we describe experiment with both synthetic data and natural data ocr and text that demonstrate the merit of our framework in particular robustness to outlier 
in this paper we show that many kernel method can be adapted to deal with indefinite kernel that is kernel which are not positive semidefinite they do not satisfy mercer s condition and they induce associated functional space called reproducing kernel kre icaron n space rkks a generalization of reproducing kernel hilbert space rkhs machine learning in rkks share many nice property of learning in rkhs such a orthogonality and projection however since the kernel are indefinite we can no longer minimize the loss instead we stabilize it we show a general representer theorem for constrained stabilization and prove generalization bound by computing the rademacher average of the kernel class we list several example of indefinite kernel and investigate regularization method to solve spline interpolation some preliminary experiment with indefinite kernel for spline smoothing are reported for truncated spectral factorization landweber fridman iteration and mr ii 
we develop a system theoretical treatment of a behavioural system that interacts with it environment in a closed loop situation such that it motor action influence it sensor input the simplest form of a feedback is a reflex reflex occur always too late i e only after a unpleasant painful dangerous reflex eliciting sensor event ha occurred this defines an objective problem which can be solved if another sensor input exists which can predict the primary reflex and can generate an earlier reaction in contrast to previous approach our linear learning algorithm allows for an analytical proof that this system learns to apply feedforward control with the result that slow feedback loop are replaced by their equivalent feed forward controller creating a forward model in other word learning turn the reactive system into a pro active system by mean of a robot implementation we demonstrate the applicability of the theoretical result which can be used in a variety of different area in physic and engineering 
many real world domain are relational in nature consisting of a set of object related to each other in complex way this paper focus on predicting the existence and the type of link between entity in such domain we apply the relational markov networkframework of taskar et al to define a joint probabilistic model over the entire link graph entity attribute and link the application of the rmn algorithm to this task requires the definition of pr obabilistic pattern over subgraph structure we apply this method to two new relational datasets one involving university webpage and the other a social network we show that the collective classification approach of rmns and the intr oduction of subgraph pattern over link label provide significant improvement s in accuracy over flat classification which attempt to predict each link in isola tion 
many interesting multiclass problem can be cast in the general framework of label ranking defined on a given set of class the eva luation for such a ranking is generally given in term of the number of violated order constraint between class in this paper we propose the preference learning modelas a unifying framework to model and solve a large class of multiclass problem in a large margin perspective in addition an original kernel based method is proposed and evaluated on a ranking dataset with state of the art result 
belief propagation on cyclic graph is an efficient algorithm for computing approximate marginal probability distribution over single node and neighboring node in the graph in this paper we propose two new algorithm for approximating joint probability of arbitrary pair of node and prove a number of desirable property that these estimate fulfill the first algorithm is a propagation algorithm which is shown to converge if belief propagation converges to a stable fixed point the second algorithm is based on matrix inversion experiment compare a number of competing method 
this paper present a new bootstrapping approach to named entity ne classification this approach only requires a few common noun pronoun seed that correspond to the concept for the target ne type e g he she man woman for person ne the entire bootstrapping procedure is implemented a training two successive learner i a decision list is used to learn the parsing based high precision ne rule ii a hidden markov model is then trained to learn string sequence based ne pattern the second learner us the training corpus automatically tagged by the first learner the resulting ne system approach supervised ne performance for some ne type the system also demonstrates intuitive support for tagging user defined ne type the difference of this approach from the co training based ne bootstrapping are also discussed 
the issue of cross channel integration and customer life time value modeling are two of the most important topic surrounding customer relationship management crm today in the present paper we describe and evaluate a novel solution that treat these two important issue in a unified framework of markov decision process mdp in particular we report on the result of a joint project between ibm research and saks fifth avenue to investigate the applicability of this technology to real world problem the business problem we use a a testbed for our evaluation is that of optimizing direct mail campaign mailing for maximization of profit in the store channel we identify a problem common to cross channel crm which we call the cross channel challenge due to the lack of explicit linking between the marketing action taken in one channel and the customer response obtained in another we provide a solution for this problem based on old and new technique in reinforcement learning our in laboratory experimental evaluation using actual customer interaction data show that a much a to per cent increase in the store profit can be expected by employing a mailing policy automatically generated by our methodology these result confirm that our approach is valid in dealing with the cross channel crm scenario in the real world 
steganography involves hiding message in innocuous medium such a image while steganalysis is the field of detecting these secret message the ultimate goal of steganalysis is two fold making a binary classification of a file a stego bearing or innocent and secondly locating the hidden message with an aim to extracting sterilizing or manipulating it almost all steganalysis approach known a attack focus on the first of these two issue in this paper we explore the difficult related problem given that we know an image file contains steganography locate which pixel contain the message we treat the hidden message location problem a outlier detection using probability energy measure of image motivated by the image restoration community pixel contributing the most to the energy calculation of an image are deemed outlier typically of the top third of one percent of most energized pixel outlier we find that are stego bearing in color image and in grayscale image in all image type only of all pixel are stego bearing indicating our technique provides a substantial lift over random guessing 
we describe a method for learning sparse multiscale image representation using a sparse prior distribution over the basis function coecien t the prior consists of a mixture of a gaussian and a dirac delta function and thus encourages coecien t to have exact zero value coecien t for an image are computed by sampling from the resulting posterior distribution with a gibbs sampler the learned basis is similar to the steerable pyramid basis and yield slightly higher snr for the same number of active coecien t denoising using the learned image model is demonstrated for some standard test image with result that compare favorably with other denoising method 
this paper introduces the point based value iteration pbvi algorithm for pomdp planning pbvi approximates an exact value iteration solution by selecting a small set of representative belief point and then tracking the value and it derivative for those point only by using stochastic trajectory to choose belief point and by maintaining only one value hyper plane per point pbvi successfully solves large problem we present result on a robotic laser tag problem a well a three test domain from the literature 
this paper present a machine learning approach to modeling human behavior in one shot game it provides a framework for representing and reasoning about the social factor that affect people s play the model predicts how a human player is likely to react to different action of another player and these prediction are used to determine the best possible strategy for that player data collection and evaluation of the model were performed on a negotiation game in which human played against each other and against computer model playing various strategy a computer player trained on human data outplayed nash equilibrium and nash bargaining computer player a well a human it also generalized to play people and game situation it had not seen before 
thesaurus ha been widely used in many application including information retrieval natural language processing and question answering in this paper we propose a novel approach to automatically constructing a domain specific thesaurus from the web using link structure information the proposed approach is able to identify new term and reflect the latest relationship between term a the web evolves first a set of high quality and representative website of a specific domain is selected after filtering out navigational link link analysis is applied to each website to obtain it content structure finally the thesaurus is constructed by merging the content structure of the selected website the experimental result on automatic query expansion based on our constructed thesaurus show improvement in search precision compared to the baseline 
this paper analyzes generalization of the classic rescorla wagner rw learning algorithm and study their relationship to maximum likelihood estimation of causal parameter we prove that the parameter of two popular causal model p and pc can be learnt by the same generalized linear rescorla wagner glrw algorithm provided genericity condition apply we characterize the fixed point of these glrw algorithm and calculate the fluctuation about them assuming that the input is a set of i i d sample from a fixed unknown distribution we describe how to determine convergence condition and calculate convergence rate for the glrw algorithm under these condition 
this paper present a neuromorphic model of two olfactory signalprocessing primitive chemotopic convergence of olfactory receptor neuron and center on off surround lateral inhibition in the olfactory bulb a self organizing model of receptor convergence onto glomerulus is used to generate a spatially organized map an olfactory image this map serf a input to a lattice of spiking neuron with lateral connection the dynamic of this recurrent network transforms the initial olfactory image into a spatio temporal pattern that evolves and stabilizes into odorand intensity coding attractor the model is validated using experimental data from an array of temperature modulated gas sensor our result are consistent with recent neurobiological finding on the antennal lobe of the honeybee and the locust 
predictive state representation psrs have recently been proposed a an alternative to partially observable markov decision process pomdps for representing the state of a dynamical system littman et al we present a learning algorithm that learns a psr from observational data our algorithm produce a variant of psrs called transformed predictive state representation tpsrs we provide an efficient principal component based algorithm for learning a tpsr and show that tpsrs can perform well in comparison to hidden markov model learned with baum welch in a real world robot tracking task for low dimensional representation and long prediction horizon 
minesweeper is a one person game which look deceptively easy to play but where average human performance is far from optimal playing the game requires logical arithmetic and probabilistic reasoning based on spatial relationship on the board simply checking a board state for consistency is an np complete problem given the difficulty of hand crafting strategy to play this and other game ai researcher have always been interested in automatically learning such strategy from experience in this paper we show that when integrating certain technique into a general purpose learning system mio the resulting system is capable of inducing a minesweeper playing strategy that beat the winning rate of average human player in addition we discus the necessary background knowledge present experimental result demonstrating the gain obtained with our technique and show the strategy learned for the game 
xml retrieval is a departure from standard document retrieval in which each individual xml element ranging from italicized word or phrase to full blown article is a potentially retrievable unit the distribution of xml element length is unlike what we usually observe in standard document collection prompting u to revisit the issue of document length normalization we perform a comparative analysis of arbitrary element versus relevant element and show the importance of length a a parameter for xml retrieval within the language modeling framework we investigate a range of technique that deal with length either directly or indirectly we observe a length bias introduced by the amount of smoothing and show the importance of extreme length prior for xml retrieval we also show that simply removing shorter element from the index by introducing a cut off value doe not create an appropriate document length normalization even after increasing the minimal size of xml element occurring in the index the importance of an extreme length bias remains 
in response to a query a search engine return a ranked list of document if the query is about a popular topic i e it match many document then the returned list is usually too long to view fully study show that user usually look at only the top to result however we can exploit the fact that the best target for popular topic are usually linked to by enthusiast in the same domain in this paper we propose a novel ranking scheme for popular topic that place the most authoritative page on the query topic at the top of the ranking our algorithm operates on a special index of expert document these are a subset of the page on the www identified a directory of link to non affiliated source on specific topic result are ranked based on the match between the query and relevant descriptive text for hyperlink on expert page pointing to a given result page we present a prototype search engine that implement our ranking scheme and discus it performance with a relatively small million page expert index our algorithm wa able to perform comparably on popular query with the best of the mainstream search engine 
we derive a probabilistic multi scale model for contour completion based on image statistic the boundary of human segmented image are used a ground truth a probabilistic formulation of contour demand a prior model and a measurement model from the image statistic of boundary contour we derive both the prior model of contour shape and the local likelihood model of image measurement we observe multi scale phenomenon in the data and accordingly propose a higher order markov model over scale for the contour continuity prior various image cue derived from orientation energy are evaluated and incorporated into the measurement model based on these model we have designed a multi scale algorithm for contour completion which exploit both contour continuity and texture experimental result are shown on a wide range of image 
the paper describes a particular approach to multiengine machine translation memt where we make use of voted language model to selectively combine translation output from multiple off the shelf mt system experiment are done using large corpus from three distinct domain the study found that the use of voted language model lead to an improved performance of memt system 
we present a web search interface designed to encourage user to interact more fully with the result of a web search wrapping around a major commercial search engine the system combine three main feature real time query biased web document summarisation the presentation of sentence highly relevant to the searcher s query and evidence captured from searcher interaction with the retrieval result 
the problem of learning a sparse conic combination of kernel function or kernel matrix for classification or regression can be ac hieved via the regularization by a block norm in this paper we present an algorithm that computes the entire regularization path for th ese problem the path is obtained by using numerical continuation technique and involves a running time complexity that is a constant time the complexity of solving the problem for one value of the regularization parameter working in the setting of kernel linear regression and kernel logistic regression we show empirically that the effect of the block norm regularization differs notably from the non block norm regularization commonly used for variable selection and that the regularization path is of particular value in the block case 
we consider a statistical framework for learning in a class of network of spiking neuron our aim is to show how optimal local learningrulescanbereadilyderivedoncetheneuraldynamicsand desired functionality of the neural assembly have been specifled in contrast to other model which assume sub optimal learning rule withinthisframeworkwederivelocalrulesforlearningtemporalsequencesinamodelofspikingneuronsanddemonstrateits superior performance to correlation hebbian based approach we further show how to include mechanism such a synaptic depression and outline how the framework is readily extensible to learninginnetworksofhighlycomplexspikingneurons astochasticquantalvesiclereleasemechanismisconsideredandimplications on the complexity of learning discussed 
a the complexity of distributed computing system increase system management task require significantly higher level of automation example include diagnosis and prediction based on real time stream of computer event setting alarm and performing continuous monitoring the core of autonomic computing a recently proposed initiative towards next generation it system capable of self healing is the ability to analyze data in real time and to predict potential problem the goal is to avoid catastrophic failure through prompt execution of remedial action this paper describes an attempt to build a proactive prediction and control system for large cluster we collected event log containing various system reliability availability and serviceability ra event and system activity report sars from a node cluster system for a period of one year the raw system health measurement contain a great deal of redundant event data which is either repetitive in nature or misaligned with respect to time we applied a filtering technique and modeled the data into a set of primary and derived variable these variable used probabilistic network for establishing event correlation through prediction algorithm we also evaluated the role of time series method rule based classification algorithm and bayesian network model in event prediction based on historical data our result suggest that it is feasible to predict system performance parameter sars with a high degree of accuracy using time series model rule based classification technique can be used to extract machine event signature to predict critical event with up to accuracy 
reagent are remotely executing agent that customize web browsing for non standard client a reagent is essentially a one shot mobile agent that act a an extension of a client dynamically launched by the client to run on it behalf at a remote more advantageous location reagent simplify the use of mobile agent technology by transparently handling data migration and run time network communication and provide a general interface for programmer to more easily implement their application specific customizing logic this is made possible by the identification of useful remote behavior i e common pattern of action that exploit the ability to process and communicate remotely example of such behavior are transformer monitor cachers and collators in this paper we identify a set ofuseful reagent behavior for interacting with web service via astandard browser describe how to program and use reagent and show that the overhead of using reagent is low and outweighed by it benefit 
we propose two new tool to address the evolution of hyperlinked corpus first we define time graph to extend the traditional notion of an evolving directed graph capturing link creation a a point phenomenon in time second we develop definition and algorithm for time dense community tracking to crystallize the notion of community evolution we develop these tool in the context of blogspace the space of weblogs or blog our study involves approximately k link among k blog we create a time graph on these blog by an automatic analysis of their internal time stamp we then study the evolution of connected component structure and microscopic community structure in this time graph we show that blogspace underwent a transition behavior around the end of and ha been rapidly expanding over the past year not just in metric of scale but also in metric of community structure and connectedness this expansion show no sign of abating although measure of connectedness must plateau within two year by randomizing link destination in blogspace but retaining source and timestamps we introduce a concept of randomized blogspace herein we observe similar evolution of a giant component but no corresponding increase in community structure having demonstrated the formation of micro community over time we then turn to the ongoing activity within active community we extend recent work of kleinberg to discover dense period of bursty intra community link creation 
there exists a huge demand for multimedia good and service in the internet currently available bandwidth speed can support sale of downloadable content like cd e book etc a well a service like video on demand in the future such service will be prevalent in the internet since cost are typically fixed maximizing revenue can maximize profit a primary determinant of revenue in such e content market is how much value the customer associate with the content though marketing survey are useful they cannot adapt to the dynamic nature of the internet market in this work we examine how to learn customer valuation in close to real time our contribution in this paper are threefold we develop a probabilistic model to describe customer behavior we develop a framework for pricing e content based on basic economic principle and we propose a price discovering algorithm that learns customer behavior parameter and suggests price to an e content provider we validate our algorithm using simulation our simulation indicate that our algorithm generates revenue close to the maximum expectation further they also indicate that the algorithm is robust to transient customer behavior 
abstract we describe a method that computes provably exact map estimate for a subclass of problem on graph with cycle the basic idea is to represent the original problem on the graph with cycle a a convex combination of tree structured problem a convexity argument the n guarantee that the optimal value of the original problem i e the log probability of the map assignment is upper bounded by the combined optimal value of the tree problem we prove that this upper bound is met with equality if and only if the tree problem share an optimal con figuration in common an important implication is that any such shared configuration must also be the map configuration for the original pr oblem next we develop a tree reweighted max product algorithm for attempting to find convex combination of tree structured problem that share a common optimum we prove that these message passing update always have a fixed point and give necessary and sufficient cond itions for the fixed point to yield the exact map estimate an attractive feature of our analysis is that it generalizes naturally to convex co mbinations of hypertree structured distribution 
cyclic definition in description logic have until now been investigated only for description logic allowing for value restriction even for the most basic language fl which allows for conjunction and value restriction only deciding subsumption in the presence of terminological cycle is a pspace complete problem this paper investigates subsumption in the presence of terminological cycle for the language el which allows for conjunction existential restriction and the topconcept in contrast to the result for fl subsumption in el remains polynomial independent of whether we use least fixpoint semantics greatest fixpoint semantics or descriptive semantics 
defining outlier by their distance to neighboring example is a popular approach to finding unusual example in a data set recently much work ha been conducted with the goal of finding fast algorithm for this task we show that a simple nested loop algorithm that in the worst case is quadratic can give near linear time performance when the data is in random order and a simple pruning rule is used we test our algorithm on real high dimensional data set with million of example and show that the near linear scaling hold over several order of magnitude our average case analysis suggests that much of the efficiency is because the time to process non outlier which are the majority of example doe not depend on the size of the data set 
we present a framework for sparse gaussian process gp method which us forward selection with criterion based on informationtheoretic principle previously suggested for active learning our goal is not only to learn d sparse predictor which can be evaluated in o d rather than o n d n n the number of training point but also to perform training under strong restriction on time and memory requirement the scaling of our method is at most o n d and in large real world classication experiment we show that it can match prediction performance of the popular support vector machine svm yet can be signican tly faster in training in contrast to the svm our approximation produce estimate of predictive probability error bar allows for bayesian model selection and is le complex in implementation 
we study an explicit parametric model of document query and relevancy assessment for information retrieval ir mean field method are applied to analyze the model and derive efficient practical algorithm to estimate the parameter in the problem the hyperparameters are estimated by a fast approximate leave one out cross validation procedure based on the cavity method the algorithm is further evaluated on several benchmark database by comparing with standard algorithm in ir 
the iterative closest point icp algorithm is a popular method for modeling d object from range data the classical icp algorithm rest on a rigid surface assumption building on recent work on nonrigid object model this paper present an icp algorithm capable of modeling nonrigid object where individual scan may be subject to local deformation we describe an integrated mathematical framework for simultaneously registering scan and recovering the surface configuration to tackle the resulting high dimensional optimization problem we introduce a hierarchical method that first match a coarse skeleton of scan point then adapts local scan patch the approach is implemented for a mobile robot capable of acquiring d model of object 
a boosting algorithm seek to minimize empirically a loss function in a greedy fashion the resulted estimator take an additive function form and is built iteratively by applying a base estimator or learner to updated sample depending on the previous iteration this paper study convergence of boosting when it is carried out over the linear span of a family of basis function for general loss function we prove the convergence of boosting s greedy optimization to the infinimum of the loss function over the linear span a a side product these result reveal the importance of restricting the greedy search step size a known in practice through the work of friedman and others 
in pattern classification task error are introduced because of difference between the true generative model and the one obtained via model estimation using likelihood ratio based classification it is possible to correct for this discrepancy by finding class pair specific term to adjust the likelihood ratio directly and that can make class pair preference relationship intransitive in this work we introduce new methodology that make necessary correction to the likelihood ratio specifically those that are necessary to achieve perfect classification but not likelihoodratio correction which is overkill the new correction while weaker than previously reported such adjustment are analytically challenging since they involve discontinuous function we therefore make several approximation one of which involves a novel measure of decision boundary error sensitivity we test a number of these new scheme on an isolated word speech recognition task a well a on the uci machine learning data set result show that by using the bias term calculated in this new way classification accuracy can substantially improve over both the baseline and over our previous result 
coordinator are coordination manager for fielded first responder each first response team is paired with a coordinator coordination manager which is running on a mobile computing device coordinator provide decision support to first response team by reasoning about who should be doing what when with what resource in support of which other team and so forth coordinator respond to the dynamic of the environment by re coordinating to determine the right course of action for the current circumstance coordinator have been implemented using wireless pda and proprietary first responder location tracking technology this paper describes coordinator the motivation for them the underlying agent architecture implementation issue and first response exercise 
there is a long history of research in automatic text summarization system by both the text retrieval and the natural language processing community but evaluation of such system output ha always presented problem one critical problem remains how to handle the unavoidable variability in human judgment at the core of all the evaluation sponsored by the darpa tide project nist launched a new text summarization evaluation effort called duc in with follow on workshop in and human judgment provided the foundation for all three evaluation and this paper examines how the variation in those judgment doe and doe not affect the result and their interpretation 
there is growing evidence from psychophysical and neurophysiological study that the brain utilizes bayesian principle for inference and decision making an important open question is how bayesian inference for arbitrary graphical model can be implemented in network of spiking neuron in this paper we show that recurrent network of noisy integrate and fire neuron can perform approximate bayesian inference for dynamic and hierarchical graphical model the membrane potential dynamic of neuron is used to implement belief propagation in the log domain the spiking probability of a neuron is shown to approximate the posterior probability of the preferred state encoded by the neuron given past input we illustrate the model using two example a motion detection network in which the spiking probability of a direction selective neuron becomes proportional to the posterior probability of motion in a preferred direction and a two level hierarchical network that produce attentional effect similar to those observed in visual cortical area v and v the hierarchical model offer a new bayesian interpretation of attentional modulation in v and v 
this paper compare the ability of human observer to detect target image curve with that of an ideal observer the target curve are sampled from a generative model which specifies probabilistically the geometry and local intensity property of the curve the ideal observer performs bayesian inference on the generative model using map estimation varying the probability model for the curve geometry enables u investigate whether human performance is best for target curve that obey specific shape statistic in particular those observed on natural shape experiment are performed with data on both rectangular and hexagonal lattice our result show that human observer performance approach that of the ideal observer and are in general closest to the ideal for condition where the target curve tends to be straight or similar to natural statistic on curve this suggests a bias of human observer towards straight curve and natural statistic 
helmholtz stereopsis ha been previously introduced a a surface reconstruction technique that doe not assume a model of surface reectance this technique relies on the use of multiple camera and light source and it ha been shown to be effective when the camera and source position are known here we take a stratied look at uncalibrated helmholtz stereopsis we derive a new photometric matching constraint that can be used to establish correspondence without any knowledge of the camera and source except that they are co located and we determine condition under which we can obtain afne and metric reconstruction an implementation and experimental result are presented 
a scalable approach to trust negotiation is required in web service environment that have large and dynamic requester population we introduce trust serv a model driven trust negotiation framework for web service the framework employ a model for trust negotiation that is based on state machine extended with security abstraction our policy model support lifecycle management an important trait in the dynamic environment that characterize web service in particular we provide a set of change operation to modify policy and migration strategy that permit ongoing negotiation to be migrated to new policy without being disrupted experimental result show the performance benefit of these strategy the proposed approach ha been implemented a a container centric mechanism that is transparent to the web service and to the developer of web service simplifying web service development and management a well a enabling scalable deployment 
airline routinely overbook flight based on the expectation that some fraction of booked passenger will not show for each flight accurate forecast of the expected number of no show for each flight can increase airline revenue by reducing the number of spoiled seat empty seat that might otherwise have been sold and the number of involuntary denied boarding at the departure gate conventional no show forecasting method typically average the no show rate of historically similar flight without the use of passenger specific information we develop two class of model to predict cabin level no show rate using specific information on the individual passenger booked on each flight the first of these model computes the no show probability for each passenger using both the cabin level historical forecast and the extracted passenger feature a explanatory variable this passenger level model is implemented using three different predictive method a c decision tree a segmented naive bayes algorithm and a new aggregation method for an ensemble of probabilistic model the second cabin level model is formulated using the desired cabin level no show rate a the response variable input to this model include the predicted cabin level no show rate derived from the various passenger level model a well a simple statistic of the feature of the cabin passenger population the cabin level model is implemented using either linear regression or a a direct probability model with explicit incorporation of the cabin level no show rate derived from the passenger level model output the new passenger based model are compared to a conventional historical model using train and evaluation data set taken from over million passenger name record standard metric such a lift curve and mean square cabin level error establish the improved accuracy of the passenger based model over the historical model all model are also evaluated using a simple revenue model and it is shown that the cabin level passenger based model can produce between and revenue gain over the conventional model depending on the revenue model parameter 
we propose a self supervised word segmentation technique for chinese information retrieval this method combine the advantage of traditional dictionary based approach with character based approach while overcoming many of their shortcoming experiment on trec data show comparable performance to both the dictionary based and the character based approach however our method is language independent and unsupervised which provides a promising avenue for constructing accurate multilingual information retrieval system that are flexible and adaptive 
for structured light range imaging color stripe can be used for increasing the number of distinguishable light pattern compared to binary bw stripe therefore an appropriate use of color pattern can reduce the number of light projection and range imaging is achievable in single video frame or in one shot on the other hand the reliability and range resolution attainable from color stripe is generally lower than those from multiply projected binary bw pattern since color contrast is affected by object color reflectance and ambient light this paper present new method for selecting stripe color and designing multiple stripe pattern for one shot and two shot imaging we show that maximizing color contrast between the stripe in one shot imaging reduces the ambiguity resulting from colored object surface and limitation in sensor projector resolution two shot imaging add an extra video frame and maximizes the color contrast between the first and second video frame to diminish the ambiguity even further experimental result demonstrate the effectiveness of the presented one shot and two shot color stripe imaging scheme 
this paper present a method to estimate geometrical photometrical and environmental information of a single viewedobject in one integrated framework under fixed viewingposition and fixed illumination direction these threetypes of information are important to render a photorealisticimage of a real object photometrical information representsthe texture and the surface roughness of an object while geometrical and environmental information representthe d shape of an object and the illumination distribution respectively the proposed method estimate the d shapeby computing the surface normal from polarization data calculates the texture of the object from the diffuse only reflectioncomponent determines the illumination directionsfrom the position of the brightest intensity in the specularreflection component and finally computes the surfaceroughness of the object by using the estimated illuminationdistribution 
the web s hyperlink are notoriously brittle and break whenever a resource migrates one solution to this problem is a transparent resource migration mechanism which separate a resource s location from it identity and help provide referential integrity however although several such mechanism have been designed they have not been widely adopted due largely to a lack of compliance with current web standard in addition these mechanism must be updated manually whenever a resource migrates limiting their effectiveness for large web site recently however new web protocol such a webdav web distributed authoring and versioning have emerged which extend the http protocol and provide a new level of control over web resource in this paper we show how we have used these protocol in the design of a new resource migration protocol rmp which enables transparent resource migration across standard web server the rmp work with a new resource migration mechanism we have developed called the resource locator service rls and is fully backwards compatible with the web s architecture enabling all web server and all web content to be involved in the migration process we describe the protocol and the new rls in full together with a prototype implementation and demonstration application that we have developed the paper concludes by presenting performance data taken from the prototype that show how the rls will scale well beyond the size of today s web 
noa is an agent architecture that support the development of agent motivated by norm obligation permission and prohibition obligation motivate a normative agent to act a motive to achieve a state of affair or to perform some action prohibition restrict an agent s behaviour whereas permission allow an agent to pursue certain activity to test the architecture noa agent arc applied to automated business transaction scenario where the correct execution of contract is paramount to create a situation of trust 
this paper present a logical framework for negotiation based on belief revision theory we consider that a negotiation process is a course or multiple course of mutual belief revision a set of agm style postulate are proposed to capture the rationality of competitive and cooperative behavior of negotiation we first show that the agm revision and it iterated extension is a special case of negotiation function then we show that a negotiation function can be constructed by two related iterated belief revision function under a certain coordination mechanism this provides a qualitative method for constructing negotiation space and rational concession it also show glimpse of how to express game theoretical concept in logical framework 
traditional word alignment approach cannot come up with satisfactory result for named entity in this paper we propose a novel approach using a maximum entropy model for named entity alignment to ease the training of the maximum entropy model bootstrapping is used to help supervised learning unlike previous work reported in the literature our work conduct bilingual named entity alignment without word segmentation for chinese and it performance is much better than that with word segmentation when compared with ibm and hmm alignment model experimental result show that our approach outperforms ibm model and hmm significantly 
prediction suffix tree pst provide a popular and effectiv e tool for task such a compression classification and language modeling in this paper we take a decision theoretic view of pst generalizing the notion of margin to pst we present an online pst learning algorithm and derive a mistake bound for it we then describe a self bounded enhancement of our learning algorithm for which the learning process aut omatically grows a bounded depthpst we also prove a similar mistake bound for the self bounded algorithm the result is an efficient algor ithm that neither relies on a priori assumption on the shape or maximal d epth of the target pst nor doe it require any parameter to our knowledge this is the first provably correct pst learning algorithm which g enerates a bounded depth pst while being competitive with any fixed pst determined in hindsight 
many machine learning algorithm for clustering or dimensionality reduction take a input a cloud of point in euclidean space and construct a graph with the input data point a vertex this graph is then partitioned clustering or used to redefine metric information dimensionality reduction there ha been much recent work on new method for graph based clustering and dimensionality reduction but not much on constructing the graph itself graph typically used include the fullyconnected graph a local fixed grid graph for image segmentation or a nearest neighbor graph we suggest that the graph should adapt locally to the structure of the data this can be achieved by a graph ensemble that combine multiple minimum spanning tree each fit to a perturbed version of the data set we show that such a graph ensemble usually produce a better representation of the data manifold than standard method and that it provides robustness to a subsequent clustering or dimensionality reduction algorithm based on the graph 
sequence alignment is an important problem in computational biology we compare two different approach to the problem of optimally aligning two or more character string bounded dynamic programming bdp and divide and conquer frontier search dcfs the approach are compared in term of time and space requirement in through dimension with sequence of varying similarity and length while bdp performs better in two and three dimension it consumes more time and memory than dcfs for higher dimensional problem 
we present an algorithm for translating xslt program into sql our context is that of virtual xml publishing in which a single xml view is defined from a relational database and subsequently queried with xslt program each xslt program is translated into a single sql query and run entirely in the database engine our translation work for a large fragment of xslt which we define that includes descendant ancestor axis recursive template mode parameter and aggregate we put considerable effort in generating correct and efficient sql query and describe several optimization technique to achieve this efficiency we have tested our system on all sql query of the tpc h database benchmark which we represented in xslt and then translated back to sql using our translator 
we present a novel method for tracking object by combiningdensity matching with shape prior density matchingis a tracking method which operates by maximizing thebhattacharyya similarity measure between the photometricdistribution from an estimated image region and a modelphotometric distribution such tracker can be expressed aspde based curve evolution which can be implemented usinglevel set shape prior can be combined with this level setimplementation of density matching by representing theshape prior a a series of level set a variational approachallows for a natural parametrization independentshape term to be derived experimental result on real imagesequences are shown 
discrete fourier transforms and other related fourier method have been practically implementable due to the fast fourier transform fft however there are many situation where doing fast fourier transforms without complete data would be desirable in this paper it is recognised that formulating the fft algorithm a a belief network allows suitable prior to be set for the fourier coecien t furthermore ecien t generalised belief propagation method between cluster of four node enable the fourier coecien t to be inferred and the missing data to be estimated in near to o n log n time where n is the total of the given and missing data point this method is compared with a number of common approach such a setting missing data to zero or to interpolation it is tested on generated data and for a fourier analysis of a damaged audio signal 
we present a novel approach to modelling the non linear and timevarying dynamic of human motion using statistical method to capture the characteristic motion pattern that exist in typical human activity our method is based on automatically clustering the body pose space into connected region exhibiting similar dynamical characteristic modelling the dynamic in each region a a gaussian autoregressive process activity that would require large number of exemplar in example based method are covered by comparatively few motion model different region correspond roughly to different action fragment and our class inference scheme allows for smooth transition between these thus making it useful for activity recognition task the method is used to track activity including walking running etc using a planar d body model it effectiveness is demonstrated by it success in tracking complicated motion like turn without any key frame or d information 
information source on the web are controlled by dierent organization or people utilize dierent text format and have varying inconsistency therefore any system that integrates information from dierent data source must consolidate data from these source data from many data source on the web may not contain enough information to accurately consolidate the data even using state of the art object consolidation system we present an approach to accurately and automatically consolidate data from various data source by utilizing a state of the art object consolidation system in conjunction with a mediator system the mediator system is able to automatically determine which secondary source need to be queried in case where the object consolidation system is unable to confidently determine whether two record refer to the same entity in turn the object consolidation system is then able to utilize this additional information to improve the accuracy of the consolidation between datasets 
semantic understanding of multimedia content is critical in enabling effective access to all form of digital medium data by making large medium repository searchable semantic content description greatly enhance the value of such data automatic semantic understanding is a very challenging problem and most medium database resort to describing content in term of low level feature or using manually ascribed annotation recent technique focus on detecting semantic concept in video such a indoor outdoor face people nature etc this approach work for a fixed lexicon for which annotated training example exist in this paper we consider the problem of using such semantic concept detection to map the video clip into semantic space this is done by constructing a model vector that act a a compact semantic representation of the underlying content we then present experiment in the semantic space leveraging such information for enhanced semantic retrieval classification visualization and data mining purpose we evaluate these idea using a large video corpus and demonstrate significant performance gain in retrieval effectiveness 
advance in satellite technology and availability of downloaded image constantly increase the size of remote sensing image archive automatic content extraction classification and content based retrieval have become highly desired goal for the development of intelligent remote sensing database the common approach for mining these database us rule created by analyst however incorporating gi information and human expert knowledge with digital image processing improves remote sensing image analysis we developed a system that us decision tree classifier for interactive learning of land cover model and mining of image archive decision tree provide a promising solution for this problem because they can operate on both numerical continuous and categorical discrete data source and they do not require any assumption about neither the distribution nor the independence of attribute value this is especially important for the fusion of measurement from different source like spectral data dem data and other ancillary gi data furthermore using surrogate split provides the capability of dealing with missing data during both training and classification and enables handling instrument malfunction or the case where one or more measurement do not exist for some location quantitative and qualitative performance evaluation showed that decision tree provide powerful tool for modeling both pixel and region content of image and mining of remote sensing image archive 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we present a statistical analysis of the auc a an evaluation criterion for classification scoring model first we consider significance test for the difference between auc score of two algorithm on the same test set we derive exact moment under simplifying assumption and use them to examine approximate practical method from the literature we then compare auc to empirical misclassification error when the prediction goal is to minimize future error rate we show that the auc may be preferable to empirical error even in this case and discus the tradeoff between approximation error and estimation error underlying this phenomenon 
we develop a framework based on bayesian model averaging to explain how animal cope with uncertainty about contingency in classical conditioning experiment traditional account of conditioning fit parameter within a fixed generative model of reinforcer delivery uncertainty over the model structure is not considered we apply the theory to explain the puzzling relationship between second order conditioning and conditioned inhibition two similar conditioning regime that nonetheless result in strongly divergent behavioral outcome according to the theory second order conditioning result when limited experience lead animal to prefer a simpler world model that produce spurious correlation conditioned inhibition result when a more complex model is justified by additional experience 
in this paper we analyze the relationship between the computational capability of randomly connected network of threshold gate in the timeseries domain and their dynamical property in particular we propose a complexity measure which we find to assume it highest value near the edge of chaos i e the transition from ordered to chaotic dynamic furthermore we show that the proposed complexity measure predicts the computational capability very well only near the edge of chaos are such network able to perform complex computation on time series additionally a simple synaptic scaling rule for self organized criticality is presented and analyzed 
agent learning to act in a partially observable domain may need to overcome the problem of perceptual aliasing i e different state that appear similar but require different response this problem is exacerbated when the agent s sensor are noisy i e sensor may produce different observation in the same state we show that many well known reinforcement learning method designed to deal with perceptual aliasing such a utile suffix memory finite size history window eligibility trace and memory bit do not handle noisy sensor well we suggest a new algorithm noisy utile suffix memory nusm based on usm that us a weighted classification of observed trajectory we compare nusm to the above method and show it to be more robust to noise 
biomedical ontology are typically structured in a biaxial way reflecting both a taxonomic and a mereological order common example such a the gene ontology and the unified medical language system umls excel in term of coverage but lack an adequate semantics of the mereological relation they incorporate this shortcoming is particularly evident a far a the non mandatory existence of part for their whole is collcerned on the one hand and the propagation of property acmss part whole hierarchy on the other hand we provide a formal specification of the semantic foundation of mereology in the biomedical domain that is closely linked to the paradigm of description logic in essence we here propose to emulate mereological reasoning by taxonomic reasoning in an attempt to capture much of the shared intuition underlying merelogical reasoning in the biomedical domain we distinguish for each mereologically relevant concept four different class of part and whole which allow for the expression of five different propagation pattern 
inductive learning of first order theory based on example ha serious bottleneck in the enormous hypothesis search space needed making existing learning approach perform poorly when compared to the propositional approach moreover in order to choose the appropiate candidate all inductive logic programming ilp system only use quantitive information e g number of example covered and length of rule which is insufficient for search space having many similar candidate this paper introduces a novel approach to improve ilp by incorporating the qualitative information into the search heuristic by focusing only on a kind of data where one instance consists of several part a well a relation among part this approach aim to find the hypothesis describing each class by using both individual and relational characteristic of part of example this kind of data can be found in various domain especially in representing chemical compound structure each compound is composed of atom a part and bond a relation between two atom we apply the proposed approach for discovering rule describing the activity of compound from their structure from two real world datasets mutagenicity in nitroaromatic compound and dopamine antagonist compound the result were compared to the existing method using ten fold cross validation and we found that the proposed method significantly produced more accurate result in prediction 
in this paper we describe a new methodology to develop mixed initiative spoken dialog system which is based on the extensive use of simulation to accelerate the development process with the help of simulation a system providing information about a database of nearly restaurant in the boston area ha been developed the simulator can produce thousand of unique dialog which benefit not only dialog development but also provide data to train the speech recognizer and understanding component in preparation for real user interaction also described is a strategy for creating cooperative response to user query incorporating an intelligent language generation capability that produce content dependent verbal description of listed item 
we study gender discrimination of human face using a combination of psychophysical classification and discrimination exper iments together with method from machine learning we reduce the dimensionality of a set of face image using principal component analysis and then train a set of linear classifier on this reduced representation li near support vector machine svms relevance vector machine rvms fisher linear discriminant fld and prototype prot classifier usin g human classification data because we combine a linear preprocessor wi th linear classifier the entire system act a a linear classifier al lowing u to visualise the decision imagecorresponding to the normal vector of the separating hyperplanes sh of each classifier we predict that th e female tomaleness transition along the normal vector for classifier closely mimicking human classification svm and rvm should be faste r than the transition along any other direction a psychophysical discrimination experiment using the decision image a stimulus is consistent with this prediction 
is the real problem in resolving correspondence using currentstereo algorithm the lack of the right matching criterion in studying the related task of reconstructing three dimensionalspace curve from their projection in multipleviews we suggest that the problem is more basic matchingand reconstruction are coupled and so reconstruction algorithmsshould exploit this rather than assuming that matchingcan be successfully performed before reconstruction torealize this coupling a generative model of curve is introducedwhich ha two key component i a prior distributionof general space curve and ii an image formation modelwhich describes how d curve are projected onto the imageplane a novel aspect of the image formation model is that ituses an exact description of the gradient field of a piecewiseconstant image based on this forward model a fully automaticalgorithm for solving the inverse problem is developedfor an arbitrary number of view the resulting algorithmis robust to partial occlusion deficiency in image curveextraction and it doe not rely on photometric information the relative motion of the camera is assumed to be given several experiment are carried out on various realistic scenario in particular we focus on scene where traditionalcorrelation based method would fail 
we propose a general framework for support vector machine svm based on the principle of multi objective optimization the learning of svms is formulated a a multiobjective program by setting two competing goal to minimize the empirical risk and minimize the model capacity distinct approach to solving the mop introduce various svm formulation the proposed framework enables a more effective minimization of the vc bound on the generalization risk we develop a feature selection approach based on the mop framework and demonstrate it effectiveness on hand written digit data 
in this poster we report on the effect of pseudo relevance feedback prf for a cross language image retrieval task using a test collection typically prf ha been shown to improve retrieval performance in previous clir experiment based on average precision at a fixed rank however our experiment have shown that query in which no relevant document are returned also increase because query reformulation for cross language is likely to be harder than with monolingual searching a great deal of user dissatisfaction would be associated with this scenario we propose that an additional effectiveness measure based on failed query may better reflect user satisfaction than average precision alone 
we extend multiclass svm to multiple prototype per class for this framework we give a compact constrained quadratic problem and we suggest an efficient algorithm for it optimization that guarantee a local minimum of the objective function an annealed process is also proposed that help to escape from local minimum finally we report experiment where the performance obtained using linear model is almost comparable to that obtained by state of art kernel based method but with a significant reduction of one or two order in response time 
we present a principled and efficient planning algorithm for cooperative multiagent dynamic system a striking feature of our method is that the coordination and communication between the agent is not imposed but derived directly from the system dynamic and function approximation architecture we view the entire multiagent system a a single large markov decision process mdp which we assume can be represented in a factored way using a dynamic bayesian network dbn the action space of the resulting mdp is the joint action space of the entire set of agent our approach is based on the use of factored linear value function a an approximation to the joint value function this factorization of the value function allows the agent to coordinate their action at runtime using a natural message passing scheme we provide a simple and efficient method for computing such an approximate value function by solving a single linear program whose size is determined by the interaction between the value function structure and the dbn we thereby avoid the exponential blowup in the state and action space we show that our approach compare favorably with approach based on reward sharing we also show that our algorithm is an efficient alternative to more complicated algorithm even in the single agent case 
this paper present an approach to automatically optimizing the retrieval quality of search engine using clickthrough data intuitively a good information retrieval system should present relevant document high in the ranking with le relevant document following below while previous approach to learning retrieval function from example exist they typically require training data generated from relevance judgment by expert this make them difficult and expensive to apply the goal of this paper is to develop a method that utilizes clickthrough data for training namely the query log of the search engine in connection with the log of link the user clicked on in the presented ranking such clickthrough data is available in abundance and can be recorded at very low cost taking a support vector machine svm approach this paper present a method for learning retrieval function from a theoretical perspective this method is shown to be well founded in a risk minimization framework furthermore it is shown to be feasible even for large set of query and feature the theoretical result are verified in a controlled experiment it show that the method can effectively adapt the retrieval function of a meta search engine to a particular group of user outperforming google in term of retrieval quality after only a couple of hundred training example 
we present an algorithm to extract feature from high dimensional gene expression profile based on the knowledge of a graph which link together gene known to participate to successive reaction in metabolic pathway motivated by the intuition that biologically relevant feature are likely to exhibit smoothness with respect to the graph topology the algorithm involves encoding the graph and the set of expression profile into kernel function and performing a generalized form of canonical correlation analysis in the corresponding reproducible kernel hilbert space function prediction experiment for the gene of the yeast s cerevisiae validate this approach by showing a consistent increase in performance when a state of the art classifier us the vector of feature instead of the original expression profile to predict the functional class of a gene 
a new kernel function between two labeled graph is presented feature vector are defined a the count of label path produced by random walk on graph the kernel computation finally boil down to obtaining the stationary state of a discrete time linear system thus is eciently performed by solving simultaneous linear equation our kernel is based on an infinite dimensional feature space so it is fundamentally dierent from other string or tree kernel based on dynamic programming we will present promising empirical result in classification of chemical compound 
the computation of classical higher order statistic such a higher order moment or spectrum is difficult for image due to the huge number of term to be estimated and interpreted we propose an alternative approach in which multiplicative pixel interaction are described by a series of wiener functionals since the functionals are estimated implicitly via polynomial kernel the combinatorial explosion associated with the classical higher order statistic is avoided first result show that image structure such a line or corner can be predicted correctly and that pixel interaction up to the order of five play an important role in natural image 
we describe a new algorithmic framework for learning multiclass categorization problem in this framework a multiclass predictor is composed of a pair of embeddings that map both instance and label into a common space in this space each instance is assigned the label it is nearest to we outline and analyze an algorithm termed bunching for learning the pair of embeddings from labeled data a key construction in the analysis of the algorithm is the notion of probabilistic output code a generalization of error correcting output code ecoc furthermore the method of multiclass categorization using ecoc is shown to be an instance of bunching we demonstrate the advantage of bunching over ecoc by comparing their performance on numerous categorization problem 
we present a general modeling method for optimal probability prediction over future observation in which model dimensionality is determined a a natural by product this new method yield several estimator and we establish theoretically that they are optimal either overall or under stated restriction when the number of free parameter is innite a a case study we investigate the problem of tting logistic model in nite sample situation simulation result on both articial and practical datasets are supportive 
we present a graphical model for beat tracking in recorded music using a probabilistic graphical model allows u to incorporate local information and global smoothness constraint in a principled manner we evaluate our model on a set of varied and difficult example and achieve impressive result by using a fast dual tree algorithm for graphical model inference our system run in le time than the duration of the music being processed 
in this poster we present an overview of the technique we used to develop and evaluate a text categorisation system to automatically classify racist text detecting racism is difficult because the presence of indicator word is insufficient to indicate racist text unlike some other text classification task support vector machine svm are used to automatically categorise web page based on whether or not they are racist different interpretation of what constitutes a term are taken and in this poster we look at three representation of a web page within an svm bag of word bigram and part of speech tag 
this paper introduces an indexing method based on static analysis of grammar rule and type signature for typed feature structure grammar tfsgs the static analysis try to predict at compile time which feature path will cause unification failure during parsing at run time to support the static analysis we introduce a new classification of the instance of variable used in tfsgs based on what type of structure sharing they create the indexing action that can be performed during parsing are also enumerated non statistical indexing ha the advantage of not requiring training and a the evaluation using large scale hpsgs demonstrates the improvement are comparable with those of statistical optimization such statistical optimization rely on data collected during training and their performance doe not always compensate for the training cost 
multimedia scheduling model provide a rich variety of tool for managing the synchronization of medium like video and audio but generally have an inflexible model for time itself in contrast modern animation model in the computer graphic community generally lack tool for synchronization and structural time but allow for a flexible concept of time including variable pacing acceleration and deceleration and other tool useful for controlling and adapting animation behavior multimedia author have been forced to choose one set of feature over the others limiting the range of presentation they can create some programming model addressed some of these problem but provided no declarative mean for author and authoring tool to leverage the functionality this paper describes a new model incorporated into smil that combine the strength of scheduling model with the flexible time manipulation of animation model the implication of this integration are discussed with respect to scheduling and structured time drawing upon experience with smil timing and synchronization and the integration with xhtml 
abstract when we model a higher order function such a learning and memory we face a difficulty of comparing neural activity with hidden variable that depend on the history of sensory and motor signal and the dynamic of the network here we propose novel method for estimating hidden variable of a learning agent such a connection weight from sequence of observable variable bayesian estimation is a method to estimate the posterior probability of hidden variable from observable data sequence using a dynamic model of hidden and observable variable in this paper we apply particle filter for estimating internal parameter and metaparameters of a reinforcement learning model we verified the effectiveness of the method using both artificial data and real animal behavioral data 
we have aligned japanese and english news article and sentence to make a large parallel corpus we first used a method based on cross language information retrieval clir to align the japanese and english article and then used a method based on dynamic programming dp matching to align the japanese and english sentence in these article however the result included many incorrect alignment to remove these we propose two measure score that evaluate the validity of alignment the measure for article alignment us similarity in sentence aligned by dp matching and that for sentence alignment us similarity in article aligned by clir they enhance each other to improve the accuracy of alignment using these measure we have successfully constructed a large scale article and sentence alignment corpus available to the public 
data clustering ha attracted a lot of research attention in the field of computational statistic and data mining in most related study the dissimilarity between two cluster is defined a the distance between their centroid or the distance between two closest or farthest data point however all of these measurement are vulnerable to outlier and removing the outlier precisely is yet another difficult task in view of this we propose a new similarity measurement referred to a cohesion to measure the inter cluster distance by using this new measurement of cohesion we design a two phase clustering algorithm called cohesion based self merging abbreviated a csm which run in linear time to the size of input data set combining the feature of partitional and hierarchical clustering method algorithm csm partition the input data set into several small subclusters in the first phase and then continuously merges the subclusters based on cohesion in a hierarchical manner in the second phase a shown by our performance study the cohesion based clustering is very robust and posse the excellent tolerance to outlier in various workload more importantly algorithm csm is shown to be able to cluster the data set of arbitrary shape very efficiently and provide better clustering result than those by prior method index term data mining data clustering hierarchical clustering partitional clustering 
a method for the definition of verb predicate is proposed the definition of the predicate is essentially tied to a semantic interpretation algorithm that determines the predicate for the verb it semantic role and adjunct a predicate definition are complete they can be tested by running the algorithm on some sentence and verifying the resolution of the predicate semantic role and adjunct in those sentence the predicate are defined semiautomatically with the help of a software environment that us several section of a corpus to provide feedback for the definition of the predicate and then for the subsequent testing and refining of the definition the method is very flexible in adding a new predicate to a list of already defined predicate for a given verb the method build on an existing approach that defines predicate for wordnet verb class and that plan to define predicate for every english verb the definition of the predicate and the semantic interpretation algorithm are being used to automatically create a corpus of annotated verb predicate semantic role and adjunct 
we seek to both detect and segment object in image to exploit both local image data a well a contextual information we introduce boosted random field brfs which us boosting to learn the graph structure and local evidence of a conditional random field crf t he graph structure is learned by assembling graph fragment in an additive model the connection between individual pixel are not very informative but by using dense graph we can pool information from large region of the image dense model also support efficient inference we show how contextual information from other object can improve detection performance both in term of accuracy and speed by using a computational cascade we apply our system to detect stuff and thing in offi ce and street scene 
we describe and analyze an online algorithm for supervised learning of pseudo metric the algorithm receives pair of instance and predicts their similarity according to a pseudo metric the pseudo metric we use are quadratic form parameterized by positive semi definite matrix the core of the algorithm is an update rule that is based on successive projection onto the positive semi definite cone and onto half space constraint imposed by the example we describe an efficient procedure for performing these projection derive a worst case mistake bound on the similarity prediction and discus a dual version of the algorithm in which it is simple to incorporate kernel operator the online algorithm also serf a a building block for deriving a large margin batch algorithm we demonstrate the merit of the proposed approach by conducting experiment on mnist dataset and on document filtering 
d morphable model a a mean to generate image of a class ofobjects and to analyze them have become increasingly popular theproblematic part of this frameworkis the registration of the modelto an image a k a the fitting the characteristic feature of afitting algorithm are it efficiency robustness accuracy andautomation many accurate algorithm based on gradient descenttechniques exist which are unfortunately short on the otherfeatures recently an efficient algorithm called inversecompositional image alignment icia algorithm able to fit dimages wa introduced in this paper we extent this algorithm tofit d morphable model using a novel mathematical notation whichfacilitates the formulation of the fitting problem thisformulation enables u to avoid a simplification so far used in theicia being a efficient and leading to improved fitting precision additionally the algorithm is robust without sacrificing itsefficiency and accuracy thereby conforming to three of the fourcharacteristics of a good fitting algorithm 
in information retrieval it is well known that the complexity of processing boolean query depends on the size of the intermediate result which could be huge and are typically on disk even though the size of the final result may be quite small in the case of inverted file the most time consuming operation is the merging or intersection of the list of occurrence we propose the keyword tree k tree and forest efficient structure to handle boolean query in keyword based information retrieval extensive simulation show that k tree is order of magnitude faster i e far fewer i o s for boolean query than the usual approach of merging the list of occurrence and incurs only a small overhead for single keyword query the k tree can be efficiently parallelized a well the construction cost of k tree is comparable to the cost of building inverted file 
in this paper we propose a scaling up method that is applicable to essentially any induction algorithm based on discrete search the result of applying the method to an algorithm is that it running time becomes independent of the size of the database while the decision made are essentially identical to those that would be made given infinite data the method work within pre specified memory limit and a long a the data is iid only requires accessing it sequentially it give anytime result and can be used to produce batch stream time changing and active learning version of an algorithm we apply the method to learning bayesian network developing an algorithm that is faster than previous one by order of magnitude while achieving essentially the same predictive performance we observe these gain on a series of large database generated from benchmark network on the kdd cup e commerce data and on a web log containing million request 
ego motion estimation for an agile single camera movingthrough general unknown scene becomes a much morechallenging problem when real time performance is requiredrather than under the off line processing conditionsunder which most successful structure from motion workhas been achieved this task of estimating camera motionfrom measurement of a continuously expanding set of self mappedvisual feature is one of a class of problem knownas simultaneous localisation and mapping slam in therobotics community and we argue that such real time mappingresearch despite rarely being camera based is morerelevant here than off line structure from motion methodsdue to the more fundamental emphasis placed on propagationof uncertainty we present a top down bayesian framework for single cameralocalisation via mapping of a sparse set of naturalfeatures using motion modelling and an information guidedactive measurement strategy in particular addressingthe difficult issue of real time feature initialisation viaa factored sampling approach real time handling of uncertaintypermits robust localisation via the creating andactive measurement of a sparse map of landmark such thatregions can be re visited after period of neglect and localisationcan continue through period when few feature arevisible result are presented of real time localisation forum hand waved camera with very sparse prior scene knowledgeand all processing carried out on a desktop pc 
state of the art machine translation technique are still far from producing high quality translation this drawback lead u to introduce an alternative approach to the translation problem that brings human expertise into the machine translation scenario in this framework namely computer assisted translation cat human translator interact with a translation system a an assistance tool that dinamically offer a list of translation that best completes the part of the sentence already translated in this paper finite state transducer are presented a a candidate technology in the cat paradigm the appropriateness of this technique is evaluated on a printer manual corpus and result from preliminary experiment confirm that human translator would reduce to le than the amount of work to be done for the same task 
we present a novel method for generic visual categorization the problem of identifying the object content of natural image while generalizing across variation inherent to the object class this bag of keypoints method is based on vector quantization of affine invariant descriptor of image patch we propose and compare two alternative implementation using different classifier na ve bayes and svm the main advantage of the method are that it is simple computationally efficient and intrinsically invariant we present result for simultaneously classifying seven semantic visual category these result clearly demonstrate that the method is robust to background clutter and produce good categorization accuracy even without exploiting geometric information 
this paper present a flexible mixture model fmm for collaborative filtering fmm extends existing partitioning clustering algorithm for collaborative filtering by clustering both user and item together simultaneously without assuming that each user and item should only belong to a single cluster furthermore with the introduction of preference node the proposed framework is able to explicitly model how user rate item which can vary dramatically even among the user with similar taste on item empirical study over two datasets of movie rating ha shown that our new algorithm outperforms five other collaborative filtering algorithm substantially 
privacy consideration often constrain data mining project this paper address the problem of association rule mining where transaction are distributed across source each site hold some attribute of each transaction and the site wish to collaborate to identify globally valid association rule however the site must not reveal individual transaction data we present a two party algorithm for efficiently discovering frequent itemsets with minimum support level without either site revealing individual transaction value 
we describe a model of short term synaptic depression that is derived from a silicon circuit implementation the dynamic of this circuit model are similar to the dynamic of some present theoretical model of shortterm depression except that the recovery dynamic of the variable describing the depression is nonlinear and it also depends on the presynaptic frequency the equation describing the steady state and transient response of this synaptic model fit the experimental result obtained from a fabricated silicon network consisting of leaky integrate and fire neuron and different type of synapsis we also show experimental data demonstrating the possible computational role of depression one possible role of a depressing synapse is that the input can quickly bring the neuron up to threshold when the membrane potential is close to the resting potential 
following futurism we show how periodic motion can be represented by a small number of eigen shape that capture the whole dynamic mechanism of periodic motion spectral decomposition of a silhouette of an object in motion serf a a basis for behavior classification by principle component analysis the boundary contour of the walking dog for example is first computed efficiently and accurately after normalization the implicit representation of a sequence of silhouette contour given by their corresponding binary image is used for generating eigen shape for the given motion singular value decomposition produce these eigen shape that are then used to analyze the sequence we show example of object a well a behavior classification based on the eigen decomposition of the binary silhouette sequence 
ensemble technique such a bagging and decorate exploit the instability of learner such a decision tree to create a diverse set of model however creating a diverse set of model for stable learner such a na ve bayes is difficult a they are relatively insensitive to training data change furthermore many popular ensemble technique do not have a rigorous underlying theory and often provide no insight into how many model to build we formally define stable learner a having a second order derivative of the posterior density function and propose an ensemble technique specifically for stable learner our ensemble technique bootstrap model averaging creates a number of bootstrap sample from the training data build a model from each and then sum the joint instance and class probability over all model built we show that for stable learner our ensemble technique for infinite bootstrap sample approximates posterior model averaging aka the optimal bayes classifier obc for finite bootstrap sample we estimate the increase over the abc error using chebychev bound we empirically illustrate our approach s usefulness for several stable learner and verify our bound s correctness 
concept learning in content based image retrieval cbir system isa challenging task this paper present an active concept learningapproach based on mixture model to deal with the two basic aspectsof a database system changing image insertion or removal natureof a database and user query to achieve concept learning wedevelop a novel model selection method based on bayesian analysisthat evaluates the consistency of hypothesized model with theavailable information the analysis of exploitation v explorationin the search space help to find optimal model efficiently experimental result on corel database show the efficacy of ourapproach 
disjunctive logic programming dlp is a very expressive formalism it allows to express every property of finite structure that is decidable in the complexity class p npnp despite the high expressiveness of dlp there are some simple property often arising in real world application which cannot be encoded in a simple and natural manner among these property requiring to apply some arithmetic operator like sum time count on a set of element satisfying some condition cannot be naturally expressed in dlp to overcome this deficiency in this paper we extend dlp by aggregate function we formally define the semantics of the new language named dlpa we show the usefulness of the new construct on relevant knowledge based problem we analyze the computational complexity of dlpa showing that the addition of aggregate doe not bring a higher cost in that respect we provide an implementation of the dlpa language in dlvthe state of the art dlp system and report on experiment which confirm the usefulness of the proposed extension also for the efficiency of the computation 
we introduce the generalized semi markov decision process gsmdp a an extension of continuous time mdps and semi markov decision process smdps for modeling stochastic decision process with asynchronous event and action using phase type distribution and uniformization we show how an arbitrary gsmdp can be approximated by a discrete time mdp which can then be solved using existing mdp technique the technique we present can also be seen a an alternative approach for solving smdps and we demonstrate that the introduction of phase allows u to generate higher quality policy than those obtained by standard smdp solution technique 
structured document rich information need and detailed information about user are becoming more pervasive within everyday computing usage application such a question answering reading tutor and xml retrieval demand more robust retrieval on richly annotated document in order to effectively serve these application the community will need a better understanding of the combination of evidence in this work i propose that the use of simple generative probabilistic model will be an effective framework for these problem statistical language model which are a special case of generative probabilistic model have been used extensively within recent information retrieval research their flexibility ha been very effective in adapting to numerous task and problem i propose to extend the statistical language modeling framework to handle rich information need and document with structural and linguistic annotation much of the prior work on combination of evidence ha had few well studied theoretical contribution so i also propose to develop a sounder theoretical basis which give more predictable result 
this paper present a method for learning a distance metric from relative comparison such a a is closer to b than a is to c taking a support vector machine svm approach we develop an algorithm that provides a flexible way of describing qualitative training data a a set of constraint we show that such constraint lead to a convex quadratic programming problem that can be solved by adapting standard method for svm training we empirically evaluate the performance and the modelling flexibility of the algorithm on a collection of text document 
the data in many real world problem can be thought of a a graph such a the web co author network and biological network we propose a general regularization framework on graph which is applicable to the classication ranking and link prediction problem we also show that the method can be explained a lazy random walk we evaluate the method on a number of experiment 
in this paper we introduce a new underlying probabilistic model for principal component analysis pca our formulation interprets pca a a particular gaussian process prior on a mapping from a latent space to the observed data space we show that if the prior s covariance function constrains the mapping to be linear the model is equivalent to pca we then extend the model by considering le restrictive covariance function which allow non linear mapping this more general gaussian process latent variable model gplvm is then evaluated a an approach to the visualisation of high dimensional data for three different data set additionally our non linear algorithm can be further kernelised leading to twin kernel pca in which a mapping between feature space occurs 
this paper introduces support envelope a new tool for analyzing association pattern and illustrates some of their property application and possible extension specifically the support envelope for a transaction data set and a specified pair of positive integer m n consists of the item and transaction that need to be searched to find any association pattern involving m or more transaction and n or more item for any transaction data set with m transaction and n item there is a unique lattice of at most m n support envelope that capture the structure of the association pattern in that data set because support envelope are not encumbered by a support threshold this support lattice provides a complete view of the association structure of the data set including association pattern that have low support furthermore the boundary of the support lattice the support boundary ha at most min m n envelope and is especially interesting since it bound the maximum size of potential association pattern not only for frequent closed and maximal itemsets but also for pattern such a error tolerant itemsets that are more general the association structure can be represented graphically a a two dimensional scatter plot of the m n value associated with the support envelope of the data set a feature that is useful in the exploratory analysis of association pattern finally the algorithm to compute support envelope is simple and computationally efficient and it is straightforward to parallelize the process of finding all the support envelope 
this paper proposes a robust estimation and validation framework for characterizing local structure in a positive multi variate continuous function approximated by a gaussian based model the new solution is robust against data with large deviation from the model and margin truncation induced by neighboring structure to this goal it unifies robust statistical estimation for parametric model fitting and multi scale analysis based on continuous scale space theory the unification is realized by formally extending the mean shift based density analysis towards continuous signal whose local structure is characterized by an anisotropic fully parameterized covariance matrix a statistical validation method based on analyzing residual error of the chi square fitting is also proposed to complement this estimation framework the strength of our solution is the aforementioned robustness experiment with synthetic d and d data clearly demonstrate this advantage in comparison with the normalized laplacian approach and the standard sample estimation approach p the new framework is applied to d volumetric analysis of lung tumor a d implementation is evaluated with high resolution ct image of patient with tumor including part solid or ground glass opacity nodule that are highly nongaussian and clinically significant our system accurately estimated d anisotropic spread and orientation for of the total tumor and also correctly rejected all the failure without any false rejection and false acceptance this system process each voxel volume of interest by an average of two second with a ghz intel cpu our framework is generic and can be applied for the analysis of blob like structure in various other application 
temporal relation resolution involves extraction of temporal information explicitly or implicitly embedded in a language this information is often inferred from a variety of interactive grammatical and lexical cue especially in chinese for this purpose inter clause relation temporal or otherwise in a multiple clause sentence play an important role in this paper a computational model based on machine learning and heterogeneous collaborative bootstrapping is proposed for analyzing temporal relation in a chinese multiple clause sentence the model make use of the fact that event are represented in different temporal structure it take into account the effect of linguistic feature such a tense aspect temporal connective and discourse structure a set of experiment ha been conducted to investigate how linguistic feature could affect temporal relation resolution 
we consider the question of predicting nonlinear time series kernel dynamical modeling kdm a new method based on kernel is proposed a an extension to linear dynamical model the kernel trick is used twice first to learn the parameter of the model and second to compute preimages of the time series predicted in the feature space by mean of support vector regression our model show strong connection with the classic kalman filter model with the kernel feature space a hidden state space kernel dynamical modeling is tested against two benchmark time series and achieves high quality prediction 
this paper present an approach to the approximate description of univariate real valued function in term of precise or imprecise reference point and interpolation between these point it is achieved by mean of gradual rule which express that the closer the variable to the abscissa of a reference point the closer the value of the function to the ordinate of this reference point gradual rule enable u to specify sophisticated gauge under the form of connected area inside of which the function belonging to the class under consideration should remain this provides a simple and efficient tool for categorizing signal this tool can be further improved by making the gauge flexible by mean of fuzzy gradual rule this is illustrated on a benchmark example 
we present an extension of isomap nonlinear dimension reduction tenenbaum et al for data with both spatial and temporal relationship our method st isomap augments the existing isomap framework to consider temporal relationship in local neighborhood that can be propagated globally via a shortest path mechanism two instantiation of st isomap are presented for sequentially continuous and segmented data result from applying st isomap to real world data collected from human motion performance and humanoid robot teleoperation are also presented 
within the initiative for the evaluation of xml retrieval inex a number of metric to evaluate the effectiveness of content oriented xml retrieval approach were developed although these metric provide a solution towards addressing the problem of overlapping result element they do not consider the problem of overlapping reference component within the recall base thus leading to skewed effectiveness score we propose alternative metric that aim to provide a solution to both overlap issue 
this paper present an unsupervised word sense learning algorithm which induces sens of target word by grouping it occurrence into a natural number of cluster based on the similarity of their context for removing noisy word in feature set feature selection is conducted by optimizing a cluster validation criterion subject to some constraint in an unsupervised manner gaussian mixture model and minimum description length criterion are used to estimate cluster structure and cluster number experimental result show that our algorithm can find important feature subset estimate model order cluster number and achieve better performance than another algorithm which requires cluster number to be provided 
the goal of clustering is to identify distinct group in a dataset the basic idea of model based clustering is to approximate the data density by a mixture model typically a mixture of gaussians and to estimate the parameter of the component density the mixing fraction and the number of component from the data the number of distinct group in the data is then taken to be the number of mixture component and the observation are partitioned into cluster estimate of the group using bayes rule if the group are well separated and look gaussian then the resulting cluster will indeed tend to be distinct in the most common sense of the word contiguous densely populated area of feature space separated by contiguous relatively empty region if the group are not gaussian however this correspondence may break down an isolated group with a non elliptical distribution for example may be modeled by not one but several mixture component and the corresponding cluster will no longer be well separated we present method for assessing the degree of separation between the component of a mixture model and between the corresponding cluster we also propose a new clustering method that can be regarded a a hybrid between model based and nonparametric clustering the hybrid clustering algorithm prune the cluster tree generated by hierarchical model based clustering starting with the tree corresponding to the mixture model chosen by the bayesian information criterion it progressively merges cluster that do not appear to correspond to different mode of the data density 
recently there ha been increasing interest in the issue of cost sensitive learning and decision making in a variety of application of data mining a number of approach have been developed that are effective at optimizing cost sensitive decision when each decision is considered in isolation however the issue of sequential decision making with the goal of maximizing total benefit accrued over a period of time instead of immediate benefit ha rarely been addressed in the present paper we propose a novel approach to sequential decision making based on the reinforcement learning framework our approach attempt to learn decision rule that optimize a sequence of cost sensitive decision so a to maximize the total benefit accrued over time we use the domain of targeted marketing a a testbed for empirical evaluation of the proposed method we conducted experiment using approximately two year of monthly promotion data derived from the well known kdd cup donation data set the experimental result show that the proposed method for optimizing total accrued benefit out performs the usual targeted marketing methodology of optimizing each promotion in isolation we also analyze the behavior of the targeting rule that were obtained and discus their appropriateness to the application domain 
a information volume in enterprise system and in the web grows rapidly how to accurately retrieve information is an important research area several corpus based smoothing technique have been proposed to address the data sparsity and synonym problem faced by information retrieval system such smoothing technique are often unable to discover and utilize the correlation among term we propose cv a correlation verification based smoothing method that considers co occurrence information in smoothing strongly correlated term in a document are identified by their co occurrence frequency in the document to avoid missing correlated term with low co occurrence frequency but specific to the theme of the document the joint distribution of term in the document are compared with those in the corpus for statistical significance a common approach to apply corpus based smoothing technique to information retrieval is by refining the vector representation of document this paper investigates the effect of corpus based smoothing on information retrieval by query expansion using term cluster generated from a term clustering process the result can also be viewed in light of the effect of smoothing on clustering empirical study show that our approach outperforms previous corpus based smoothing technique it improves retrieval effectiveness by the result demonstrate that corpus based smoothing can be used for query expansion by term clustering 
developer of artificial agent commonly take the view that we can only specify agent behavior via the expensive process of implementing new skill this paper offer an alternative expressed by the separation hypothesis that the behavioral difference among individual are due to the action of distinct preference over the same set of skill we test this hypothesis in a simulated automotive domain by using a reinforcement learning algorithm to induce vehicle control policy given a structured skill for driving that contains option and a user supplied reward function we show that qualitatively distinct reward function produce agent with qualitatively distinct behavior over the same set of skill this lead to a new development metaphor we call programming by reward 
learning reusable sequence can support the development of expertise in many domain either by improving decision making quality or decreasing execution speed this paper introduces and evaluates a method to learn action sequence for generalized state from prior problem experience from experienced sequence the method induces the context that underlies a sequence of action empirical result indicate that the sequence and context learned for a class of problem are actually those deemed important by expert for that particular class and can be used to select appropriate action sequence when solving problem there 
a framework is introduced for assessing the encoding accuracy and the discriminational ability of a population of neuron upon simultaneous presentation of multiple stimulus minimal square estimation error are obtained from a fisher information analysis in an abstract compound space comprising the feature of all stimulus even for the simplest case of linear superposition of response and gaussian tuning the symmetry in the compound space are very dieren t from those in the case of a single stimulus the analysis allows for a quantitative description of attentional eects and can be extended to include neural nonlinearities such a nonclassical receptive eld 
the concept of consistency ha pervaded study of the constraint satisfiction problem we introduce two concept which are inspired by consistency for the more general framework of the quantified constraint satisfaction problem qcsp we use these concept to derive in a uniform fashion proof of polynomial time tractability and corresponding algorithm for certain case of the qcsp where the type of allowed relation are restricted we not only unify existing tractability result and algorithm but also identify new class of tractable qcsps 
we revisit the problem of revising probabilistic belief using uncertain evidence and report result on several major issue relating to this problem how should one specify uncertain evidence how should one revise a probability distribution how should one interpret informal evidential statement should and do iterated belief revision commute and what guarantee can be offered on the amount of belief change induced by a particular revision our discussion is focused on two main method for probabilistic revision jeffrey s rule of probability kinematics and pearl s method of virtual evidence where we analyze and unify these method from the perspective of the question posed above 
choice based conjoint analysis build model of consumer preference over product with answer gathered in questionnaire our main goal is to bring tool from the machine learning community to solve this problem more efficiently thus we propose two algorithm to quickly and accurately estimate consumer preference 
we present a learning framework for markovian decision process that is based on optimization in the policy space instead of using relatively slow gradient based optimization algorithm we use the fast cross entropy method the suggested framework is described for several reward criterion and it eectiveness is demonstrated for a grid world navigation task and for an inventory control problem 
this paper present a method for admission control and request scheduling for multiply tiered e commerce web site achieving both stable behavior during overload and improved response time our method externally observes execution cost of request online distinguishing different request type and performs overload protection and preferential scheduling using relatively simple measurement and a straight forward control mechanism unlike previous proposal which require extensive change to the server or operating system our method requires no modification to the host o s web server application server or database since our method is external it can be implemented in a proxy we present such an implementation called gatekeeper using it with standard software component on the linux operating system we evaluate the proxy using the industry standard tpc w workload generator in a typical three tiered e commerce environment we show consistent performance during overload and throughput increase of up to percent response time improves by up to a factor of with only a percent penalty to large job 
we propose a novel method for constructing utility model by learning from observed negotiation action in particular we show how offer and counter offer in negotiation can be transformed into gamble question providing the basis for eliciting utility function result of experiment and evaluation are briefly described 
parsing system which rely on hand coded linguistic description can only perform adequately in a far a these description are correct and complete the paper describes an error mining technique to discover problem in hand coded linguistic description for parsing such a grammar and lexicon by analysing parse result for very large unannotated corpus the technique discovers missing incorrect or incomplete linguistic description the technique us the frequency of n gram of word for arbitrary value of n it is shown how a new combination of suffix array and perfect hash finite automaton allows an efficient implementation 
in earlier work we proposed a way for a web server to detect connectivity information about client accessing it in order to take tailored action for a client request this paper describes the design implementation and evaluation of such a working system a web site ha a strong incentive to reduce the time to glass to retain user who may otherwise lose interest and leave the site we have performed a measurement study from multiple client site around the world with various level of connectivity to the internet communicating with modified apache web server under our control the result show that client can be classified in a correct and stable manner and that user perceived latency can be reduced via tailored action our measurement show that classification and determination of server action are done without significant overhead on the web server we explore a variety of modified action ranging from selecting a lower quality version of the resource to altering the manner of content delivery by studying numerous performance related factor in a single unified framework and examining both individual action a well a combination of action our modified web server implementation show the efficacy of various server action 
peer to peer p p system are very large computer network where peer collaborate to provide a common service providing large scale information retrieval ir e g for searching the word wide web is an appealing application for p p system the research community ha presented several proposal for p p ir however so far the concept of p p and of ir have been intermingled in this paper we propose an architecture to structure p p ir system we difierentiate between concept belonging to the construction and maintenance of a p p overlay network and those belonging to ir furthermore we distinguish basic p p ir concept which are likely to be needed in all p p ir system and advanced p p ir concept that rather depend on the avor of the system this decomposition of the p p retrieval process is an important step towards a structured implementation of such system furthermore it allows a systematic sharing of method and resource needed to perform retrieval the next generation of global information retrieval system will combine these distributed resource in new way to provide more e cient web search keywords peer to peer information retrieval p p ir architecture key based routing kbr p p web search 
multiagent learning is a key problem in ai in the presence of multiple nash equilibrium even agent with non conflicting interest may not be able to learn an optimal coordination policy the problem is exaccerbated if the agent do not know the game and independently receive noisy payoff so multiagent reinforfcement learning involves two interrelated problem identifying the game and learning to play in this paper we present optimal adaptive learning the first algorithm that converges to an optimal nash equilibrium with probability in any team markov game we provide a convergence proof and show that the algorithm s parameter are easy to set to meet the convergence condition 
causality is typically treated an all or nothing concept either a is a cause of b or it is not we extend the definition of causality introduced by halpern and pearl a to take into account the degree of responsibility of a for b for example if someone win an election then each person who vote for him is le responsible for the victory than if he had won we then define a notion of degree of blame which take into account an agent s epistemic state roughly speaking the degree of blame of a for b is the expected degree of responsibility of a for b taken over the epistemic state of an agent 
over the last few year the network community ha started to rely heavily on the use of novel concept such a fractal self similarity long range dependence power law especially evidence of fractal self similarity and long range dependence in network trac have been widely observed despite their wide use there is still much confusion regarding the identification of such phenomenon in real network trac 
the objective of active recognition is to iteratively collect the next best measurement e g camera angle or viewpoint to maximally reduce ambiguity in recognition however existing work largely overlooked feature interaction issue feature selection on the other hand focus on the selection of a subset of measurement for a given classification task but is not context sensitive i e the decision doe not depend on the current input this paper proposes a unified perspective through conditional feature sensitivity analysis taking into account both current context and feature interaction based on different representation of the contextual uncertainty we present three treatment model and exploit their joint power for dealing with complex feature interaction synthetic example are used to systematically test the validity of the proposed model a practical application in medical domain is illustrated using an echocardiography database with more than video segment with both subjective from expert and objective validation 
mining frequent tree is very useful in domain like bioinformatics web mining mining semistructured data and so on we formulate the problem of mining embedded subtrees in a forest of rooted labeled and ordered tree we present treeminer a novel algorithm to discover all frequent subtrees in a forest using a new data structure called scope list we contrast treeminer with a pattern matching tree mining algorithm patternmatcher we conduct detailed experiment to test the performance and scalability of these method we find that treeminer outperforms the pattern matching approach by a factor of to and ha good scaleup property we also present an application of tree mining to analyze real web log for usage pattern 
we describe method for computing an implicit model of a hypersurface that is given only by a finite sampling the method work by mapping the sample point into a reproducing kernel hilbert space and then determining region in term of hyperplanes 
the crossed slit x slit camera is defined by two non intersectingslits which replace the pinhole in the commonperspective camera each point in space is projected to theimage plane by a ray which pass through the point and thetwo slit the x slit projection model includes the pushb roomcamera a a special case in addition it describesa certain class of panoramic image which are generatedfrom sequence obtained by translating pinhole camera in this paper we develop the epipolar geometry of the x slitsprojection model we show an object which is similarto the fundamental matrix our matrix however describesa quadratic relation between corresponding image point using the veronese mapping similarly the equivalent ofepipolar line are conic in the image plane unlike the pin holecase epipolar surface do not usually exist in the sensethat matching epipolar line lie on a single surface we analyzethe case when epipolar surface exist and characterizetheir property finally we demonstrate the matchingof point in pair of x slit panoramic image 
this paper give a new iterative algorithm for kernel logistic regression it is based on the solution of a dual problem using idea similar to those of the sequential minimal optimization algorithm for support vector machine asymptotic convergence of the algorithm is proved computational experiment show that the algorithm is robust and fast the algorithmic idea can also be used to give a fast dual algorithm for solving the optimization problem arising in the inner loop of gaussian process classifier 
this paper investigates a new learning model in which the input data is corrupted with noise we present a general statistical framework to tackle this problem based on the statistical reasoning we propose a novel formulation of support vector classification which allows uncertainty in input data we derive an intuitive geometric interpretation of the proposed formulation and develop algorithm to efficiently solve it empirical result are included to show that the newly formed method is superior to the standard svm for problem with noisy input 
the paper present a method for pruning frequent itemsets based on background knowledge represented by a bayesian network the interestingness of an itemset is defined a the absolute difference between it support estimated from data and from the bayesian network efficient algorithm are presented for finding interestingness of a collection of frequent itemsets and for finding all attribute set with a given minimum interestingness practical usefulness of the algorithm and their efficiency have been verified experimentally 
richly interlinked machine understandable data constitute the basis for the semantic web we provide a framework cream that allows for creation of metadata while the annotation mode of cream allows to create metadata for existing web page the authoring mode let author create metadata almost for free while putting together the content of a page a a particularity of our framework cream allows to create relational metadata i e metadata that instantiate interrelated definition of class in a domain ontology rather than a comparatively rigid template like schema asm dublin core we discus some of the requirement one ha to meet when developing such an ontology based framework e g the integration of a metadata crawler inference service document management and a meta ontology and describe it implementation viz ont o mat a component based ontology driven web page authoring and annotation tool 
in this paper linear multilayer ica lmica is proposed for extracting independent component from quite high dimensional observed signal such a large size natural scene there are two phase in each layer of lmica one is the mapping phase where a one dimensional mapping is formed by a stochastic gradient algorithm which make more highlycorrelated non independent signal be nearer incrementally another is the local ica phase where each neighbor namely highly correlated pair of signal in the mapping is separated by the maxkurt algorithm because lmica separate only the highly correlated pair instead of all one it can extract independent component quite efficiently from appropriate observed signal in addition it is proved that lmica always converges some numerical experiment verify that lmica is quite efficient and effective in large size natural image processing 
many description logic dl combine knowledge representation on an abstract logical level with an interface to concrete domain like number and string with built in predicate such a 
term dependence is a natural consequence of language use it successful representation ha been a long standing goal for information retrieval research we present a methodology for the construction of a concept hierarchy that take into account the three basic dimension of term dependence we also introduce a document evaluation function that allows the use of the concept hierarchy a a user profile for information filtering initial experimental result indicate that this is a promising approach for incorporating term dependence in the way document are filtered 
semi supervised clustering employ a small amount of labeled data to aid unsupervised learning previous work in the area ha utilized supervised data in one of two approach constraint based method that guide the clustering algorithm towards a better grouping of the data and distance function learning method that adapt the underlying similarity metric used by the clustering algorithm this paper provides new method for the two approach a well a present a new semi supervised clustering algorithm that integrates both of these technique in a uniform principled framework experimental result demonstrate that the unified approach produce better cluster than both individual approach a well a previously proposed semi supervised clustering algorithm 
semi structured data such a xml and html is attracting considerable attention it is important to develop various kind of data mining technique that can handle semistructured data in this paper we discus application of kernel method for semistructured data we model semi structured data by labeled ordered tree and present kernel for classifying labeled ordered tree based on their tag structure by generalizing the convolution kernel for parse tree introduced by collins and duy we give algorithm to eciently compute the kernel for labeled ordered tree we also apply our kernel to node marking problem that are special case of information extraction from tree preliminary experiment using artificial data and real html document show encouraging result 
this paper provides a foundation for multi task learning using reproducing kernel hilbert space of vector valued function in this setting the kernel is a matrix valued function some explicit example will be described which go beyond our earlier result in in particular we characterize class of matrixvalued kernel which are linear and are of the dot product or the translation invariant type we discus how these kernel can be used to model relation between the task and present linear multi task learning algorithm finally we present a novel proof of the representer theorem for a minimizer of a regularization functional which is based on the notion of minimal norm interpolation 
this paper investigates the problem of augmenting labeled data with unlabeled data to improve classification accuracy this is significant for many application such a image classification where obtaining classification label is expensive while large unlabeled example are easily available we investigate an expectation maximization em algorithm for learning from labeled and unlabeled data the reason why unlabeled data boost learning accuracy is because it provides the information about the joint probability distribution a theoretical argument show that the more unlabeled example are combined in learning the more accurate the result we then introduce b em algorithm based on the combination of em with bootstrap method to exploit the large unlabeled data while avoiding prohibitive i o cost experimental result over both synthetic and real data set that the proposed approach ha a satisfactory performance 
conventional method used for the interpretation of activation data provided by functional neuroimaging technique provide useful insight on what the network of cerebral structure are and when and how much they activate however they do not explain how the activation of these large scale network derives from the cerebral information processing mechanism involved in cognitive function at this global level of representation the human brain can be considered a a dynamic biological system dynamic bayesian network seem currently the most promising modeling paradigm our modeling approach is based on the anatomical connectivity of cerebral region the information processing within cerebral area and the causal influence that connected region exert on each other the capability of the formalism s current version are illustrated by the modeling of a phonemic categorization process explaining the different cerebral activation in normal and dyslexic subject the simulation data are compared to experimental result ruff et al 
in recent year the technological advance in mapping gene have made it increasingly easy to store and use a wide variety of biological data such data are usually in the form of very long string for which it is difficult to determine the most relevant feature for a classification task for example a typical dna string may be million of character long and there may be thousand of such string in a database in many case the classification behavior of the data may be hidden in the compositional behavior of certain segment of the string which cannot be easily determined apriori another problem which complicates the classification task is that in some case the classification behavior is reflected in global behavior of the string whereas in others it is reflected in local pattern given the enormous variation in the behavior of the string over different data set it is useful to develop an approach which is sensitive to both the global and local behavior of the string for the purpose of classification for this purpose we will exploit the multi resolution property of wavelet decomposition in order to create a scheme which can mine classification characteristic at different level of granularity the resulting scheme turn out to be very effective in practice on a wide range of problem 
estimating the number of people in a crowded environment is acentral task in civilian surveillance most vision based countingtechniques depend on detecting individual in order to count anunrealistic proposition in crowded setting we propose analternative approach that directly estimate the number of people in our system group of image sensor segment foreground objectsfrom the background aggregate the resulting silhouette over anetwork and compute a planar projection of the scene s visualhull we introduce a geometric algorithm that calculates bound onthe number of person in each region of the projection afterphantom region have been eliminated the computationalrequirements scale well with the number of sensor and the numberof people and only limited amount of data are transmitted overthe network because of these property our system run inreal time and can be deployed a an untethered wireless sensornetwork we describe the major component of our system and reportpreliminary experiment with our first prototype implementation 
prior knowledge in the form of multiple polyhedral set each belonging to one of two category is introduced into a reformulation of a linear support vector machine classier the resulting formulation lead to a linear program that can be solved ecien tly real world example from dna sequencing and breast cancer prognosis demonstrate the eectiv ene of the proposed method numerical result show improvement in test set accuracy after the incorporation of prior knowledge into ordinary data based linear support vector machine classiers one experiment also show that a linear classier based solely on prior knowledge far outperforms the direct application of prior knowledge rule to classify data keywords use and renement of prior knowledge support vector machine linear programming 
we present two result which arise from a model based approach to hierarchical agglomerative clustering first we show formally that the common heuristic agglomerative clustering algorithm single link complete link groupaverage and ward s method are each equivalent to a hierarchical model based method this interpretation give a theoretical explanation of the empirical behavior of these algorithm a well a a principled approach to resolving practical issue such a number of cluster or the choice of method second we show how a model based approach can be used to extend these basic agglomerative algorithm we introduce adjusted complete link mahalanobis link and line link a variant of the classical agglomerative method and demonstrate their utility 
in sequence modeling we often wish to represent complex interaction between label such a when performing multiple cascaded labeling task on the same sequence or when long range dependency exist we present dynamic conditional random field dcrfs a generalization of linear chain conditional random field crfs in which each time slice contains a set of state variable and edge a distributed state representation a in dynamic bayesian network dbns and parameter are tied across slice since exact inference can be intractable in such model we perform approximate inference using several schedule for belief propagation including tree based reparameterization trp on a natural language chunking task we show that a dcrf performs better than a series of linear chain crfs achieving comparable performance using only half the training data 
in the multi armed bandit problem an online algorithm must choose from a set of strategy in a sequence of n trial so a to minimize the total cost of the chosen strategy while nearly tight upper and lower bound are known in the case when the strategy set is flnite much le is known when there is an inflnite strategy set here we consider the case when the set of strategy is a subset ofrd and the cost function are continuous in the d case we improve on the best known upper and lower bound closing the gap to a sublogarithmic factor we also consider the case where d and the cost function are convex adapting a recent online convex optimization algorithm of zinkevich to the sparser feedback model of the multi armed bandit problem 
data referring to cultural calendar such a the widespread gregorian date but also date after the chinese hebrew or islamic calendar a well a data referring to professional calendar like fiscal year or teaching term are omnipresent on the web formalism such a xml schema have acknowledged this by offering a rather extensive set of gregorian date and time a basic data type this article introduces into catts the calendar and time type system catts go far beyond predefined date and time type after the gregorian calendar a supported by xml schema catts first give rise to declaratively specify more or le complex cultural or professional calendar including specificity such a leap second leap year and time zone catts further offer a tool for the static type checking of data typed after calendar s defined in catts catts finally offer a language for declaratively expressing and a solver for efficiently solving temporal constraint referring to calendar s expressed in catts catts complement data modeling and reasoning method designed for generic semantic web application such a rdf or owl with method specific to the particular application domain of calendar and time 
anticipating the availability of large questionanswer datasets we propose a principled datadriven instance based approach to question answering most question answering system incorporate three major step classify question according to answer type formulate query for document retrieval and extract actual answer under our approach strategy for answering new question are directly learned from training data we learn model of answer type query content and answer extraction from cluster of similar question we view the answer type a a distribution rather than a class in an ontology in addition to query expansion we learn general content feature from training data and use them to enhance the query finally we treat answer extraction a a binary classification problem in which text snippet are labeled a correct or incorrect answer we present a basic implementation of these concept that achieves a good performance on trec test data 
several unsupervised learning algorithm based on an eigendecomposition provide either an embedding or a clustering only for given training point with no straightforward extension for out of sample example short of recomputing eigenvectors this paper provides algorithm for such an extension for local linear embedding lle isomap laplacian eigenmaps multi dimensional scaling all algorithm which provide lower dimensional embedding for dimensionality reduction a well a for spectral clustering which performs non gaussian clustering these extension stem from a unified framework in which these algorithm are seen a learning eigenfunctions of a kernel lle and isomap pose special challenge a the kernel is training data dependent numerical experiment on real data show that the generalization performed have a level of error comparable to the variability of the embedding algorithm to the choice of training data 
we present an efficient approach to adding soft constraint in the form of preference to disjunctive temporal problem dtps and their subclass temporal constraint satisfaction problem tcsps specifically we describe an algorithm for checking the consistency of and finding optimal solution to such probkms the algorithm borrows concept from previous algorithm for solving tcsps and simple temporal problem with preference stpps in both case using technique for projecting and solving component sub problem we show that adding preference to dtps and tcsps requires only slightly more time than corresponding algorithm for tcsps and dtps without preference thus for problem where dtps and tcsps make sense adding preference provides a substantial gain in expressiveness for a marginal cost 
we present in this paper the problem of discovering set of attribute value pair in high dimensional data set that are of interest not because of co occurrence alone but due to their value in serving a core for potential classifier of cluster we present our algorithm in the context of a gene expression dataset gene expression data in most situation is insufficient for clustering algorithm and any statistical inference because for gene typically only s and at most s of data point become available it is difficult to use statistical technique to design a classifier for such immensely under specified data the observed data though statistically insufficient contains some information about the domain our goal is to discover a much information about all potential classifier a possible from the data and then summarize this knowledge this summarization provides insight into the composition of potential classifier we present here algorithm and method for mining a high dimensional data set exemplified by a gene expression data set for mining such information 
context free grammar cannot be identified in the limit from positive example gold yet natural language grammar are more powerful than context free grammar and human learn them with remarkable ease from positive example marcus identifiability result for formal language ignore a potentially powerful source of information available to learner of natural language namely meaning this paper explores the learnability of syntax i e context free grammar given positive example and knowledge of lexical semantics and the learnability of lexical semantics given knowledge of syntax the long term goal is to develop an approach to learning both syntax and semantics that bootstrap itself using limited knowledge about syntax to infer additional knowledge about semantics and limited knowledge about semantics to infer additional knowledge about syntax 
within this paper a new framework for bayesian tracking ispresented which approximates the posterior distribution atmultiple resolution we propose a tree based representationof the distribution where the leaf define a partition ofthe state space with piecewise constant density the advantageof this representation is that region with low probabilitymass can be rapidly discarded in a hierarchical search and the distribution can be approximated to arbitrary precision we demonstrate the effectiveness of the technique byusing it for tracking d articulated and non rigid motionin front of cluttered background more specifically we areinterested in estimating the joint angle position and orientationof a d hand model in order to drive an avatar 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
the assumption behind linear classifier for categorical data are examined and reformulated in the context of the multinomial manifold the simplex of multinomial model furnished with the riemannian structure induced by the fisher information this lead to a new view of hyperplane classifier which together with a generalized margin concept show how to adapt existing margin based hyperplane model to multinomial geometry experiment show the new classification framework to be effective for text classification where the categorical structure of the data is modeled naturally within the multinomial family 
we apply the replica method of statistical physic combined with a variational method to the approximate analytical computation of bootstrap average for estimating the generalization error we demonstrate our approach on regression with gaussian process and compare our result with average obtained by monte carlo sampling 
the growth of the web ha posed new challenge for information retrieval ir most of the current system are based on traditional model which have been developed for atomic and independent document and are not adapted to the web a promising research orientation consists of studying the impact of the web structure on indexing the hyperdocument model presented in this article is based on essential aspect of information comprehension content composition and linear non linear reading 
one significant challenge in the construction of visualdetection system is the acquisition of sufficient labeleddata this paper describes a new technique for trainingvisual detector which requires only a small quantity of labeleddata and then us unlabeled data to improve performanceover time unsupervised improvement is based onthe co training framework of blum and mitchell in whichtwo disparate classifier are trained simultaneously unlabeledexamples which are confidently labeled by one classifierare added with label to the training set of the otherclassifier experiment are presented on the realistic task ofautomobile detection in roadway surveillance video in thisapplication co training reduces the false positive rate by afactor of to from the classifier trained with labeled dataalone 
this paper compare document blind feedback and passage blind feedback in information retrieval ir based on the work during the nrrc reliable information access summer workshop the analysis of our experimental result show overall consistency on the performance impact of using passage and document for blind feedback however it is observed that the behavior of passage blind feedback compared to document blind feedback is both system dependent and topic dependent the relationship between the performance impact of passage blind feedback and the number of feedback term and the topic s average relevant document length respectively are examined to illustrate these dependency 
recently mining data stream with concept drift for actionable insight ha become an important and challenging task for a wide range of application including credit card fraud protection target marketing network intrusion detection etc conventional knowledge discovery tool are facing two challenge the overwhelming volume of the streaming data and the concept drift in this paper we propose a general framework for mining concept drifting data stream using weighted ensemble classifier we train an ensemble of classification model such a c ripper naive beyesian etc from sequential chunk of the data stream the classifier in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time evolving environment thus the ensemble approach improves both the efficiency in learning the model and the accuracy in performing classification our empirical study show that the proposed method have substantial advantage over single classifier approach in prediction accuracy and the ensemble framework is effective for a variety of classification model 
this paper explores the computational consequence of simultaneous intrinsic and synaptic plasticity in individual model neuron it proposes a new intrinsic plasticity mechanism for a continuous activation model neuron based on low order moment of the neuron s ring rate distribution the goal of the intrinsic plasticity mechanism is to enforce a sparse distribution of the neuron s activity level in conjunction with hebbian learning at the neuron s synapsis the neuron is shown to discover sparse direction in the input we consider an individual continuous activation model neuron with a non linear transfer function that ha adjustable parameter we are proposing a simple intrinsic learning mechanism based on estimate of low order moment of the activity distribution that allows the model neuron to adjust the parameter of it non linear transfer function to obtain an approximately exponential distribution of it activity we then show that if combined with a standard hebbian learning rule employing multiplicative weight normalization this lead to the extraction of sparse feature from the input this is in sharp contrast to standard hebbian learning in linear unit with multiplicative weight normalization which lead to 
many technique in the social science and graph theory deal with the problem of examining and analyzing pattern found in the underlying structure and association of a group of entity however much of this work assumes that this underlying structure is known or can easily be inferred from data which may often be an unrealistic assumption for many real world problem below we consider the problem of learning and querying a graph based model of this underlying structure the model is learned from noisy observation linking set of entity we explicitly allow different type of link representing different type of relation and temporal information indicating when a link wa observed we quantitatively compare this representation and learning method against other algorithm on the task of predicting future link and new friendship in a variety of real world data set 
data on individual and entity are being collected widely these data can contain information that explicitly identifies the individual e g social security number data can also contain other kind of personal information e g date of birth zip code gender that are potentially identifying when linked with other available data set data are often shared for business or legal reason this paper address the important issue of preserving the anonymity of the individual or entity during the data dissemination process we explore preserving the anonymity by the use of generalization and suppression on the potentially identifying portion of the data we extend earlier work in this area along various dimension first satisfying privacy constraint is considered in conjunction with the usage for the data being disseminated this allows u to optimize the process of preserving privacy for the specified usage in particular we investigate the privacy transformation in the context of data mining application like building classification and regression model second our work improves on previous approach by allowing more flexible generalization for the data lastly this is combined with a more thorough exploration of the solution space using the genetic algorithm framework these extension allow u to transform the data so that they are more useful for their intended purpose while satisfying the privacy constraint 
web link analysis ha proven to be a significant enhancement for quality based web search most existing link can be classified into two category intra type link e g web hyperlink which represent the relationship of data object within a homogeneous data type web page and inter type link e g user browsing log which represent the relationship of data object across different data type user and web page unfortunately most link analysis research only considers one type of link in this paper we propose a unified link analysis framework called link fusion which considers both the interand intratype link structure among multiple type inter related data object and brings order to object in each data type at the same time the pagerank and hit algorithm are shown to be special case of our unified link analysis framework experiment on an instantiation of the framework that make use of the user data and web page extracted from a proxy log show that our proposed algorithm could improve the search effectiveness over the hit and directhit algorithm by and respectively 
distance based method in machine learning and pattern recognition have to rely on a metric distance between point in the input space instead of specifying a metric a priori we seek to learn the metric from data via kernel method and multidimensional scaling md technique under the classification setting we define discriminant kernel on the joint space of input and output space and present a specific family of discriminant kernel this family of discriminant kernel is attractive because the induced metric are euclidean and fisher separable and md technique can be used to find the lowdimensional euclidean representation also called feature vector of the induced metric since the feature vector incorporate information from both input point and their corresponding label and they enjoy fisher separability they are appropriate to be used in distance based classifier 
abstract we present an analytic solution to the problem of estimating multiple d and d motion model from two view correspondence or optical flow the key to our approach is to view the estimation of multiple motion model a the estimation of a single multibody motion model this is possible thanks to two im portant algebraic fact first we show that all the image measurement regardless of their associated motion model can be fit with a real or complex polynomial second we show that the parameter of the motion model associated with an im age measurement can be obtained from the derivative of the polynomial at the measurement this lead to a novel motion segmentation algorithm that applies to most of the two view motion model adopted in computer vision our experi ments show that the proposed algorithm outperforms existing algebraic method in term of efficiency and robustness and provides a good initialization for itera tive technique such a em which is strongly dependent on correct initialization 
in this paper we propose a new approach for topic distillation on world wide web topic distillation is to find quality document related to the user query topic our approach is based on bharat s topic distillation algorithm we present the analysis of hyperlink graph structure using hierarchy concept tree to solve the mixed hub problem that is also remained in the bharat s algorithm for assigning better weight to hyperlink which point to relevant document among hyperlink in a document we try to find the relationship in document connected by hyperlink using content analysis and we assign weight to hyperlink based on the relationship we evaluated this algorithm using topic on wt g corpus and obtained improved result 
to analyze the effect of the ocean and atmosphere on land climate earth scientist have developed climate index which are time series that summarize the behavior of selected region of the earth s ocean and atmosphere in the past earth scientist have used observation and more recently eigenvalue analysis technique such a principal component analysis pca and singular value decomposition svd to discover climate index however eigenvalue technique are only useful for finding a few of the strongest signal furthermore they impose a condition that all discovered signal must be orthogonal to each other making it difficult to attach a physical interpretation to them this paper present an alternative clustering based methodology for the discovery of climate index that overcomes these limitiations and is based on cluster that represent region with relatively homogeneous behavior the centroid of these cluster are time series that summarize the behavior of the ocean or atmosphere in those region some of these centroid correspond to known climate index and provide a validation of our methodology other centroid are variant of known index that may provide better predictive power for some land area and still other index may represent potentially new earth science phenomenon finally we show that cluster based index generally outperform svd derived index both in term of area weighted correlation and direct correlation with the known index 
estimation of gaussian mixture model is an ecien t and popular technique for clustering and density estimation an em procedure is widely used to estimate the model parameter in this paper we show how side information in the form of equivalence constraint can be incorporated into this procedure leading to improved clustering result equivalence constraint are prior knowledge concerning pair of data point indicating if the point arise from the same source positive constraint or from dieren t source negative constraint such constraint can be gathered automatically in some learning problem and are a natural form of supervision in others we present a closed form em procedure for handling positive constraint and a generalized em procedure using a markov net for the incorporation of negative constraint using publicly available data set we demonstrate that such side information may lead to considerable improvement in clustering task and that our algorithm is preferable to another suggested method using this type of side information 
abstract density estimation with gaussian mixture model is a popular gener ative technique used also for clustering we develop a framework to incorporate side information in the form of equivalence constraint into the model estimation procedure equivalence constraint are defined on pair of data point indicating whether the point arise from the same source positive constraint or from different source negative con straints such constraint can be gathered automatically in some learn ing problem and are a natural form of supervision in others for the estimation of model parameter we present a closed form em procedure which handle positive constraint and a gene 
in mixed reality especially in augmented virtuality whichvirtualizes real object it is important to estimate objectsurface reflectance property to render the object underarbitrary illumination condition though several method have beenexplored to estimate the surface reflectance property it isstill difficult to estimate surface reflectance parametersfaithfully for complex object which have non uniform surfacereflectance property and exhibitinter reflection this paperdescribes a new method for densely estimating non uniform surfacereflectance property of real object constructed of convex andconcave surface with interreflection we use registered range andsurface color texture image obtained by a laser range finder experiment show the usefulness of the proposed method 
abstract there are several reinforcement learning algorithm that yield approximatesolutions for the problem of policy evaluation when thevalue function is represented with a linear function approximator 
in this paper we consider the image taken from pair ofparabolic catadioptric camera separated by discrete motion despite the nonlinearity of the projection model theepipolar geometry arising from such a system like the perspectivecase can be encoded in a bilinear form the catadioptricfundamental matrix we show that all such matriceshave equal lorentzian singular value and they definea nine dimensional manifold in the space of matrix furthermore this manifold can be identified with a quotientof two lie group we present a method to estimate a matrixin this space so a to obtain an estimate of the motion we show that the estimation procedure are robust to modestdeviations from the ideal assumption 
wireless access with mobile or handheld device is a promising addition to the www and traditional electronic business mobile device provide convenience and portable access to the huge information space on the internet without requiring user to be stationary with network connection however the limited screen size narrow network bandwidth small memory capacity and low computing power are the shortcoming of handheld device loading and visualizing large document on handheld device become impossible the limited resolution restricts the amount of information to be displayed the download time is intolerably long in this paper we introduce the fractal summarization model for document summarization on handheld device fractal summarization is developed based on the fractal theory it generates a brief skeleton of summary at the first stage and the detail of the summary on different level of the document are generated on demand of user such interactive summarization reduces the computation load in comparing with the generation of the entire summary in one batch by the traditional automatic summarization which is ideal for wireless access three tier architecture with the middle tier conducting the major computation is also discussed visualization of summary on handheld device is also investigated 
we apply a decision tree based approach to pronoun resolution in spoken dialogue our system deal with pronoun with np and non np antecedent we present a set of feature designed for pronoun resolution in spoken dialogue and determine the most promising feature we evaluate the system on twenty switchboard dialogue and show that it compare well to byron s manually tuned system 
we investigate how user interact with the result page of a www search engine using eye tracking the goal is to gain insight into how user browse the presented abstract and how they select link for further exploration such understanding is valuable for improved interface design a well a for more accurate interpretation of implicit feedback e g clickthrough for machine learning the following present initial result focusing on the amount of time spent viewing the presented abstract the total number of abstract viewed a well a measure of how thoroughly searcher evaluate their result set 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we aim to define an event ontology that allows natural representation of complex spatio temporal event common in the physical world by a composition of simpler event the event are abstracted into three hierarchy primitive event are defined directly from the mobile object property single thread composite event are a number of primitive event with temporal sequencing multi thread composite event are a number of single thread event with temporal spatial logical relationship this hierarchical event representation naturally lead to a language description of the event we define an event recognition language erl which allows the user to define the event of interest conveniently without interacting with the low level processing in the program we will also briefly mention some approach to compute the proposed representation 
privacy is an important issue in data mining and knowledge discovery in this paper we propose to use the randomized response technique to conduct the data mining computation specially we present a method to build decision tree classifier from the disguised data we conduct experiment to compare the accuracy of our decision tree with the one built from the original undisguised data our result show that although the data are disguised our method can still achieve fairly high accuracy we also show how the parameter used in the randomized response technique affect the accuracy of the result 
the exponential growth of data demand scalable infrastructure capable of indexing and searching rich content such a text music and image a promising direction is to combine information re trieval with peer to peer technology for scalability fault tolerance and low administration cost one pioneering work along this di rection is psearch psearch place document onto a peer topeer overlay network according to semantic vector produced using latent semantic indexing lsi the search cost for a query is reduced since document related to the query are likely to be co located on a small number of node unfortunately because of it reliance on lsi psearch also inherits the limitation of lsi when the corpus is large and heterogeneous lsi s retrieval quality is inferior to method such a okapi the singular value decomposition svd used in lsi is unscalable in term of both memory consumption and computation time this paper address the above limitation of lsi and make the following contribution to reduce the cost of svd we reduce the size of it input matrix through document clustering and term selection our method retains the retrieval quality of lsi but is several order of magnitude more efficient through extensive experimentation we found that proper normalization of semantic vector for term and document improves recall by to further improve retrieval quality we use low dimensional subvectors of semantic vector to cluster document in the overlay and then use okapi to guide the search and document selection 
this paper evaluates an extraction based approach to answering definitional question our system extracted useful linguistic construct called linguistic feature from raw text using information extraction tool and formulated answer based on such feature the feature employed include appositives copula structured pattern relation proposition and raw sentence the feature were ranked based on feature type and similarity to a question profile redundant feature were detected using a simple heuristic based strategy the approach achieved state of the art performance at the trec qa evaluation component analysis of the system wa carried out using an automatic scoring function called rouge lin and hovy major finding include answer using linguistic feature are significantly better than those using raw sentence the most useful feature are appositives and copula question profile a a mean of modeling user interest can significantly improve system performance the rouge score are closely correlated with subjective evaluation result indicating the suitability of using rouge for evaluating definitional qa system 
coalition formation is a key problem in automated negotiation among self interested agent in order for coalition formation to be successful a key question that must be answered is how the gain from cooperation are to be distributed various solution concept have been proposed but the computational question around these solution concept have received little attention we study a concise representation of characteristic function which allows for the agent to be concerned with a number of independent issue that each coalition of agent can address for example there may be a set of task that the capacity unconstrained agent could undertake where accomplishing a task generates a certain amount of value possibly depending on how well the task is accomplished given this representation we show how to quickly compute the shapley value a seminal value division scheme that distributes the gain from cooperation fairly in a certain sense we then show that in distributed marginal contribution based value division scheme which are known to be vulnerable to manipulation of the order in which the agent are added to the coalition this manipulation is np complete thus computational complexity serf a a barrier to manipulating the joining order finally we show that given a value division determining whether some subcoalition ha an incentive to break away in which case we say the division is not in the core is np complete so computational complexity serf to increase the stability of the coalition 
many collaborative music recommender system cmrs have succeeded in capturing the similarity among user or item based on rating however they have rarely considered about the available information from the multimedia such a genre let alone audio feature from the medium stream such information is valuable and can be used to solve several problem in r in this paper we design a cmrs based on audio feature of the multimedia stream in the cmrs we provide recommendation service by our proposed method where a clustering technique is used to integrate the audio feature of music into the collaborative filtering cf framework in hope of achieving better performance experiment are carried out to demonstrate that our approach is feasible 
in a categorized information space predicting user information need at the category level can facilitate personalization caching and other topic oriented service this paper present a two phase model to predict the category of a user s next access based on previous access phase generates a snapshot of a user s preference among category based on a temporal and frequency analysis of the user s access history phase us the computed preference to make prediction at different category granularity several alternative for each phase are evaluated using the rating behavior of on line raters a the form of access considered the result show that a method based on re access pattern and frequency analysis of a user s whole history ha the best prediction quality even over a path based method markov model that us the combined history of all user 
automating the scheduling of sport league ha received considerable attention in recent year a these application involve significant revenue and generate challenging combinatorial optimization problem this paper considers the traveling tournament problem ttp which abstract the salient feature of major league baseball mlb in the united state it proposes a simulated annealing algorithm ttsa for the ttp that explores both feasible and infeasible schedule us a large neighborhood with complex move and includes advanced technique such a strategic oscillation and reheats to balance the exploration of the feasible and infeasible region and to escape local minimum at very low temperature ttsa match the best known solution on the small instance of the ttp and produce significant improvement over previous approach on the larger instance moreover ttsa is shown to be robust because it worst solution quality over run is always smaller or equal to the best known solution 
the benzodiazepine midazolam cause dense but temporary anterograde amnesia similar to that produced by hippocampal damage doe the action of midazolam on the hippocampus cause le storage or le accurate storage of information in episodic long term memory we used a simple variant of the rem model to fit data collected by hirshman fisher henthorn arndt and passannante on the effect of midazolam study time and normative word frequency on both yes no and remember know recognition memory that a simple strength model fit well wa contrary to the expectation of hirshman et al more important within the bayesian based rem modeling framework the data were consistent with the view that midazolam cause le accurate storage rather than le storage of information in episodic memory 
nested dichotomy are a standard statistical technique for tackling certain polytomous classification problem with logistic regression they can be represented a binary tree that recursively split a multi class classification task into a system of dichotomy and provide a statistically sound way of applying two class learning algorithm to multi class problem assuming these algorithm generate class probability estimate however there are usually many candidate tree for a given problem and in the standard approach the choice of a particular tree is based on domain knowledge that may not be available in practice an alternative is to treat every system of nested dichotomy a equally likely and to form an ensemble classifier based on this assumption we show that this approach produce more accurate classification than applying c and logistic regression directly to multi class problem our result also show that ensemble of nested dichotomy produce more accurate classifier than pairwise classification if both technique are used with c and comparable result for logistic regression compared to error correcting output code they are preferable if logistic regression is used and comparable in the case of c an additional benefit is that they generate class probability estimate consequently they appear to be a good general purpose method for applying binary classifier to multi class problem 
the need for natural language interface to database nlis ha become increasingly acute a more and more people access information through their web browser pda and cell phone yet nlis are only usable if they map natural language question to sql query correctly people are unwilling to trade reliable and predictable user interface for intelligent but unreliable one we describe a reliable nli precise that incorporates a modern statistical paser and a semantic module precise provably handle a large class of natural language question correctly on the benchmark atis data set precise achieves accuracy 
in this paper we propose a bayesian framework which construct shared state triphone hmms based on a variational bayesian approach and recognizes speech based on the bayesian prediction classification variational bayesian estimation and clustering for speech recognition vbec an appropriate model structure with high recognition performance can be found within a vbec framework unlike conventional method including bic or mdl criterion based on the maximum likelihood approach the proposed model selection is valid in principle even when there are insufficient amount of data because it doe not use an asymptotic assumption in isolated word recognition experiment we show the advantage of vbec over conventional method especially when dealing with small amount of data 
four statistical visual feature index are proposed slm shot length mean the average length of each shot in a video sld shot length deviation the standard deviation of shot length for a video onm object number mean the average number of object per frame of the video and ond object number deviation the standard deviation of the number of object per frame across the video each of these index provides a unique perspective on video content a novel video retrieval interface ha been developed a a platform to examine our assumption that the new index facilitate some video retrieval task initial feedback is promising and formal experiment are planned 
obtaining an accurate multiple alignment of protein sequence is a difficult computational problem for which many heuristic technique sacrifice optimality to achieve reasonable running time the most commonly used heuristic is progressive alignment which merges sequence into a multiple alignment by pairwise comparison along the node of a guide tree to improve accuracy consistency based method take advantage of conservation across many sequence to provide a stronger signal for pairwise comparison in this paper we introduce the concept of probabilistic consistency for multiple sequence alignment we also present probcons an hmm based protein muhiple sequence aligner based on an approximation of the probabilistic consistency objective function on the balibase benchmark alignment database probcons demonstrates a statistically significant improvement in accuracy compared to several leading alignment program while maintaining practical running time source code and program update are freely available under the gnu public license at http probcons stanford edu 
computer animated agent and robot bring a social dimension to human computer interaction and force u to think in new way about how computer could be used in daily life face to face communication is a real time process operating at a time scale of le than a second in this paper we present progress on a perceptual primitive to automatically detect frontal face in the video stream and code them with respect to dimension in real time neutral anger disgust fear joy sadness surprise the face finder employ a cascade of feature detector trained with boosting technique the expression recognizer employ a novel combination of adaboost and svm s the generalization performance to new subject for a way forced choice wa and correct on two publicly available datasets the output of the classifier change smoothly a a function of time providing a potentially valuable representation to code facial expression dynamic in a fully automatic and unobtrusive manner the system wa deployed and evaluated for measuring spontaneous facial expression in the field in an application for automatic assessment of human robot interaction computer animated agent and robot bring a social dimension to human computer interaction and force u to think in new way about how computer could be used in daily life face to face communication is a real time process operating at a time scale of le than a second thus fulfilling the idea of machine that interact face to face with u requires development of robust real time perceptive primitive in this paper we present first step towards the development of one such primitive a system that automatically find face in the visual video stream and code facial expression dynamic in real time the system automatically detects frontal face and code them with respect to dimension joy sadness surprise anger disgust fear and neutral speed and accuracy are enhanced by a novel technique that combine feature selection based on adaboost with feature integration based on support vector machine we host an online demo of the system at http mplab ucsd edu 
we discus an idea for collecting data in a relatively efficient manner our point of view is bayesian and information theoretic on any given trial we want to adaptively choose the input in such a way that the mutual information between the unknown state of the system and the stochastic output is maximal given any prior information including data collected on any previous trial we prove a theorem that quantifies the effectiveness of this strategy and give a few illustrative example comparing the performance of this adaptive technique to the more usual nonadaptive experimental design 
we describe an efficient robust method for selecting and optimizing term for a classification or filtering task term are extracted from positive example in training data based on several alternative term selection algorithm then combined additively after a simple term score normalization step to produce a merged and ranked master term vector the score threshold for the master vector is set via beta gamma regulation over all the available training data the process avoids para meter calibration and protracted training it also result in compact profile for run time evaluation of test new document result on trec filtering task datasets demonstrate substantial improvement over trec median result and rival both idealized ir based result and optimized and expensive svm based classifier in general effectiveness 
in this paper we focus on the adaptation of boosting to grammatical inference we aim at improving the performance of state merging algorithm in the presence of noisy data by using in the update rule additional information provided by an oracle this strategy requires the construction of a new weighting scheme that take into account the confidence in the label of the example we prove that our new framework preserve the theoretical property of boosting using the state merging algorithm rpni we describe an experimental study on various datasets showing a dramatic improvement of performance 
in the web extractor agent process class of page like call for paper page researcher page etc neglecting the relevant fact that some of them are interrelated forming cluster e g science we propose here an architecture for cognitive multi agent system to retrieve and classify page from these cluster based on data extraction to enable cooperation two design requirement are crucial a a web vision coupling a vision for content class and attribute to be extracted to a functional vision the role of page in information presentation b explicit representation of agent knowledge and ability in the form of ontology both about the cluster s domain and agent task employing this web vision and agent cooperation can accelerate the retrieval of useful page we got encouraging result with two agent for the page class of scientific event and article a comparison of result to similar system come up with two requirement for such system functional categorization and a thoroughly detailed ontology of the cluster 
this paper give an algorithm for detecting and reading text in natural image the algorithm is intended for use by blind and visually impaired subject walking through city scene we first obtain a dataset of city image taken by blind and normally sighted subject from this dataset we manually label and extract the text region next we perform statistical analysis of the text region to determine which image feature are reliable indicator of text and have low entropy i e feature response is similar for all text image we obtain weak classifier by using joint probability for feature response on and off text these weak classifier are used a input to an adaboost machine learning algorithm to train a strong classifier in practice we trained a cascade with strong classifier containg feature an adaptive binarization and extension algorithm is applied to those region selected by the cascade classifier an commercial ocr software is used to read the text or reject it a a non text region the overall algorithm ha a success rate of over evaluated by complete detection and reading of the text on the test set and the unread text is typically small and distant from the viewer 
a standard method for approximating average in probabilistic model is to construct a markov chain in the product space of the random variable with the desired equilibrium distribution since the number of configuration in this space grows exponentially with the number of random variable we often need to represent the distribution with sample in this paper we show that if one is interested in average over single variable only an alternative markov chain defined on the much smaller union space which can be evolved exactly becomes feasible the transition kernel of this markov chain is based on conditional distribution for pair of variable and we present way to approximate them using approximate inference algorithm such a mean field factorized neighbor and belief propagation robustness to these approximation and error bound on the estimate follow from stability analysis for markov chain we also present idea on a new class of algorithm that iterate between increasingly accurate estimate for conditional and marginal distribution experiment validate the proposed method 
the ability to find table and extract information from them is a necessary component of data mining question answering and other information retrieval task document often contain table in order to communicate densely packed multi dimensional information table do this by employing layout pattern to efficiently indicate field and record in two dimensional form their rich combination of formatting and content present difficulty for traditional language modeling technique however this paper present the use of conditional random field crfs for table extraction and compare them with hidden markov model hmms unlike hmms crfs support the use of many rich and overlapping layout and language feature and a a result they perform significantly better we show experimental result on plain text government statistical report in which table are located with f and their constituent line are classified into table related category with accuracy we also discus future work on undirected graphical model for segmenting column finding cell and classifying them a data cell or label cell 
we use clustering to derive new relation which augment database schema used in automatic generation of predictive feature in statistical relational learning entity derived from cluster increase the expressivity of feature space by creating new first class concept which contribute to the creation of new feature for example in citeseer paper can be clustered based on word or citation giving topic and author can be clustered based on document they co author giving community such cluster derived concept become part of more complex feature expression out of the large number of generated feature those which improve predictive accuracy are kept in the model a decided by statistical feature selection criterion we present result demonstrating improved accuracy on two task venue prediction and link prediction using citeseer data 
we propose a new set of criterion for learning algorithm in multi agent system one that is more stringent and we argue better justified than previous proposed criterion our criterion which apply most straightforwardly in repeated game with average reward consist of three requirement a against a specified class of opponent this class is a parameter of the criterion the algorithm yield a payoff that approach e the payoff of the best response b against other opponent the algori thm s payoff at least approach and possibly exceed the security level payoff or maximin value and c subject to these requirement the algo rithm achieve a close to optimal payoff in self play we furthermore require that these average payoff be achieved quickly we then present a novel algorithm and show that it meet these new criterion for a particular par ameter class the class of stationary opponent finally we show that the algorithm is effective not only in theory but also empirically using a recently introduced comprehensive game theoretic test suite we show that the algorithm almost universally outperforms previous learning algorithm 
much work in information retrieval focus on using a model of document and query to derive retrieval algorithm model based development is a useful alternative to heuristic development because in a model the assumption are explicit and can be examined and refined independent of the particular retrieval algorithm we explore the explicit assumption underlying the na ve framework by performing computational analysis of actual corpus and query to devise a generative document model that closely match text our thesis is that a model so developed will be more accurate than existing model and thus more useful in retrieval a well a other application we test this by learning from a corpus the best document model we find the learned model better predicts the existence of text data and ha improved performance on certain ir task 
in this paper we present two way to improve the precision of hit based algorithm on web document first by analyzing the limitation of current hit based algorithm we propose a new weighted hit based method that assigns appropriate weight to in link of root document then we combine content analysis with hit based algorithm and study the effect of four representative relevance scoring method vsm okapi tl and cdr using a set of broad topic query our experimental result show that our weighted hit based method performs significantly better than bharat s improved hit algorithm when we combine our weighted hit based method or bharat s hit algorithm with any of the four relevance scoring method the combined method are only marginally better than our weighted hit based method between the four relevance scoring method there is no significant quality difference when they are combined with a hit based algorithm 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
the scrubber system monitor problem in the local loop of the telephonenetwork making automated decision on ten of million of case a year many of which lead to automated action scrubber save bell atlantic millionsof dollar annually by reducing the number of inappropriate technician dispatch scrubber s core knowledge base the trouble isolation module tim is a probability estimation tree constructed via several data mining process tim currently is deployed in the 
in this paper we present result of a study on brain computer interfacing we adopted an approach of farwell donchin which we tried to improve in several aspect the main objective wa to improve the transfer rate based on offline analysis of eeg data but within a more realistic setup closer to an online realization than in the original study the objective wa achieved along two different track on the one hand we used state of the art machine learning technique for signal classification and on the other hand we augmented the data space by using more electrode for the interface for the classification task we utilized svms and a motivated by recent finding on the learning of discriminative density we accumulated the value of the classification function in order to combine several classification which finally lead to significantly improved rate a compared with technique applied in the original work in combination with the data space augmentation we achieved competitive transfer rate at an average of bit min and with a maximum of bit min 
learning in a multiagent system is a challenging problem due to two key factor first if other agent are simultaneously learning then the environment is no longer stationary thus undermining convergence guarantee second learning is often susceptible to deception where the other agent may be able to exploit a learner s particular dynamic in the worst case this could result in poorer performance than if the agent wa not learning at all these challenge are identifiable in the two most common evaluation criterion for multiagent learning algorithm convergence and regret algorithm focusing on convergence or regret in isolation are numerous in this paper we seek to address both criterion in a single algorithm by introducing giga wolf a learning algorithm for normalform game we prove the algorithm guarantee at most zero average regret while demonstrating the algorithm converges in many situation of self play we prove convergence in a limited setting and give empirical result in a wider variety of situation these result also suggest a third new learning criterion combining convergence and regret which we call negative non convergence regret nnr 
we seek insight into latent semantic indexing by establishing a method to identify the optimal number of factor in the reduced matrix for representing a keyword this method is demonstrated empirically by duplicating all document containing a term t and inserting new document in the database that replace t with t by examining the number of time term t is identified for a search on term t precision using differing range of dimension we find that lower ranked dimension identify related term and higher ranked dimension discriminate between the synonym 
computer system are rapidly becoming so complex that maintaining them with human support staff will be prohibitively expensive and inefficient in response visionary have begun proposing that computer system be imbued with the ability to configure themselves diagnose failure and ultimately repair themselves in response to these failure however despite convincing argument that such a shift would be desirable a of yet there ha been little concrete progress made towards this goal we view these problem a fundamentally machine learning challenge hence this article present a new network simulator designed to study the application of machine learning method from a system wide perspective we also introduce learning based method for addressing the problem of job routing and scheduling in the network we simulate our experimental result verify that method using machine learning outperform heuristic and hand coded approach on an example network designed to capture many of the complexity that exist in real system 
in order to investigate the deep structure of gaussian scale space image one need to understand the behaviour of critical point under the influence of parameter driven blurring during this evolution two different type of special point are encountered the so called scale space saddle and the catastrophe point the latter describing the pairwise annihilation and creation of critical point the mathematical framework of catastrophe theory is used to model nongeneric event that might occur due to e g local symmetry in the image it is shown how this knowledge can be exploited in conjunction with the scale space saddle point yielding a scale space hierarchy tree that can be used for segmentation furthermore the relevance of creation of pair of critical point with respect to the hierarchy is discussed we clarify the theory with an artificial image and a simulated mr image 
web page classification is much more difficult than pure text classification due to a large variety of noisy information embedded in web page in this paper we propose a new web page classification algorithm based on web summarization for improving the accuracy we first give empirical evidence that ideal web page summary generated by human editor can indeed improve the performance of web page classification algorithm we then propose a new web summarization based classification algorithm and evaluate it along with several other state of the art text summarization algorithm on the looksmart web directory experimental result show that our proposed summarization based classification algorithm achieves an approximately improvement a compared to pure text based classification algorithm we further introduce an ensemble classifier using the improved summarization algorithm and show that it achieves about improvement over pure text based method 
pervasive computing aim to build an aggregated environment around a user by knitting diverse computing and communicating device and software service into a single homogeneous unit our work is to develop a pervasive computing framework which harness the power of semantic web and web service facilitating the development of effective and intelligent pervasive environment this paper present a high level view of the framework and how different pervasive service can be built on this framework 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
current model of the classification problem do not effectively handle burst of particular class coming in at different time in fact the current model of the classification problem simply concentrate on method for one pas classification modeling of very large data set our model for data stream classification view the data stream classification problem from the point of view of a dynamic approach in which simultaneous training and testing stream are used for dynamic classification of data set this model reflects real life situation effectively since it is desirable to classify test stream in real time over an evolving training and test stream the aim here is to create a classification system in which the training model can adapt quickly to the change of the underlying data stream in order to achieve this goal we propose an on demand classification process which can dynamically select the appropriate window of past training data to build the classifier the empirical result indicate that the system maintains a high classification accuracy in an evolving data stream while providing an efficient solution to the classification task 
intelligence agency are under increasing pressure to connect the dot between fragment of evidence from disparate source to enable preemption of potential threat such a terrorist attack most system for threat detection in use today provide only data visualization tool for manual link analysis leading to method that do not scale to massive data set the cadre system continuous analysis and discovery from relational evidence address this deficiency by automating the link analysis process cadre combine an expressive knowledge representation of threat pattern with efficient constraint based abductive reasoning algorithm to automatically infer link and construct coherent threat hypothesis from structured data a compact factored representation of multiple hypothesis avoids redundant storage and enables scaling to large data set cadre efficiently manages the growth of the hypothesis using probabilistic evaluation model and a consistency checking algorithm to prune unlikely hypothesis 
it is widely recognized that distributed denial of service ddos attack can disrupt electronic commerce and cause large revenue loss however effective defense continue to be mostly unavailable we describe and evaluate vipnet a novel value added network service for protecting e commerce and other transaction based site from ddos attack in vipnet e merchant pay internet service provider isps to carry the packet of the e merchant best client called vip in a privileged class of service co protected from congestion whether malicious or not in the regular co vipnet reward vip with not only better quality of service but also greater availability because vip right are clientand server specific cannot be forged are usage limited and are only replenished after successful client transaction e g purchase it is impractical for attacker to mount and sustain ddos attack against an e merchant s vip vipnet can be deployed incrementally and doe not require universal adoption experiment demonstrate vipnet s benefit 
arabic morphology represents a special type of morphological system it is generally considered to be of the nonconcatenative type which depends on manipulating root letter in a nonconcatenative manner in addition to prefixation and suffixation inflectional and derivational process may cause stem to undergo infixational modification in the presence of different syntactic feature a well a certain stem consonant the basic problem then is the large number of variant that must be analyzed or generated in this paper we seek to reduce the complexity of arabic morphology using the lexeme based morphology theory to represent the linguistic resource and morphe a a computational tool to implement them we show that the space of rule can be kept small if we consider the stem a the phonological domain of realisation rule the reduction in the number of rule keep the system small and also increase it understandability and maintainability we primarily focus on generation of verb and broken plural 
in p manolios and j moore show that a tail recursive dening equation for a new function can always be consistently added to acl this is done by constructing a function that satises the proposed tail recursive dening equation their construction is extended to many primitive recursive dening equation this extends the known recursive scheme that can be consistently introduced into acl s logic exactly what is meant by primitive recursive and the exact restriction placed on the denitions are explained below 
in this poster we incorporate user query history a context information to improve the retrieval performance in interactive retrieval experiment using the trec data show that incorporating such context information indeed consistently improves the retrieval performance in both average precision and precision at document 
in many medical computer vision task the relevant data isattached to a specific tissue such a the colon or the cortex this situation call for regularization technique whichare defined over surface we introduce in this paper thebeltrami flow over implicit manifold this new regularizationtechnique overcomes the over smoothing of the l flow and the staircasing effect of the l flow that wererecently suggested via the harmonic map method the keyof our approach is first to clarify the link between the intrinsic polyakov action and the implicit harmonic energyfunctional and then use the geometrical understanding ofthe beltrami flow to generalize it to image on implicitlydefined non flat surface it is shown that once again thebeltrami flow interpolates between the l and l flow onnon flat surface the implementation scheme of this flowis presented and various experimental result obtained on aset of various real image illustrate the performance of theapproach a well a the difference with the harmonic mapflows this extension of the beltrami flow to the case of nonflat surface open new perspective in the regularization ofnoisy data defined on manifold 
the equivalent kernel is a way of understanding how gaussian process regression work for large sample size based on a continuum limit in this paper we show how to approximate the equivalent kernel of the widely used squared exponential or gaussian kernel and related kernel and how analysis using the equivalent kernel help to understand the learning curve for gaussian process 
much of the work in machine learning ha focused on demonstrating the efficacy of learning technique using training and testing phase on line learning over the long term place different demand on symbolic machine learning technique and raise a different set of question for symbolic learning than for empirical learning we have instrumented soar to collect data and characterize the long term learning behavior of soar and demonstrate an effective approach to the utility problem in this paper we describe our approach and provide result 
we present a new approach to estimating mixture model based on a new inference principle we have proposed the latent maximum entropy principle lme lme is different both from jaynes maximum entropy principle and from standard maximum likelihood estimation we demonstrate the lme principle by deriving new algorithm for mixture model estimation and show how robust new variant of the em algorithm can be developed our experiment show that estimation based on lme generally yield better result than maximum likelihood estimation particularly when inferring latent variable model from small amount of data 
xml is the w c standard document format for writing and exchanging information on the web rdf is the w c standard model for describing the semantics and reasoning about information on the web unfortunately rdf and xml although very close to each other are based on two different paradigm we argue that in order to lead the semantic web to it full potential the syntax and the semantics of information need to work together to this end we develop a model theoretic semantics for the xml xquery and xpath data model which provides a unified model for both xml and rdf this unified model can serve a the basis for web application that deal with both data and semantics we illustrate the use of this model on a concrete information integration scenario our approach enables each side of the fence to benefit from the other notably we show how the rdf world can take advantage of xml query language and how the xml world can take advantage of the reasoning capability available for rdf 
traditional non parametric statistical learning technique are often computationally attractive but lack the same generalization and model selection ability a state of the art bayesian algorithm which however are usually computationally prohibitive this paper make several important contribution that allow bayesian learning to scale to more complex real world learning scenario firstly we show that backfitting a traditional non parametric yet highly efficient regression tool can be derived in a novel formulation within an expectation maximization em framework and thus can finally be given a probabilistic interpretation secondly we show that the general framework of sparse bayesian learning and in particular the relevance vector machine rvm can be derived a a highly efficient algorithm using a bayesian version of backfitting at it core a we demonstrate on several regression and classification benchmark bayesian backfitting offer a compelling alternative to current regression method especially when the size and dimensionality of the data challenge computational resource 
we propose a soft greedy learning algorithm for building small conjunction of simple threshold function called ray deflned on single real valued attribute we also propose a pac bayes risk bound which is minimized for classiflers achieving a non trivial tradeofi between sparsity the number of ray used and the magnitude of the separating margin of each ray finally we test the soft greedy algorithm on four dna micro array data set 
tahonen hadid mkp ee oulu fi http www ee oulu fi mvg abstract in this work we present a novel approach to face recognition which considers both shape and texture information to represent face image the face area is first divided into small region from which local binary pattern lbp histogram are extracted and concatenated into a 
pairwise coupling is a popular multi class classification method that combine all comparison for each pair of class this paper present two approach for obtaining class probability both method can be reduced to linear system and are easy to implement we show conceptually and experimentally that the proposed approach are more stable than the two existing popular method voting and the method by hastie and tibshirani 
this poster describes initial work exploring a relatively unexamined area of data fusion fusing the result of retrieval system whose collection have no overlap between them many of the effective meta search data fusion strategy gain much of their success from exploiting document overlap across the source system being merged when the intersection of the collection is the empty set the strategy generally degrade to a simpler form in order to address such situation two strategy were examined re ranking of merged result using a locally run search on the text fragment returned by the source search engine and re ranking based on cross document similarity again using text fragment presented in the retrieved list result from experiment which go beyond previous work indicate that both strategy improve fusion effectiveness 
abstract in reinforcement learning rl there ha been some experimental evidence that the residual gradient algorithm converges slower than the td algorithm in this paper we use the concept of asymptotic convergence rate to prove that under certain condition the synchronous o policy td algorithm converges faster than the synchronous o policy residual gradient algorithm if the value function is represented in tabular form this is the rst theoretical result comparing the convergence behaviour of two rl algorithm we also show that a soon a linear function approximation is involved no general state ment concerning the superiority of one of the algorithm can be made 
we present a novel approach to modeling human gait such a walking and running we represent the trajectory of a certain number of salient feature on the human body a the output of a dynamical system driven by an unknown stochastic input we present technique for inferring model parameter and input signal distribution corresponding to difierent optimality criterion and evaluate the corresponding model for accuracy and predictive power in particular we exploit the higherorder statistical information content in motion capture data to arrive at input signal with independent component we show that human gait synthesized from nongaussian input best capture the dynamic complexity of the original gait data 
we present an account of human concept learning that is learning of category from example based on the principle of minimum description length mdl in support of this theory we tested a wide range of two dimensional concept type including both regular simple and highly irregular complex structure and found the mdl theory to give a good account of subject performance this suggests that the intrinsic complexity of a concept that is it description length systematically influence it learnability 
teamwork demand agreement among teammembers to collaborate and coordinate effectively when a disagreement between teammate occurs due to failure team member should ideally diagnose it cause to resolve the disagreement such diagnosis of social failure can be expensive in communication and computation overhead which previous work did not address we present a novel design space of diagnosis algorithm distinguishing several phase in the diagnosis process and providing alternative algorithm for each phase we then combine these algorithm in different way to empirically explore specific design choice in a complex domain on thousand of failure case the result show that centralizing the diagnosis disambiguation process is a key factor in reducing communication while run time is affected mainly by the amount of reasoning about other agent these result contrast sharply with previous work in disagreement detection in which distributed algorithm reduce communication 
this paper present an optimization framework for estimating the motion and underlying physical parameter of a rigid body in free flight from video the algorithm take a video clip of a tumbling rigid body of known shape and generates a physical simulation of the object observed in the video clip this solution is found by optimizing the simulation parameter to best match the motion observed in the video sequence these simulation parameter include initial position and velocity environment parameter like gravity direction and parameter of the camera a global objective function computes the sum squared difference between the silhouette of the object in simulation and the silhouette obtained from video at each frame application include creating interesting rigid body animation tracking complex rigid body motion in video and estimating camera parameter from video 
an important trend in web information processing is the support of multimedia retrieval however the most prevailing paradigm for multimedia retrieval content based retrieval cbr is a rather conservative one whose performance depends on a set of specifically defined low level feature and a carefully chosen sample object in this paper an aggressive search mechanism called octopus is proposed which address the retrieval of multi modality data using multifaceted knowledge in particular octopus promotes a novel scenario in which the user supply seed object of arbitrary modality a the hint of his information need and receives a set of multi modality object satisfying his need the foundation of octopus is a multifaceted knowledge base constructed on a layered graph model lgm which describes the relevance between medium object from various perspective link analysis based retrieval algorithm is proposed based on the lgm a unique relevance feedback technique is developed to update the knowledge base by learning from user behavior and to enhance the retrieval performance in a progressive manner a prototype implementing the proposed approach ha been developed to demonstrate it feasibility and capability through illustrative example 
knowledge of the anaphoricity of a noun phrase might be profitably exploited by a coreference system to bypass the resolution of non anaphoric noun phrase perhaps surprisingly recent attempt to incorporate automatically acquired anaphoricity information into coreference system however have led to the degradation in resolution performance this paper examines several key issue in computing and using anaphoricity information to improve learning based coreference system in particular we present a new corpus based approach to anaphoricity determination experiment on three standard coreference data set demonstrate the effectiveness of our approach 
abstract knowledge of the anaphoricity of a noun phrase might be profitably exploited by a coreference system to bypass the resolution of non anaphoric noun phrase perhaps surprisingly recent attempt to incorporate automatically acquired anaphoricity information into coreference system however have led to the degradation in resolution performance this paper examines several key issue in computing and using anaphoricity information to improve learning based coreference system in particular we present a new corpus based approach to anaphoricity determination experiment on three standard coreference data set demonstrate the effectiveness of our approach 
finding an equivalent fsm with minimal number of state is generally referred a state minimization or state reduction sr problem state minimization is an effective approach in logic synthesis to optimize sequential circuit design in term of area and power kam the sr problem of fsm is np complete and can be treated a a special case of the constraint satisfaction problem csp where the transition function defines all the constraint that need to be satisfied interestingly we observe that not all the constraint are re quired to obtain a given sr solution identifying the redun dancy in the fsm will be useful in the following occasion first it help to understand the nature of the np complete sr problem and to build fsm benchmark to test the effec tiveness of sr solver second the redundancy can be uti lized to hide information and thus provide security protec tion to the fsm simulation result on real life fsms reveal the existence of extremely rich redundancy 
xml messaging is at the heart of web service providing the flexibility required for their deployment composition and maintenance yet current approach to web service development hide the messaging layer behind java or c apis preventing the application to get direct access to the underlying xml information to address this problem we advocate the use of a native xml language namely xquery a an integral part of the web service development infrastructure the main contribution of the paper is a binding between wsdl the web service description language and xquery the approach enables the use of xquery for both web service deployment and composition we present a simple command line tool that can be used to automatically deploy a web service from a given xquery module and extend the xquery language itself with a statement for accessing one or more web service the binding provides tight coupling between wsdl and xquery yielding additional benefit notably the ability to use wsdl a an interface language for xquery and the ability to perform static typing on xquery program that include web service call last but not least the proposal requires only minimal change to the existing infrastructure we report on our experience implementing this approach in the galax xquery processor 
we formulate and propose the template detection problem and suggest a practical solution for it based on counting frequent item set we show that the use of template is pervasive on the web we describe three principle which characterize the assumption made by hypertext information retrieval ir and data mining dm system and show that template are a major source of violation of these principle a a consequence basic pure implementation of simple search algorithm coupled with template detection and elimination show surprising increase in precision at all level of recall 
subject or prepositional content ha been the focus of most classification research genre or style on the other hand is a different and important property of text and automatic text genre classification is becoming important for classification and retrieval purpose a well a for some natural language processing research in this paper we present a method for automatic genre classification that is based on statistically selected feature obtained from both subject classified and genre classified training data the experimental result show that the proposed method outperforms a direct application of a statistical learner often used for subject classification we also observe that the deviation formula and discrimination formula using document frequency ratio also work a expected we conjecture that this dual feature set approach can be generalized to improve the performance of subject classification a well 
this work is concerned with the question of how to combine online an ensemble of active learner so a to expedite the learning progress in pool based active learning we develop an active learning master algorithm based on a known competitive algorithm for the multi armed bandit problem a major challenge in successfully choosing top performing active learner online is to reliably estimate their progress during the learning session to this end we propose a simple maximum entropy criterion that provides effective estimate in realistic setting we study the performance of the proposed master algorithm using an ensemble containing two of the best known active learning algorithm a well a a new algorithm the resulting active learning master algorithm is empirically shown to consistently perform almost a well a and sometimes outperform the best algorithm in the ensemble on a range of classification problem 
we investigate the explore exploit trade off in reinforcement learning using competitive analysis applied to an abstract model we state and prove lower and upper bound on the competitive ratio the essential conclusion of our analysis is that optimizing the explore exploit trade off is much easier with a few piece of extra knowledge such a the stopping time or upper and lower bound on the value of the optimal exploitation policy 
real world application of text categorization often require a system to deal with ten of thousand of category defined over a large taxonomy this paper address the problem with respect to a set of popular algorithm in text categorization including support vector machine k nearest neighbor ridge regression linear least square fit and logistic regression by providing a formal analysis of the computational complexity of each classification method followed by an investigation on the usage of different classifier in a hierarchical setting of categorization we show how the scalability of a method depends on the topology of the hierarchy and the category distribution in addition we are able to obtain tight bound for the complexity by using the power law to approximate category distribution over a hierarchy experiment with knn and svm classifier on the ohsumed corpus are reported on a concrete example 
we are developing a technique to predict travel time of a vehicle for an objective road section based on real time traffic data collected through a probe car system in the area of intelligent transport system it travel time prediction is an important subject probe car system is an upcoming data collection method in which a number of vehicle are used a moving sensor to detect actual traffic situation it can collect data concerning much larger area compared with traditional fixed detector our prediction technique is based on statistical analysis using ar model with seasonal adjustment and mdl minimum description length criterion seasonal adjustment is used to handle periodicity of hour in traffic data alternatively we employ state space model which can handle time series with periodicity it is important to select really effective data for prediction among the data from widespread area which are collected via probe car system we do this using mdl criterion that is we find the explanatory variable that really have influence on the future travel time in this paper we experimentally show effectiveness of our method using probe car data collected in nagoya metropolitan area in 
we present analog neuromorphic circuit for implementing bistable synapsis with spike timing dependent plasticity stdp property in these type of synapsis the short term dynamic of the synaptic efficacy are governed by the relative timing of the preand post synaptic spike while onlongtime scale the efficacy tendasymptotically to eithera potentiated state or to a depressed one test result from a prototype vlsi chip containing a learning synapse connected to a preand post synaptic integrate andfire neuron demonstrate the synapse s stdp learning property and it long term bistable characteristic 
given a noisy dataset how to locate erroneous instance and attribute and rank suspicious instance based on their impact on the system performance is an interesting and important research issue we provide in this paper an error detection and impact sensitive instance ranking edir mechanism to address this problem given a noisy dataset d we first train a benchmark classifier t from d the instance that cannot be effectively classified by t are treated a suspicious and forwarded to a subset s for each attribute ai we switch ai and the class label c to train a classifier api for ai given an instance ik in s we use api and the benchmark classifier t to locate the erroneous value of each attribute ai to quantitatively rank instance in s we define an impact measure based on the information gain ratio ir we calculate iri between attribute ai and c and use iri a the impact sensitive weight of ai the sum of impact sensitive weight from all located erroneous attribute of ik indicates it total impact value the experimental result demonstrate the effectiveness of our strategy 
brain computer interface bci are an interesting emerging technology that is driven by the motivation to develop an effective communication interface translating human intention into a control signal for device like computer or neuroprostheses if this can be done bypassing the usual human output pathway like peripheral nerve and muscle it can ultimately become a valuable tool for paralyzed patient most activity in bci research is devoted to finding suitable feature and algorithm to increase information transfer rate itrs the present paper study the implication of using more class e g left v right hand v foot for operating a bci we contribute by a theoretical study showing under some mild assumption that it is practically not useful to employ more than three or four class two extension of the common spatial pattern csp algorithm one interestingly based on simultaneous diagonalization and controlled eeg experiment that underline our theoretical finding and show excellent improved itrs 
this paper present a new dependence language modeling approach to information retrieval the approach extends the basic language modeling approach based on unigram by relaxing the independence assumption we integrate the linkage of a query a a hidden variable which express the term dependency within the query a an acyclic planar undirected graph we then assume that a query is generated from a document in two stage the linkage is generated first and then each term is generated in turn depending on other related term according to the linkage we also present a smoothing method for model parameter estimation and an approach to learning the linkage of a sentence in an unsupervised manner the new approach is compared to the classical probabilistic retrieval model and the previously proposed language model with and without taking into account term dependency result show that our model achieves substantial and significant improvement on trec collection 
we present a probabilistic approach to learning objectrepresentations based on the content and style bilineargenerative model of tenenbaum and freeman in contrastto their earlier svd based approach our approach modelsimages using particle filter we maintain separate particlefilters to represent the content and style space allowing usto define arbitrary weighting function over the particle tohelp estimate the content style density we combine thisapproach with a new em based method for learning basisvectors that describe content style mixing using a particle basedrepresentation permit good reconstruction despitereduced dimensionality and increase storage capacity andcomputational efficiency we describe how learning the distributionsusing particle filter allows u to efficiently computea probabilistic novelty term our example applicationconsiders a dataset of face under different lightingconditions the system classifies face of people it ha seenbefore and can identify previously unseen face a new content using a probabilistic definition of novelty in conjunctionwith learning content style separability provides a crucialbuilding block for designing real world real time objectrecognition system 
a text corpus become larger tradeos between speed and accuracy become critical slow but accurate method may not complete in a practical amount of time in order to make the training data a manageable size a data reduction technique may be necessary subsampling for example speed up a classier by randomly removing training point in this paper we describe an alternate method for reducing the number of training point by combining training point such that important statistical information is retained our algorithm keep the same statistic that fast linear time text algorithm like rocchio and naive bayes use we provide empirical result that show our data reduction technique compare favorably to three other data reduction technique on four standard text corpus 
we present an approach for learning criterion for part of speech classification by induction over the lexicon contained within the cyc knowledge base this produce good result using a decision tree that incorporates semantic feature e g cyc ontological type a well a syntactic feature e g headword morphology accurate result are achieved for the special case of deciding whether lexical mapping should use count noun or mass noun headword for this special case comparable result are also obtained using opencyc the publicly available version of cyc and the cyc to wordnet translation of the semantic speech part criterion 
the fundamental difference between standard information retrieval and xml retrieval is the unit of retrieval in traditional ir the unit of retrieval is fixed it is the complete document in xml retrieval every xml element in a document is a retrievable unit this make xml retrieval more difficult besides being relevant a retrieved unit should be neither too large nor too small the research presented here a comparative analysis of two approach to xml retrieval aim to shed light on which xml element should be retrieved the experimental evaluation us data from the initiative for the evaluation of xml retrieval inex 
in the paper we introduce a quantitative measure of autonomy in multiagent interaction we quantify and analyse different type of agent autonomy a decision autonomy versus action autonomy b autonomy with respect to an agent s user c autonomy with respect to other agent and group of agent and d a measure of group autonomy that account for the degree with which one group depends on another group we analyse the problem of composing multiagent group with maximum overall autonomy and we prove that this problem is np complete therefore finding the optimal group or agent with whom to share a task or to whom to delegate a task is in general computationally hard 
most information extraction ie system treat separate potential extraction a independent however in many case considering inuences between different potential extraction could improve overall accuracy statistical method based on undirected graphical model such a conditional random eld crfs have been shown to be an effective approach to learning accurate ie system we present a new ie method that employ relational markov network which can represent arbitrary dependency between extraction this allows for collective information extraction that exploit the mutual inuence between possible extraction experiment on learning to extract protein name from biomedical text demonstrate the advantage of this approach 
face detection is a canonical example of a rare event detection problem in which target pattern occur with much lower frequency than nontargets out of million of face sized window in an input image for example only a few will typically contain a face viola and jones recently proposed a cascade architecture for face detection which successfully address the rare event nature of the task a central part of their method is a feature selection algorithm based on adaboost we present a novel cascade learning algorithm based on forward feature selection which is two order of magnitude faster than the viola jones approach and yield classifier of equivalent quality this faster method could be used for more demanding classification task such a on line learning 
the application of latent hidden variable dynamic bayesian network is constrained by the complexity of marginalising over latent variable for this reason either small latent dimension or gaussian latent conditional table linearly dependent on past state are typically considered in order that inference is tractable we suggest an alternative approach in which the latent variable are modelled using deterministic conditional probability table this specialisation ha the advantage of tractable inference even for highly complex non linear non gaussian visible conditional probability table this approach enables the consideration of highly complex latent dynamic whilst retaining the benet of a tractable probabilistic model 
we present a new method for image retrieval by shape similarity able to deal with real image with not uniform background and possible touching occluding object first of all we perform a sketch driven segmentation of the scene by mean of a deformation tolerant version of the generalized hough transform dtght using the dtght we select in the image some candidate segment to be matched with the user sketch the candidate segment are then matched with the sketch checking the consistency of the corresponding shape 
abstract we consider the general problem of utilizing both labeled and unlabeleddata to improve classification accuracy under the assumptionthat the data lie on a submanifold in a high dimensional space we develop an algorithmic framework to classify a partially labeleddata set in a principled manner the central idea of our approach isthat classification function are naturally defined only on the submanifoldin question rather than the total ambient space using thelaplace beltrami 
in this paper a bayesian self calibration approach using sequential importance sampling si is proposed given a set of feature correspondence tracked through an image sequence the joint posterior distribution of both camera extrinsic and intrinsic parameter a well a the scene structure are approximated by a set of sample and their corresponding weight the critical motion sequence are explicitly considered in the design of the algorithm the probability of the existence of the critical motion sequence is inferred from the sample and weight set obtained from the si procedure no initial guess for the calibration parameter is required the proposed approach ha been extensively tested on both synthetic and real image sequence and satisfactory performance ha been observed 
advance in imaging technique have led to large repository of image there is an increasing demand for automated system that can analyze complex medical image and extract meaningful information for mining pattern here we describe a real life image mining application to the problem of tumour cell counting the quantitative analysis of tumour cell is fundamental to characterizing the activity of tumour cell existing approach are mostly manual time consuming and subjective effort to automate the process of cell counting have largely focused on using image processing technique only our study indicate that image processing alone is unable to give accurate result in this paper we examine the use of extracted feature rule to aid in the process of tumor cell counting we propose a robust local adaptive thresholding and dynamic water immersion algorithm to segment region of interesting from background meaningful feature are then extracted from the segmented region a number of base classifier are built to generate feature rule to help identify the tumor cell two voting strategy are implemented to combine the base classifier into a meta classifier experiment result indicate that this process of using extracted feature rule to help identify tumor cell lead to better accuracy than pure image processing technique alone 
many clustering algorithm fail when dealing with high dimensional data principal component analysis pca is a popular dimensionality reduction algorithm however it assumes a single multivariate gaussian model which provides a global linear projection of the data mixture of probabilistic principal component analyzer ppca provides a better model to the clustering paradigm it provides a local linear pca projection for each multivariate gaussian cluster component we extend this model to build hierarchical mixture of ppca hierarchical clustering provides a flexible representation showing relationship among cluster in various perceptual level we introduce an automated hierarchical mixture of ppca algorithm which utilizes the integrated classification likelihood a a criterion for splitting and stopping the addition of hierarchical level an automated approach requires automated method for initialization determining the number of principal component dimension and determining when to split cluster we address each of these in the paper this automated approach result in a coarse to fine local component model with varying projection and with different number of dimension for each cluster 
this paper address the problem of classification in situation where the data distribution is not homogeneous data instance might come from different location or time and therefore are sampled from related but different distribution in particular feature s may appear in some part of the data that are rarely or never seen in others in most situation with nonhomogeneous data the training data is not representative of the distribution under which the classifier must operate we propose a method based on probabilistic graphical model for utilizing unseen feature during classification our method introduces for each such unseen feature a continuous hidden variable describing it influence on the class whether it tends to be associated with some label we then use probabilistic inference over the test data to infer a distribution over the value of this hidden variable intuitively we learn the role of this unseen feature from the test set generalizing from those instance whose label we are fairly sure about our overall probabilistic model is learned from the training data in particular we also learn model for characterizing the role of unseen feature these model use meta feature of those feature such a word in the neighborhood of an unseen feature to infer it role we present result for this framework on the task of classifying news article and web page showing significant improvement over model that do not use unseen feature 
this paper study the problem of categorical data clustering especially for transactional data characterized by high dimensionality and large volume starting from a heuristic method of increasing the height to width ratio of the cluster histogram we develop a novel algorithm clope which is very fast and scalable while being quite effective we demonstrate the performance of our algorithm on two real world datasets and compare clope with the state of art algorithm 
analyzing fluid motion is essential in number of domain and can rarely be handled using generic computer vision technique in this particular application context we address two distinct problem first we describe a dedicated dense motion estimator the approach relies on constraint issuing from fluid motion property and allows u to recover dense motion field of good quality secondly we address the problem of analyzing such velocity field we present a kind of motion based segmentation relying on an analytic representation of the motion field that permit to extract important quantity such a singularity stream function or velocity potential the proposed method ha the advantage to be robust simple and fast 
we describe an approach to building brain computer interface bci based on graphical model for probabilistic inference and learning we show how a dynamic bayesian network dbn can be used to infer probability distribution over brainand body state during planning and execution of action the dbn is learned directly from observed data and allows measured signal such a eeg and emg to be interpreted in term of internal state such a intent to move preparatory activity and movement execution unlike traditional classification based approach to bci the proposed approach allows continuous tracking and prediction of internal state over time and generates control signal based on an entire probability distribution over state rather than binary yes no decision we present preliminary result of brainand body state estimation using simultaneous eeg and emg signal recorded during a self paced left right hand movement task 
the corpus analysis method in chinese keyword extraction look on the corpus a a single sample of language stochastic process but the distribution of keywords in the whole corpus and in each document are very different from each other the extraction based on global statistical information only can get significant keywords in the whole corpus max duplicated string contain the local significant keywords in each document in this paper we designed an efficient algorithm to extract the max duplicated string by building pat tree for the document so that the keywords can be picked out from the max duplicated string by their sig value in the corpus 
although discriminatively trained classifi er are usually more accurate when labeled training data is abundant previous work ha shown that when training data is limited generative classifi er can out perform them this paper describes a hybrid model in which a high dimensional subset of the parameter are trained to maximize generative likelihood and another small subset of parameter are discriminatively trained to maximize conditional likelihood we give a sample complexity bound showing that in order to fi t the discriminative parameter well the number of training example required depends only on the logarithm of the number of feature occurrence and feature set size experimental result show that hybrid model can provide lower test error and can produce better accuracy coverage curve than either their purely generative or purely discriminative counterpart we also discus several advantage of hybrid model and advocate further work in this area 
clustering is a very well studied problem that attempt to group similar data point most traditional clustering algorithm assume that the data is provided without measurement error often however real world data set have such error and one can obtain estimate of these error we present a clustering method that incorporates information contained in these error estimate we present a new distance function that is based on the distribution of error in data using a gaussian model for error the distance function follows a chi square distribution and is easy to compute this distance function is used in hierarchical clustering to discover meaningful cluster the distance function is scale invariant so that clustering result are independent of unit of measuring data in the special case when the error distribution is the same for each attribute of data point the rank order of pair wise distance is the same for our distance function and the euclidean distance function the clustering method is applied to the seasonality estimation problem and experimental result are presented for the retail industry data a well a for simulated data where it outperforms classical clustering method 
term weighting method have been shown to give significant increase in information retrieval performance the presence of pronomial reference in document reduces the term frequency of associated word with a consequent effect on term weight and information retrieval behaviour this investigation explores the impact on information retrieval performance of broad coverage automatic pronoun resolution result indicate that this approach ha potential to improve both precision at fixed cutoff level and average precision 
broad coverage repository of semantic relation between verb could benefit many nlp task we present a semi automatic method for extracting fine grained semantic relation between verb we detect similarity strength antonymy enablement and temporal happens before relation between pair of strongly associated verb using lexicosyntactic pattern over the web on a set of strongly associated verb pair our extraction algorithm yielded accuracy analysis of error type show that on the relation strength we achieved accuracy we provide the resource called verbocean for download at http semantics isi edu ocean 
we present a novel data driven method for integrated shallow and deep parsing mediated by an xml based multi layer annotation architecture we interleave a robust but accurate stochastic topological field parser of german with a constraint based hpsg parser our annotation based method for dovetailing shallow and deep phrasal constraint is highly flexible allowing targeted and fine grained guidance of constraint based parsing we conduct systematic experiment that demonstrate substantial performance gain 
the naive classifier is a well established mathematical model whose simplicity speed and accuracy have made it a popular choice for classification in ai and engineering in this paper we show that given n feature of interest it is possible to perform tractable exact model averaging ma over all n possible feature set model in fact we show that it is possible to calculate parameter for a single naive classifier c such that c produce prediction equivalent to those obtained by the full model averaging and we show that c can be constructed using the same time and space complexity required to construct a single naive classifier with map parameter we present experimental result which show that on average the ma classifier typically outperforms the map classifier on simulated data and we characterize how the relative performance varies with number of variable number of training record and complexity of the generating distribution finally we examine the performance of the ma naive model on the real world alarm and hepar network and show ma improved classification here a well 
we present a geometric approach to statistical shape analysis of closed curve in image the basic idea is to specify a space of closed curve satisfying given constraint and exploit the differential geometry of this space to solve optimization and inference problem we demonstrate this approach by i defining and computing statistic of observed shape ii defining and learning a parametric probability model on shape space and iii designing a binary hypothesis test on this space 
over the last year significant effort have been made to develop kernel that can be applied to sequence data such a dna text speech video and image the fisher kernel and similar variant have been suggested a good way to combine an underlying generative model in the feature space and discriminant classifier such a svm s in this paper we suggest an alternative procedure to the fisher kernel for systematically finding kernel function that naturally handle variable length sequence data in multimedia domain in particular for domain such a speech and image we explore the use of kernel function that take full advantage of well known probabilistic model such a gaussian mixture and single full covariance gaussian model we derive a kernel distance based on the kullback leibler kl divergence between generative model in effect our approach combine the best of both generative and discriminative method and replaces the standard svm kernel we perform experiment on speaker identification verification and image classification task and show that these new kernel have the best performance in speaker verification and mostly outperform the fisher kernel based svm s and the generative classifier in speaker identification and image classification 
regularization play a central role in the analysis of modern data where non regularized fitting is likely to lead to over fitted model useless for both prediction and interpretation we consider the design of incremental algorithm which follow path of regularized solution a the regularization varies these approach often result in method which are both efficient and highly flexible we suggest a general path following algorithm based on second order approximation prove that under mild condition it remains very close to the path of optimal solution and illustrate it with example 
astronomy increasingly face the issue of massive unwieldly data set the sloan digital sky survey sd ha so far generated ten of million of image of distant galaxy of which only a tiny fraction have been morphologically classified morphological classification in this context is achieved by fitting a parametric model of galaxy shape to a galaxy image this is a nonlinear regression problem whose challenge are threefold blurring of the image caused by atmosphere and mirror imperfection large number of local minimum and massive data set our strategy is to use the eigenimages of the parametric model to form a new feature space and then to map both target image and the model parameter into this feature space in this low dimensional space we search for the best image to parameter match to search the space we sample it by creating a database of many random parameter vector prototype and mapping them into the feature space the search problem then becomes one of finding the best prototype match so the fitting process a nearest neighbor search in addition to the saving realized by decomposing the original space into an eigenspace we can use the fact that the model is a linear sum of function to reduce the prototype further the only prototype stored are the component of the model function a modified form of nearest neighbor is used to search among them additional complication arise in the form of missing data and heteroscedasticity both of which are addressed with weighted linear regression compared to existing technique speed ups ach ieved are between and order of magnitude this should enable the analysis of the entire sd dataset 
the scalable vector graphic format svg is already substantially improving graphic delivery on the web but some important issue still remain to be addressed in particular svg doe not support client side adaption of document to different viewing condition such a varying screen size style preference or different device capability based on our earlier work we show how svg can be extended with constraint based specification of document layout to augment it with adaptive capability the core of our proposal is to include one way constraint into svg which offer more expressiveness than the previously suggested class of linear constraint and at the same time require substantially le computational effort 
accurate cross language information retrieval requires that query term be correctly translated several new technique to improve the translation of out of vocabulary term in english chinese cross language information retrieval have been developed however these require query and a document collection to enable translation disambiguation although effective they involve much processing and searching of the web at query time and may not be practical in a production web search engine in this work we consider what task maybe carried out beforehand the goal being to reduce the processing required at query time we have successfully developed new technique to extract and translate out of vocabulary term using the web and add them into a translation dictionary prior to query time 
link discovery is a new challenge in data mining whose primary concern are to identify strong link and discover hidden relationship among entity and organization based on low level incomplete and noisy evidence data to address this challenge we are developing a hybrid link discovery system called kojak that combine state of the art knowledge representation and reasoning kr r technology with statistical clustering and analysis technique from the area of data mining in this paper we report on the architecture and technology of it first fully completed module called the kojak group finder the group finder is capable of finding hidden group and group member in large evidence database our group finding approach address a variety of important ld challenge such a being able to exploit heterogeneous and structurally rich evidence handling the connectivity curse noise and corruption a well a the capability to scale up to very large realistic data set the first version of the kojak group finder ha been successfully tested and evaluated on a variety of synthetic datasets 
the connectivity of the nervous system of the nematode caenorhabditis elegans ha been described completely but the analysis of the neuronal basis of behavior in this system is just beginning here we used an optimization algorithm to search for pattern of connectivity sufficient to compute the sensorimotor transformation underlying c elegans chemotaxis a simple form of spatial orientation behavior in which turning probability is modulated by the rate of change of chemical concentration optimization produced differentiator network with inhibitory feedback among all neuron further analysis showed that feedback regulates the latency between sensory input and behavior common pattern of connectivity between the model and biological network suggest new function for previously identified connection in the c elegans nervous system 
we introduce a new method for disambiguating word sens that exploit a nonlinear kernel principal component analysis kpca technique to achieve accuracy superior to the best published individual model we present empirical result demonstrating significantly better accuracy compared to the state of the art achieved by either na ve bayes or maximum entropy model on senseval data we also contrast against another type of kernel method the support vector machine svm model and show that our kpca based model outperforms the svm based model it is hoped that these highly encouraging first result on kpca for natural language processing task will inspire further development of these direction 
providing video on demand vod service over the internet in a scalable way is a challenging problem in this paper we propose p cast an architecture that us a peer to peer approach to cooperatively stream video using patching technique while only relying on unicast connection among peer we address the following two key technical issue in p cast constructing an application overlay appropriate for streaming and providing continuous stream playback without glitch in the face of disruption from an early departing client our simulation experiment show that p cast can serve many more client than traditional client server unicast service and that it generally out performs multicast based patching if client can cache more than of a stream s initial portion we handle disruption by delaying the start of playback and applying the shifted forwarding technique a threshold on the length of time during which arriving client are served in a single session in p cast serf a a knob to adjust the balance between the scalability and the client viewing quality in p cast 
we have implemented a real time front end for detecting voiced speech and estimating it fundamental frequency the front end performs the signal processing for voice driven agent that attend to the pitch contour of human speech and provide continuous audiovisual feedback the algorithm we use for pitch tracking ha several distinguishing feature it make no use of ffts or autocorrelation at the pitch period it update the pitch incrementally on a sample by sample basis it avoids peak picking and doe not require interpolation in time or frequency to obtain high resolution estimate and it work reliably over a four octave range in real time without the need for postprocessing to produce smooth contour the algorithm is based on two simple idea in neural computation the introduction of a purposeful nonlinearity and the error signal of a least square fit the pitch tracker is used in two real time multimedia application a voice to midi player that synthesizes electronic music from vocalized melody and an audiovisual karaoke machine with multimodal feedback both application run on a laptop and display the user s pitch scrolling across the screen a he or she sings into the computer 
soboroff nicholas and cahan recently proposed a method for evaluating the performance of retrieval system without relevance judgment they demonstrated that the system evaluation produced by their methodology are correlated with actual evaluation using relevance judgment in the trec competition in this work we propose an explanation for this phenomenon we devise a simple measure for quantifying the similarity of retrieval system by assessing the similarity of their retrieved result then given a collection of retrieval system and their retrieved result we use this measure to ass the average similarity of a system to the other system in the collection we demonstrate that evaluating retrieval system according to average similarity yield result quite similar to the methodology proposed by soboroff et al and we further demonstrate that these two technique are in fact highly correlated thus the technique are effectively evaluating and ranking retrieval system by popularity a opposed to performance 
there exist many approach to clustering but the important issue of feature selection i e selecting the data attribute that are relevant for clustering is rarely addressed feature selection for clustering is difficult due to the absence of class label we propose two approach to feature selection in the context of gaussian mixture based clustering in the first one instead of making hard selection we estimate feature saliency an expectation maximization em algorithm is derived for this task the second approach extends koller and sahami s mutual informationbased feature relevance criterion to the unsupervised case feature selection is then carried out by a backward search scheme this scheme can be classified a a wrapper since it wrap mixture estimation in an outer layer that performs feature selection experimental result on synthetic and real data show that both method have promising performance 
user make journey through the web web travel encompasses the task of orientation and navigation the environment and the purpose of the journey the ease of travel it mobility varies from page to page and site to site for visually impaired user in particular mobility is reduced the object that support travel are inaccessible or missing altogether web development tool need to include support to increase mobility we present a framework for finding and classifying travel object within web page the evaluation carried out ha shown that this framework support a systematic and consistent method for assessing travel upon the web we propose that such a framework can provide the foundation for a semi automated tool for the support of travel upon the web 
in many real world environment automatic speech recognition asr technology fail to provide adequate performance for application such a human robot dialog despite substantial evidence that speech recognition in human is performed in a top down a well a bottom up manner asr system typically fail to capitalize on this instead relying on a purely statistical bottom up methodology in this paper we advocate the use of a knowledge based approach to improving asr in domain such a mobile robotics a simple implementation is presented which us the visual recognition of object in a robot s environment to increase the probability that word and sentence related to these object will be recognized 
the security of a network configuration is based not just on the security of it individual component and their direct interconnection but it is also based on the potential for system to interoperate indirectly across network route such interoperation ha been shown to provide the potential for cascading path that violate security in a circuitous manner across a network in this paper we show how constraint programming provides a natural approach to expressing the necessary constraint to ensure multilevel security across a network configuration in particular soft constraint are used to detect and eliminate the cascading network path that violate security taking this approach result in practical advancement over existing solution to this problem in particular constraint satisfaction highlight the set of all cascading path upon which we can compute in polynomial time an optimal reconfiguration of the network and ensure security 
irregular so called broken plural identification in modern standard arabic is a problematic issue for information retrieval ir and language engineering application but their effect on the performance of ir ha never been examined broken plural bps are formed by altering the singular a in english toothteeth through an application of interdigitating pattern on stem and singular word cannot be recovered by standard affix stripping stemming technique we developed several method for bp detection and evaluated them using an unseen test set we incorporated the bp detection component into a new light stemming algorithm that conflates both regular and broken plural with their singular form we also evaluated the new light stemming algorithm within the context of information retrieval comparing it performance with other stemming algorithm 
we study how to learn to play a pareto optimal strict nash equilibrium when there exist multiple equilibrium and agent may have different preference among the equilibrium we focus on repeated coordination game of non identical interest where agent do not know the game structure up front and receive noisy payoff we design efficient near optimal algorithm for both the perfect monitoring and the imperfect monitoring setting where the agent only observe their own payoff and the joint action 
abstract we propose an approach to learning the semantics of image which allows u to automatically annotate an image with keywords and to retrieve image based on text query we do this using a formalism that model the generation of annotated image we assume that every image is divided into region each described by a continuous valued feature vector given a training set of image with annotation we compute a joint probabilistic model of image feature and word which allow u to predict the probability of generating a word given the image region this may be used to automatically annotate and retrieve image given a word a a query experiment show that our model significantly outperforms the best of the previously reported result on the task of automatic image annotation and retrieval 
one of the central challenge in reinforcement learning is to balance the exploration exploitation tradeoff while scaling up to large problem although model based reinforcement learning ha been le prominent than value based method in addressing these challenge recent progress ha generated renewed interest in pursuing modelbased approach theoretical work on the exploration exploitation tradeoff ha yielded provably sound model based algorithm such a e and rmax while work on factored mdp representation ha yielded model based algorithm that can scale up to large problem recently the benefit of both achievement have been combined in the factored e algorithm of kearns and koller in this paper we address a significant shortcoming of factored e namely that it requires an oracle planner that cannot be feasibly implemented we propose an alternative approach that us a practical approximate planner approximate linear programming that maintains desirable property further we develop an exploration strategy that is targeted toward improving the performance of the linear programming algorithm rather than an oracle planner this lead to a simple exploration strategy that visit state relevant to tightening the lp solution and achieves sample efficiency logarithmic in the size of the problem description our experimental result show that the targeted approach performs better than using approximate planning for implementing either factored e or factored rmax 
we consider the problem of unsupervised learning from a matrix of data vector where in each row the observed value are randomly permuted in an unknown fashion such problem arise naturally in area such a computer vision and text modeling where measurement need not be in correspondence with the correct feature we provide a general theoretical characterization of the dicult y of unscrambling the value of the row for such problem and relate the optimal error rate to the well known concept of the bayes classication error rate for known parametric distribution we derive closed form expression for the optimal error rate that provide insight into what make this problem difcult in practice finally we show how the expectation maximization procedure can be used to simultaneously estimate both a probabilistic model for the feature a well a a distribution over the correspondence of the row value 
online mechanism design md considers the problem of providing incentive to implement desired system wide outcome in system with self interested agent that arrive and depart dynamically agent can choose to misrepresent their arrival and departure time in addition to information about their value for dieren t outcome we consider the problem of maximizing the total longterm value of the system despite the self interest of agent the online md problem induces a markov decision process mdp which when solved can be used to implement optimal policy in a truth revealing bayesian nash equilibrium 
we present a set of algorithm that recovers detailed building surface structure from large set of urban image containing severe occlusion and lighting variation an iterative weighted average algorithm is introduced to recover high quality consensus facade texture d and d method are combined to extract microstructures facilitating urban model renemen t and visualization 
automatically understanding event happening at a site is the ultimate goal of visual surveillance system we investigate the challenge faced by automated surveillance system operating in hostile condition and demonstrate the developed algorithm via a system that detects water crisis within highly dynamic aquatic environment an efficient segmentation algorithm based on robust block based background modelling and thresholding with hysteresis methodology enables swimmer to be reliably detected amid reflection ripple splash and rapid lighting change partial occlusion are resolved using a markov random field framework that enhances the tracking capability of the system visual indicator of water crisis are identified based on professional knowledge of water crisis detection based on which a set of swimmer descriptor ha been defined through seamlessly fusing the extracted swimmer descriptor based on a novel functional link network the system achieves promising result for water crisis detection the developed algorithm have been incorporated into a live system with robust performance for different hostile environment faced by an outdoor swimming pool 
we are developing technology for generating english textual summary of time series data in three domain weather forecast gas turbine sensor reading and hospital intensive care data our weather forecast generator is currently operational and being used daily by a meteorological company we generate summary in three step a selecting the most important trend and pattern to communicate b mapping these pattern onto word and phrase and c generating actual text based on these word and phrase in this paper we focus on the first step a selecting the information to communicate and describe how we perform this using modified version of standard data analysis algorithm such a segmentation the modification arose out of empirical work with user and domain expert and in fact can all be regarded a application of the gricean maxim of quality quantity relevance and manner which describe how a cooperative speaker should behave in order to help a hearer correctly interpret a text the gricean maxim are perhaps a key element of adapting data analysis algorithm for effective communication of information to human user and should be considered by other researcher interested in communicating data to human user 
we have developed a method for recommending item that combine content and collaborative data under a single probabilistic framework we benchmark our algorithm against a na ve bayes classifier on the cold start problem where we wish to recommend item that no one in the community ha yet rated we systematically explore three testing methodology using a publicly available data set and explain how these method apply to specific real world application we advocate heuristic recommenders when benchmarking to give competent baseline performance we introduce a new performance metric the croc curve and demonstrate empirically that the various component of our testing strategy combine to obtain deeper understanding of the performance characteristic of recommender system though the emphasis of our testing is on cold start recommending our method for recommending and evaluation are general 
model schema language msl is an attempt to formalize some of the core idea in xml schema the benefit of a formal description are that it is both concise and precise msl ha already proved helpful in work on the design of xml query we expect that similar technique can be used to extend msl to include most or all xml schema 
a wide variety of technique for visual navigation using robot mounted camera have been described over the past several decade yet adoption of optical flow navigation technique ha been slow this demo illustrates what visual navigation ha to offer robust hazard detection including precipice and obstacle high accuracy open loop odometry and stable closed loop motion control implemented via an optical flow based visual odometry system this work is based on open source vision code common computing hardware and inexpensive consumer quality camera and a such should be accessible to many robot builder 
we have developed willex a tool that help grammar developer to work efficiently by using annotated corpus and recording parsing error willex ha two major new function first it decrease ambiguity of the parsing result by comparing them to an annotated corpus and removing wrong partial result both automatically and manually second willex accumulates parsing error a data for the developer to clarify the defect of the grammar statistically we applied willex to a large scale hpsg style grammar a an example 
to what extent can three dimensional shape and radiancebe inferred from a collection of image can the two be estimatedseparately while retaining optimality how shouldthe optimality criterion be computed when is it necessaryto employ an explicit model of the reflectance property ofa scene in this paper we introduce a separation principlefor shape and radiance estimation that applies to lambertianscenes and hold for any choice of norm when thescene is not lambertian however shape cannot be decoupledfrom radiance and therefore matching image to imageis not possible directly we employ a rank constraint onthe radiance tensor which is commonly used in computergraphics and construct a novel cost functional whose minimizationleads to an estimate of both shape and radiancefor non lambertian object which we validate experimentally 
inference and adaptation in noisy and changing rich sensory environment are rife with a variety of specific sort of variability experimental and theoretical study suggest that these different form of variability play different behavioral neural and computational role and may be reported by different notably neuromodulatory system here we refine our previous theory of acetylcholine s role in cortical inference in the oxymoronic term of expected uncertainty and advocate a theory for norepinephrine in term of unexpected uncertainty we suggest that norepinephrine report the radical divergence of bottom up input from prevailing top down interpretation to influence inference and plasticity we illustrate this proposal using an adaptive factor analysis model 
in this paper we explore the use of random forest rf in the structured language model slm which us rich syntactic information in predicting the next word based on word already seen the goal in this work is to construct rf by randomly growing decision tree dts using syntactic information and investigate the performance of the slm modeled by the rf in automatic speech recognition rf which were originally developed a classifier are a combination of decision tree classifier each tree is grown based on random training data sampled independently and with the same distribution for all tree in the forest and a random selection of possible question at each node of the decision tree our approach extends the original idea of rf to deal with the data sparseness problem encountered in language modeling rf have been studied in the context of n gram language modeling and have been shown to generalize well to unseen data we show in this paper that rf using syntactic information can also achieve better performance in both perplexity ppl and word error rate wer in a large vocabulary speech recognition system compared to a baseline that us kneser ney smoothing 
we discus feature latent semantic analysis flsa an extension to latent semantic analysis lsa lsa is a statistical method that is ordinarily trained on word only flsa add to lsa the richness of the many other linguistic feature that a corpus may be labeled with we applied flsa to dialogue act classification with excellent result we report result on three corpus callhome spanish maptask and our own corpus of tutoring dialogue 
source separation is an important problem at the intersection of several field including machine learning signal processing and speech technology here we describe new separation algorithm which are based on probabilistic graphical model with latent variable in contrast with existing method these algorithm exploit detailed model to describe source property they also use subband filtering idea to model the reverberant environment and employ an explicit model for background and sensor noise we leverage variational technique to keep the computational complexity per em iteration linear in the number of frame 
privacy and security concern can prevent sharing of data derailing data mining project distributed knowledge discovery if done correctly can alleviate this problem the key is to obtain valid result while providing guarantee on the non disclosure of data we present a method for k mean clustering when different site contain different attribute for a common set of entity each site learns the cluster of each entity but learns nothing about the attribute at other site 
we examine the problem of generating state space compression of pomdps in a way that minimally impact decision quality we analyze the impact of compression on decision quality observing that compression tha t allow accurate policy evaluation prediction of expected future reward will not affect decision quality we derive a set of sufficient condition that ensure accu rate prediction in this respect illustrate interesting mathematical property these confer on lossless linear compression and use these to derive an iterative proce dure for finding good linear lossy compression we also elaborate on how structured representation of a pomdp can be used to find such compression 
we present an novel algorithm that reconstructs voxels of a general d specular surface from multiple image of a calibrated camera acalibrated scene i e point whose d coordinate are known isreflected by the unknown specular surface onto the image plane ofthe camera for every viewpoint surface normal are associated tothe voxels traversed by each projection ray formed by thereflection of a scene point a decision process then discardsvoxels whose associated surface normal are not consistent with oneanother the output of the algorithm is a collection of voxels andsurface normal in d space whose quality and size depend onuser set threshold the method ha been tested on synthetic andreal image visual and quantified experimental result arepresented 
we address appropriate user modeling in order to generate cooperative response to each user in spoken dialogue system unlike previous study that focus on user s knowledge or typical kind of user the user model we propose is more comprehensive specifically we set up three dimension of user model skill level to the system knowledge level on the target domain and the degree of hastiness moreover the model are automatically derived by decision tree learning using real dialogue data collected by the system we obtained reasonable classification accuracy for all dimension dialogue strategy based on the user modeling are implemented in kyoto city bus information system that ha been developed at our laboratory experimental evaluation show that the cooperative response adaptive to individual user serve a good guidance for novice user without increasing the dialogue duration for skilled user 
multimodal interface are becoming increasingly ubiquitous with the advent of mobile device accessibility consideration and novel software technology that combine diverse interaction medium in addition to improving access and delivery capability such interface enable flexible and personalized dialog with website much like a conversation between human in this paper we present a software framework for multimodal web interaction management that support mixed initiative dialog between user and website a mixed initiative dialog is one where the user and the website take turn changing the flow of interaction the framework support the functional specification and realization of such dialog using staging transformation a theory for representing and reasoning about dialog based on partial input it support multiple interaction interface and offer sessioning caching and co ordination function through the use of an interaction manager two case study are presented to illustrate the promise of this approach 
previous study have demonstrated that the appearance ofan object under varying illumination condition can be representedby a low dimensional linear subspace a set ofbasis image spanning such a linear subspace can be obtainedby applying the principal component analysis pca for a large number of image taken under different lightingconditions while the approach based on pca havebeen used successfully for object recognition under varyingillumination condition little is known about how many imageswould be required in order to obtain the basis imagescorrectly in this study we present a novel method for analyticallyobtaining a set of basis image of an object forarbitrary illumination from input image of the object takenunder a point light source the main contribution of ourwork is that we show that a set of lighting direction canbe determined for sampling image of an object dependingon the spectrum of the object s brdf in the angularfrequency domain such that a set of harmonic image canbe obtained analytically based on the sampling theorem onspherical harmonic in addition unlike the previously proposedtechniques based on spherical harmonic our methoddoes not require the d shape and reflectance property ofan object used for rendering harmonic image of the objectsynthetically 
abstract nonlinear partial differential equation pde are now widely used to regularize image they allow to eliminate noise and artifact while preserving large global feature such a object contour in this context we propose a geometric framework to design pde flow actingon constrained datasets we focus our interest on flow of matrixvalued function undergoing orthogonal and spectral constraint the correspondingevolution pde s are found by minimization of cost functionals and depend on the natural metric of the underlyingconstrained manifold viewed a lie group or homogeneous space suitable numerical scheme that fit the constraint are also presented we illustrate this theoretical framework through a recent and challenging problem in medical imaging the regularization of diffusion tensor volume dtmri 
since cdn simulation are known to be highly memory intensive in this paper we argue the need for reducing the memory requirement of such simulation we propose a novel memory efficient data structure that store cache state for a small subset of popular object accurately and us approximation for storing the state for the remaining object since popular object receive a large fraction of the request while le frequently accessed object consume much of the memory space this approach yield large memory saving and reduces error we use bloom filter to store approximate state and show that careful choice of parameter can substantially reduce the probability of error due to approximation we implement our technique into a user library for constructing proxy cache in cdn simulator our experimental result show up to an order of magnitude reduction in memory requirement of cdn simulation while incurring a error 
this paper describes an extension of the semantic grammar used in conventional statistical spoken language interface to allow the probability of derived analysis to be conditioned on the meaning or denotation of input utterance in the context of an interface s underlying application environment or world model since these denotation will be used to guide disambiguation in interactive application they must be efficiently shared among the many possible analysis that may be assigned to an input utterance this paper therefore present a formal restriction on the scope of variable in a semantic grammar which guarantee that the denotation of all possible analysis of an input utterance can be calculated in polynomial time without undue constraint on the expressivity of the derived semantics empirical test show that this model theoretic interpretation yield a statistically significant improvement on standard measure of parsing accuracy over a baseline grammar not conditioned on denotation 
in this paper we introduce coverage map a a new way of representing the environment of a mobile robot coverage map store for each cell of a given grid a posterior about the amount the corresponding cell is covered by an obstacle using this representation a mobile robot can more accurately reason about it uncertainty in the map of the environment than with standard occupancy grid we present a model for proximity sensor designed to update coverage map upon sensory input we also describe how coverage map can be used to formulate a decision theoretic approach for mobile robot exploration we present experiment carried out with real robot in which accurate map are build from noisy ultrasound data finally we present a comparison of different view point selection strategy for mobile robot exploration 
although interactive query reformulation ha been actively studied in the laboratory little is known about the actual behavior of web searcher who are offered terminological feedback along with their search result we analyze log session for two group of user interacting with variant of the altavista search engine a baseline group given no terminological feedback and a feedback group to whom twelve refinement term are offered along with the search result we examine uptake refinement effectiveness condition of use and refinement type preference although our measure of overall session success show no difference between outcome for the two group we find evidence that a subset of those user presented with terminological feedback do make effective use of it on a continuing basis 
a online document collection continue to expand both on the web and in proprietary environment the need for duplicate detection becomes more critical the goal of this work is to facilitate a investigation into the phenomenon of near duplicate and b algorithmic approach to minimizing it negative effect on search result harnessing the expertise of both client user and professional searcher we establish principled method to generate a test collection for identifying and handling inexact duplicate document 
we empirically evaluate several state of theart method for constructing ensemble of heterogeneous classiers with stacking and show that they perform at best comparably to selecting the best classier from the ensemble by cross validation we then propose a new method for stacking that us multi response model tree at the meta level and show that it clearly outperforms existing stacking approach and selecting the best classier by cross validation the work presented in this paper is set in the stacking framework we argue that selecting the best of the classiers in an ensemble generated by applying different learning algorithm should be considered a a baseline to which the stacking performance should be compared our empirical evaluation of several recent stacking approach show that they perform comparably to the best of the individual classiers a selected by cross validation but not better we then propose a new stacking method based on classication by using model tree and show that this method doe perform better than other combining approach a well a better than selecting the best individual classier section rst summarizes the stacking framework then survey some recent result and nally introduces our stacking approach based on classication via model tree the setup for the experimental comparison of several stacking method voting and selecting the best classier is described in section section present and discus the experimental result and section concludes 
the standard norm svm is known for it good performance in twoclass classification in this paper we consider the norm svm we argue that the norm svm may have some advantage over the standard norm svm especially when there are redundant noise feature we also propose an efficient algorithm that computes the whole s olution path of the norm svm hence facilitates adaptive selection of the tuning parameter for the norm svm 
we compare standard global ir searching with user centric localized technique to address the database selection problem we conduct a series of experiment to compare the retrieval effectiveness of three separate search mode applied to a hierarchically structured data environment of textual database representation the data environment is represented a a tree like directory containing over unique database and over total leaf node our search mode consist of varying degree of browse and search from a global search at the root node to a refined search at a sub node using dynamically calculated inverse document frequency idf to score candidate database for probable relevance our finding indicate that a browse and search approach that relies upon localized searching from sub node is capable of producing the most effective result 
we present a new type of multi class learning algorithm called a linear max algorithm linearmax algorithm learn with a special type of attribute called a sub expert a sub expert is a vector attribute that ha a value for each output class the goal of the multi class algorithm is to learn a linear function combining the sub expert and to use this linear function to make correct class prediction the main contribution of this work is to prove that in the on line mistake bounded model of learning a multi class sub expert learning algorithm ha the same mistake bound a a related two class linear threshold algorithm we apply these technique to three linear threshold algorithm perceptron winnow and romma we show these algorithm give good performance on artificial and real datasets 
in this paper we present a new co training strategy that make use of unlabelled data it train two predictor in parallel with each predictor labelling the unlabelled data for training the other predictor in the next round both predictor are support vector machine one trained using data from the original feature space the other trained with new feature that are derived by clustering both the labelled and unlabelled data hence unlike standard co training method our method doe not require a priori the existence of two redundant view either of which can be used for classification nor is it dependent on the availability of two different supervised learning algorithm that complement each other we evaluated our method with two classifier and three text benchmark webkb reuters newswire article and newsgroups our evaluation show that our co training technique improves text classification accuracy especially when the number of labelled example are very few 
performance profile tree have recently been proposed a a theoretical basis for fully normative deliberation control in this paper we conduct the first experimental study of their feasibility and accuracy in making stopping decision for anytime algorithm on optimization problem using data and algorithm from two different real world domain we compare performance profile tree to other well established deliberation control technique we show that performance profile tree are feasible in practice and lead to significantly better deliberation control decision we then conduct experiment using performance profile tree where deliberation control decision are made using conditioning on multiple feature of the solution to illustrate that such an approach is feasible in practice 
query ambiguity is a generally recognized problem particularly in web environment where query are commonly only one or two word in length in this study we explore one technique that find commonly occurring pattern of part of speech near a one word query and allows them to be transformed into clarification question we use a technique derived from statistical language modeling to show that the clarification query will reduce ambiguity much of the time and often quite substantially 
this paper evaluates the xlink format in comparison with other linking format the comparison is based on xspect an implementation of xlink xspect handle transformation between an open hypermedia format ohif and xlink and the paper discus this isomorphic transformation and generalises it to include another open hypermedia format fohm the xspect system based on xslt and javascript provides user with an interface to browse and merge linkbases xspect support navigational hypermedia in the form of link inserted on the fly into web page a well a guided tour presented a svg xspect ha two implementation one server side and one running on the client both implementation provide the user with an interface for the creation of annotation the main result of the paper is a critique of xlink xlink is shown to be a format well suited for navigational hypermedia but lacking in more advanced construct more problematic are the issue regarding large scale use such a evaluating validity and credibility of linkbases and ensuring general support for a format a flexible a xlink 
we propose a variational method for segmenting imagesequences into spatio temporal domain of homogeneousmotion to this end we formulate the problem of motionestimation in the framework of bayesian inference using aprior which favor domain boundary of minimal surfacearea we derive a cost functional which depends on a surfacein space time separating a set of motion region aswell a a set of vector modeling the motion in each region we propose a multiphase level set formulation of thisfunctional in which the surface and the motion region arerepresented implicitly by a vector valued level set function joint minimization of the proposed functional result in aneigenvalue problem for the motion model of each region andin a gradient descent evolution for the separating interface numerical result on real world sequence demonstratethat minimization of a single cost functional generates asegmentation of space time into multiple motion region 
decomposition method are used to convert general constraint satisfaction problem into an equivalent tree structured problem that can be solved more effectively recently diagnosis algorithm for treestructured system have been introduced but the prerequisite of coupling these algorithm to the outcome of decomposition method have not been analyzed in detail thus limiting their diagnostic applicability in this paper we generalize the tree algorithm and show how to use hypertree decomposition outcome a input to the algorithm to compute the diagnosis of a general diagnosis problem 
this paper examines the relative performance of additive and multiplicative clause weighting scheme for propositional satisfiability testing starting with one of the most recently developed multiplicative algorithm sap an experimental study wa constructed to isolate the effect of multiplicative in comparison to additive weighting while controlling other key feature of the two approach namely the use of random versus flat move deterministic versus probabilistic weight smoothing and multiple versus single inclusion of literal in the local search neighborhood a a result of this investigation we developed a pure additive weighting scheme paw which can outperform multiplicative weighting on a range of difficult problem whtle requiring considerably le effort in term of parameter turning we conclude that additive weighting show better scaling property because it make le distinction between cost and so considers a larger domain of possible move 
automatic construction of shape model from example hasbeen the focus of intense research during the last coupleof year these method have proved to be useful forshape segmentation tracking and shape understanding inthis paper novel theory to automate shape modelling is described the theory is intrinsically defined for curve althoughcurves are infinite dimensional object the theoryis independent of parameterisation and affine transformation we suggest a method for implementing the ideasand compare it to minimising the description length of themodel mdl it turn out that the accuracy of the two methodsis comparable both the mdl and our approach can getstuck at local minimum our algorithm is le computationalexpensive and relatively good solution are obtained after afew iteration the mdl is however better suited at fine tuningthe parameter given good initial estimate to theproblem it is shown that a combination of the two methodsoutperforms either on it own 
cheap camera and fast processor have made it possible to visually exploit geometric constraint in real time it ha been shown that the fast depth segmentation fds algorithm successfully exploit geometric constraint to perform visual foreground background segmentation in environment where other vision routine fail this paper present new insight into the operation of the fds algorithm that lead to the concept of a virtual surface margin we then show how surface margin can be used to extend the fds algorithm and thereby enable a class of logical volume operation that go far beyond simple background segmentation task an example application called touchit is demonstrated touchit utilizes surface margin to create a virtual volume configuration that is useful for detecting physical proximity to a surface we also present refinement in the the implementation of the fds algorithm that make these volumetric computation practical for interactive application by taking advantage of the single instruction multiple data simd instruction set extension that have recently become commonly available in consumer grade microprocessor 
this paper present a novel local feature selection approach for text categorization it construct a feature set for each category by first selecting a set of term highly indicative of membership a well a another set of term highly indicative of non membership then unifying the two set the size ratio of the two set wa empirically chosen to obtain optimal performance this is in contrast with the standard local feature selection approach that either only select the term most indicative of membership or implicitly but not optimally combine the term most indicative of membership with non membership the experimental comparison between the proposed approach and standard approach wa conducted on four feature selection metric chisquare correlation coefficient odds ratio and g coefficient the result show that the proposed approach improves text categorization performance 
we present a new approach to recognizing event invideos we first detect and track moving object in thescene based on the shape and motion property ofthese object we infer probability of primitive eventsframe by frame by using bayesian network compositeevents consisting of multiple primitive event overextended period of time are analyzed by using a hidden semi markov finite state model this result in morereliable event segmentation compared to the use of standardhmms in noisy video sequence at the cost of someincrease in computational complexity we describe ourapproach to reducing this complexity we demonstratethe effectiveness of our algorithm using both real worldand pertubed data 
there are many challenge associated with the integration ofsynthetic and real imagery one particularly difficult problem isthe automatic extraction of salient parameter of natural phenomenain real video footage for subsequent application to syntheticobjects can we ensure that the hair and clothing of a syntheticactor placed in a meadow of swaying grass will move consistentlywith the wind that moved that grass the video footage can be seenas a controller for the motion of synthetic feature a concept wecall video input driven animation vida we propose a schema thatanalyzes an input video sequence extract parameter from themotion of object in the video and us this information to drivethe motion of synthetic object to validate the principle ofvida we approximate the inverse problem to harmonic oscillation which we use to extract parameter of wind and of regular waterwaves we observe the effect of wind on a tree in a video estimatewind speed parameter from it motion and then use this to makesynthetic object move we also extract water elevation parametersfrom the observed motion of boat and apply the resulting waterwaves to synthetic boat 
this article present a novel representation for dynamic scene composed of multiple rigid object that may undergo different motion and are observed by a moving camera multi view constraint associated with group of affine co variant scene patch and a normalized description of their appearance are used to segment a scene into it rigid component construct three dimensional model of these component and match instance of model recovered from different image sequence the proposed approach ha been applied to the detection and matching of moving object in video sequence and to shot matching i e the identification of shot that depict the s ame scene in a video clip 
we propose a new statistical approach to extracting personal name from a corpus one of the key point of our approach is that it can both automatically learn the characteristic of personal name from a large training corpus and make good use of human empirical knowledge e g context free grammar furthermore our approach also assigns confidence measure to the extracted personal name compared with traditional simple true false determination another main contribution of this work is that we have applied the personal name extraction technology into a real application which is a chinese inputting system and have achieved an approximately error rate reduction for all character and error rate reduction for personal name 
randomization is an economical and efficient approach for privacy preserving data mining ppdm in order to guarantee the performance of data mining and the protection of individual privacy optimal randomization scheme need to be employed this paper demonstrates the construction of optimal randomization scheme for privacy preserving density estimation we propose a general framework for randomization using mixture model the impact of randomization on data mining is quantified by performance degradation and mutual information loss while privacy and privacy loss are quantified by interval based metric two different type of problem are defined to identify optimal randomization for ppdm illustrative example and simulation result are reported 
abstract high retrieval precision in content based image retrieval can be attained by adopting relevance feedback mechanism these mechanism require that the user judge the quality of the result of the query by marking all the retrieved image a being either relevant or not then the search engine exploit this information to adapt the search to better meet user s need at present the vast majority of proposed relevance feedback mechanism are formulated in term of search model that ha to be optimized such an optimization involves the modification of some search parameter so that the nearest neighbor of the query vector contains the largest number of relevant image in this paper a different approach to relevance feedback is proposed after the user provides the first feedback following retrieval are not based on knn search but on the computation of a relevance score for each image of the database this score is computed a a function of two distance namely the distance from the nearest non relevant image and the distance from the nearest relevant one image are then ranked according to this score and the top k image are displayed reported result on three image data set show that the proposed mechanism outperforms other state of the art relevance feedback mechanism in t rod u ct i on 
we present an emerging indoor assisted navigation system for the visually impaired the core of the system is a mobile robotic base with a sensor suite mounted on it the sensor suite consists of an rfid reader and a laser range finder small passive rfid sensor are manually inserted in the environment we describe how the system wa deployed in two indoor environment and evaluated by visually impaired participant in a series of pilot experiment 
this work evaluates a few search strategy for arabic monolingual and cross lingual retrieval using the trec arabic corpus a the test bed the release by nist in of an arabic corpus of nearly k document with both monolingual and cross lingual query and relevance judgment ha been a new enabler for empirical study experimental result show that spelling normalization and stemming can significantly improve arabic monolingual retrieval character tri gram from stem improved retrieval modestly on the test corpus but the improvement is not statistically significant to further improve retrieval we propose a novel thesaurus based technique different from existing approach to thesaurus based retrieval ours formulates word synonym a probabilistic term translation that can be automatically derived from a parallel corpus retrieval result show that the thesaurus can significantly improve arabic monolingual retrieval for cross lingual retrieval clir we found that spelling normalization and stemming have little impact 
dynamically balancing robot have recently been made available by segway llc in the form of the segway rmp robot mobility platform we have addressed the challenge of using these rmp robot to play soccer building up upon our extensive previous work in this multi robot research domain in this paper we make three contribution first we present a new domain called segway soccer for investigating the coordination of dynamically formed mixed human robot team within the realm of a team task that requires real time decision making and response segway soccer is a game of soccer between two team consisting of both segway riding human and segway rmps we believe segway soccer is the first game involving both human and robot in cooperative role and with similar capability in conjunction with this new domain we present our work towards developing a soccer playing robot using the rmp platform with vision a it primary sensor our third contribution is that of skill acquisition from a human teacher where the learned skill is then used seamlessly during robot execution a part of it control hierarchy skill acquisition and use address the challenge or rapidly developing the low level action that are environment dependent and are not transferable across robot 
pearl s probabilistic causal model ha been used in many domain to reason about causality pearl s treatment of action is very diffewnt from the way action are represented explicitly in action language in this paper we show how to encode pearl s probabilistic causal model in the action language pal thus relating this two distinct approach to reasoning about action 
we show that two important property of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequence the property are simple cell like receptive field and complex cell like pooling of simple cell output which emerge when we apply two different approach to temporal coherence in the first approach we extract receptive field whose output are a temporally coherent a possible this approach yield simple cell like receptive field oriented localized multiscale thus temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive field the second approach is based on a two layer statistical generative model of natural image sequence in addition to modeling the temporal coherence of individual simple cell this model includes inter cell temporal dependency estimation of this model from natural data yield both simple cell like receptive field and complex cell like pooling of simple cell output in this completely unsupervised learning both layer of the generative model are estimated simultaneously from scratch this is a significant improvement on earlier statistical model of early vision where only one layer ha been learned and others have been fixed a priori 
this paper present an algorithm for computing optical flow shape motion lighting and albedo from an image sequenceof a rigidly moving lambertian object under distant illumination the problem is formulated in a manner that subsumesstructure from motion multi view stereo and photometricstereo a special case the algorithm utilizes bothspatial and temporal intensity variation a cue the formerconstrains flow and the latter constrains surface orientation combining both cue enables dense reconstruction ofboth textured and texture le surface the algorithm worksby iteratively estimating affine camera parameter illumination shape and albedo in an alternating fashion result aredemonstrated on video of hand held object moving in frontof a fixed light and camera 
although the v support vector machine v svm schslkopf et al ha the advantage of using a single parameter v to control both the number of support vector and the fraction of margin error there are two issue that prevent it from being used in many real world application first unlike the c svm that allows asymmetric misclassification cost v svm us a symmetric misclassification cost while lower error rate is promoted by this symmetric mi qclassification cost it is not always the preferred measure in many application second the additional constraint from vsvm make it training more difficult sequential minimal optimization smo algorithm that are very easy to implement and scalable to very large problem do not exist in a good form for v svm in this paper we proposed two new v svm formulation these formulation introduce mean to control the misclassification cost ratio between false positive and false negative while preserving the intuitive parameter v we also propose a smo algorithm for the v svm classification problem experiment show that our new v svm formulation is effective in incorporating asymmetric misclassification cost and the smo algorithm for v svm is comparable in speed to that for c svm 
pseudo feedback is a commonly used technique to improve information retrieval performance it assumes a few top ranked document to be relevant and learns from them to improve the retrieval accuracy a serious problem is that the performance is often very sensitive to the number of pseudo feedback document in this poster we address this problem in a language modeling framework we propose a novel two stage mixture model which is le sensitive to the number of pseudo feedback document than an effective existing feedback model the new model can tolerate a more flexible setting of the number of pseudo feedback document without the danger of losing much retrieval accuracy 
statistical background modelling and subtraction ha proved to be apopular and effective class of algorithm for segmentingindependently moving foreground object outfrom a staticbackground without requiring any a priori information of theproperties of foreground object this paper present twocontributions on this topic aimed towards robotics where an activehead is mounted on a mobile vehicle in period when the vehicle swheels are not driven camera translation is virtually zero andbackground subtraction technique are applicable part of thiswork are also highly relevant to surveillance and videoconferencing the first part of the paper present an efficientprobabilistic framework for when the camera pan and tilt aunified approach is developed for handling various source oferror including motion blur sub pixel camera motion mixed pixelsat object boundary and also uncertainty in backgroundstabilisation caused by noise unmodelled radial distortion andsmall translation of the camera the second contribution regard abayesian approach to specifically incorporate uncertaintyconcerning whether the background ha yet been uncovered by movingforeground object this is an important requirement duringinitialisation of a system we cannot assume that a backgroundmodel is available in advance since that would involve storingmodels for each possible position in every room of the robot soperating environment instead the background model must begenerated online very possibly in the presence of moving object 
web accessibility is an important goal however most approach to it attainment are based on unrealistic economic model in which web content developer are required to spend too much for which they receive too little we believe this situation is due in part to the overly narrow definition given both to those who stand to benefit from enhanced access to the web and what is meant by this enhanced access in this paper we take a broader view discussing a complementary approach that cost developer le and provides greater advantage to a larger community of user while we have quite specific aim in our technical work we hope it can also serve a an example of how the technical conversation regarding web accessibility can move beyond the narrow confines of limited adaptation for small population 
this paper devise a novel kernel function for structured natural language data in the field of natural language processing feature extraction consists of the following two step syntactically and semantically analyzing raw data i e character string then representing the result a discrete structure such a parse tree and dependency graph with part of speech tag creating possibly high dimensional numerical feature vector from the discrete structure the new kernel called hierarchical directed acyclic graph hdag kernel directly accept dag whose node can contain dag hdag data structure are needed to fully reflect the syntactic and semantic structure that natural language data inherently have in this paper we define the kernel function and show how it permit efficient calculation experiment demonstrate that the proposed kernel are superior to existing kernel function e g sequence kernel tree kernel and bag of word kernel 
multi agent system result from interaction between individual agent through these interaction different kind of relationship are formed which can impact substantially on the overall system performance however the behaviour of agent cannot always be anticipated especially when dealing with open and complex system open agent system must incorporate relationship management mechanism to constrain agent action and allow only desirable interaction in consequence in this paper we tackle two important issue firstly in addressing management we identify the range of different control mechanism that are required and when they should be applied secondly in addressing relationship we present a model for identifying and characterising relationship in a manner that is application neutral and amenable to automation 
observation from a unique investigation of failure analysis of information retrieval ir research engine are presented the reliable information access ria workshop invited seven leading ir research group to supply both their system and their expert to an effort to analyze why their system fail on some topic and whether the failure are due to system flaw approach flaw or the topic itself there were surprising result from this cross system failure analysis one is that despite system retrieving very different document the major cause of failure for any particular topic wa almost always the same across all system another is that relationship between aspect of a topic are not especially important for state of the art system the system are failing at a much more basic level where the top retrieved document are not reflecting some aspect at all 
a default conditional ha most often been informally interpreted a a defeasible version of a classical conditional usually the material conditional that is the intuition is that a default should behave implicitly or explicitly a it say material counterpart by default or unless explicitly overridden in this paper we develop an alternative interpretation in which a default is regarded more like a rule leading from premise to conclusion to this end a general semantic framework under a rule based interpretation is developed and a family of weak conditional logic is specified along with associated proof theory nonmonotonic inference is defined very easily in these logic one obtains a rich set of nonmonotonic inference concerning the incorporation of irrelevant property and of property inheritance moreover this interpretation resolve problem that have been associated with previous approach 
we develop and test new machine learning method for the predictionof topological representation of protein structure in the formof coarseor fine grained contact or distance map that are translationand rotation invariant the method are based on generalizedinput output hidden markov model giohmms and generalizedrecursive neural network grnns the method are used to predicttopology directly in the fine grained case and in the coarsegrainedcase indirectly by first 
we provide a reformulation of the constraint hierarchy chs framework based on the notion of error indicator adapting the generalized view of local consistency in semiring based constraint satisfaction problem s csps we define constraint hierarchyk consistency ch k c and give a ch c enforcement algorithm we demonstrate how the ch c algorithm can be seamlessly integrated into the ordinary branch and bound algorithm to make it a finite d omain ch solver experimentation confirms the efficiency and robustness of our proposed solver prototype unlike other finite domain ch solver our pr oposed method work for both local and global comparators in addition our solver can support arbitrary error function 
a new distance measure between probability density function pdfs is introduced which we refer to a the laplacian pdf distance the laplacian pdf distance exhibit a remarkable connection to mercer kernel based learning theory via the parzen window technique for density estimation in a kernel feature space dened by the eigenspectrum of the laplacian data matrix this pdf distance is shown to measure the cosine of the angle between cluster mean vector the laplacian data matrix and hence it eigenspectrum can be obtained automatically based on the data at hand by optimal parzen window selection we show that the laplacian pdf distance ha an interesting interpretation a a risk function connected to the probability of error 
in this paper we define and study a novel text mining problem which we refer to a comparative text mining ctm given a set of comparable text collection the task of comparative text mining is to discover any latent common theme across all collection a well a summarize the similarity and difference of these collection along each common theme this general problem subsumes many interesting application including business intelligence and opinion summarization we propose a generative probabilistic mixture model for comparative text mining the model simultaneously performs cross collection clustering and within collection clustering and can be applied to an arbitrary set of comparable text collection the model can be estimated efficiently using the expectation maximization em algorithm we evaluate the model on two different text data set i e a news article data set and a laptop review data set and compare it with a baseline clustering method also based on a mixture model experiment result show that the model is quite effective in discovering the latent common theme across collection and performs significantly better than our baseline mixture model 
biochemical signal transduction network are the biological information processing system by which individual cell from neuron to amoeba perceive and respond to their chemical environment we introduce a simplified model of a single biochemical relay and analyse it capacity a a communication channel a diffusible ligand is released by a sending cell and received by binding to a transmembrane receptor protein on a receiving cell this receptor ligand interaction creates a nonlinear communication channel with non gaussian noise we model this channel numerically and study it response to input signal of different frequency in order to estimate it channel capacity stochastic effect introduced in both the diffusion process and the receptor ligand interaction give the channel low pas characteristic we estimate the channel capacity using a water filling formula adapted from the additive white noise gaussian channel 
abstract in many application of graphical model arising in computer vision the hi dden variable of interest are most naturally specified by continuous non gaussian distribution there ex ist inference algorithm for discrete approximation to these continuous distribution but fo r the high dimensional variable typically of interest discrete inference becomes infeasible stochastic method such a particle filter provide an appealing alternative however existing technique fail to exp loit the rich structure of the graphical model describing many vision problem drawing on idea from regularized particle filter and belief propagation b p this paper develops a nonparametric belief propagation nbp algorithm applicable to general graph each nbp iteration us an efficient sampling procedure to update kernel based approximation t o the true continuous likelihood the algorithm can accomodate an extremely broad class of potential function including nonparametric representation thus nbp extends particle filtering method t o the more general vision problem that graphical model can describe we apply the nbp algorithm to infer component interrelationship in a part based face model allowing location and reconstruction o f occluded feature this report describes research done within the laboratory for information and decision system and the artificial intelli 
this paper proposes a solution for the automatic detection and tracking of human motion in image sequence due to the complexity of the human body and it motion automatic detection of d human motion remains an open and important problem existing approach for automatic detection and tracking focus on d cue and typically exploit object appearance color distribution shape or knowledge of a static background in contrast we exploit d optical flow information which provides rich descriptive cue while being independent of object and background appearance to represent the optical flow pattern of people from arbitrary viewpoint we develop a novel representation of human motion using low dimensional spatio temporal model that are learned using motion capture data of human subject in addition to human motion the foreground we probabilistically model the motion of generic scene the background these statistical model are defined a gibbsian field specified from the first order derivative of motion observation detection and tracking are posed in a principled bayesian framework which involves the computation of a posterior probability distribution over the model parameter i e the location and the type of the human motion given a sequence of optical flow observation particle filtering is used to represent and predict this non gaussian posterior distribution over time the model parameter of sample from this distribution are related to the pose parameter of a d articulated model e g the approximate joint angle and movement direction thus the approach prof suitable for initializing more complex probabilistic model of human motion a shown by experiment on real image sequence our method is able to detect and track people under different viewpoint with complex background 
variational inference method including mean field method and loopy belief propagation have been widely used for approximate probabilistic inference in graphical model while often le accurate than mcmc variational method provide a fast deterministic approximation to marginal and conditional probability such approximation can be particularly useful in high dimensional problem where sampling method are too slow to be effective a limitation of current method however is that they are restricted to parametric probabilistic model mcmc doe not have such a limitation indeed mcmc sampler have been developed for the dirichlet process dp a nonparametric distribution on distribution ferguson that is the cornerstone of bayesian nonparametric statistic escobar west neal in this paper we develop a mean field variational approach to approximate inference for the dirichlet process where the approximate posterior is based on the truncated stick breaking construction ishwaran james we compare our approach to dp sampler for gaussian dp mixture model 
we have been investigating an interactive approach for open domain qa odqa and have constructed a spoken interactive odqa system the system derives disambiguating query dqs that draw out additional information to test the efficiency of additional information requested by the dqs the system reconstructs the user s initial question by combining the addition information with question the combination is then used for answer extraction experimental result revealed the potential of the generated dqs 
a memory based heuristic is a heuristic function that is stored in a lookup table very accurate heuristic have been created by building very large lookup table sometimes called pattern database most previous work assumes that a memory based heuristic is computed for the entire state space and the cost of computing it is amortized over many problem instance but in some case it may be useful to compute a memory based heuristic for a single problem instance if the start and goal state of the problem instance are used to restrict the region of the state space for which the heuristic is needed the time and space used to compute the heuristic may be substantially reduced in this paper we review recent work that us this idea to compute space efficient heuristic for the multiple sequence alignment problem we then describe a novel development of this idea that is simpler and more general our approach lead to improved performance in solving the multiple sequence alignment problem and is general enough to apply to other domain 
syntactic information potentially play a much more important role in question answering than it doe in information retrieval although many people have used syntactic evidence in question answering there haven t been many detailed experiment reported in the literature the aim of the experiment described in this paper is to study the impact of a particular approach for using syntactic information on question answering effectiveness our result indicate that a combination of syntactic information with heuristic for ranking potential answer can perform better than the ranking heuristic on their own 
in this paper we describe the concept of federated information sharing community fisc and associated architecture which provide a way for organisation distributed workgroups and individual to build up a federated community based on their common interest over the world wide web to support community we develop capability that go beyond the generic retrieval of document to include the ability to retrieve people their interest and inter relationship we focus on providing social awareness in the large to help user understand the member within a community and the relationship between them within the fisc framework we provide viewpoint retrieval to enable a user to construct visual contextual view of the community from the perspective of any community member to evaluate these idea we develop test bed to compare individual component technology such a user and group profile construction and similarity matching and we develop prototype to explore the broader architecture and usage issue 
despite increasing deployment of agent technology in several business and industry domain user confidence in fully automated agent driven application is noticeably lacking the main reason for such lack of trust in complete automation are scalability and nonexistence of reasonable guarantee in the performance of selfadapting software in this paper we address the latter issue in the context of learning agent in a multiagent system ma performance guarantee for most existing on line multiagent learning mal algorithm are realizable only in the limit thereby seriously limiting it practical utility our goal is to provide certain meaningful guarantee about the performance of a learner in a ma while it is learning in particular we present a novel mal algorithm that i converges to a best response against stationary opponent ii converges to a nash equilibrium in self play and iii achieves a constant bounded expected regret at any time no average regret asymptotically in arbitrary sized general sum game with non negative payoff and against any number of opponent 
given a set of image acquired from known viewpoint wedescribe a method for synthesizing the image which wouldbe seen from a new viewpoint in contrast to existing technique which explicity reconstruct the d geometry of thescene we transform the problem to the reconstruction ofcolour rather than depth this retains the benefit of geometricconstraints but project out the ambiquities in depthestimation which occur in textureless region on the other hand regularization is still needed in orderto generate high quality image the paper s secondcontribution is to constrain the generated view to lie in thespace of image whose texture statistic are those of the inputimages this amount to a image based prior on thereconstruction which regularizes the solution yielding realisticsynthetic view example are given of new viewgeneration for camera interpolated between the acquisitionviewpoints which enables synthetic steadicam stabilizationof a sequence with a high level of realism 
given a set of image acquired from known viewpoint we describe a method for synthesizing the image which would be seen from a new viewpoint in contrast to existing technique which explicitly reconstruct the d geometry of the scene we transform the problem to the reconstruction of colour rather than depth this retains the benefit of geometric constraint but project out the ambiguity in depth estimation which occur in textureless region on the other hand regularization is still needed in order to generate high quality image the paper s second contribution is to constrain the generated view to lie in the space of image whose texture statistic are those of the input image this amount to an image based prior on the reconstruction which regularizes the solution yielding realistic synthetic view example are given of new view generation for camera interpolated between the acquisition viewpoint which enables synthetic steadicam stabilization of a sequence with a high level of realism 
abstract to exploit the similarity information hidden in the hyperlinkstructure of the web this paper introduces algorithmsscalable to graph with billion of vertex on a distributedarchitecture the similarity of multi step neighborhood ofvertices are numerically evaluated by similarity function includingsimrank a recursive refinement of cocitation psimrank a novel variant with better theoretical characteristic and the jaccard coe cient extended to multi stepneighborhoods our 
vision algorithm utilizing camera network with a commonfield of view are becoming increasingly feasible andimportant calibration of such camera network is a challengingand cumbersome task the current approach forcalibration using plane or a known d target may not befeasible a these object may not be simultaneously visiblein all the camera in this paper we present a new algorithmto calibrate camera using occluding contour of sphere in general an occluding contour of a sphere project to anellipse in the image our algorithm us the projection ofthe occluding contour of three sphere and solves for theintrinsic parameter and the location of the sphere theproblem is formulated in the dual space and the parametersare solved for optimally and efficiently using semi definiteprogramming the technique is flexible accurate and easyto use in addition since the contour of a sphere is simultaneouslyvisible in all the camera our approach cangreatly simplify calibration of multiple camera with a commonfield of view experimental result from computer simulateddata and real world data both for a single cameraand multiple camera are presented 
we consider a sequence of three model for skin detection built from a large collection of labelled image each model is a maximum entropy model with respect to constraint concerning marginal distribution our model are nested the first model is well known from practitioner pixel are considered a independent the second model is a hidden markov model it includes constraint that force smoothness of the solution the third model is a first order model the full color gradient is included parameter estimation a well a optimization cannot be tackled without approximation we use thoroughly bethe tree approximation of the pixel lattice within it parameter estimation is eradicated and the belief propagation algorithm permit to obtain exact and fast solution for skin probability at pixel location we then ass the performance on the compaq database 
we present an algorithm to overcome the local maximum problem in estimating the parameter of mixture model it combine existing approach from both em and a robust tting algorithm ransac to give a data driven stochastic learning scheme minimal subset of data point sufcient to constrain the parameter of the model are drawn from proposal density to discover new region of high likelihood the proposal density are learnt using em and bias the sampling toward promising solution the algorithm is computationally efcient a well a effective at escaping from local maximum we compare it with alternative method including em and ransac on both challenging synthetic data and the computer vision problem of alpha matting 
recent result on sparse coding and independent component analysis suggest that human vision first represents a visual image by a linear superposition of a relatively small number of localized elongate oriented image base with this representation the sketch of an image consists of the location orientation and elongation of the image base and the sketch can be visually illustrated by depicting each image base by a linelet of the same length and orientation built on the insight of sparse and independent component analysis we propose a two level generative model for texture at the bottom level the texture image is represented by a linear superposition of image base at the top level a markov model is assumed for the placement of the image base or the sketch and the model is characterized by a set of simple geometrical feature statistic 
it is not easy to tokenize agglutinative language like japanese and chinese into word many ir system start with a dictionary based morphology program like chasen unfortunately dictionary cannot cover all possible word unknown word such a proper noun are important for ir this paper proposes a statistical dictionary free method for selecting index string based on recent work on adaptive language modeling 
in this paper we introduce new algorithm for unsupervised learningbased on the use of a kernel matrix all the information requiredby such algorithm is contained in the eigenvectors of thematrix or of closely related matrix we use two dierent but relatedcost function the alignment and the cut cost the rstone is discussed in a companion paper the second one is basedon graph theoretic concept both function measure the level ofclustering of a labeled dataset or 
in this paper we propose a multi criterion based active learning approach and effectively apply it to named entity recognition active learning target to minimize the human annotation effort by selecting example for labeling to maximize the contribution of the selected example we consider the multiple criterion informativeness representativeness and diversity and propose measure to quantify them more comprehensively we incorporate all the criterion using two selection strategy both of which result in le labeling cost than single criterion based method the result of the named entity recognition in both muc and genia show that the labeling cost can be reduced by at least without degrading the performance 
complex object can often be conveniently represented by fin ite set of simpler component such a image by set of patch or text by bag of word we study the class of positive definite p d kerne l for two such object that can be expressed a a function of the merger of their respective set of component we prove a general integral representation of such kernel and present two particular example one of them lead to a kernel for set of point living in a space endowed itself with a positive definite kernel we provide experimental result o n a benchmark experiment of handwritten digit image classification whic h illustrate the validity of the approach 
we describe a self configuring neuromorphic chip that us a model of activity dependent axon remodeling to automatically wire topographic map based solely on input correlation axon are guided by growth cone which are modeled in analog vlsi for the first time growth cone migrate up neurotropin gradient which are represented by charge diffusing in transistor channel virtual axon move by rerouting address event we refined an initially gross topographic projection by simulating retinal wave input 
a unied biophysically motivated calcium dependent learning model ha been shown to account for various rate based and spike time dependent paradigm for inducing synaptic plasticity here we investigate the property of this model for a multi synapse neuron that receives input with dieren t spike train statistic in addition we present a physiological form of metaplasticity an activity driven regulation mechanism that is essential for the robustness of the model a neuron thus implemented develops stable and selective receptive eld given various input statistic 
d surface classification is a fundamental problem incomputer vision and computational geometry surface canbe classified by different transformation group traditionalclassification method mainly use topological transformationgroups and euclidean transformation group this paperintroduces a novel method to classify surface by conformaltransformation group conformal equivalent classis refiner than topological equivalent class and coarser thanisometric equivalent class making it suitable for practicalclassification purpose for general surface the gradientfields of conformal map form a vector space which hasa natural structure invariant under conformal transformation we present an algorithm to compute this conformalstructure which can be represented a matrix and use itto classify surface the result is intrinsic to the geometry invariant to triangulation and insensitive to resolution tothe best of our knowledge this is the first paper to classifysurfaces with arbitrary topology by global conformal invariant the method introduced here can also be used forsurface matching problem 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
typically markov decision problem mdps assume a single action is executed per decision epoch but in the real world one may frequently execute certain action in parallel this paper explores concurrent mdps mdps which allow multiple non conflicting action to be executed simultaneously and present two new algorithm our first approach exploit two provably sound pruning rule and thus guarantee solution optimality our second technique is a fast sampling based algorithm which produce c ose to optimal solution extremely quickly experiment show that our approach outperform the existing algorithm producing up to two order of magnitude speedup 
abstract recursive conditioning rc is an any space algorithm for exact inference in bayesian network which can trade space for time in increment of the size of a floating point number this smooth tradeoff is possible by varying the algorithm s cache size when rc is run with a constrained cache size an important problem arises which specific result should be cached in order to minimize the running time of the algorithm rc is driven by a structure known a a dtree and many such dtrees exist for a given bayesian network in this paper we examine the problem of searching for an optimal caching scheme for a given dtree and present some optimal time space tradeoff curve for given dtrees of several published bayesian network we also compare these curve to the memory requirement of state of the art algorithm based on jointrees our result show that the memory requirement of these network can be significantly reduced with only a minimal cost in time allowing for exact inference in situation previously impractical they also show that probabilistic reasoning system can be efficiently designed to run under varying amount of memory 
in general term the evaluation of a summary depends on how close it is to the chief point in the source text this begets the question a to what are the chief point in the source text and how is this information used in itself in identifying the source text this is crucially important when we discus automatic evaluation of summary so the question of main point is the source text typically this would be around a nucleus of keywords however the salience the frequency and the relationship of the text with other text in the collection of these keywords is perhaps are important text categorisation using neural network explicates these point well and also ha a practical impact 
to bridge the gap between low level feature and high level semantic query in image retrieval detecting meaningful visual entity e g face sky foliage building etc based on trained pattern classifier ha become an active research trend however a drawback of the supervised learning approach is the human effort to provide labeled region a training sample in this paper we propose a new three stage hybrid framework to discover local semantic pattern and generate their sample for training with minimal human intervention support vector machine svm are first trained on local image block from a small number of image labeled a several semantic category then to bootstrap the local semantics image block that produce high svm output are grouped into discovered semantic region dsrs using fuzzy c mean clustering the training sample for these dsrs are automatically induced from cluster membership and subject to support vector machine learning to form local semantic detector for dsrs an image is then indexed a a tessellation of dsr histogram and matched using histogram intersection we evaluate our method against the linear fusion of color and texture feature using semantic query on heterogeneous consumer photo the dsr model achieved a promising improvement in average precision over that of the feature fusion approach 
we present a novel strategy for automatically debugging program given sampled data from thousand of actual user run our goal is to pinpoint those feature that are most correlated with crash this is accomplished by maximizing an appropriately defined utility function it ha analogy with intuitive debugging heuristic and a we demonstrate is able to deal with various type of bug that occur in real program 
multi robot learning face all of the challenge of robot learning with all of the challenge of multiagent learning there ha been a great deal of recent research on multiagent reinforcement learning in stochastic game which is the intuitive extension of mdps to multiple agent this recent work although general ha only been applied to small game with at most hundred of state on the other hand robot task have continuous and often complex state and action space robot learning task demand approximation and generalization technique which have only received extensive attention in single agent learning in this paper we introduce grawolf a general purpose scalable multiagent learning algorithm it combine gradient based policy learning technique with the wolf win or learn fast variable learning rate we apply this algorithm to an adversarial multi robot task with simultaneous learning we show result of learning both in simulation and on the real robot these result demonstrate that grawolf can learn successful policy overcoming the many challenge in multi robot learning 
we investigate the generalization performance of some learning problem in hilbert function space we introduce a concept of scalesensitive effective data dimension and show that it characterizes the convergence rate of the underlying learning problem using this concept we can naturally extend result for parametric estimation problem in finite dimensional space to non parametric kernel learning method we derive upper bound on the generalization performance and show that the resulting convergent rate are optimal under various circumstance 
government regulation are semi structured text document that are often voluminous heavily cross referenced between provision and even ambiguous multiple source of regulation lead to difficulty in both understanding and complying with all applicable code in this work we propose a framework for regulation management and similarity analysis an online repository for legal document is created with the help of text mining tool and user can access regulatory document either through the natural hierarchy of provision or from a taxonomy generated by knowledge engineer based on concept our similarity analysis core identifies relevant provision and brings them to the user s attention and this is performed by utilizing both the hierarchical and referential structure of regulation to provide a better comparison between provision preliminary result show that our system reveals hidden similarity that are not apparent between provision based on node content comparison 
a program that make an existing website look like a database is called a wrapper wrapper learning is the problem of learning website wrapper from example we present a wrapper learning system called wl that can exploit several different representation of a document example of such different representation include dom level and token level representation a well a two dimensional geometric view of the rendered page for tabular data and representation of the visual appearance of text asm it will be rendered additionally the learning system is modular and can be easily adapted to new domain and task the learning system described is part of an industrial strength wrapper management system that is in active use at whizbang lab controlled experiment show that the learner ha broader coverage and a faster learning rate than earlier wrapper learning system 
hypermedia system and more specifically open hypermedia system oh provide a rich set of implementation of different hypertext flavor such a navigational hypertext spatial hypertext or taxonomic hypertext additionally these system offer component based modular architecture and address interoperability between hypertext domain despite multiple effort of integrating web client a widespread adoption of oh technology by web developer ha not taken place in this paper it is argued that web service which offer a component model for web application can be integrated in oh an architectural integration is proposed a step by step process is outlined and an example of integration is provided this very approach is aimed to benefit both world the web community with new rich hypermedia functionality that extends the current navigational hypermedia and the oh community by opening it tool and platform to the many developer group of the web community 
we present a fast iterative algorithm for identifying the support vector of a given set of point our algorithm work by maintaining a candidate support vector set it us a greedy approach to pick point for inclusion in the candidate set when the addition of a point to the candidate set is blocked because of other point already present in the set we use a backtracking approach to prune away such point to speed up convergence we initialize our algorithm with the nearest pair of point from opposite class we then use an optimization based approach to increment or prune the candidate support vector set the algorithm make repeated pass over the data to satisfy the kkt constraint the memory requirement of our algorithm scale a o s in the average case where s is the size of the support vector set we show that the algorithm is extremely competitive a compared to other conventional iterative algorithm like smo and the npa we present result on a variety of real life datasets to validate our claim 
we describe scot a spoken conversational tutor which ha been implemented in order to investigate the advantage of natural language in tutoring especially spoken language scot us a generic architecture for conversational intelligence which ha capability such a turn management and coordination of multi modal input and output scot also includes a set of domain independent tutorial recipe a domain specific production rule knowledge base and many natural language component including a bi directional grammar a speech recognizer and a text to speech synthesizer scot lead a reflective tutorial discussion based on the detail of a problem solving session with a real time navy shipboard damage control simulator the tutor attempt to identify and remediate gap in the student s understanding of damage control doctrine by decomposing it tutorial goal into dialogue act which are then acted on by the dialogue manager to facilitate the conversation 
we have recently embarked on a three year project funded by the nasa astep program to develop robotic astrobiology in the process of learning the limit of life in the atacama desert of chile we see this a an opportunity to develop a more science aware rover one that on encountering a new area can select interesting feature perform initial experiment and selectively return relevant data all before receiving feedback from the science team several component of the proposed science autonomy system can make use of classiers is this the kind of rock we are looking for and clustering algorithm is this rock like anything we have already sampled the unknown character of unexplored area motivates use of on line learning technique 
to model combinatorial decision problem involving uncertainty and probability we extend the stochastic constraint programming framework proposed in walsh along a number of important dimension e g to multiple chance constraint and to a range of new objective we also provide a new but equivalent semantics based on scenario using this semantics we can compile stochastic constraint program down into conventional nonstochastic constraint program this allows u to exploit the full power of existing constraint solver we have implemented this framework for decision making under uncertainty in stochastic opl a language which is based on the opl constraint modelling language hentenryck et al to illustrate the potential of this framework we model a wide range of problem in area a diverse a finance agriculture and production 
the relation between answer set programming asp and propositional satisfiability sat is at the center of many research paper partly because of the tremendous performance boost of sat solver during last year various translation from asp to sat are known but the resulting sat formula either includes many new variable or may have an unpractical size there are also well known result showing a one to one correspondence between the answer set of a logic program and the model of it completion unfortunately these result only work for specific class of problem in this paper we present a sat based decision procedure for answer set programming that i deal with any non disjunctive logic program ii work on a sat formula without additional variable and iii is guaranteed to work in polynomial space further our procedure can be extended to compute all the answer set still working in polynomial space the experimental result of a prototypical implementation show that the approach can pay off sometimes by order of magnitude 
we introduce an information theoretic method for nonparametric nonlinear dimensionality reduction based on the infinite cluster limit of rate distortion theory by constraining the information available to manifold coordinate a natural probabilistic map emerges that assigns original data to corresponding point on a lower dimensional manifold with only the information distortion trade off a a parameter our method determines the shape of the manifold it dimensionality the probabilistic map and the prior that provide optimal description of the data 
abstract cycorp ha developed a knowledge acquisitionsystem based on cyc that can engage a user in anatural language mixed initiative dialogue inorder to achieve a intelligent dialogue with theuser it employ explicit topicand user modeling a system of prioritized interaction and atransparent agenda to which either the user or thesystem can add interaction at any time 
tracking human motion is an integral part to developing powerful human computer interface several successful tracking algorithm were developed that model human body a an articulated tree we propose a learning based method for creating such articulated model from observation of multiple rigid motion this paper is concerned with recovering topology of the articulated model when the rigid motion of constituent segment is known our approach is based on finding the maximum likelihood tree shaped factorization of the joint probability density function pdf of rigid segment motion the topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body we demonstrate the performance of our algorithm both on synthetic and real motion capture data 
abstract many researcher in artificial intelligence are beginning to explore the use of soft constraint to express a set of possibly conflicting problem requirement a soft constraint is a function defined on a collection of variable which associate some measure of desirability with each possible combination of value for those variable however the crucial question of the computational complexity of finding the optimal solution to a collection of soft constraint ha so far received very 
this paper address the problem of large scale multiview registration of range image captured from unknown viewing direction to reduce the computational burden we decouple the local problem of pairwise registration on neighboring view from the global problem of distribution of accumulated error we define the global problem over the graph of neighboring view and we show that this graph can be decomposed into a set of cycle such that the optimal transformation parameter for each cycle can be solved in closed form we then describe an iterative procedure that can be used to integrate the solution for the set of cycle across the graph this method for error distribution doe not require point correspondence between view and therefore can be used together with robot odometry or any method of pairwise registration experimental result demonstrate the effectiveness of this technique on range image of an indoor facility 
in this paper we investigate the general problem of discovering recurrent pattern that are embedded in categorical sequence an important real world problem of this nature is motif discovery in dna sequence we investigate the fundamental aspect of this data mining problem that can make discovery easy or hard we present a general framework for characterizing learning in this context by deriving the bayes error rate for this problem under a markov assumption the bayes error framework demonstrates why certain pattern are much harder to discover than others it also explains the role of different parameter such a pattern length and pattern frequency in sequential discovery we demonstrate how the bayes error can be used to calibrate existing discovery algorithm providing a lower bound on achievable performance we discus a number of fundamental issue that characterize sequential pattern discovery in this context present a variety of empirical result to complement and verify the theoretical analysis and apply our methodology to real world motif discovery problem in computational biology 
abstract it is a well known classical result that given the image projection of three known world point it is possible to solve for the pose of a calibrated perspective camera to up to four pair of solution we solve the generalised problem where the camera is allowed to sample ray in some arbitrary but known fashion and is not assumed to perform a central perspective projection that is given three back projected ray that emanate from a camera or multi camera rig in an arbitrary but known fashion we seek the possible pose of the camera such that the three ray meet three known world point we show that the generalised problem ha up to eight solution that can be found a the intersection between a circle and a ruled quartic surface a minimal and efficient constructive numerical algorithm is given to find the solution the algorithm derives an octic polynomial whose root correspond to the solution in the classical case when the three ray are concurrent the ruled quartic surface and the circle posse a reflection symmetry such that their intersection come in symmetric pair this manifest itself in that the odd order term of the octic polynomial vanish a a result the up to four pair of solution can be found in closed form the proposed algorithm can be used to solve for the pose of any type of calibrated camera or camera rig the intended use for the algorithm is in a hypothesise and test architecture 
we introduce a family of classifier based on a physical analogy to an electrostatic system of charged conductor the family called coulomb classifier includes the two best known support vector machine svms the svm and the c svm in the electrostatics analogy a training example corresponds to a charged conductor at a given location in space the classification function corresponds to the electrostatic potential function and the training objective function corresponds to the coulomb energy the electrostatic framework provides not only a novel interpretation of existing algorithm and their interrelationship but it suggests a variety of new method for svms including kernel that bridge the gap between polynomial and radial basis function objective function that do not require positive definite kernel regularization technique that allow for the construction of an optimal classifier in minkowski space based on the framework we propose novel svms and perform simulation study to show that they are comparable or superior to standard svms the experiment include classification task on data which are represented in term of their pairwise proximity where a coulomb classifier outperformed standard svms 
one of the major hurdle in maintaining long lived electronic system is that electronic part become obsolete no longer available from the original supplier when this occurs an engineer is tasked with resolving the problem by finding a replacement that is a similar a possible to the original part the current approach involves a laborious manual search through several electronic portal and data book the search is difficult because potential replacement may differ from the original and from each other by one or more parameter worse still the cumbersome nature of this process may cause the engineer to miss appropriate solution amid the many thousand of part listed in industry catalog in this paper we address this problem by introducing the notion of a parametric distance between electronic component we use this distance to search a large part data set and recommend likely replacement recommendation are based on an adaptive nearest neighbor search through the parametric data set for each user we learn how to scale the ax of the feature space in which the nearest neighbor are sought this allows the system to learn each user s judgment of the phrase a similar a possible 
we present ongoing work on a project for automatic recognition of spontaneous facial action spontaneous facial expression differ substantially from posed expression similar to how continuous spontaneous speech differs from isolated word produced on command previous method for automatic facial expression recognition assumed image were collected in controlled environment in which the subject deliberately faced the camera since people often nod or turn their head automatic recognition of spontaneous facial behavior requires method for handling out of image plane head rotation here we explore an approach based on d warping of image into canonical view we evaluated the performance of the approach a a front end for a spontaneous expression recognition system using support vector machine and hidden markov model this system employed general purpose learning mechanism that can be applied to recognition of any facial movement the system wa tested for recognition of a set of facial action defined by the facial action coding system facs we showed that d tracking and warping followed by machine learning technique directly applied to the warped image is a viable and promising technology for automatic facial expression recognition one exciting aspect of the approach presented here is that information about movement dynamic emerged out of filter which were derived from the statistic of image 
the problem of structure from motion is a central problem in vision given the d location of certain point we wish to recover the camera motion and the d coordinate of the point under simplied camera model the problem reduces to factorizing a measurement matrix into the product of two low rank matrix each element of the measurement matrix contains the position of a point in a particular image when all element are observed the problem can be solved trivially using svd but in any realistic situation many element of the matrix are missing and the one that are observed have a dieren t directional uncertainty under these condition most existing factorization algorithm fail while human perception is relatively unchanged in this paper we use the well known em algorithm for factor analysis to perform factorization this allows u to easily handle missing data and measurement uncertainty and more importantly allows u to place a prior on the temporal trajectory of the latent variable the camera position we show that incorporating this prior give a signican t improvement in performance in challenging image sequence 
accurate dependency recovery ha recently been reported for a number of wide coverage statistical parser using combinatory categorial grammar ccg however overall figure give no indication of a parser s performance on specific construction nor how suitable a parser is for specific application in this paper we give a detailed evaluation of a ccg parser on object extraction dependency found in wsj text we also show how the parser can be used to parse question for question answering the accuracy of the original parser on question is very poor and we propose a novel technique for porting the parser to a new domain by creating new labelled data at the lexical category level only using a supertagger to assign category to word trained on the new data lead to a dramatic increase in question parsing accuracy 
in recent year variational method have become a popular tool for approximate inference and learning in a wide variety of probabilistic model for each new application however it is currently necessary rst to derive the variational update equation and then to implement them in application specic code each of these step is both time consuming and error prone in this paper we describe a general purpose inference engine called vibe variational inference for bayesian network which allows a wide variety of probabilistic model to be implemented and solved variationally without recourse to coding new model are specied either through a simple script or via a graphical interface analogous to a drawing package vibe then automatically generates and solves the variational equation we illustrate the power and exibilit y of vibe using example from bayesian mixture modelling 
this paper study the dynamic of agent mediated combinatorial trading at the macroscopic level the combinatorial marketplace consists of a retailer who wish to sell bundle of item and a large number of agent with different purchasing goal these agent dynamically form coalition to exploit the benefit of grouping based on their complementary need a novel physic based dynamic equation is proposed to capture the essence of the movement of agent among different sized coalition simulation experiment are performed to study the global behavior of the agent and the effectiveness of the agent mediated combinatorial trading 
we present a semi parametric latent variable model based technique for density modelling dimensionality reduction and visualization unlike previous method we estimate the latent distribution non parametrically which enables u to model data generated by an underlying low dimensional multimodal distribution in addition we allow the component of latent variable model to be drawn from the exponential family which make the method suitable for special data type for example binary or count data simulation on real valued binary and count data show favorable comparison to other related scheme both in term of separating different population and generalization to unseen sample 
we present a modified version of the perceptron learning algorithm pla which solves semidefinite program sdps in polynomial time the algorithm is based on the following three observation i semidefinite program are linear program with infinitely many linear constraint ii every linear program can be solved by a sequence of constraint satisfaction problem with linear constraint iii in general the perceptron learning algorithm solves a constraint satisfaction problem with linear constraint in finitely many update combining the pla with a probabilistic rescaling algorithm which on average increase the size of the feasable region result in a probabilistic algorithm for solving sdps that run in polynomial time we present preliminary result which demonstrate that the algorithm work but is not competitive with state of the art interior point method 
inner product operator often referred to a kernel in statistical learning define a mapping from some input space into a feature space the focus of this paper is the construction of biologically motivated kernel for cortical activity the kernel we derive termed spikernels map spike count sequence into an abstract vector space in which we can perform various prediction task we discus in detail the derivation of spikernels and describe an efficient algorithm for computing their value on any two sequence of neural population spike count we demonstrate the merit of our modeling approach using the spikernel and various standard kernel for the task of predicting hand movement velocity from cortical recording in all of our experiment all the kernel we tested outperform the standard scalar product used in regression with the spikernel consistently achieving the best performance 
this paper present an algorithm for learning the time varying shape of a non rigid d object from uncalibrated d tracking data we model shape motion a a rigid component rotation and translation combined with a nonrigid deformation reconstruction is ill posed if arbitrary deformation are allowed we constrain the problem by assuming that the object shape at each time instant is drawn from a gaussian distribution based on this assumption the algorithm simultaneously estimate d shape and motion for each time frame learns the parameter of the gaussian and robustly fill in missing data point we then extend the algorithm to model temporal smoothness in object shape thus allowing it to handle severe case of missing data 
one fundamental challenge for mining recurring subgraphs from semi structured data set is the overwhelming abundance of such pattern in large graph database the total number of frequent subgraphs can become too large to allow a full enumeration using reasonable computational resource in this paper we propose a new algorithm that mine only maximal frequent subgraphs i e subgraphs that are not a part of any other frequent subgraphs this may exponentially decrease the size of the output set in the best case in our experiment on practical data set mining maximal frequent subgraphs reduces the total number of mined pattern by two to three order of magnitude our method first mine all frequent tree from a general graph database and then reconstructs all maximal subgraphs from the mined tree using two chemical structure benchmark and a set of synthetic graph data set we demonstrate that in addition to decreasing the output size our algorithm can achieve a five fold speed up over the current state of the art subgraph mining algorithm 
address event representation aer originally proposed a a mean to communicate sparse neural event between neuromorphic chip ha proven efficient in implementing large scale network with arbitrary configurable synaptic connectivity in this work we further extend the functionality of aer to implement arbitrary configurable synaptic plasticity in the address domain a proof of concept we implement a biologically inspired form of spike timing dependent plasticity stdp based on relative timing of event in an aer framework experimental result from an analog vlsi integrate and fire network demonstrate address domain learning in a task that requires neuron to group correlated input 
this paper present a new measure of semantic relatedness between concept that is based on the number of shared word overlap in their definition gloss this measure is unique in that it extends the gloss of the concept under consideration to include the gloss of other concept to which they are related according to a given concept hierarchy we show that this new measure reasonably correlate to human judgment we introduce a new method of word sense disambiguation based on extended gloss overlap and demonstrate that it fare well on the senseval lexical sample data 
a fast growing body of research in the ai and machine learning community address learning in game where there are multiple learner with different interest this research add to more established research on learning in game conducted in economics in part because of a clash of field there are widely varying requirement on learning algorithm in this domain the goal of this paper is to demonstrate how communication complexity can be used a a lower bound on the required learning time or cost because this lower bound doe not assume any requirement on the learning algorithm it is universal applying under any set of requirement on the learning algorithm we characterize exactly the communication complexity of various solution concept from game theory namely nash equilibrium iterated dominant strategy both strict and weak and backwards induction this give the tighest lower bound on learning in game that can be obtained with this method 
we perform noun phrase bracketing by using a local maximum entropy based tagging model which produce bracketing hypothesis these hypothesis are subsequently fed into a reranking framework based on support vector machine we solve the problem of hierarchical structure in our tagging model by modeling underspecified tag which are fully determined only at decoding time the tagging model performs comparably to competing approach and the subsequent reranking increase our system s performance from an f score of to surpassing the best reported result to date of 
abstract we describe a nonparametric bayesian approach to generalizing from few labeled example guided by a larger set of unlabeled object and the assumption of a latent tree structure to the domain the tree or a distribution over tree may be inferred using the unlabeled data a prior over concept generated by a mutation process on the inferred tree s allows efficient computation of the optimal bayesian classification function from the labeled example we test our approach on eight real world datasets 
we present a novel representation of shape for closedplanar contour explicitly designed to posse a linearstructure this greatly simplifies linear operation suchas averaging principal component analysis or differentiation in the space of shape the representation reliesupon embedding the contour on a subset of the space ofharmonic function of which the original contour is thezero level set 
various problem in computer vision become dicult due to a strong influence of lighting on the image of an object recent work showed analytically that the set of all image of a convex lambertian object can be accurately approximated by the low dimensional linear subspace constructed using spherical harmonic function in this paper we present two major contribution first we extend previous analysis of spherical harmonic approximation to the case of arbitrary object second we analyze it applicability for near light we begin by showing that under distant lighting with uniform distribution of light source the average accuracy of spherical harmonic representation can be bound from below this bound hold for object of arbitrary geometry and color and for general illumination consisting of any number of light source we further examine the case when light is coming from above and provide an analytic expression for the accuracy obtained in this case finally we show that low dimensional representation using spherical harmonic provide an accurate approximation also for fairly near light our analysis assumes lambertian reflectance and account for attached but not for cast shadow we support this analysis by simulation and real experiment including an example of a d shape reconstruction by photometric stereo under very close unknown lighting 
traditional intrusion detection system id detect attack by comparing current behavior to signature of known attack one main drawback is the inability of detecting new attack which do not have known signature in this paper we propose a learning algorithm that construct model of normal behavior from attack free network traffic behavior that deviate from the learned normal model signal possible novel attack our id is unique in two respect first it is nonstationary modeling probability based on the time since the last event rather than on average rate this prevents alarm flood second the id learns protocol vocabulary at the data link through application layer in order to detect unknown attack that attempt to exploit implementation error in poorly tested feature of the target software on the darpa id evaluation data set we detect of attack with false alarm about evenly divided between user behavioral anomaly ip address and port a modeled by most other system and protocol anomaly because our method are unconventional there is a significant non overlap of our id with the original darpa participant which implies that they could be combined to increase coverage 
we present and evaluate a nic based network intrusion detection system intrusion detection at the nic make the system potentially tamper proof and is naturally extensible to work in a distributed setting simple anomaly detection and signature detection based model have been implemented on the nic firmware which ha it own processor and memory we empirically evaluate such system from the perspective of quality and performance bandwidth of acceptable message under varying condition of host load the preliminary result we obtain are very encouraging and lead u to believe that such nic based security scheme could very well be a crucial part of next generation network security system 
manycontrolproblemstakeplaceincontinuousstate actionspaces e g a in manipulator robotics where the control objective is oftendeflnedasflndingadesiredtrajectorythatreachesaparticular goalstate whilereinforcementlearningofiersatheoreticalframeworktolearnsuchcontrolpoliciesfromscratch itsapplicabilityto higher dimensional continuous state action space remains rather limited to date instead of learning from scratch in this paper we suggest to learn a desired complex control policy by transforming an existing simple canonical control policy for this purpose we represent canonical policy in term of difierential equation with well deflned attractor property by nonlinearly transforming the canonicalattractordynamicsusingtechniquesfromnonparametric regression almost arbitrary new nonlinear policy can be generated without losing the stability property of the canonical system we demonstrate our technique in the context of learning a set of movement skill for a humanoid robot from demonstration of a human teacher policy are acquired rapidly and due to the propertiesofwellformulateddifierentialequations canbere used and modifled on line under dynamic change of the environment thelinearparameterizationofnonparametricregressionmoreover lends itself to recognize and classify previously learned movement skill evaluation in simulation and on an actual degree offreedom humanoid robot exemplify the feasibility and robustness of our approach 
recent development in grid based and point based approximation algorithm for pomdps have greatly improved the tractability of pomdp planning these approach operate on set of belief point by individually learning a value function for each point in reality belief point exist in a highly structured metric simplex but current pomdp algorithm do not exploit this property this paper present a new metric tree algorithm which can be used in the context of pomdp planning to sort belief point spatially and then perform fast value function update over group of point we present result showing that this approach can reduce computation in point based pomdp algorithm for a wide range of problem 
most data mining algorithm require the setting of many input parameter two main danger of working with parameter laden algorithm are the following first incorrect setting may cause an algorithm to fail in finding the true pattern second a perhaps more insidious problem is that the algorithm may report spurious pattern that do not really exist or greatly overestimate the significance of the reported pattern this is especially likely when the user fails to understand the role of parameter in the data mining process data mining algorithm should have a few parameter a possible ideally none a parameter free algorithm would limit our ability to impose our prejudice expectation and presumption on the problem at hand and would let the data itself speak to u in this work we show that recent result in bioinformatics and computational theory hold great promise for a parameter free data mining paradigm the result are motivated by observation in kolmogorov complexity theory however a a practical matter they can be implemented using any off the shelf compression algorithm with the addition of just a dozen or so line of code we will show that this approach is competitive or superior to the state of the art approach in anomaly interestingness detection classification and clustering with empirical test on time series dna text video datasets 
noun extraction is very important for many nlp application such a information retrieval automatic text classification and information extraction most of the previous korean noun extraction system use a morphological analyzer or a part of speech po tagger therefore they require much of the linguistic knowledge such a morpheme dictionary and rule e g morphosyntactic rule and morphological rule this paper proposes a new noun extraction method that us the syllable based word recognition model it find the most probable syllable tag sequence of the input sentence by using automatically acquired statistical information from the po tagged corpus and extract noun by detecting word boundary furthermore it doe not require any labor for constructing and maintaining linguistic knowledge we have performed various experiment with a wide range of variable influencing the performance the experimental result show that without morphological analysis or po tagging the proposed method achieves comparable performance with the previous method 
this paper address the problem of outdoor terrain modeling for the purpose of mobile robot navigation we propose an approach in which a robot acquires a set of terrain model at differing resolution our approach address one of the major shortcoming of bayesian reasoning when applied to terrain modeling namely artifact that arise from the limited spatial resolution of robot perception limited spatial resolution cause small obstacle to be detectable only at close range hence a bayes filter estimating the state of terrain segment must consider the range at which that terrain is observed we develop a multi resolution approach that maintains multiple navigation map and derive rational argument for the number of layer and their resolution we show that our approach yield significantly better result in a practical robot system capable of acquiring detailed d map in large scale outdoor environment 
time based medium centric web presentation can be described declaratively in the xml world through the development of language such a smil it is difficult however to fully integrate them in a complete document transformation processing chain in order to achieve the desired processing of data driven time based medium centric presentation the text flow based formatting vocabulary used by style language such a xsl cs and dsssl need to be extended the paper present a selection of use case which are used to derive a list of requirement for a multimedia style and transformation formatting vocabulary the boundary of applicability of existing text based formatting model for medium centric transformation are analyzed the paper then discus the advantage and disadvantage of a fully fledged time based multimedia formatting model finally the discussion is illustrated by describing the key property of the example multimedia formatting vocabulary currently implemented in the back end of our cuypers multimedia transformation engine 
quantum learning hold great promise for the field of machine intelligence the most studied quantum learning algorithm is the quantum neural network many such model have been proposed yet none ha become a standard in addition these model usually leave out many detail often excluding how they intend to train their network this paper discus one approach to the problem and what advantage it would have over classical network 
abstract we consider the problem of deriving class size independent generalization bound for some regularized discriminative multi category classification method in particular we obtain an expected generalization bound for a standard formulation of multi category support vector machine based on the theoretical result we argue that the formulation over penalizes misclassification error which in theory may lead to poor generalization performance a remedy based on a generalization of multi category logistic regression conditional maximum entropy is then proposed and it theoretical property are examined 
discovering coherent gene expression pattern in time series gene expression data is an important task in bioinformatics research and biomedical application in this paper we propose an interactive exploration framework for mining coherent expression pattern in time series gene expression data we develop a novel tool coherent pattern index graph to give user highly confident indication of the existence of coherent pattern to derive a coherent pattern index graph we devise an attraction tree structure to record the gene in the data set and summarize the information needed for the interactive exploration we present fast and scalable algorithm to construct attraction tree and coherent pattern index graph from gene expression data set we conduct an extensive performance study on some real data set to verify our design the experimental result strongly show that our approach is more effective than the state of the art method in mining real gene expression data and is scalable in mining large data set 
in this paper we introduce textrank a graph based ranking model for text processing and show how this model can be successfully used in natural language application in particular we propose two innovative unsupervised method for keyword and sentence extraction and show that the result obtained compare favorably with previously published result on established benchmark 
multiple projection of a scene cannot be arbitrary the allowedconfigurations being given by matching constraint this paper present new matching constraint on multipleprojections of a rigid point set by uncalibrated camera obtainedby formulation in the oriented projective rather thanprojective geometry they follow from consistency of orientationsof camera ray and from the fact that the scene is theaffine rather that projective space for their non parametricnature we call them combinatorial the constraint are derivedin a unified theoretical framework using the theory oforiented matroids for example we present constraint on point correspondence for d camera resectioning on correspondence in two d camera and on correspondencesin two d camera 
okapi bm scoring of anchor text surrogate document ha been shown to facilitate effective ranking in navigational search task over web data we hypothesize that even better ranking can be achieved in certain important case particularly when anchor score must be fused with content score by avoiding length normalisation and by reducing the attentuation of score associated with high tf preliminary result are presented 
we study the computational complexity of reasoning with global constraint we show that reasoning with such constraint is intractable in general we then demonstrate how the same tool of computational complexity can be used in the design and analysis of specific global constraint in particular we illustrate how computational complexity can be used to determine when a lesser level of local consistency should be enforced when decomposing constraint will lose pruning and when combining constraint is tractable we also show how the same tool can be used to study symmetry breaking meta constraint like the cardinality constraint and learning nogoods 
this paper analysis the contrastive divergence algorithm for learning statistical parameter we relate the algorithm to the stochastic approximation literature this enables u to specify condition under which the algorithm is guaranteed to converge to the optimal solution with probability this includes necessary and sufficient condition for the solution to be unbiased 
traditionally text classifier are built from labeled training example labeling is usually done manually by human expert or the user which is a labor intensive and time consuming process in the past few year researcher investigated various form of semi supervised learning to reduce the burden of manual labeling in this paper we propose a different approach instead of labeling a set of document the proposed method label a set of representative word for each class it then us these word to extract a set of document for each class from a set of unlabeled document to form the initial training set the em algorithm is then applied to build the classifier the key issue of the approach is how to obtain a set of representative word for each class one way is to ask the user to provide them which is difficult because the user usually can only give a few word which are insufficient for accurate learning we propose a method to solve the problem it combine clustering and feature selection the technique can effectively rank the word in the unlabeled set according to their importance the user then selects label some word from the ranked list for each class this process requires le effort than providing word with no help or manual labelillg of document our result show that the new method is highly effective and promising 
linear discriminant analysis lda is a well known scheme for feature extraction and dimension reduction it ha been used widely in many application involving high dimensional data such a face recognition and image retrieval an intrinsic limitation of classical lda is the so called singularity problem that is it fails when all scatter matrix are singular a well known approach to deal with the singularity problem is to apply an intermediate dimension reduction stage using principal component analysis pca before lda the algorithm called pca lda is used widely in face recognition however pca lda ha high cost in time and space due to the need for an eigen decomposition involving the scatter matrix in this paper we propose a novel lda algorithm namely dlda which stand for dimensional linear discriminant analysis dlda overcomes the singularity problem implicitly while achieving efficiency the key difference between dlda and classical lda lie in the model for data representation classical lda work with vectorized representation of data while the dlda algorithm work with data in matrix representation to further reduce the dimension by dlda the combination of dlda and classical lda namely dlda lda is studied where lda is preceded by dlda the proposed algorithm are applied on face recognition and compared with pca lda experiment show that dlda and dlda lda achieve competitive recognition accuracy while being much more efficient 
we consider the problem of eliminating redundant boolean feature for a given data set where a feature is redundant if it separate the class le well than another feature or set of feature lavra ccaron et al proposed the algorithm reduce that work by pairwise comparison of feature i e it eliminates a feature if it is redundant with respect to another feature their algorithm operates in an ilp setting and is restricted to two class problem in this paper we improve their method and extend it to multiple class central to our approach is the notion of a neighbourhood of example a set of example of the same class where the number of different feature between example is relatively small redundant feature are eliminated by applying a revised version of the reduce method to each pair of neighbourhood of different class we analyse the performance of our method on a range of data set 
we compare two approach for describing and generating body of rule used for natural language parsing in today s parser rule body do not exist a priori but are generated on the fly usually with method based on n gram which are one particular way of inducing probabilistic regular language we compare two approach for inducing such language one is based on n gram the other on minimization of the kullback leibler divergence the inferred regular language are used for generating body of rule inside a parsing procedure we compare the two approach along two dimension the quality of the probabilistic regular language they produce and the performance of the parser they were used to build the second approach outperforms the first one along both dimension 
we compare two approach for describing and generating body of rule used for natural language parsing in today s parser rule body do not exist a priori but are generated on the y usually with method based on n gram which are one particular way of inducing probabilistic regular language we compare two approach for inducing such language one is based on n gram the other on minimization of the kullback leibler divergence the inferred regular language are used for generating body of rule inside a parsing procedure we compare the two approach along two dimension the quality of the probabilistic regular language they produce and the performance of the parser they were used to build the second approach outperforms the rst one along both dimension 
internet server selection mechanism attempt to optimize subject to a variety of constraint the distribution of client request to a geographically and topologically diverse pool of server research on server selection ha thus far focused primarily on technique for choosing a server from a group administered by single entity like a content distribution network provider in a federated multi provider computing system however selection must occur over distributed server set deployed by the participating provider without the benefit of the full information available in the single provider case intelligent server set selection algorithm will require a model of the expected performance client would receive from a candidate server set in this paper we study whether the complex policy and dynamic of intelligent server selection can be effectively modeled in order to predict client performance for server set we introduce a novel server set distance metric and use it in a measurement study of several million server selection transaction to develop simple model of existing server selection scheme we then evaluate these model in term of their ability to accurately predict performance for a second larger set of distributed client we show that our model are able to predict performance within m for over of the observed sample our analysis demonstrates that although existing deployment use a variety of complex and dynamic server selection criterion most of which are proprietary these scheme can be modeled with surprising accuracy 
this paper describes a novel approach to automatically recovercorresponding feature point and epipolar geometry over two widebaseline frame our contribution consist of several aspect first the use of an affine invariant feature edge corner isintroduced to provide a robust and consistent matching primitive second based on svd decomposition of affine matrix the affinematching space between two corner can be approximately dividedinto two independent space by rotation angle and scaling factor employing this property a two stage affine matching algorithm isdesigned to obtain robust match over two frame third using theepipolar geometry estimated by these match more correspondingfeature point are determined based on these robustcorrespondences the fundamental matrix is refined and a series ofvirtual view of the scene are synthesized finally severalexperiments are presented to illustrate that a number of robustcorrespondences can be stably determined for two wide baselineimages under significant camera motion with illumination change occlusion and self similarity after testing a number ofexamples and comparing with the existing method the experimentalresults strongly demonstrate that our matching method outperformsthe state of art algorithm for all of the test case 
the framenet project ha developed a lexical knowledge base providing a unique level of detail a to the the possible syntactic realization of the specific semantic role evoked by each predicator for roughly lexical unit on the basis of annotating more than example sentence extracted from corpus an interim version of the framenet data wa released in october and is being widely used a new more portable version of the framenet software is also being made available to researcher elsewhere including the spanish framenet project this demo and poster will briefly explain the principle of frame semantics and demonstrate the new unified tool for lexicon building and annotation and also framesql a search tool for finding pattern in annotated sentence we will discus the content and format of the data release and how the software and data can be used by other nlp researcher 
in contrast to traditional document retrieval a web page a a whole is not a good information unit to search because it often contains multiple topic and a lot of irrelevant information from navigation decoration and interaction part of the page in this paper we propose a vision based page segmentation vip algorithm to detect the semantic content structure in a web page compared with simple dom based segmentation method our page segmentation scheme utilizes useful visual cue to obtain a better partition of a page at the semantic level by using our vip algorithm to assist the selection of query expansion term in pseudo relevance feedback in web information retrieval we achieve performance improvement on web track dataset 
this paper present vlsi circuit with continuous valued probabilistic behaviour realized by injecting noise into each computing unit neuron interconnecting the noisy neuron form a continuous restricted boltzmann machine crbm which ha shown promising performance in modelling and classifying noisy biomedical data the minimising contrastive divergence learning algorithm for crbm is also implemented in mixed mode vlsi to adapt the noisy neuron parameter on chip 
ontology play a prominent role on the semantic web they make possible the widespread publication of machine understandable data opening myriad opportunity for automated information processing however because of the semantic web s distributed nature data on it will inevitably come from many different ontology information processing across ontology is not possible without knowing the semantic mapping between their element manually finding such mapping is tedious error prone and clearly not possible at the web scale hence the development of tool to assist in the ontology mapping process is crucial to the success of the semantic web we describe glue a system that employ machine learning technique to find such mapping given two ontology for each concept in one ontology glue find the most similar concept in the other ontology we give well founded probabilistic definition to several practical similarity measure and show that glue can work with all of them this is in contrast to most existing approach which deal with a single similarity measure another key feature of glue is that it us multiple learning strategy each of which exploit a different type of information either in the data instance or in the taxonomic structure of the ontology to further improve matching accuracy we extend glue to incorporate commonsense knowledge and domain constraint into the matching process for this purpose we show that relaxation labeling a well known constraint optimization technique used in computer vision and other field can be adapted to work efficiently in our context our approach is thus distinguished in that it work with a variety of well defined similarity notion and that it efficiently incorporates multiple type of knowledge we describe a set of experiment on several real world domain and show that glue proposes highly accurate semantic mapping 
large datasets arise in various application such a market basket analysis and information retrieval we concentrate on the study of topic model aiming at result which indicate why certain method succeed or fail we describe simple algorithm for finding topic model from data we give theoretical result showing that the algorithm can discover the epsilon separable topic model of papadimitriou et al we present empirical result showing that the algorithm find natural topic in real world data set we also briefly discus the connection to matrix approach including nonnegative matrix factorization and independent component analysis 
many real world domain are relational in nature consisting of a set of entity linked to each other in complex way two important task in such data are predicting entity label and link between entity we present a flexible framework that build on conditional markov network and successfully address both task by capturing complex dependency in the data these model can compactly represent probabilistic pattern over subgraph structure and use them to predict label and link effectively we show how to train these model and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entity and link we evaluate our framework on several relational datasets including university webpage and social network our approach achieves significantly better performance than flat classification which attempt to predict each label and link in isolation 
simple lexicographic decision heuristic that consider cue one at a time in a particular order and stop searching for cue a soon a a decision can be made have been shown to be both accurate and frugal in their use of information but much of the simplicity and success of these heuristic come from using an appropriate cue order for instance the take the best heuristic us validity order for cue which requires considerable computation potentially undermining the computational advantage of the simple decision mechanism but many cue order can achieve good decision performance and study of sequential search for data record have proposed a number of simple ordering rule that may be of use in constructing appropriate decision cue order a well here we consider a range of simple cue ordering mechanism including tallying swapping and move to front rule and show that they can find cue order that lead to reasonable accuracy and considerable frugality when used with lexicographic decision heuristic 
recent development in computer vision have shown that localfeatures can provide efficient representation suitable for robustobject recognition support vector machine have been establishedas powerful learning algorithm with good generalizationcapabilities in this paper we combine these two approach andpropose a general kernel method for recognition with localfeatures we show that the proposed kernel satisfies the mercercondition and that it is suitable for many established localfeature framework large scale recognition result are presentedon three different database which demonstrate that svms with theproposed kernel perform better than standard matching technique onlocal feature in addition experiment on noisy and occludedimages show that local feature representation significantlyoutperform global approach 
the information resource on the web are vast but much of the web is based on a browsing paradigm that requires someone to actively seek information instead one would like to have information agent that continuously attend to one s personal information need such agent need to be able to extract the relevant information from web source integrate data across site and execute efficiently in a networked environment in this paper i describe the technology we have developed to rapidly construct and deploy information agent on the web this includes wrapper learning to convert online source into agent friendly resource query planning and record linkage to integrate data across different site and streaming dataflow execution to efficiently execute agent plan i also describe how we applied this work within the electric elf project to deploy a set of agent for continuous monitoring of travel itinerary 
we propose a novel method of dimensionality reduction for supervised learning given a regression or classification problem in which we wish to predict a variable y from an explanatory vector x we treat the problem of dimensionality reduction a that of finding a low dimensional effective subspace ofx which retains the statistical relationship between x and y we show that this problem can be formulated in term of conditional independence to turn this formulation into an optimization problem we characterize the notion of conditional independence using covariance operator on reproducing kernel hilbert space this allows u to derive a contrast function for estimation of the effective subspace unlike many conventional method the proposed method requires neither assumption on the marginal distribution of x nor a parametric model of the conditional distribution of y 
in this paper we describe a resource light system for the automatic morphological analysis and tagging of russian we eschew the use of extensive resource particularly large annotated corpus and lexicon exploiting instead i pre existing annotated corpus of czech ii an unannotated corpus of russian we show that our approach ha benefit and present what we believe to be one of the first full evaluation of a russian tagger in the openly available literature krasiv a beautiful short adjective feminine muz a husband noun masc sing genitive husband noun masc sing accusative okn a window noun neuter sing genitive window noun neuter pl nominative window noun neuter pl accusative knig a book noun fem sing nominative dom a house noun masc sing genitive house noun masc pl nominative house noun masc pl accusative skazal a say verb fem sing past tense dv a two numeral masc nominative 
capitalizing on the intuitive underlying assumption of language modelling for ad hoc retrieval we present a novel approach that is capable of injecting the user s context of the document collection into the retrieval process the preliminary finding from the evaluation undertaken suggest that improved ir performance is possible under certain circumstance this motivates further investigation to determine the extent and significance of this improved performance 
this paper address the problem of constructing good action selection policy for agent acting in partially observable environment a class of problem generally known a partially observable markov decision process we present a novel approach that us a modification of the well known baum welch algorithm for learning a hidden markov model hmm to predict both percept and utility in a non deterministic world this enables an agent to make decision based on it previous history of action observation and reward our algorithm called utile distinction hidden markov model udhmm handle the creation of memory well in that it tends to create perceptual and utility distinction only when needed while it can still discriminate state based on history of arbitrary length the experimental result in highly stochastic problem domain show very good performance 
we propose a method called rule based esp resp for utilizing prior knowledge evolving artificial neural network anns first kbann like technique are used to transform a set of rule into an ann then the ann is trained using the enforced subpopulation esp neuroevolution method empirical result in the prey capture domain show that resp can reach higher level of performance than esp the result also suggest that incremental learning is not necessary with resp and it is often easier to design a set of rule than an incremental evolution scheme in addition an experiment with some of the rule deleted suggests that resp is robust even with an incomplete knowledge base i esp therefore provides a robust methodology for scaling up neuroevolution to harder task by utilizing existing knowledge about the domain 
this paper present two new formulation of multiple instance learning a a maximum margin problem the proposed extension of the support vector machine svm learning approach lead to mixed integer quadratic program that can be solved heuristically our generalization of svms make a state of the art classication technique including non linear classication via kernel available to an area that up to now ha been largely dominated by special purpose method we present experimental result on a pharmaceutical data set and on application in automated image indexing and document categorization 
rule mining is an important data mining task that ha been applied to numerous real world application often a rule mining system generates a large number of rule and only a small subset of them is really useful in application although there exist some system allowing the user to query the discovered rule they are le suitable for complex ad hoc querying of multiple data mining rulebases to retrieve interesting rule in this paper we propose a new powerful rule query language rule ql for querying multiple rulebases that is modeled after sql and ha rigorous theoretical foundation of a rule based calculus in particular we first propose a rule based calculus rc based on the first order logic and then present the language rule ql that is at least a expressive a the safe fragment of rc we also propose a number of efficient query evaluation technique for rule ql and test them experimentally on some representative query to demonstrate the feasibility of rule ql 
during initial development kdd solution often focus heavily on algorithm architecture software hardware and system engineering challenge without first thoroughly exploring how end user will employ the new kdd technology a a result of such system centered design many useless feature are implemented that prolong development and significantly add to life cycle cost while making the system hard to operate and use this presentation will describe an alternate user centered approach borrowed from the consumer product industry that can produce kdd solution with shorter development cycle lower cost and much better usability 
from the standpoint of the automated extraction of scientific knowledge an important but little studied part of scientific publication are the figure and accompanying caption caption are dense in information but also contain many extra grammatical construct making them awkward to process with standard information extraction method we propose a scheme for understanding caption in biomedical publication by extracting and classifying image pointer reference to the accompanying image we evaluate a number of automated method for this task including hand coded method method based on existing learning technique and method based on novel learning technique the best of these method lead to a usefully accurate tool for caption understanding with both recall and precision in excess of on the most important single class in a combined extraction classification task 
a binary constraint network is tree convex if we can construct a tree for the domain of the variable so that for any constraint no matter what value one variable take all the value allowed for the other variable form a subtree of the constructed tree it is known that a tree convex network is globally consistent if it is path consistent however if a tree convex network is not path consistent enforcing path consistency on it may not make it globally consistent in this paper we identify a subclass of tree convex network which are locally chain convex and union closed this class of problem can be made globally consistent by path consistency and thus is tractable more interestingly we also find that some scene labeling problem can be modeled by tree convex constraint in a natural and meaningful way 
ontology alignment is a foundational problem area for semantic interoperability we discus the complexity faced by automated alignment solution and describe an ontology based approach for describing and evaluating alignment 
this paper address the problem of recognizing three dimensional d object in photograph and image sequence it revisits viewpoint invariant a a local representation of shape and appearance and proposes a unified framework for object recognition where object model consist of a collection of small planar patch their invariant and a description of their d spatial relationship this approach is applied to two fundamental instance of the d object recognition problem modeling rigid d object from a small set of unregistered picture and recognizing them in cluttered photograph taken from unconstrained viewpoint and recognizing non uniform texture pattern despite appearance variation due to non rigid transformation and change in viewpoint it is validated through several experiment and extension to the analysis of video sequence and the recognition of object category are briefly discussed 
symmetry is an effective geometric cue to facilitate conventionalsegmentation technique on image of man madeenvironment based on three fundamental principle thatsummarize the relation between symmetry and perspectiveimaging namely structure from symmetry symmetry hypothesistesting and global symmetry testing we developa prototype system which is able to automatically segmentsymmetric object in space from single d perspective image the result of such a segmentation is a hierarchy ofgeometric primitive called symmetry cell and complex whose d structure and pose are fully recovered such ageometrically meaningful segmentation may greatly facilitateapplications such a feature matching and robot navigation 
we discus the problem of ranking instance with the use of a large margin principle we introduce two main approach the first is the fixed margin policy in which the margin of the closest neighboring class is being maximized which turn out to be a direct generalization of svm to ranking learning the second approach allows for different margin where the sum of margin is maximized this approach is shown to reduce to svm when the number of class both approach are optimal in size of where is the total number of training example experiment performed on visual classification and collaborative filtering show that both approach outperform existing ordinal regression algorithm applied for ranking and multi class svm applied to general multi class classification 
the simplicity of the basic client server model of web service led quickly to it widespread adoption but also to scalability and performance problem the technological response to these problem ha been the development of technology for the creation of surrogate for web server starting with simple proxy cache and reverse proxy and leading more recently to the development of content distribution network surrogate technology based on cache have proved quite successful in reducing the load due to delivery of cacheable content html file and image but in general they cannot replicate service that are implemented through the execution of program at the server a full service surrogate is a technology that is designed to address this issue directly because it is a copy or mirror of the server that is created managed and updated automatically one of the central issue in the creation of full service surrogate is portability of interpreted content and the representation of metadata necessary to support execution in this paper we describe the portable channel representation pcr which is an extensible markup language resource definition framework encoded data model developed to enable full service surrogate and we discus the implication of the increasing importance of executable web service 
in this paper we present a framework and a system that extract event relevant to a query from a collection c of document and place such event along a timeline each event is represented by a sentence extracted from c based on the assumption that important event are widely cited in many document for a period of time within which these event are of interest in our experiment we used query that are event type earthquake and person name e g george bush evaluation wa performed using g leader name a query comparison made by human evaluator between manually and system generated timeline showed that although manually generated timeline are on average more preferable system generated timeline are sometimes judged to be better than manually constructed one 
the web pose itself a the largest data repository ever available in the history of humankind major effort have been made in order to provide efficient access to relevant information within this huge repository of data although several technique have been developed to the problem of web data extraction their use is still not spread mostly because of the need for high human intervention and the low quality of the extraction result in this paper we present a domain oriented approach to web data extraction and discus it application to automatically extracting news from web site our approach is based on a highly efficient tree structure analysis that produce very effective result we have tested our approach with several important brazilian on line news site and achieved very precise result correctly extracting of the news in a set of page distributed among different site 
the success of the semantic web depends on the availability of ontology a well a on the proliferation of web page annotated with metadata conforming to these ontology thus a crucial question is where to acquire these metadata from in this paper wepropose pankow pattern based annotation through knowledge on theweb a method which employ an unsupervised pattern based approach to categorize instance with regard to an ontology the approach is evaluated against the manual annotation of two human subject the approach is implemented in ontomat an annotation tool for the semantic web and show very promising result 
in complicated interacting auction a fundamental problem is the prediction of price of good in the auction and more broadly the modeling of uncertainty regarding these price in this paper we present a machine learning approach to this problem the technique is based on a new and general boosting based algorithm for conditional density estimation problem of this kind i e supervised learning problem in which the goal is to estimate the entire conditional distribution of the real valued label this algorithm which we present in detail is at the heart of a top scoring agent in the recent trading agent competition tac we describe how work the result of the competition and controlled experiment evaluating the effectiveness of price prediction in auction 
list question answering qa offer a unique challenge in effectively and efficiently locating a complete set of distinct answer from huge corpus or the web in trec the median average f performance of list qa system wa only this paper exploit the wealth of freely available text and link structure on the web to seek complete answer to list question we employ natural language parsing web page classification and clustering to find reliable list answer we also study the effectiveness of web page classification on both the recall and uniqueness of answer for web based list qa 
we present a new texture classification scheme whichis invariant to surface rotation many textureclassification approach have been presented in the pastthat are image rotation invariant however imagerotation is not necessarily the same a surface rotation we have therefore developed a classifier that usesinvariants that are derived from surface property ratherthan image property previously we developed ascheme that used surface gradient normal fieldsestimated using photometric stereo in this paper weaugment these data with albedo information and an alsoemploy an additional feature set the radial spectrum we used real texture to test the new classifier aclassification accuracy of wa achieved whenalbedo and gradient d polar and radial feature werecombined the best performance wa also achieved byusing d albedo and gradient spectrum the classificationaccuracy is 
recent work in question answering ha focused on web based system that extract answer using simple lexico syntactic pattern we present an alternative strategy in which pattern are used to extract highly precise relational information offline creating a data repository that is used to efficiently answer question we evaluate our strategy on a challenging subset of question i e who is question against a state of the art web based question answering system result indicate that the extracted relation answer more question correctly and do so three order of magnitude faster than the state of the art system 
the algorithm presented in this paper aim to segment theforeground object in video e g people given time varying textured background example of time varying background includewaves on water cloud moving tree waving in the wind automobiletraffic moving crowd escalator etc we have developed a novelforeground background segmentation algorithm that explicitlyaccounts for the non stationary nature and clutter like appearanceof many dynamic texture the dynamic texture is modeled byanautoregressive moving average model arma a robust kalman filteralgorithm iteratively estimate the intrinsic appearance of thedynamic texture a well a the region of the foreground object preliminary experiment with this method have demonstratedpromising result 
in this work we consider the task of relaxing the i i d assumption in online pattern recognition or classification aiming to make existing learning algorithm applicable to a wider range of task online pattern recognition is predicting a sequence of label based on object given for each label and on example pair of object and label learned so far traditionally this task is considered under the assumption that example are independent and identically distributed however it turn out that many result of pattern recognition theory carry over under a much weaker assumption namely under the assumption of conditional independence and identical distribution of object only while the only condition on the distribution of label is that the rate of occurrence of each label should be above some positive threshold we find a broad class of learning algorithm for which estimation of the probability of a classification error achieved under the classical i i d assumption can be generalised to the similar estimate for the case of conditionally i i d distributed example 
getting trapped in suboptimal local minimum is a perennial problem in model based vision especially in application like monocular human body tracking where complex nonlinear parametric model are repeatedly fitted to ambiguous image data we show that the trapping problem can be attacked by building roadmaps of nearby minimum linked by transition pathway path leading over low col or pass in the cost surface found by locating the transition state codimension saddle point at the top of the pas and then sliding downhill to the next minimum we know of no previous vision or optimization work on numerical method for locating transition state but such method do exist in computational chemistry where transition are critical for predicting reaction parameter we present two family of method originally derived in chemistry but here generalized clarified and adapted to the need of model based vision eigenvector tracking is a modified form of damped newton minimization while hypersurface sweeping sweep a moving hypersurface through the space tracking minimum within it experiment on the challenging problem of estimating d human pose from monocular image show that our algorithm find nearby transition state and minimum very efficiently but also underline the disturbingly large number of minimum that exist in this and similar model based vision problem 
it is crucial for cross language information retrieval clir system to deal with the translation of unknown query due to that real query might be short the purpose of this paper is to investigate the feasibility of exploiting the web a the corpus source to translate unknown query for clir we propose an online translation approach to determine effective translation for unknown query term via mining of bilingual search result page obtained from web search engine this approach can alleviate the problem of the lack of large bilingual corpus translate many unknown query term provide flexible query specification and extract semantically close translation to benefit clir task especially for cross language web search 
conditional random field crfs lafferty mccallum pereira provide a flexible and powerful model for learning to assign label to element of sequence in such application a part of speech tagging text to speech mapping protein and dna sequence analysis and information extraction from web page however existing learning algorithm are slow particularly in problem with large number of potential input feature this paper describes a new method for training crfs by applying friedman s gradient tree boosting method in tree boosting the crf potential function are represented a weighted sum of regression tree regression tree are learned by stage wise optimization similar to adaboost but with the objective of maximizing the conditional likelihood p y x of the crf model by growing regression tree interaction among feature are introduced only a needed so although the parameter space is potentially immense the search algorithm doe not explicitly consider the large space a a result gradient tree boosting scale linearly in the order of the markov model and in the order of the feature interaction rather than exponentially like previous algorithm based on iterative scaling and gradient descent 
the majority of the existing algorithm for learning decision tree are greedy a tree is induced top down making locally optimal decision at each node in most case however the constructed tree is not globally optimal furthermore the greedy algorithm require a fixed amount of time and are not able to generate a better tree if additional time is available to overcome this problem we present two lookahead based algorithm for anytime induction of decision tree thus allowing tradeoff between tree quality and learning time the first one is depth k lookahead where a larger time allocation permit larger k the second algorithm us a novel strategy for evaluating candidate split a stochastic version of id is repeatedly invoked to estimate the size of the tree in which each split result and the one that minimizes the expected size is preferred experimental result indicate that for several hard concept our proposed approach exhibit good anytime behavior and yield significantly better decision tree when more time is available 
we describe an approach to object and scene retrievalwhich search for and localizes all the occurrence of auser outlined object in a video the object is represented bya set of viewpoint invariant region descriptor so that recognitioncan proceed successfully despite change in viewpoint illumination and partial occlusion the temporalcontinuity of the video within a shot is used to track theregions in order to reject unstable region and reduce theeffects of noise in the descriptor the analogy with text retrieval is in the implementationwhere match on descriptor are pre computed using vectorquantization and inverted file system and documentrankings are used the result is that retrieval is immediate returning a ranked list of key frame shot in the manner ofgoogle the method is illustrated for matching on two full lengthfeature film 
association rule have received a lot of attention in the data mining community since their introduction the classical approach to find rule whose item enjoy high support appear in a lot of the transaction in the data set is however filled with shortcoming it ha been shown that support can be misleading a an indicator of how interesting the rule is alternative measure such a lift have been proposed more recently a paper by dumouchel et al proposed the use of all two factor loglinear model to discover set of item that cannot be explained by pairwise association between the item involved this approach however ha it limitation since it stop short of considering higher order interaction other than pairwise among the item in this paper we propose a method that examines the parameter of the fitted loglinear model to find all the significant association pattern among the item since fitting loglinear model for large data set can be computationally prohibitive we apply graph theoretical result to divide the original set of item into component set of item that are statistically independent from each other we then apply loglinear modeling to each of the component and find the interesting association among item in them the technique is experimentally evaluated with a real data set insurance data and a series of synthetic data set the result show that the technique is effective in finding interesting association among the item involved 
conventional tracking approach assume proximity inspace time and appearance of object in successive observation however observation of object are often widelyseparated in time and space when viewed from multiplenon overlapping camera to address this problem wepresent a novel approach for establishing object correspondenceacross non overlapping camera our multi cameratracking algorithm exploit the redundance in path thatpeople and car tend to follow e g road walk way orcorridors by using motion trend and appearance of object to establish correspondence our system doe notrequire any inter camera calibration instead the systemlearns the camera topology and path probability of objectsusing parzen window during a training phase oncethe training is complete correspondence are assigned usingthe maximum a posteriori map estimation framework the learned parameter are updated with changing trajectorypatterns experiment with real world video are reported which validate the proposed approach 
we describe the application of probabilistic model based learning to the problem of automatically identifying class of galaxy based on both morphological and pixel intensity characteristic the em algorithm can be used to learn how to spatially orient a set of galaxy so that they are geometrically aligned we augment this ordering model with a mixture model on object and demonstrate how class of galaxy can be learned in an unsupervised manner using a two level em algorithm the resulting model provide highly accurate classi cation of galaxy in cross validation experiment 
we investigate the change in performance of automatic subcategorization acquisition when a word sense disambiguation wsd system is employed to guide the acquisition process a a subgoal this involves creating a probabilistic wsd system which we evaluate on the senseval english all word task data we carry out an evaluation of the enriched subcategorization acquisition system using difficult english verb which show that wsd help to improve the acquisition performance 
this paper describes a dynamic service reconfiguration model where the proxy is composed of a chain of service object called mobilets pronounced a mo be let which can be deployed onto the network actively this model offer flexibility because the chain of mobilets can be dynamically reconfigured to adapt to the vigorous change in the characteristic of the wireless environment without interrupting the service provision for other mobile node furthermore mobilets can also be migrated to a new proxy server when the mobile node move to a different network domain we have realized the dynamic service reconfiguration model by crafting it design into a programmable infrastructure that form the baseline architecture of the webpads short for web proxy for actively deployable service system 
the sequential information bottleneck sib algorithm cluster co occurrence data such a text document v word we introduce a variant that model sparse co occurrence data by a generative process this turn the objective function of sib mutual information into a bayes factor while keeping it intact asymptotically for non sparse data experimental performance of the new algorithm is comparable to the original sib for large data set and better for smaller sparse set 
web page often contain clutter such a pop up ad unnecessary image and extraneous link around the body of an article that distracts a user from actual content extraction of useful and relevant content from web page ha many application including cell phone and pda browsing speech rendering for the visually impaired and text summarization most approach to removing clutter or making content more readable involve changing font size or removing html and data component such a image which take away from a webpage s inherent look and feel unlike content reformatting which aim to reproduce the entire webpage in a more convenient form our solution directly address content extraction we have developed a framework that employ easily extensible set of technique that incorporate advantage of previous work on content extraction our key insight is to work with the dom tree rather than with raw html markup we have implemented our approach in a publicly available web proxy to extract content from html web page 
in this paper we obtain convergence bound for the concentration of bayesian posterior distribution around the true distribution using a novel method that simplifies and enhances previous result based on the analysis we also introduce a generalized family of bayesian posterior and show that the convergence behavior of these generalized posterior is completely determined by the local prior structure around the true distribution this important and surprising robustness property doe not hold for the standard bayesian posterior in that it may not concentrate when there exist bad prior structure even at place far away from the true distribution 
we consider the problem of decentralized detection under constraint on the number of bit that can be transmitted by each sensor in contrast to most previous work in which the joint distribution of sensor observation is assumed to be known we address the problem when only a set of empirical sample is available we propose a novel algorithm using the framework of empirical risk minimization and marginalized kernel and analyze it computational and statistical property both theoretically and empirically we provide an efficient implementation of the algorithm and demonstrate it performance on both simulated and real data set 
an important issue in reinforcement learning is how to incorporate expert knowledge in a principled manner especially a we scale up to real world task in this paper we present a method for incorporating arbitrary advice into the reward structure of a reinforcement learning agent without altering the optimal policy this method extends the potentialbased shaping method proposed by ng et al to the case of shaping function based on both state and action this allows for much more specic information to guide the agent which action to choose without requiring the agent to discover this from the reward on state alone we develop two qualitatively dieren t method for converting a potential function into advice for the agent we also provide theoretical and experimental justications for choosing between these advice giving algorithm based on the property of the potential function 
automatically generated html a produced by wysiwyg program typically contains much repetitive and unnecessary markup thispaper identifies aspect of such html that may be altered whileleaving a semantically equivalent document and proposes technique to achieve optimizing modification these technique include attribute re arrangement via dynamic programming the use of style class and dead coderemoval these technique produce document a small a of original size the size decrease obtained are still significant when the technique are used in combination with conventional text based compression 
crawling the web is deceptively simple the basic algorithm is a fetch a page b parse it to extract all linked url c for all the url not seen before repeat a c however the size of the web estimated at over billion page and it rate of change estimated at per week move this plan from a trivial programming exercise to a serious algorithmic and system design challenge indeed these two factor alone imply that for a reasonably fresh and complete crawl of the web step a must be executed about a thousand time per second and thus the membership test c must be done well over ten thousand time per second against a set too large to store in main memory this requires a distributed architecture which further complicates the membership test a crucial way to speed up the test is to cache that is to store in main memory a dynamic subset of the seen url the main goal of this paper is to carefully investigate several url caching technique for web crawling we consider both practical algorithm random replacement static cache lru and clock and theoretical limit clairvoyant caching and infinite cache we performed about simulation using these algorithm with various cache size using actual log data extracted from a massive day web crawl that issued over one billion http request our main conclusion is that caching is very effective in our setup a cache of roughly entry can achieve a hit rate of almost interestingly this cache size fall at a critical point a substantially smaller cache is much le effective while a substantially larger cache brings little additional benefit we conjecture that such critical point are inherent to our problem and venture an explanation for this phenomenon 
when mining temporal sequence knowledge discovery technique can be applied that discover interesting pattern of interaction existing approach use frequency and sometimes length a measurement for interestingness because these are temporal sequence additional characteristic such a periodicity may also be interesting we propose that information theoretic principle can be used to evaluate interesting characteristic of time ordered input sequence in this paper we present a novel data mining technique based on the minimum description length principle that discovers interesting feature in a time ordered sequence we discus feature of our real time mining approach show application of the knowledge mined by the approach and present a technique to bootstrap a decision maker from the mined pattern 
the direct neural control of external device such a computer display or prosthetic limb requires the accurate decoding of neural activity rep resenting continuous movement we develop a real time control system using the spiking activity of approximately neuron recorded with an electrode array implanted in the arm area of primary motor cortex in contrast to previous work we develop a control theoretic approach that explicitly model the motion of the hand and the probabilistic re lationship between this motion and the mean ring rate of the cell in m bin we focus on a realistic cursor control task in which the sub ject must move a cursor to hit randomly placed target on a computer monitor encoding and decoding of the neural data is achieved with a kalman lter which ha a number of advantage over previous linear ltering technique in particular the kalman lter reconstruction of hand trajectory in off line experiment are more accurate than previ ously reported result and the model provides insight into the nature of the neural coding of movement 
a system capable of performing robust live ego motion estimationfor perspective camera is presented the system is powered byrandom sample consensus with preemptive scoring of the motionhypotheses a general statement of the problem of efficientpreemptive scoring is given then a theoretical investigation ofpreemptive scoring under a simple inlier outlier model isperformed a practical preemption scheme is proposed and it isshown that the preemption is powerful enough to enable robust livestructure and motion estimation 
we present a probabilistic parsing model for german trained on the negra treebank we observe that existing lexicalized parsing model using head head dependency while successful for english fail to outperform an unlexicalized baseline model for german learning curve show that this effect is not due to lack of training data we propose an alternative model that us sister head dependency instead of head head dependency this model out performs the baseline achieving a labeled precision and recall of up to this indicates that sister head dependency are more appropriate for treebanks with very flat structure such a negra 
different qualitative model have been proposed for decision under uncertainty in artificial intelligence but they generally fail to satisfy the principle of strict pareto dominance or principle of efficiency in contrast to the classical numerical criterion expected utility among the most prominent example of qualitative model are the qualitative possibilistic utility qpu and the order of magnitude expected utility omeu they are both appealing but inefficient in the above sense the question is whether it is possible to reconcile qualitative criterion and efficiency the present paper show that the answer is yes and that it lead to special kind of expected utility it is also shown that although numerical these expected utility remain qualitative they lead to different decision procedure based on min max and reverse operator only generalizing the leximin and leximax ordering of vector 
caching the result of frequent query pattern can improve the performance of query evaluation this paper describes a pas mining algorithm called pxminer to discover frequent xml query pattern we design data structure to expedite the mining process experiment result indicate that pxminer is both efficient and scalable 
abstract we present a syntax based language model for use in noisy channel machine translation in particular a languagemodel based upon that described in cha is combined with the syntax based translation model described in yk the resulting system wa used to translate sentence from chinese to english and compared with theresults of an ibm model based system a well a that of yk all trained on the same data the translationswere sorted into four group good bad syntax 
covariance and correlation estimate have important application in data mining in the presence of outlier classical estimate of covariance and correlation matrix are not reliable a small fraction of outlier in some case even a single outlier can distort the classical covariance and correlation estimate making them virtually useless that is correlation for the vast majority of the data can be very erroneously reported principal component transformation can be misleading and multidimensional outlier detection via mahalanobis distance can fail to detect outlier there is plenty of statistical literature on robust covariance and correlation matrix estimate with an emphasis on affine equivariant estimator that posse high breakdown point and small worst case bias all such estimator have unacceptable exponential complexity in the number of variable and quadratic complexity in the number of observation in this paper we focus on several variant of robust covariance and correlation matrix estimate with quadratic complexity in the number of variable and linear complexity in the number of observation these estimator are based on several form of pairwise robust covariance and correlation estimate the estimator studied include two fast estimator based on coordinate wise robust transformation embedded in an overall procedure recently proposed by we show that the estimator have attractive robustness property and give an example that us one of the estimator in the new insightful miner data mining product 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
collaborative filtering cf is valuable in e commerce and for direct recommendation for music movie news etc but today s system have several disadvantage including privacy risk a we move toward ubiquitous computing there is a great potential for individual to share all kind of information about place and thing to do see and buy but the privacy risk are severe in this paper we describe a new method for collaborative filtering which protects the privacy of individual data the method is based on a probabilistic factor analysis model privacy protection is provided by a peer to peer protocol which is described elsewhere but outlined in this paper the factor analysis approach handle missing data without requiring default value for them we give several experiment that suggest that this is most accurate method for cf to date the new algorithm ha other advantage in speed and storage over previous algorithm finally we suggest application of the approach to other kind of statistical analysis of survey or questionaire data 
we propose a new clustering algorithm called symp which is based on synchronization of pulse coupled oscillator symp represents each data point by an integrate and fire oscillator and us the relative similarity between the point to model the interaction between the oscillator symp is robust to noise and outlier determines the number of cluster in an unsupervised manner identifies cluster of arbitrary shape and can handle very large data set the robustness of symp is an intrinsic property of the synchronization mechanism to determine the optimum number of cluster symp us a dynamic resolution parameter to identify cluster of various shape symp model each cluster by multiple gaussian component the number of component is automatically determined using a dynamic intra cluster resolution parameter cluster with simple shape would be modeled by few component while cluster with more complex shape would require a larger number of component the scalable version of symp us an efficient incremental approach that requires a simple pas through the data set the proposed clustering approach is empirically evaluated with several synthetic and real data set and it performance is compared with cure 
evaluating sum of multivariate gaussians is a common computational task in computer vision and pattern recognition including in the general and powerful kernel density estimation technique the quadratic computational complexity of the summation is a significant barrier to the scalability of this algorithm to practical application the fast gauss transform fgt ha successfully accelerated the kernel density estimation to linear running time for low dimensional problem unfortunately the cost of a direct extension of the fgt to higher dimensional problem grows exponentially with dimension making it impractical for dimension above we develop an improved fast gauss transform to efficiently estimate sum of gaussians in higher dimension where a new multivariate expansion scheme and an adaptive space subdivision technique dramatically improve the performance the improved fgt ha been applied to the mean shift algorithm achieving linear computational complexity experimental result demonstrate the efficiency and effectiveness of our algorithm 
there ha been a surge of interest in learning using a mix of labeled and unlabeled data general approach include semi supervised learning and tranductive inference in this paper we look at some of the unique way in which unlabeled data can improve performance when doing link based classification the classification of object making use of both object description and the link between object 
in this paper we present a novel customizable ie paradigm that take advantage of predicate argument structure we also introduce a new way of automatically identifying predicate argument structure which is central to our ie paradigm it is based on an extended set of feature and inductive decision tree learning the experimental result prove our claim that accurate predicate argument structure enable high quality ie result 
this paper present an energy normalization transform a a method to reduce system error in the lf asd brain computer interface the energy normalization transform ha two major benefit to the system performance first it can increase class separation between the active and idle eeg data second it can desensitize the system to the signal amplitude variability for four subject in the study the benefit resulted in the performance improvement of the lf asd in the range from to while for the fifth subject who had the highest non normalized accuracy of the performance did not change notably with normalization 
classifying m example using a support vector machine containing l support vector traditionally requires exactly m l kernel computation we introduce a computational geometry method for which classification cost becomes roughly proportional to the difficulty of each example e g distance from the discriminant hyperplane it produce exactly the same classification while typically requiring much e g time fewer kernel computation than ma l related educed set method e g burges scholkopf et al scholkopf et al similarly lower the effective l but provide neither proportionality with difficulty nor guaranteed preservation of classification 
this paper present an in depth analysis of a state of the art question answering system several scenario are examined the performance of each module in a serial baseline system the impact of feedback and the insertion of a logic prover and the impact of various retrieval strategy and lexical resource the main conclusion is that the overall performance depends on the depth of natural language processing resource and the tool used for answer finding 
we present an algorithm based on convex optimization for constructing kernel for semi supervised learning the kernel matrix are derived from the spectral decomposition of graph laplacians and combine labeled and unlabeled data in a systematic fashion unlike previous work using diffusion kernel and gaussian random field kernel a nonparametric kernel approach is presented that incorporates order constraint during optimization this result in flexible kernel and av oids the need to choose among different parametric form our approach relies on a quadratically constrained quadratic program qcqp and is computationally feasible for large datasets we evaluate the ker nels on real datasets using support vector machine with encouraging result 
markov chain monte carlo mcmc technique revolutionized statistical practice in the s by providing an essential toolkit for making the rigor and flexibility of bayesian analysis computationally practical at the same time the increasing prevalence of massive datasets and the expansion of the field of data mining ha created the need to produce statistically sound method that scale to these large problem except for the most trivial example current mcmc method require a complete scan of the dataset for each iteration eliminating their candidacy a feasible data mining technique in this article we present a method for making bayesian analysis of massive datasets computationally feasible the algorithm simulates from a posterior distribution that condition on a smaller more manageable portion of the dataset the remainder of the dataset may be incorporated by reweighting the initial draw using importance sampling computation of the importance weight requires a single scan of the remaining observation while importance sampling increase efficiency in data access it come at the expense of estimation efficiency a simple modification based on the rejuvenation step used in particle filter for dynamic system model sidestep the loss of efficiency with only a slight increase in the number of data access to show proof of concept we demonstrate the method on a mixture of transition model that ha been used to model web traffic and robotics for this example we show that estimation efficiency is not affected while offering a reduction in data access 
the optimal setting of retrieval parameter often depend on both the document collection and the query and are usually found through empirical tuning in this paper we propose a family of two stage language model for information retrieval that explicitly capture the different influence of the query and document collection on the optimal setting of retrieval parameter a a special case we present a two stage smoothing method that allows u to estimate the smoothing parameter completely automatically in the first stage the document language model is smoothed using a dirichlet prior with the collection language model a the reference model in the second stage the smoothed document language model is further interpolated with a query background language model we propose a leave one out method for estimating the dirichlet parameter of the first stage and the use of document mixture model for estimating the interpolation parameter of the second stage evaluation on five different database and four type of query indicates that the two stage smoothing method with the proposed parameter estimation method consistently give retrieval performance that is close to or better than the best result achieved using a single smoothing method and exhaustive parameter search on the test data 
in this paper we describe the development of a fielded application for detecting malicious executables in the wild we gathered benign and malicious executables and encoded each a a training example using n gram of byte code a feature such processing resulted in more than million distinct n gram after selecting the most relevant n gram for prediction we evaluated a variety of inductive method including naive bayes decision tree support vector machine and boosting ultimately boosted decision tree outperformed other method with an area under the roc curve of result also suggest that our methodology will scale to larger collection of executables to the best of our knowledge ours is the only fielded application for this task developed using technique from machine learning and data mining 
quadrature filter are a well known method of low level computer vision for estimating certain property of the signal a there are local amplitude and local phase however d quadrature filter suffer from being not rotation invariant furthermore they do not allow to detect truly d feature a corner and junction unless they are combined to form the structure tensor the present paper deal with a new d generalization of quadrature filter which is rotation invariant and allows to analyze intrinsically d signal hence the new approach can be considered a the union of property of quadrature filter and of the structure tensor the proposed method first estimate the local orientation of the signal which is then used for steering some basis filter response certain linear combination of these filter response are derived which allow to estimate the local isotropy and two perpendicular phase of the signal the phase model is based on the assumption of an angular band limitation in the signal a an application a simple and efficient point of interest operator is presented and it is compared to the plessey detector 
the probability estimate of a naive bayes classifier are inaccurate if some of it underlying independence assumption are violated the decision criterion for using these estimate for classification therefore ha to be learned from the data this paper proposes the use of roc curve for this purpose for two class the algorithm is a simple adaptation of the algorithm for tracing a roc curve by sorting the instance according to their predicted probability of being positive a there is no obvious way to upgrade this algorithm to the multi class case we propose a hillclimbing approach which adjusts the weight for each class in a pre defined order experiment on a wide range of datasets show the proposed method lead to significant improvement over the naive bayes classifier s accuracy finally we discus an method to find the global optimum and show how it computational complexity would make it untractable 
model for the process by which idea and influence propagate through a social network have been studied in a number of domain including the diffusion of medical and technological innovation the sudden and widespread adoption of various strategy in game theoretic setting and the effect of word of mouth in the promotion of new product recently motivated by the design of viral marketing strategy domingo and richardson posed a fundamental algorithmic problem for such social network process if we can try to convince a subset of individual to adopt a new product or innovation and the goal is to trigger a large cascade of further adoption which set of individual should we target we consider this problem in several of the most widely studied model in social network analysis the optimization problem of selecting the most influential node is np hard here and we provide the first provable approximation guarantee for efficient algorithm using an analysis framework based on submodular function we show that a natural greedy strategy obtains a solution that is provably within of optimal for several class of model our framework suggests a general approach for reasoning about the performance guarantee of algorithm for these type of influence problem in social network we also provide computational experiment on large collaboration network showing that in addition to their provable guarantee our approximation algorithm significantly out perform node selection heuristic based on the well studied notion of degree centrality and distance centrality from the field of social network 
query length in best match information retrieval ir system is well known to be positively related to effectiveness in the ir task when measured in experimental non interactive environment however in operational interactive ir system query length is quite typically very short on the order of two to three word we report on a study which tested the effectiveness of a particular query elicitation technique in increasing initial searcher query length and which tested the effectiveness of query elicited using this technique and the relationship in general between query length and search effectiveness in interactive ir result show that the specific technique result in longer query than a standard query elicitation technique that this technique is indeed usable that the technique result in increased user satisfaction with the search and that query length is positively correlated with user satisfaction with the search 
kernel based learning e g support vector machine ha been successfully applied to many hard problem in natural language processing nlp in nlp although feature combination are crucial to improving performance they are heuristically selected kernel method change this situation the merit of the kernel method is that effective feature combination is implicitly expanded without loss of generality and increasing the computational cost kernel based text analysis show an excellent performance in term in accuracy however these method are usually too slow to apply to large scale text analysis in this paper we extend a basket mining algorithm to convert a kernel based classifier into a simple and fast linear classifier experimental result on english basenp chunking japanese word segmentation and japanese dependency parsing show that our new classifier are about to time faster than the standard kernel based classifier 
natural scene contain rich stochastic motion pattern which are characterized by the movement of a large number of small element such a falling snow raining flying bird firework and waterfall in this paper we call these motion pattern textured motion and present a generative method that combine statistical model and algorithm from both texture and motion analysis the generative method includes the following three aspect photometrically an image is represented a a superposition of linear base in atomic decomposition using an overcomplete dictionary such a gabor or laplacian such base representation is known to be generic for natural image and it is low dimensional a the number of base is often time smaller than the number of pixel geometrically each moving element called moveton such a the individual snowflake and bird is represented by a deformable template which is a group of several spatially adjacent base such template are learned through clustering dynamically the movetons are tracked through the image sequence by a stochastic algorithm maximizing a posterior probability a classic second order markov chain model is adopted for the motion dynamic the source and sink of the movetons are modeled by birth and death map we adopt an em like stochastic gradient algorithm for inference of the hidden variable base movetons birth death map parameter of the dynamic the learned model are also verified through synthesizing random textured motion sequence which bear similar visual appearance with the observed sequence 
natural scene contain rich stochastic motion pattern which are characterized by the movement of a large number of small element such a falling snow raining flying bird firework and waterfall in this paper we call these motion pattern textured motion and present a generative method that combine statistical model and algorithm from both texture and motion analysis the generative method includes the following three aspect photometrically an image is represented a a superposition of linear base in atomic decomposition using an over complete dictionary such a gabor or laplacian such base representation is known to be generic for natural image and it is low dimensional a the number of base is often time smaller than the number of pixel geometrically each moving element called moveton such a the individual snowflake and bird is represented by a deformable template which is a group of several spatially adjacent base such template are learned through clustering dynamically the movetons are tracked through the image sequence by a stochastic algorithm maximizing a posterior probability a classic second order markov chain model is adopted for the motion dynamic the source and sink of the movetons are modeled by birth and death map we adopt an em like stochastic gradient algorithm for inference of the hidden variable base movetons birth death map parameter of the dynamic the learned model are also verified through synthesizing random textured motion sequence which bear similar visual appearance with the observed sequence 
we study a new model free form of approximate policy iteration which us sarsa update with linear state action value function approximation for policy evaluation and a policy improvement operator to generate a new policy based on the learned state action value we prove that if the policy improvement operator produce soft policy and is lipschitz continuous in the action value with a constant that is not too large then the approximate policy iteration algorithm converges to a unique solution from any initial policy to our knowledge this is the first convergence result for any form of approximate policy iteration under similar computational resource assumption 
in this work we present a genetic algorithm ga system evolutionary counterpoint evoc that generates contrapuntal music counterpoint is the construction of a musical piece by superimposing multiple melody indirectly forming an underlying harmonic structure here we include a description of the underlying algorithm fitness function and overall system module 
we measure the wt g test collection used in the trec and trec web track with common measure used in the web topology community in order to see if wt g look like the web this is not an idle question characteristic of the web such a power law relationship diameter and connected component have all been observed within the scope of general web crawl constructed by blindly following link in contrast wt g wa carved out from a larger crawl specifically to be a web search test collection within the reach of university researcher doe such a collection retain the property of the larger web in the case of wt g yes 
a product price become increasingly available on the world wide web consumer attempt to understand how corporation vary these price over time however corporation change price based on proprietary algorithm and hidden variable e g the number of unsold seat on a flight is it possible to develop data mining technique that will enable consumer to predict price change under these condition this paper report on a pilot study in the domain of airline ticket price where we recorded over price observation over a day period when trained on this data hamlet our multi strategy data mining algorithm generated a predictive model that saved simulated passenger by advising them when to buy and when to postpone ticket purchase remarkably a clairvoyant algorithm with complete knowledge of future price could save at most in our simulation thus hamlet s saving were of optimal the algorithm s saving of represents an average saving of for the passenger for whom saving are possible overall hamlet saved of the ticket price averaged over the entire set of simulated passenger our pilot study suggests that mining of price data available over the web ha the potential to save consumer substantial sum of money per annum 
super resolution aim to produce a high resolution image from a set of one or more low resolution image by recovering or inventing plausible high frequency image content typical approach try to reconstruct a high resolution image using the sub pixel displacement of several lowresolution image usually regularized by a generic smoothness prior over the high resolution image space other method use training data to learn low to high resolution match and have been highly successful even in the single input image case here we present a domain specific image prior in the form of a p d f based upon sampled image and show that for certain type of super resolution problem this sample based prior give a significant improvement over other common multiple image super resolution technique 
we show how to build hierarchical reduced rank representation for large stochastic matrix and use this representation to design an efficient algorithm for computing the largest eigenvalue and the corresponding eigenvectors in particular the eigen problem is first solved at the coarsest level of the representation the approximate eigen solution is then interpolated over successive level of the hierarchy a small number of power iteration are employed at each stage to correct the eigen solution the typical speedup obtained by a matlab implementation of our fast eigensolver over a standard sparse matrix eigensolver are at least a factor of ten for large image size the hierarchical representation ha proven to be effective in a min cut based segmentation algorithm that we proposed recently 
we describe a system that localizes a single dipole to reasonable accuracy from noisy magnetoencephalographic meg measurement in real time at it core is a multilayer perceptron mlp trained to map sensor signal and head position to dipole location including head position overcomes the previous need to retrain the mlp for each subject and session the training dataset wa generated by mapping randomly chosen dipole and head position through an analytic model and adding noise from real meg recording after training a localization took m with an average error of cm a few iteration of a levenberg marquardt routine using the mlp s output a it initial guess took m and improved the accuracy to cm only slightly above the statistical limit on accuracy imposed by the noise we applied these method to localize single dipole source from meg component isolated by blind source separation and compared the estimated location to those generated by standard manually assisted commercial software 
this paper describes a data source and methodology for producing customized test suite for molecular biology entity identification system the data consists of a a set of gene name and symbol classified by a taxonomy of feature that are relevant to the performance of entity identification system and b a set of sentential environment into which name and symbol are inserted to create test data and the associated gold standard we illustrate the utility of test set producible by this methodology by applying it to five entity identification system and describing the error pattern uncovered by it and investigate relationship between performance on a customized test suite generated from this data and the performance of a system on two corpus 
schema learning is a way to discover probabilistic constructivist predictive action model schema from experience it includes method for finding and using hidden state to make prediction mor e accurate we extend the original schema mechanism to handle arbitrary discrete valued sensor improve the original learning cr iteria to handle pomdp domain and better maintain hidden state by using schema prediction these extension show large improvement over the original schema mechanism in several rewardless pomdps and achievevery low prediction error in a difficult speech modeling task furthe r we compare the extended schema learner to the recently introduced predictive state representation and find their prediction of next ste p action effect to be approximately equal in accuracy this work lay the foundation for a schema based system of integrated learning and planning 
forming test collection relevance judgment from the pooled output of multiple retrieval system ha become the standard process for creating resource such a the trec clef and ntcir test collection this paper present a series of experiment examining three different way of building test collection where no system pooling is used first a collection formation technique combining manual feedback and multiple system is adapted to work with a single retrieval system second an existing method based on pooling the output of multiple manual search is re examined testing a wider range of searcher and retrieval system than ha been examined before third a new approach is explored where the ranked output of a single automatic search on a single retrieval system is assessed for relevance no pooling whatsoever using established technique for evaluating the quality of relevance judgment in all three case test collection are formed that are a good a trec 
recently proposed algorithm for nonlinear dimensionality reduction fall broadly into two category which have different advantage and disadvantage global isomap and local locally linear embedding laplacian eigenmaps we present two variant of isomap which combine the advantage of the global approach with what have previously been exclusive advantage of local method computational sparsity and the ability to invert conformal map 
a novel method for simultaneous keyphrase extraction and generic text summarization is proposed by modeling text document a weighted undirected and weighted bipartite graph spectral graph clustering algorithm are useed for partitioning sentence of the document into topical group with sentence link prior being exploited to enhance clustering quality within each topical group saliency score for keyphrases and sentence are generated based on a mutual reinforcement principle the keyphrases and sentence are then ranked according to their saliency score and selected for inclusion in the top keyphrase list and summary of the document the idea of building a hierarchy of summary for document capturing different level of granularity is also briefly discussed our method is illustrated using several example from news article news broadcast transcript and web document 
this paper present a parametric system devised and implemented to perform hierarchical planning by delegating the actual search to an external planner the parameter at any level of abstraction including the ground one aimed at giving a better insight of whether or not the exploitation of abstract space can be used for solving complex planning problem comparison have been made between instance of the hierarchical planner and their non hierarchical counterpart to improve the significance of the result three different planner have been selected and used while performing experiment to facilitate the setting of experimental environment a novel semi automatic technique used to generate abstraction hierarchy starting from ground level domain description is also described 
the proliferation of objectionable information on the internet ha reached a level of serious concern to empower end user with the choice of blocking undesirable and offensive website we propose a multimodal information filter named morf in this paper we present morf s core component it confidence based classifier a cross bagging ensemble scheme and multimodal classification algorithm empirical study and initial statistic collected from the morf filter deployed at site in the u s and asia show that morf is both efficient and effective due to our classification method 
we present a novel approach for finding discontinuity that outperforms previously published result on this task rather than using a deeper grammar formalism our system combine a simple unlexicalized pcfg parser with a shallow pre processor this pre processor which we call a trace tagger doe surprisingly well on detecting where discontinuity can occur without using phase structure information 
world steel trade becomes more competitive every day and new high international quality standard and productivity level can only be achieved by applying the latest computational technology data driven analysis of complex process is necessary in many industrial application where analytical modeling is not possible this paper present the deployment of kdd technology in one real industrial problem the development of new tinplate quality diagnostic model the electrodeposition of tin on steel strip is the most critical stage of a complex process that involves a great amount of variable and operating condition it optimization is not only a great commercial and economic challenge but also a compulsion due to the social impact of the tinplate product more than of the production is used for food packaging the necessary certification with standard like iso requires the use of diagnostic model to minimize the cost and the environmental impact this aim ha been achieved following the multi stage dm methodology crisp dm and a novel application of pro active maintenance method a fmea for the identification of the specific process anomaly three dm tool have been used for the development of the model the final result include two ann tinplate quality diagnostic model that provide the estimated quality of the final product just second after it production and only based on the process data the result have much better performance than the classical faraday s model widely used for the estimation 
abstract dimensionality reduction of empirical co occurrence data is a fundamental problem in unsuper vised learning it is also a well studied problem in statistic known a the analysis of cross classified data one principled approach to this problem is to represent the data in low dimension with min imal loss of mutual information contained in the original data in this paper we introduce an information theoretic nonlinear method for finding such a most informative dimension reduction in contrast with previously introduced clustering based approach here we extract continuous fea ture function directly from the co occurrence matrix in a sense we automatically extract function of the variable that serve a approximate sufficient statistic for a sample of one variable about the other one our method is different from dimensionality reduction method which are based on a specific sometimes arbitrary metric or embedding another interpretation of our method is a generalized multi dimensional non linear regression where rather than fitting one regression function through two dimensional data we extract d regression function whose expectation val ues capture the information among the variable it thus present a new learning paradigm that unifies aspect from both supervised and unsupervised learning the resulting dimension reduction can be described by two conjugate d dimensional differential manifold that are coupled through maximum entropy i projection the riemannian metric of these manifold are determined by the observed expectation value of our extracted feature following this geometric interpretation we present an iterative information projection algorithm for finding such feature and prove it convergence our algorithm is similar to the method of association analysis in statistic though the feature extraction context a well a the information theoretic and geometric interpretation are new the algorithm is illustrated by various synthetic co occurrence data it is then demonstrated for text categorization and information retrieval and prof effective in selecting a small set of feature often improving performance over the original feature set 
text normalization is an important aspect of successful information retrieval from medical document such a clinical note radiology report and discharge summary in the medical domain a significant part of the general problem of text normalization is abbreviation and acronym disambiguation numerous abbreviation are used routinely throughout such text and knowing their meaning is critical to data retrieval from the document in this paper i will demonstrate a method of automatically generating training data for maximum entropy me modeling of abbreviation and acronym and will show that using me modeling is a promising technique for abbreviation and acronym normalization i report on the result of an experiment involving training a number of me model abbreviation and acronym on a sample of rheumatology note with accuracy 
motion layer estimation ha recently emerged a apromising object tracking method in this paper we extendprevious research on layer based tracker by introducingthe concept of background occluding layer and explicitlyinferring depth ordering of foreground layer thebackground occluding layer lie in front of behind and inbetween foreground layer each pixel in the backgroundregions belongs to one of these layer and occludes all theforeground layer behind it together with the foregroundordering the complete information necessary for reliablytracking object through occlusion is included in ourrepresentation an map estimation framework isdeveloped to simultaneously update the motion layerparameters the ordering parameter and the backgroundoccluding layer experimental result show that undervarious condition with occlusion including situationswith moving object undergoing complex motion orhaving complex interaction our tracking algorithm isable to handle many difficult tracking task reliably 
rdf based p p network have a number of advantage compared with simpler p p network such a napster gnutella or with approach based on distributed index such a can and chord rdf based p p network allow complex and extendable description of resource instead of fixed and limited one and they provide complex query facility against these metadata instead of simple keyword based search in previous paper we have described the edutella infrastructure and different kind of edutella peer implementing such an rdf based p p network in this paper we will discus these rdf based p p network a a specific example of a new type of p p network schema based p p network and describe the use of super peer based topology for these network super peer based network can provide better scalability than broadcast based network and do provide perfect support for inhomogeneous schema based network which support different metadata schema and ontology crucial for the semantic web furthermore a we will show in this paper they are able to support sophisticated routing and clustering strategy based on the metadata schema attribute and ontology used especially helpful in this context is the rdf functionality to uniquely identify schema attribute and ontology the resulting routing index can be built using dynamic frequency counting algorithm and support local mediation and transformation rule and we will sketch some first idea for implementing these advanced functionality a well 
unknown environment where unexpected condition can with each new rover mission to mar rover are traveling significantly longer distance in some case distance are increasing by order of magnitude from previous mission this increase enables not only the collection of more science data but cause a large rise in the number of new and different science collection opportunity in this paper we describe the oasis system which provides autonomous capability for dynamically pursuing these science collection opportunity during longrange rover traverse oasis utilizes technique from both machine learning and planning and scheduling to address this goal machine learning technique are applied to analyze data a it is collected and quickly determine new science task and priority on these task planning and scheduling technique are used to alter the rover s behavior so new science measurement can be performed while still obeying resource and other mission constraint in addition to describing our system we also discus how we are testing oasis including the use of mar rover prototype and validation using data gathered from expert planetary geologist 
in this study we show experimental result on using independent component analysis ica and the self organizing map som in document analysis our document are segment of spoken dialogue carried out over the telephone in a customer service transcribed into text the task is to analyze the topic of the discussion and to group the discussion into meaningful subset the quality of the grouping is studied by comparing to a manual topical classification of the document 
in this poster we present a model of the flow of information among bioinformatics resource in the context of a specific scientific problem combining task analysis with traditional qualitative research we determined the extent to which the bioinformatics analysis process could be automated the model represents a semi automated process involving fourteen distinct data processing step and form the framework for an interface to bioinformatics information 
the minimax probability machine classification mpmc framework lanckriet et al build classifier by minimizing the maximum probability of misclassification and give direct estimate of the probabilistic accuracy bound the only assumption that mpmc make is that good estimate of mean and covariance matrix of the class exist however a with support vector machine mpmc is computationally expensive and requires extensive cross validation experiment to choose kernel and kernel parameter that give good performance in this paper we address the computational cost of mpmc by proposing an algorithm that construct nonlinear sparse mpmc smpmc model by incrementally adding basis function i e kernel one at a time greedily selecting the next one that maximizes the accuracy bound smpmc automatically chooses both kernel parameter and feature weight without using computationally expensive cross validation therefore the smpmc algorithm simultaneously address the problem of kernel selection and feature selection i e feature weighting based solely on maximizing the accuracy bound experimental result indicate that we can obtain reliable bound a well a test set accuracy that are comparable to state of the art classification algorithm 
this paper considers vehicle routing problem vrp where customer location and service time are random variable that are realized dynamically during plan execution it proposes a multiple scenario approach msa that continuously generates plan consistent with past decision and anticipating future request the approach which combine ai and or technique in novel way is compared with the best available heuristic that model long distance courier mail service larsen et al experimental result show that msa may significantly decrease travel time and is robust wrt reasonably noisy distribution 
layout analysis is the process of extracting a hierarchical structure describing the layout of a page in the system wisdom the layout analysis is performed in two step firstly the global analysis determines possible area containing paragraph section column figure and table and secondly the local analysis group together block that possibly fall within the same area the result of the local analysis process strongly depends on the quality of the result of the first step we investigate the possibility of supporting the user during the correction of the result of the global analysis this is done by automatically generating training example of action selection from the sequence of user action and then by learning action selection rule for layout correction rule are expressed a a logic program whose induction demand the careful application of ilp technique experimental result on a set of multi page document shed evidence on the difficulty of the learning task tackled and pose new problem in learning control rule for adaptive interface 
this paper provides an objective evaluation of the performance impact of binary xml encoding using a fast stream based xquery processor a our representative application instead of proposing one binary format and comparing it against standard xml parser we investigate the individual effect of several binary encoding technique that are shared by many proposal our goal is to provide a deeper understanding of the performance impact of binary xml encoding in order to clarify the ongoing and often contentious debate over their merit particularly in the domain of high performance xml stream processing 
this paper present a novel domain independent text segmentation method which identifies the boundary of topic change in long text document and or text stream the method consists of three component a a preprocessing step we eliminate the document dependent stop word a well a the generic stop word before the sentence similarity is computed this step assist in the discrimination of the sentence semantic information then the cohesion information of sentence in a document or a text stream is captured with a sentence distance matrix with each entry corresponding to the similarity between a sentence pair the distance matrix can be represented with a gray scale image thus a text segmentation problem is converted into an image segmentation problem we apply the anisotropic diffusion technique to the image representation of the distance matrix to enhance the semantic cohesion of sentence topical group a well a sharpen topical boundary at last the dynamic programming technique is adapted to find the optimal topical boundary and provide a zoom in and zoom out mechanism for topic access by segmenting text in variable number of sentence topical group our approach involves no domain specific training and it can be applied to text in a variety of domain the experimental result show that our approach is effective in text segmentation and outperforms several state of the art method 
this paper describes technique for fusing the output of multiple cue to robustly and accurately segment foreground object from the background in image sequence two different method for cue integration are presented and tested the first is a probabilistic approach which at each pixel computes the likelihood of observation over all cue before assigning pixel to foreground or background layer using bayes rule the second method allows each cue to make a decision independent of the other cue before fusing their output with a weighted sum a further important contribution of our work concern demonstrating how model for some cue can be learnt and subsequently adapted online in particular region of coherent motion are used to train distribution for colour and for a simple texture descriptor an additional aspect of our framework is in providing mechanism for suppressing cue when they are believed to be unreliable for instance during training or when they disagree with the general consensus result on extended video sequence are presented 
the bayesian paradigm provides a natural and effective mean of exploiting prior knowledge concerning the time frequency structure of sound signal such a speech and music something which ha often been overlooked in traditional audio signal processing approach here after constructing a bayesian model and prior distribution capable of taking into account the time frequency characteristic of typical audio waveform we apply markov chain monte carlo method in order to sample from the resultant posterior distribution of interest we present speech enhancement result which compare favourably in objective term with standard time varying filtering technique and in several case yield superior performance both objectively and subjectively moreover in contrast to such method our result are obtained without an assumption of prior knowledge of the noise power 
we review a query log of hundred of million of query that constitute the total query traffic for an entire week of a general purpose commercial web search service previously query log have been studied from a single cumulative view in contrast our analysis show change in popularity and uniqueness of topically categorized query across the hour of the day we examine query traffic on an hourly basis by matching it against list of query that have been topically pre categorized by human editor this represents of the query traffic we show that query traffic from particular topical category differs both from the query stream a a whole and from other category this analysis provides valuable insight for improving retrieval effectiveness and efficiency it is also relevant to the development of enhanced query disambiguation routing and caching algorithm 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
the paper describes two parsing scheme a shallow approach based on machine learning and a cascaded finite state parser with a hand crafted grammar it discus several way to combine them and present evaluation result for the two individual approach and their combination an underspecification scheme for the output of the finite state parser is introduced and shown to improve performance 
we introduce a framework which we call divide by db for extending support vector machine svm to multi class problem db offer an alternative to the standard one against one and one against rest algorithm for an n class problem db produce an n node binary decision tree where node represent decision boundary formed by n svm binary classifier this tree structure allows u to present a generalization and a time complexity analysis of db our analysis and related experiment show that db is faster than one against one and one against rest algorithm in term of testing time significantly faster than one against rest in term of training time and that the cross validation accuracy of db is comparable to these two method 
this work applies boosted wrapper induction bwi a machine learning algorithm for information extraction from semi structured document to the problem of named entity recognition the default feature set of bwi is augmented with feature based on distributional term cluster induced from a large unlabeled text corpus using no traditional linguistic resource such a syntactic tag or specialpurpose gazetteer this approach yield result near the state of the art in the muc named entity domain supervised learning using feature derived through unsupervised corpus analysis may be regarded a an alternative to bootstrapping method 
we have constructed a second generation cpg chip capable of generating the necessary timing to control the leg of a walking machine we demonstrate improvement over a previous chip by moving toward a significantly more versatile device this includes a larger number of silicon neuron more sophisticated neuron including voltage dependent charging and relative and absolute refractory period and enhanced programmability of neural network this chip build on the basic result achieved on a previous chip and expands it versatility to get closer to a self contained locomotion controller for walking robot 
a key challenge facing it organization today is their evolution towards adopting e business practice that give rise to the need for reengineering their underlying software system any reengineering effort ha to be aware of the functional requirement of the subject system in order not to violate the integrity of it intended us however a software system get regularly maintained throughout their lifecycle the documentation of their requirement often become obsolete or get lost to address this problem of software requirement loss we have developed an interaction pattern mining method for the recovery of functional requirement a usage scenario our method analyzes trace of the run time system user interaction to discover frequently recurring pattern these pattern correspond to the functionality currently exercised by the system user represented a usage scenario the discovered scenario provide the basis for reengineering the software system into web accessible component each one supporting one of the discovered scenario in this paper we describe ipm our interaction pattern discovery algorithm we illustrate it with a case study from a real application and we give an overview of the reengineering process in the context of which it is employed 
in this paper we present a hybrid system combining technique from symbolic planning and reinforcement learning planning is used to automatically construct task hierarchy for hierarchical reinforcement learning based on abstract model of the behaviour purpose and to perform intelligent termination improvement when an executing behaviour is no longer appropriate reinforcement learning is used to produce concrete implementation of abstractly defined behaviour and to learn the best possible choice of behaviour when plan are ambiguous two new hierarchical reinforcement learning algorithm are presented planned hierarchical semi markov q learning p hsmq a variant of the hsmq algorithm dietterich b which us plan built task hierarchy and teleoreactive q learning trq a more complex algorithm which implement hierarchical reinforcement learning with teleo reactive execution semantics nilsson each algorithm is demonstrated in a simple grid world domain localised goal policy are learnt in term of these abstract behaviour rather than directly in term of primitive action simply adding behaviour blindly doe not solve the problem an agent with a diverse repertoire of behaviour with overlapping applicability space may have just a much trouble learning a policy a an agent learning a primitive policy directly most hierarchical reinforcement learning algorithm are model free they require no prior model of their behaviour effect nor do they build one and the agent must explore them to learn their effect yet behaviour are designed with a purpose this purpose serf a an abstract model which tell u that from all the applicable behaviour in a particular state some are appropriate and some are not to save the learning agent from blindly exploring inappropriate behaviour we need to implement some of this knowledge most existing algorithm achieve this by including some kind of task hierarchy which structure the agent s decision process limiting the set of choice it can make to those that might be productive essentially this is a function which map the agent s state to a set of appropriate behaviour at present this function is implemented by hand by the trainer a more ambitious problem are tackled this is likely to become an increasingly difficult task in this paper we provide a mean to specify abstract symbolic model of an agent s behaviour and goal behaviour are represented by planning operator symbolic description are used to allow the trainer to specify behaviour in a high level language these operator are then used to automatically construct taskhierarchies through planning this hybrid of planning and learning allows u to have the best of both world using background knowledge to automatically structure our policy through planning and reinforcement learning to produce concrete policy for behaviour and optimise choice in the plan 
dividing web page into fragment ha been shown to provide significant benefit for both content generation and caching in order for a web site to use fragment based content generation however good method are needed for dividing web page into fragment manual fragmentation of web page is expensive error prone and unscalable this paper proposes a novel scheme to automatically detect and flag fragment that are cost effective cache unit in web site serving dynamic content we consider the fragment to be interesting if they are shared among multiple document or they have different lifetime or personalization characteristic our approach ha three unique feature first we propose a hierarchical and fragment aware model of the dynamic web page and a data structure that is compact and effective for fragment detection second we present an efficient algorithm to detect maximal fragment that are shared among multiple document third we develop a practical algorithm that effectively detects fragment based on their lifetime and personalization characteristic we evaluate the proposed scheme through a series of experiment showing the benefit and cost of the algorithm we also study the impact of adopting the fragment detected by our system on disk space utilization and network bandwidth consumption 
accurate spectral decomposition is essential for the analysis and diagnosis of histologically stained tissue section in this paper we present the first automated system for performing this decomposition we compare the performance of our system with ground truth data and report favorable result 
we present a sufficient a well a a necessary condition for the equivalence between answer set and model of completion for logic program with nested expression in the body of rule this condition is the weakest among all that we are aware of even for normal logic program to obtain this result we present a polynomial time reduction from this class of nested logic program to extended program consequently answer set for these nested program can be computed by an answer set generator for extended program on the one hand and characterized in term of model of completion on the other 
there have been significant advance in cross language information retrieval clir in recent year one of the major remaining reason that clir doe not perform a well a monolingual retrieval is the presence of out of vocabulary oov term previous work ha either relied on manual intervention or ha only been partially successful in solving this problem we use a method that extends earlier work in this area by augmenting this with statistical analysis and corpus based translation disambiguation to dynamically discover translation of oov term the method can be applied to both chinese english and english chinese clir correctly extracting translation of oov term from the web automatically and thus is a significant improvement on earlier work 
kernel k mean and spectral clustering have both been used to identify cluster that are non linearly separable in input space despite significant research these method have remained only loosely related in this paper we give an explicit theoretical connection between them we show the generality of the weighted kernel k mean objective function and derive the spectral clustering objective of normalized cut a a special case given a positive definite similarity matrix our result lead to a novel weighted kernel k mean algorithm that monotonically decrease the normalized cut this ha important implication a eigenvector based algorithm which can be computationally prohibitive are not essential for minimizing normalized cut b various technique such a local search and acceleration scheme may be used to improve the quality a well a speed of kernel k mean finally we present result on several interesting data set including diametrical clustering of large gene expression matrix and a handwriting recognition data set 
we present an image based approach to infer d structureparameters using a probabilistic shape structure model the d shape of an object class is represented by setsof contour from silhouette view simultaneously observedfrom multiple calibrated camera while structural featuresof interest on the object are denoted by a number of d location a prior density over the multi view shape and correspondingstructure is constructed with a mixture of probabilisticprincipal component analyzer given a novelset of contour we infer the unknown structure parametersfrom the new shape s bayesian reconstruction modelmatching and parameter inference are done entirely in theimage domain and require no explicit d construction ourshape model enables accurate estimation of structure despitesegmentation error or missing view in the input silhouette and it work even with only a single input view using a training set of thousand of pedestrian image generatedfrom a synthetic model we can accurately infer the d location of joint on the body based on observedsilhouette contour from real image 
the heterogeneous web exacerbates ir problem and short user query make them worse the content of web document are not enough to find good answer document link information and url information compensates for the insufficiency of content information however static combination of multiple evidence may lower the retrieval performance we need different strategy to find target document according to a query type we can classify user query a three category the topic relevance task the homepage finding task and the service finding task in this paper a user query classification scheme is proposed this scheme us the difference of distribution mutual information the usage rate a anchor text and the po information for the classification after we classified a user query we apply different algorithm and information for the better result for the topic relevance task we emphasize the content information on the other hand for the homepage finding task we emphasize the link information and the url information we could get the best performance when our proposed classification method with the okapi scoring algorithm wa used 
web application are becoming increasingly popular for mobile wireless system however wireless network can have high packet loss rate which can degrade web browsing performance on wireless system an alternative approach is wireless thin client computing in which the web browser run on a remote thin server with a more reliable wired connection to the internet a mobile client then maintains a connection to the thin server to receive display update over the lossy wireless network to ass the viability of this thin client approach we compare the web browsing performance of thin client against fat client that run the web browser locally in lossy wireless network our result show that thin client can operate quite effectively over lossy network compared to fat client running web browser locally our result show surprisingly that thin client can be faster and more resilient on web application over lossy wireless lan despite having to send more data over the network we characterize and analyze different design choice in various thin client system and explain why these approach can yield superior web browsing performance in lossy wireless network 
a new feature detection technique is presented that utilises local radial symmetry to identify region of interest within a scene this transform is significantly faster than existing technique using radial symmetry and offer the possibility of real time implementation on a standard processor the new transformis shown to perform well on a wide variety of image and it performance is tested against leading technique from the literature both a a facial feature detector and a a generic region of interest detector the new transformis seen to offer equal or superior performance to contemporary technique whilst requiring drastically le computational effort 
in this paper we revisit the problem of inducing a process model from time series data we illustrate this task with a realistic ecosystem model review an initial method for it induction then identify three challenge that require extension of this method these include dealing with unobservable variable finding numeric condition on process and preventing the creation of model that overfit the training data we describe response to these challenge and present experimental evidence that they have the desired effect after this we show that this extended approach to inductive process modeling can explain and predict time series data from battery on the international space station in closing we discus related work and consider direction for future research 
this paper report work on automated meta data creation for multimedia content the approach result in the generation of a conceptual index of the content which may then be searched via semantic category instead of keywords the novelty of the work is to exploit multiple source of information relating to video content in this case the rich range of source covering important sport event news commentary and web report covering international football game in multiple language and multiple modality is analysed and the resultant data merged this merging process lead to increased accuracy relative to individual source 
we propose a method that allows for a rigorous statistical analysis of neural response to natural stimulus which are non gaussian and exhibit strong correlation we have in mind a model in which neuron are selective for a small number of stimulus dimension out of the high dimensional stimulus space but within this subspace the response can be arbitrarily nonlinear therefore we maximize the mutual information between the sequence of elicited neural response and an ensemble of stimulus that ha been projected on trial direction in the stimulus space the procedure can be done iteratively by increasing the number of direction with respect to which information is maximized those direction that allow the recovery of all of the information between spike and the full unprojected stimulus describe the relevant subspace if the dimensionality of the relevant subspace indeed is much smaller than that of the overall stimulus space it may become experimentally feasible to map out the neuron s input output function even under fully natural stimulus condition this contrast with method based on correlation function reverse correlation spike triggered covariance which all require simplified stimulus statistic if we are to use them rigorously 
the representation of acoustic signal at the cochlear nerve must serve a wide range of auditory task that require exquisite sensitivity in both time and frequency lewicki demonstrated that many of the filtering property of the cochlea could be explained in term of efficient coding of natural sound this model however did not account for property such a phase locking or how sound could be encoded in term of action potential here we extend this theoretical approach with algorithm for learning efficient auditory code using a spiking population code here we propose an algorithm for learning efficient auditory code using a theoretical model for coding sound in term of spike in this model each spike encodes the precise time position and magnitude of a localized time varying kernel function by adapting the kernel function to the statistic natural sound we show that compared to conventional signal representation the spike code achieves far greater coding efficiency furthermore the inferred kernel show both striking similarity to measured cochlear filter and a similar bandwidth versus frequency dependence 
in this poster we describe an experiment exploring the effectiveness of a pen based text input device for use in query construction standard trec query were written recognised and subsequently retrieved upon comparison between retrieval effectiveness based on the recognised writing and a typed text baseline were made on average effectiveness wa of the baseline other statistic on the quality and nature of recognition are also reported 
this paper present domain relevance estimation dre a fully unsupervised text categorization technique based on the statistical estimation of the relevance of a text with respect to a certain category we use a pre dened set of category we call them domain which have been previously associated to wordnet word sens given a certain domain dre distinguishes between relevant and non relevant text by mean of a gaussian mixture model that describes the frequency distribution of domain word inside a large scale corpus then an expectation maximization algorithm computes the parameter that maximize the likelihood of the model on the empirical data the correct identication of the domain of the text is a crucial point for domain driven disambiguation an unsupervised word sense disambiguation wsd methodology that make use of only domain information therefore dre ha been exploited and evaluated in the context of a wsd task result are comparable to those of state ofthe art unsupervised wsd system and show that dre provides an important contribution 
we describe a novel method for human detection in single image which can detect full body a well a close up view in the presence of clutter and occlusion human are modeled a flexible assembly of part and robust part detection is the key to the approach the part are represented by co occurrence of local feature which capture the spatial layout of the part s appearance feature selection and the part detector are learnt from training image using adaboost the detection algorithm is very efficient a i all part detector use the same initial feature ii a coarse to fine cascade approach is used for part detection iii a part assembly strategy reduces the number of spurious detection and the search space the result outperform existing human detector 
the purpose of this paper is to automatically create multilingual translation lexicon with regional variation we propose a transitive translation approach to determine translation variation across language that have insufficient corpus for translation via the mining of bilingual search result page and clue of geographic information obtained from web search engine the experimental result have shown the feasibility of the proposed approach in efficiently generating translation equivalent of various term not covered by general translation dictionary it also revealed that the created translation lexicon can reflect different cultural aspect across region such a taiwan hong kong and mainland china 
information filtering ha made considerable progress in recent year the predominant approach are content based method and collaborative method researcher have largely concentrated on either of the two approach since a principled unifying framework is still lacking this paper suggests that both approach can be combined under a hierarchical bayesian framework individual content based user profile are generated and collaboration between various user model is achieved via a common learned prior distribution however it turn out that a parametric distribution e g gaussian is too restrictive to describe such a common learned prior distribution we thus introduce a nonparametric common prior which is a sample generated from a dirichlet process which assumes the role of a hyper prior we describe effective mean to learn this nonparametric distribution and apply it to learn user information need the resultant algorithm is simple and understandable and offer a principled solution to combine content based filtering and collaborative filtering within our framework we are now able to interpret various existing technique from a unifying point of view finally we demonstrate the empirical success of the proposed information filtering method 
machine learning method are often applied to the problem of learning a map from a robot s sensor data but they are rarely applied to the problem of learning a robot s motion model the motion model which can be influenced by robot idiosyncrasy and terrain property is a crucial aspect of current algorithm for simultaneous localization and mapping slam in this paper we concentrate on generating the correct motion model for a robot by applying em method in conjunction with a current slam algorithm in contrast to previous calibration approach we not only estimate the mean of the motion but also the interdependency between motion term and the variance in these term this can be used to provide a more focused proposal distribution to a particle filter used in a slam algorithm which can reduce the resource needed for localization while decreasing the chance of losing track of the robot s position we validate this approach by recovering a good motion model despite initialization with a poor one further experiment validate the generality of the learned model in similar circumstance 
language acquisition is a special kind of learning problem because the outcome of learning of one generation is the input for the next that make it possible for language to adapt to the particularity of the learner in this paper i show that this type of language change ha important consequence for model of the evolution and acquisition of syntax 
building on recent progress in modeling lter responsestatistics of natural image we integrate a statistical modelinto a variational framework for image segmentation incorporatedin a sound probabilistic distance measure themodel drive level set toward meaningful segmentationsof complex texture and natural scene since each regioncomprises two model parameter only the approachis computationally ef cient and enables the application ofvariational segmentation to a considerably larger class ofreal world image we validate the statistical basis of ourapproach on thousand of natural image and demonstratethat our model outperforms recent variational segmentationmethods based on second order statistic 
evolutionary computation is a useful technique for learning behavior in multiagent system among the several type of evolutionary computation one natural and popular method is to coevolve multi agent behavior in multiple cooperating population recent research ha suggested that revolutionary system may favor stability rather than performance in some domain in order to improve upon existing method this paper examines the idea of modifying traditional coevolution biasing it to search for maximal reward we introduce a theoretical justification of the improved method and present experiment in three problem domain we conclude that biasing can help coevolution find better result in some multiagent problem domain 
the so called expert algorithm constitute a methodology for choosing action repeatedly when the reward depend both on the choice of action and on the unknown current state of the environment an expert algorithm ha access to a set of strategy expert each of which may recommend which action to choose the algorithm learns how to combine the recommendation of individual expert so that in the long run for any fixed sequence of state of the environment it doe a well a the best expert would have done relative to the same sequence this methodology may not be suitable for situation where the evolution of state of the environment depends on past chosen action a is usually the case for example in a repeated non zero sum game a new expert algorithm is presented and analyzed in the context of repeated game it is shown that asymptotically under certain condition it performs a well a the best available expert this algorithm is quite different from previously proposed expert algorithm it represents a shift from the paradigm of regret minimization and myopic optimization to consideration of the long term effect of a player s action on the opponent s action or the environment the importance of this shift is demonstrated by the fact that this algorithm is capable of inducing cooperation in the repeated prisoner s dilemma game whereas previous expert algorithm converge to the suboptimal non cooperative play 
periodicy detection in time series data is a challenging problem of great importance in many application most previous work focused on mining synchronous periodic pattern and did not recognize the misaligned presence of a pattern due to the intervention of random noise in this paper we propose a more flexible model of asynchronous periodic pattern that may be present only within a subsequence and whose occurrence may be shifted due to disturbance two parameter min rep and max dis are employed to specify the minimum number of repetition that is required within each segment of nondisrupted pattern occurrence and the maximum allowed disturbance between any two successive valid segment upon satisfying these two requirement the longest valid subsequence of a pattern is returned a two phase algorithm is devised to first generate potential period by distance based pruning followed by an iterative procedure to derive and validate candidate pattern and locate the longest valid subsequence we also show that this algorithm cannot only provide linear time complexity with respect to the length of the sequence but also achieve space efficiency 
we present a novel method for learning with gaussian process regression in a hierarchical bayesian framework in a first step kernel matrix on a fixed set of input point are learned from data using a simple and efficient em algorithm this step is nonparametric in that it doe not require a parametric form of covariance function in a second step kernel function are fitted to approximate the learned covariance matrix using a generalized nystr om method which result in a complex data driven kernel we evaluate our approach a a recommendation engine for art image where the proposed hierarchical bayesian method lead to excellent prediction performance 
variability and diverseness among incoming request to a service hosted on a finite capacity resource necessitates sophisticated request admission control technique for providing guaranteed quality of service qos we propose in this paper a service time based online admission control methodology for maximizing profit of a service provider the proposed methodology chooses a subset of incoming request such that the revenue of the provider is maximized admission control decision in our proposed system is based upon an estimate of the service time of the request qos bound prediction of arrival and service time of request to come in the short term future and reward associated with servicing a request within it qos bound effectiveness of the proposed admission control methodology is demonstrated using experiment with a content based messaging middleware service 
predictive state representation psrs are a recently proposed way of modeling controlled dynamical system psr based model use prediction of observable outcome of test that could be done on the system a their state representation and have model parameter that define how the predictive state representation change over time a action are taken and observation noted learning psr based model requires solving two subproblems discovery of the test whose prediction constitute state and learning the model parameter that define the dynamic so far there have been no result available on the discovery subproblem while for the learning subproblem an approximate gradient algorithm ha been proposed singh et al with mixed result it work on some domain and not on others in this paper we provide the first discovery algorithm and a new learning algorithm for linear psrs for the special class of controlled dynamical system that have a reset operation we provide experimental verification of our algorithm finally we also distinguish our work from prior work by jaeger on observable operator model ooms 
we address the issue of judging the significance of rare event a it typically arises in statistical naturallanguage processing we first define a general approach to the problem and we empirically compare result obtained using log likelihood ratio and fisher s exact test applied to measuring strength of bilingual word association 
abstract we consider bayesian mixture approach where a predictor is constructedbyformingaweightedaverageofhypothesesfromsome space of function while such procedure are known to lead to optimalpredictorsinseveralcases wheresu cientlyaccurateprior information is available it ha not been clear how they perform whensomeofthepriorassumptionsareviolated inthispaperwe establish data dependent bound for such procedure extending previous randomized approach such a the gibbs algorithm to a fully bayesian setting the flnite sample guarantee established inthisworkenabletheutilizationofbayesianmixtureapproaches in agnostic setting where the usual assumption of the bayesian paradigmfailtohold moreover theboundsderivedcanbedirectly applied to non bayesian mixture approach such a bagging and boosting 
previous work on understanding user web search behavior ha focused on how people search and what they are searching for but not why they are searching in this paper we describe a framework for understanding the underlying goal of user search and our experience in using the framework to manually classify query from a web search engine our analysis suggests that so called navigational search are le prevalent than generally believed while a previously unexplored resource seeking goal may account for a large fraction of web search we also illustrate how this knowledge of user search goal might be used to improve future web search engine 
pattern discovery ha emerged a a direct result of increased data storage and analytic capability available to the data analyst without a massive amount of data we do not have the evidence to support the discovery of the local deterministic structure that we call pattern a such pattern discovery is one of the few area of data mining that cannot be considered simply a a scaling up of current statistical methodology to analyze large data set however the philosophy of hypothesis testing and modeling in traditional statistic do lend themselves to forming a framework for pattern discovery and we can also draw from idea relating to outlier discovery and residual analysis to discover pattern we illustrate an iterative strategy in a statistical framework by way of it application to one simulated and two real data set 
we consider learning to classify cognitive state of human subject based on their brain activity observed via functional magnetic resonance imaging fmri this problem is important because such classifier constitute virtual sensor of hidden cognitive state which may be useful in cognitive science research and clinical application in recent work mitchell et al have demonstrated the feasibility of training such classifier for individual human subject e g to distinguish whether the subject is reading an ambiguous or unambiguous sentence or whether they are reading a noun or a verb here we extend that line of research exploring how to train classifier that can be applied across multiple human subject including subject who were not involved in training the classifier we describe the design of several machine learning approach to training multiple subject classifier and report experimental result demonstrating the success of these method in learning cross subject classifier for two different fmri data set 
we perform a systematic comparison of sat and csp model for a challenging combinatorial problem quasigroup completion qcp our empirical result clearly indicate the superiority of the d sat encoding kautz et al with various solver over other sat and csp model we propose a partial explanation of the observed performance analytically we focus on the relative conciseness of the d model and the pruning power of unit propagation empirically the focus is on the role of the unit propagation heuristic of the best performing solver satz li anbulagan which prof crucial to it success and result in a significant improvement in scalability when imported into the csp solver our result strongly suggest that sat encoding of permutation problem hnich smith walsh may well prove quite competitive in other domain in particular when compared with the currently preferred channeling csp model 
in traditional text classification a classifier is built using labeled training document of every class this paper study a different problem given a set p of document of a particular class called positive class and a set u of unlabeled document that contains document from class p and also other type of document called negative class document we want to build a classifier to classify the document in u into document from p and document not from p the key feature of this problem is that there is no labeled negative document which make traditional text classification technique inapplicable in this paper we propose an effective technique to solve the problem it combine the rocchio method and the svm technique for classifier building experimental result show that the new method outperforms existing method significantly 
in this paper we present a mobile streaming medium cdn content delivery network architecture in which content segmentation request routing pre fetch scheduling and session handoff are controlled by smil synchronized multimedia integrated language modification in this architecture mobile client simply follow modified smil file downloaded from a streaming portal server these modification enable multimedia content to be delivered to the mobile client from the best surrogate in the cdn the key component of this architecture are content segmentation with smil modification on demand rewriting of url in smil pre fetch scheduling based on timing information derived from smil smil update by soap simple object access protocol messaging for session handoff due to client mobility we also introduce qos control with a network agent called an rtp monitoring agent to enable appropriate control of medium quality based on both network congestion and radio link condition the current status of our prototyping on a mobile qos testbed mobiq is reported in this paper we are currently designing the soap based apis application programmable interface needed for the mobile streaming medium cdn and building the cdn over the current testbed 
an important objective of the semantic web is to make electronic commerce interaction more flexible and automated to achieve this standardization of ontology message content and message protocol will be necessary in this paper we investigate how semantic and web service technology can be used to support service advertisement and discovery in e commerce in particular we describe the design and implementation of a service matchmaking prototype which us a daml s based ontology and a description logic reasoner to compare ontology based service description we also present the result of initial experiment testing the performance of this prototype implementation in a realistic agent based e commerce scenario 
the bootstrap ha become a popular method for exploring model structure uncertainty our experiment with articial and realworld data demonstrate that the graph learned from bootstrap sample can be severely biased towards too complex graphical model accounting for this bias is hence essential e g when exploring model uncertainty we nd that this bias is intimately tied to well known spurious dependence induced by the bootstrap the leading order bias correction equal one half of akaike s penalty for model complexity we demonstrate the eect of this simple bias correction in our experiment we also relate this bias to the bias of the plug in estimator for entropy a well a to the dierence between the expected test and training error of a graphical model which asymptotically equal akaike s penalty rather than one half 
spectral clustering us eigenvectors of the laplacian of the similarity matrix they are most conveniently applied to way clustering problem when applying to multi way clustering either the way spectral clustering is recursively applied or an embedding to spectral space is done and some other method are used to cluster the point here we propose and study a k way cluster assignment method the method transforms the problem to find valley and peak of a d quantity called cluster crossing which measure the symmetric cluster overlap across a cut point along a linear ordering of the data point the method can either determine k cluster in one shot or recursively split a current cluster into several smaller one we show that a linear ordering based on a distance sensitive objective ha a continuous solution which is the eigenvector of the laplacian showing the close relationship between clustering and ordering the method relies on the connectivity matrix constructed a the truncated spectral expansion of the similarity matrix useful for revealing cluster structure the method is applied to newsgroups to illustrate introduced concept experiment show it outperforms the recursive way clustering and the standard k mean clustering 
we present a common variational framework for dense depth recoveryand dense three dimensional motion field estimation from multiplevideo sequence which is robustto camera spectral sensitivitydifferences and illumination change for this purpose we firstshow that both problem reduce to a generic image matching problemafter backprojecting the input image onto suitable surface wethen solve this matching problem in the case of statisticalsimilarity criterion that can handle frequently occurring non affineimage intensity dependency our method lead to an efficientand elegant implementation based on fast recursive filter weobtain good result on real image 
we study local interchangeability of value in constraint network based on a new approach where a single value in the domain of a variable can be treated a a combination of subvalues we present an algorithm for breaking up value and combining identical fragment experimental result show that the transformed problem take le time to solve for all solution and yield more compactly representable but equivalent solution set we obtain new theoretical result on context dependent interchangeability and full interchangeability and suggest some other application 
we extend previous work on tree kernel to estimate the similarity between the dependency tree of sentence using this kernel within a support vector machine we detect and classify relation between entity in the automatic content extraction ace corpus of news article we examine the utility of different feature such a wordnet hypernym part of speech and entity type and find that the dependency tree kernel achieves a f improvement over a bag of word kernel 
various constrained frequent pattern mining problem formulation and associated algorithm have been developed that enable the user to specify various itemset based constraint that better capture the underlying application requirement and characteristic in this paper we introduce a new class of block constraint that determine the significance of an itemset pattern by considering the dense block that is formed by the pattern s item and it associated set of transaction block constraint provide a natural framework by which a number of important problem can be specified and make it possible to solve numerous problem on binary and real valued datasets however developing computationally efficient algorithm to find these block constraint pose a number of challenge a unlike the different itemset based constraint studied earlier these block constraint are tough a they are neither anti monotone monotone nor convertible to overcome this problem we introduce a new class of pruning method that significantly reduce the overall search space and present a computationally efficient and scalable algorithm called cbminer to find the closed itemsets that satisfy the block constraint 
in this paper we develop two method for improving the performance of the standard distributed breakout algorithm yokoo et al using the notion of interchangeability we study the performance of this algorithm on the problem of distributed sensor network in particular we consider how neighborhood interchangeability and neighborhood partial interchangeability freuder can be used to keep conflict localized and avoid chain reaction where a conflict originating in one part of the problem spread to neighboring area we see from the experimental result that such technique can bring about significant improvement in term of the number of cycle required to solve the problem and therefore improvement in term of communication and time requirement especially for difficult problem moreover the improved algorithm are able to solve a higher proportion of the test problem 
current e book browser provide minimal support for comprehending the organization narrative structure and theme of large complex book in order to build an understanding of such book reader should be provided with user interface that present and relate the organizational narrative and thematic structure we propose adapting information retrieval technique for the purpose of discovering these structure and sketch three distinctive visualization for presenting these structure to the e book reader these visualization are presented within an initial design for an e book browser 
we address the issue of discovering significant binary relationship in transaction datasets in a weighted setting traditional model of association rule mining is adapted to handle weighted association rule mining problem where each item is allowed to have a weight the goal is to steer the mining focus to those significant relationship involving item with significant weight rather than being flooded in the combinatornal explosion of insignificant relationship we identify the challenge of using weight in the iterative process of generating large itemsets the problem of invalidation of the downward closure property in the weighted setting is solved by using an improved model of weighted support measurement and exploiting a weighted downward closure property a new algorithm called warm weighted association rule mining is developed based on the improved model the algorithm is both scalable and efficient in discovering significant relationship in weighted setting a illustrated by experiment performed on simulated datasets 
topic tracking and information filtering are model of interactive task but their evaluation are generally done in a way that doe not reflect likely usage the model either force frequent judgment or disallow any at all assume the user is always available to make a judgment and do not allow for user fatigue in this study we extend the evaluation framework for topic tracking to incorporate those more realistic issue we demonstrate that tracking can be done in a realistic interactive setting with minimal impact on tracking cost and with substantial reduction in required interaction 
we present an algorithm pref ac that limit arc consistency ac to the preferred choice of a tree search procedure and that make constraint solving more efficient without changing the pruning and shape of the search tree arc consistency thus becomes more scalable and usable for many realworld constraint satisfaction problem such a configuration and scheduling moreover pref ac directly computes a preferred solution for treelike constraint satisfaction problem 
there ha been substantial progress in the past decade in the development of object classifier for image for example of face human s and vehicle here we address the problem of contamination e g occlusion shadow in test image which have not explicitly been encountered in training data the variational ising classifier vic algor ithm model contamination a a mask a field of binary variable with a st rong spatial coherence prior variational inference is used to marg inalize over contamination and obtain robust classification in this way the vic approach can turn a kernel classifier for clean data into one tha t can tolerate contamination without any specific training on contaminat ed positive 
we propose a general framework for parsing image into region and object in this framework the detection and recognition of object proceed simultaneously with image segmentation in a competitive and cooperative manner we illustrate our approach on natural image of complex city scene where the object of primary interest are face and text this method make use of bottom up proposal combined with top down generative model using the data driven markov chain monte carlo ddmcmc algorithm which is guaranteed to converge to the optimal estimate asymptotically more precisely we define generative model for face text and generic regionse g shading texture and clutter these model are activated by bottom up proposal the proposal for face and text are learnt using a probabilistic version of adaboost the ddmcmc combine reversible jump and diffusion dynamic to enable the generative model to explain the input image in a competitive and cooperative manner our experiment illustrate the advantage and importance of combining bottom up and top down model and of performing segmentation and object detection recognition simultaneously 
a new technique is introduced linguistic profiling in which large number of count of linguistic feature are used a a text profile which can then be compared to average profile for group of text the technique prof to be quite effective for authorship verification and recognition the best parameter setting yield a false accept rate of at a false reject rate equal to zero for the verification task on a test corpus of student essay and a way recognition accuracy on the same corpus 
web personalization is the process of customizing a web site to the need of each specific user or set of user taking advantage of the knowledge acquired through the analysis of the user s navigational behavior integrating usage data with content structure or user profile data enhances the result of the personalization process in this paper we present sewep a system that make use of both the usage log and the semantics of a web site s content in order to personalize it web content is semantically annotated using a conceptual hierarchy taxonomy we introduce c log an extended form of web usage log that encapsulates knowledge derived from the link semantics c log are used a input to the web usage mining process resulting in a broader yet semantically focused set of recommendation 
we propose an ecient alignment method for textured doosabin subdivision surface template a variation of the inverse compositional image alignment is derived by introducing smooth adjustment in the parametric space of the surface and relating them to the control point increment the convergence property of the proposed method are improved by a coarse to fine multiscale matching the method is applied to real time tracking of specially marked surface from a single camera view 
we present a supervised machine learning algorithm for metonymy resolution which exploit the similarity between example of conventional metonymy we show that syntactic head modifier relation are a high precision feature for metonymy recognition but suffer from data sparseness we partially overcome this problem by integrating a thesaurus and introducing simpler grammatical feature thereby preserving precision and increasing recall our algorithm generalises over two level of contextual similarity resulting inference exceed the complexity of inference undertaken in word sense disambiguation we also compare automatic and manual method for syntactic feature extraction 
an information retrieval method is proposed using a hierarchical dirichlet process a a prior on the parameter of a set of multinomial distribution the resulting method naturally includes a number of feature found in other popular method specifically tf idf like term weighting and document length normalisation are recovered the new method is compared with okapi bm and the twenty one model on trec data and is shown to give better performance 
in kernel method an interesting recent development seek to learn a good kernel from empirical data automatically in this paper by regarding the transductive learning of the kernel matrix a a missing data problem we propose a bayesian hierarchical model for the problem and devise the tanner wong data augmentation algorithm for making inference on the model the tanner wong algorithm is closely related to gibbs sampling and it also bear a strong resemblance to the expectation maximization em algorithm for an efficient implementation we propose a simplified bayesian hierarchical model and the corresponding tanner wong algorithm we express the relationship between the kernel on the input space and the kernel on the output space a a symmetric definite generalized eigenproblem based on this eigenproblem an efficient approach to choosing the base kernel matrix is presented the effectiveness of our bayesian model with the tanner wong algorithm is demonstrated through some classification experiment showing promising result 
abstract classification with partially labeled data requires using a large number of unlabeled example or an estimated marginal p x to further constrain the conditional p y x beyond a few available labeled example we formulate a regularization approach to linking the marginal and the conditional in a general way the regularization penalty measure the information that is implied about the label over covering region no parametric assumption are required and the approach remains tractable even for continuous marginal density p x we develop algorithm for solving the regularization problem for finite cover establish a limiting differential equation and exemplify the behavior of the new regularization approach in simple case 
in order to effectively use machine learning algorithm e g neural network for the analysis of survival data the correct treatment of censored data is crucial the concordance index ci is a typical metric for quantifying the predictive ability of a survival model we propose a new algorithm that directly us the ci a the objective function to train a model which predicts whether an event will eventually occur or not directly optimizing the ci allows the model to make complete use of the information from both censored and non censored observation in particular we approximate the ci via a differentiable function so that gradient based method can be used to train the model we applied the new algorithm to predict the eventual recurrence of prostate cancer following radical prostatectomy compared with the traditional cox proportional hazard model and several other algorithm based on neural network and support vector machine our algorithm achieves a significant improvement in being able to identify high risk and low risk group of patient 
we consider the bias and variance of value function estimation that are caused by using an empirical model instead of the true model we analyze these bias and variance for markov process from a classical frequentist statistical point of view and in a bayesian setting using a second order approximation we provide explicit expression for the bias and variance in term of the transition count and the reward statistic we present supporting experiment with artificial markov chain and with a large transactional database provided by a mail order catalog firm 
we propose to study link between three important classification algorithm perceptrons multi layer perceptrons mlps and support vector machine svms we first study way to control the capacity of perceptrons mainly regularization parameter and early stopping using the margin idea introduced with svms after showing that under simple condition a perceptron is equivalent to an svm we show it can be computationally expensive in time to train an svm and thus a perceptron with stochastic gradient descent mainly because of the margin maximization term in the cost function we then show that if we remove this margin maximization term the learning rate or the use of early stopping can still control the margin these idea are extended afterward to the case of mlps moreover under some assumption it also appears that mlps are a kind of mixture of svms maximizing the margin in the hidden layer space finally we present a very simple mlp based on the previous finding which yield better performance in generalization and speed than the other model 
automatically segmenting unstructured text string into structured record is necessary for importing the information contained in legacy source and text collection into a data warehouse for subsequent querying analysis mining and integration in this paper we mine table present in data warehouse and relational database to develop an automatic segmentation system thus we overcome limitation of existing supervised text segmentation approach which require comprehensive manually labeled training data our segmentation system is robust accurate and efficient and requires no additional manual effort thorough evaluation on real datasets demonstrates the robustness and accuracy of our system with segmentation accuracy exceeding state of the art supervised approach 
despite the connotation of the word browsing and surfing web usage often follows routine pattern of access however few mechanism exist to assist user with these routine task bookmark or portal site must be maintained manually and are insensitive to the user s browsing context to fill this void we designed and implemented the montage system a web montage is an ensemble of link and content fused into a single view such a coalesced view can be presented to the user whenever he or she open the browser or return to the start page we pose a number of hypothesis about how user would interact with such a system and test these hypothesis with a fielded user study our finding support some design decision such a using browsing context to tailor the montage raise question about others and point the way toward future work 
recent stereo algorithm have achieved impressive resultsby modelling the disparity image a a markov randomfield mrf an important component of an mrf basedapproach is the inference algorithm used to find the mostlikely setting of each node in the mrf algorithm havebeen proposed which use graph cut or belief propagationfor inference these stereo algorithm differ in both theinference algorithm used and the formulation of the mrf it is unknown whether to attribute the responsibility for differencesin performance to the mrf or the inference algorithm we address this through controlled experiment bycomparing the belief propagation algorithm and the graphcuts algorithm on the same mrf s which have been createdfor calculating stereo disparity we find that the labellingsproduced by the two algorithm are comparable the solution produced by graph cut have a lower energythan those produced with belief propagation but this doesnot necessarily lead to increased performance relative tothe ground truth 
this paper address the problem of merging result obtained from different database and search engine in a distributed information retrieval environment the prior research on this problem either assumed the exchange of statistic necessary for normalizing score cooperative solution or is heuristic both approach have disadvantage we show that the problem in uncooperative environment is simpler when viewed a a component of a distributed ir system that us query based sampling to create resource description document sampled for creating resource description can also be used to create a sample centralized index and this index is a source of training data for adaptive result merging algorithm a variety of experiment demonstrate that this new approach is more effective than a well known alternative and that it allows query by query tuning of the result merging function 
discovering the significant relation embedded in document would be very useful not only for information retrieval but also for question answering and summarization prior method for relation discovery however needed large annotated corpus which cost a great deal of time and effort we propose an unsupervised method for relation discovery from large corpus the key idea is clustering pair of named entity according to the similarity of context word intervening between the named entity our experiment using one year of newspaper reveals not only that the relation among named entity could be detected with high recall and precision but also that appropriate label could be automatically provided for the relation 
there have been many proposal to compute similarity between word based on their distribution in context however these approach do not distinguish between synonym and antonym we present two method for identifying synonym among distributionally similar word 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we propose an information theoretic clustering approach that incorporates a pre known partition of the data aiming to identify common cluster that cut across the given partition in the standard clustering setting the formation of cluster is guided by a single source of feature information the newly utilized pre partition factor introduces an additional bias that counterbalance the impact of the feature whenever they become correlated with this known partition the resulting algorithmic framework wa applied successfully to synthetic data a well a to identifying text based cross religion correspondence 
the semantic web is vitally dependent on a formal meaning for the construct of it language for semantic web language to work well together their formal meaning must employ a common view or thesis of representation otherwise it will not be possible to reconcile document written in different language the thesis of representation underlying rdf and rdfs is particularly troublesome in this regard a it ha several unusual aspect both semantic and syntactic a more standard thesis of representation would result in the ability to reuse existing result and tool in the semantic web 
automatic document classification dc is essential for the management of information and knowledge this paper explores two practical issue in dc each document ha it context of discussion and both the content and vocabulary of the document database is intrinsically evolving the issue call for adaptive document classification adc that adapts a dc system to the evolving contextual requirement of each document category so that input document may be classified based on their context of discussion we present an incremental context mining technique to tackle the challenge of adc theoretical analysis and empirical result show that given a text hierarchy the mining technique is efficient in incrementally maintaining the evolving contextual requirement of each category based on the contextual requirement mined by the system higher precision dc may be achieved with better efficiency 
many real life sequence database grow incrementally it is undesirable to mine sequential pattern from scratch each time when a small set of sequence grow or when some new sequence are added into the database incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database update however it is nontrivial to mine sequential pattern incrementally especially when the existing sequence grow incrementally because such growth may lead to the generation of many new pattern due to the interaction of the growing subsequence with the original one in this study we develop an efficient algorithm incspan for incremental mining of sequential pattern by exploring some interesting property our performance study show that incspan outperforms some previously proposed incremental algorithm a well a a non incremental one with a wide margin 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
abstract we consider here the problem of image classification when morethan one visual feature are available in these case bayes fusionoffers an attractive solution by combining the result of differentclassifiers one classifier per feature this is a general form of theso called quot naive bayes quot approach analyzing the performance ofbayes fusion with respect to a bayesian classifier over the jointfeature distribution however is tricky on the one hand it is wellknownthat the latter ha 
this paper describes a real time system for multi target tracking and classification in image sequence from a single stationary camera several target can be tracked simultaneously in spite of split and merges amongst the foreground object and presence of clutter in the segmentation result in result we show tracking of upto target simultaneously the algorithm combine kalman filter based motion and shape tracking with an efficient pattern matching algorithm the latter facilitates the use of a dynamic programming strategy to efficiently solve the data association problem in presence of multiple split and merges the system is fully automatic and requires no manual input of any kind for initialization of tracking the initialization for tracking is done using attributed graph the algorithm give stable and noise free track initialization the image based tracking result are used a input to a bayesian network based classifier to classify the target into different category after classification a simple d model for each class is used along with camera calibration to obtain d tracking result for the target we present result on a large number of real world image sequence and accurate d tracking result compared with the reading from the speedometer of the vehicle the complete tracking system including segmentation of moving target work at about hz for resolution color image on a ghz pentium desktop 
this paper introduces rankopt a linear binary classifier which optimises the area under the roc curve the auc unlike standard binary classifier rankopt adopts the auc statistic a it objective function and optimises it directly using gradient descent the problem with using the auc statistic a an objective function are that it is non differentiable and of complexity o n in the number of data observation rankopt us a differentiable approximation to the auc which is accurate and computationally efficient being of complexity o n this enables the gradient descent to be performed in reasonable time the performance of rankopt is compared with a number of other linear binary classifier over a number of different classification problem in almost all case it is found that the performance of rankopt is significantly better than the other classifier tested 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
travel and tourism is the leading application field in the b c e commerce it represents nearly of the total b c turnover already in the past travel application were at the forefront of information technology i e the airline computerized reservation system in the early s the industry and it product have rather specific feature which explain this circumstance the product is a confidence good consumer decision are solely based on information beforehand and the industry is highly networked based on world wide cooperation of very different type of stakeholder consequently this industry depends on advanced it application a such travel and tourism may serve a an example of what happens and will happen in the emerging e market pointing at structural change a well a challenging application scenario the paper provides an overview about the industry describes ongoing structural change outline domain specific requirement and discus achievement and challenge in the field following an ai and e commerce point of view it finish with consideration regarding a future it scenario 
this paper is about a variant of k nearest neighbor classification on large many class high dimensional datasets k nearest neighbor remains a popular classification technique especially in area such a computer vision drug activity prediction and astrophysics furthermore many more modern classifier such a kernel based bayes classifier or the prediction phase of svms require computational regime similar to k nn we believe that tractable k nn algorithm therefore continue to be important this paper relies on the insight that even with many class the task of finding the majority class among the k nearest neighbor of a query need not require u to explicitly find those k nearest neighbor this insight wa previously used in liu et al in two algorithm called kns and kns which dealt with fast classification in the case of two class in this paper we show how a different approach ioc standing for the international olympic committee can apply to the case of n class where n ioc assumes a slightly different processing of the datapoints in the neighborhood of the query this allows it to search a set of metric tree one for each class during the search it is possible to quickly prune away class that cannot possibly be the majority we give experimental result on datasets of up to x record and x attribute frequently showing an order of magnitude acceleration compared with each of i conventional linear scan ii a well known independent sr tree implementation of conventional k nn and iii a highly optimized conventional k nn metric tree search 
the correction of bias in magnetic resonance image is an important problem in medical image processing most previous approach have used a maximum likelihood method to increase the likelihood of the pixel in a single image by adaptively estimating a correction to the unknown image bias field the pixel likelihood are defined either in t erms of a pre existing tissue model or non parametrically in term of the image s own pixel value in both case the specific location of a pix el in the image is not used to calculate the likelihood we suggest a new approach in which we simultaneously eliminate the bias from a set of image of the same anatomy but from different patient we use the statistic from the same location across different image rather than within an image to eliminate bias field from all of the image simultaneously the method build a multi resolution non parametric tissue model conditioned on image location while eliminating the bias field associated with the original image set we present experiment on both synthetic and real mr data set and present comparison with other method 
kernel conditional random field kcrfs are introduced a a framework for discriminative modeling of graph structured data a representer theorem for conditional graphical model is given which show how kernel conditional random field arise from risk minimization procedure defined using mercer kernel on labeled graph a procedure for greedily selecting clique in the dual representation is then proposed which allows sparse representation by incorporating kernel and implicit feature space into conditional graphical model the framework enables semi supervised learning algorithm for structured data through the use of graph kernel the framework and clique selection method are demonstrated in synthetic data experiment and are also applied to the problem of protein secondary structure prediction 
approach to increase training example to hopefully improve classification effectiveness are proposed in this work the approach were verified by use of two chinese collection classified by two top performing classifier 
we describe a language called abet that allows rapid conversion of on line human readable bilingual dictionary to machine readable form 
we address the problem of learning distance metric using side information in the form of group of similar point we propose to use the rca algorithm which is a simple and efficient algorithm for learning a full ranked mahalanobis metric shental et al we first show that rca obtains the solution to an interesting optimization problem founded on an information theoretic basis if the mahalanobis matrix is allowed to be singular we show that fisher s linear discriminant followed by rca is the optimal dimensionality reduction algorithm under the same criterion we then show how this optimization problem is related to the criterion optimized by another recent algorithm for metric learning xing et al which us the same kind of side information we empirically demonstrate that learning a distance metric using the rca algorithm significantly improves clustering performance similarly to the alternative algorithm since the rca algorithm is much more efficient and cost effective than the alternative a it only us closed form expression of the data it seems like a preferable choice for the learning of full rank mahalanobis distance 
eigenvoice speaker adaptation ha been shown effective when only a small amount of adaptation data is available at the heart of the method is principal component analysis pca employed to find the most important eigenvoices in this paper we postulate that nonlinear pca in particular kernel pca may be even more effective one major challenge is on how to map the feature space eigenvoices back to the observation space so that the state observation likelihood during estimation of eigenvoice weight and subsequent decoding can be computed our solution is to compute kernel pca using composite kernel and we will call our new method kernel eigenvoice on the tidigits corpus we found that compared with a speaker independent model our kernel eigenvoice adaptation method can reduce the word error rate by while the conventional eigenvoice approach can only match the performance of the speaker independent model 
we propose a method for constructing a video sequence of high space time resolution by combining information from multiple low resolution video sequence of the same dynamic scene super resolution is performed simultaneously in time and in space by temporal super resolution we mean recovering rapid dynamic event that occur faster than regular frame rate such dynamic event are not visible or else observed incorrectly in any of the input sequence even if these are played in slow motion the spatial and temporal dimension are very different in nature yet are inter related this lead to interesting visual tradeoff in time and space and to new video application these include i treatment of spatial artifact e g motion blur by increasing the temporal resolution and ii combination of input sequence of different space time resolution e g ntsc pal and even high quality still image to generate a high quality video sequence 
we introduce a new perceptron based discriminative learning algorithm for labeling structured data such a sequence tree and graph since it is fully kernelized and us pointwise label prediction large feature including arbitrary number of hidden variable can be incorporated with polynomial time complexity this is in contrast to existing labelers that can handle only feature of a small number of hidden variable such a maximum entropy markov model and conditional random field we also introduce several kernel function for labeling sequence tree and graph and efficient algorithm for them 
discriminative method have shown significant improvement over traditional generative method in many machine learning application but there ha been difficulty in extending them to natural language parsing one problem is that much of the work on discriminative method conflates change to the learning method with change to the parameterization of the problem we show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model we present three method for training a neural network to estimate the probability for a statistical parser one generative one discriminative and one where the probability model is generative but the training criterion is discriminative the latter model outperforms the previous two achieving state of the art level of performance f measure on constituent 
we present a domain independent topic segmentation algorithm for multi party speech our feature based algorithm combine knowledge about content using a text based algorithm a a feature and about form using linguistic and acoustic cue about topic shift extracted from speech this segmentation algorithm us automatically induced decision rule to combine the different feature the embedded text based algorithm build on lexical cohesion and ha performance comparable to state of the art algorithm based on lexical information a significant error reduction is obtained by combining the two knowledge source 
this paper considers the use of computational stylistics for performing authorship attribution of electronic message addressing categorization problem with a many a different class author effective stylistic characterization of text is potentially useful for a variety of task a language style contains cue regarding the authorship purpose and mood of the text all of which would be useful adjunct to information retrieval or knowledge management task we focus here on the problem of determining the author of an anonymous message based only on the message text several multiclass variant of the winnow algorithm were applied to a vector representation of the message text to learn model for discriminating different author we present result comparing the classification accuracy of the different approach the result show that stylistic model can be accurately learned to determine an author s identity 
we present a new technique for achieving source separation when given only a single channel recording the main idea is based on exploiting the inherent time structure of sound source by learning a priori set of basis filter in time domain that encode the source in a statistically efficient manner we derive a learning algorithm using a maximum likelihood approach given the observed single channel data and set of basis filter for each time point we infer the source signal and their contribution factor this inference is possible due to the prior knowledge of the basis filter and the associated coefficient density a fle xible model for density estimation allows accurate modeling of the observation and our experimental result exhibit a high level of separation performance for mixture of two music signal a well a the separation of two voice signal 
symmetry breaking in csps ha attracted considerable attention in recent year various general scheme have been proposed to eliminate symmetry during search in general these scheme may take exponential space or time to eliminate all symmetry this paper study class of csps for which symmetry breaking is tractable it identifies several csp class which feature various form of value interchangeability and show that symmetry breaking can be performed in constant time and space during search using dedicated search procedure experimental result also show the benefit of symmetry breaking on these csps which encompass many practical application 
for the manual semantic markup of document to become wide spread usersmust be able to express annotation that conform to ontology orschemas that have shared meaning however a typical user is unlikelyto be familiar with the detail of the term a defined by the ontology author in addition the idea to be expressed may not fit perfectly within a pre defined ontology the ideal tool should help user find apartial formalization that closely follows the ontology where possiblebut deviate from the formal representation where needed we describe animplemented approach to help user create semi structured semantic annotation for a document according to an extensible owl ontology in our approach user enter a short sentence in free text to describe allor part of a document and the system present a set of potential paraphrase of the sentence that are generated from valid expression inthe ontology from which the user chooses the closest match we use a combination of off the shelf parsing tool and breadth first search of expression in the ontology to help user create valid annotation starting from free text the user can also define new term to augmentthe ontology so the potential match can improve over time 
we present a method for shape reconstruction from severalimages of a moving object the reconstruction is dense up to image resolution the method assumes that themotion is known e g by tracking a small number of featurepoints on the object the object is assumed lambertian completely matte light source should not be veryclose to the object but otherwise arbitrary and no knowledgeof lighting condition is required an object changesits appearance significantly when it change it orientationrelative to light source causing violation of the commonbrightness constancy assumption while a lot of effort isdevoted to deal with this violation we demonstrate howto exploit it to recover d structure from d image wepropose a new correspondence measure that enables pointmatching across view of a moving object the method hasbeen tested both on computer simulated example and on areal object 
using visualization technique to explore and understand high dimensional data is an efficient way to combine human intelligence with the immense brute force computation power available nowadays several visualization technique have been developed to study the cluster structure of data i e the existence of distinctive group in the data and how these cluster are related to each other however only few of these technique lend themselves to studying how this structure change if the feature describing the data are changed understanding this relationship between the feature and the cluster structure mean understanding the feature themselves and is thus a useful tool in the feature extraction phase in this paper we present a novel approach to visualizing how modification of the feature with respect to weighting or normalization change the cluster structure we demonstrate the application of our approach in two music related data mining project 
a new approach to automatically extract the main feature in colorfundus image are proposed in this paper optic disk is localizedby the principal component analysis pca and it shape is detectedby a modified active shape model asm exudate are extracted bythe combined region growing and edge detection a fundus coordinatesystem is further set up based on the fovea localization to providea better description of the feature in fundus image the successrates achieved are and for disk localization diskboundary detection and fovea localization respectively thesensitivity and specificity for exudate detection are and the success of the proposed algorithm can be attributed to theutilization of the model based method 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
the understanding of semantic web document is built upon ontology that define concept and relationship of data hence the correctness of ontology is vital ontology reasoner such a racer and fact have been developed to reason ontology with a high degree of automation however complex ontology related property may not be expressible within the current web ontology language consequently they may not be checkable by racer and fact we propose to use the software engineering technique and tool i e z eve and alloy analyzer to complement the ontology tool for checking semantic web document in this approach z eve is first applied to remove trivial syntax and type error of the ontology next racer is used to identify any ontological inconsistency whose origin can be traced by alloy analyzer finally z eve is used again to express complex ontology related property and reveal error beyond the modeling capability of the current web ontology language we have successfully applied this approach to checking a set of military plan ontology 
this article proposes a technique for correcting chinese ocr error to support retrieval of scanned document the technique us a completely automatic technique no manually constructed lexicon or confusion resource to identify both keywords and confusable term improved retrieval effectiveness on a single term query experiment is demonstrated 
this paper present a new scenario recognition algorithm for video interpretation we represent a scenario model by specifying the character involved in the scenario the sub scenario composing the scenario and the constraint combining the sub scenario various type of constraint can be used including spatio temporal and logical constraint in this paper we focus on the performance of the recognition algorithm our goal is to propose an efficient algorithm for processing temporal constraint and combining several actor defined within the scenario by efficient we mean that the recognition process is linear in function of the number of sub scenario and in most of the case in function of the number of character to validate this algorithm in term of correctness robustness and processing time in function of scenario and scene property e g number of person in the scene we have tested the algorithm on several video of a bank branch and of an office in on line and off line mode and on simulated data we conclude by comparing our algorithm with the state of the art and showing how the definition of scenario model can influence the result of the real time scenario recognition 
more and more resource are becoming available on the web and there is a growing need for infrastructure that based on advertised description are able to semantically match demand with supply we formalize general property a matchmaker should have then we present a matchmaking facilitator compliant with desired property the system embeds a neoclassic reasoner whose structural subsumption algorithm ha been modified to allow match categorization into potential and partial and ranking of match within category experiment carried out show the good correspondence between user and system ranking 
in this work we present two new method for approximating thekullback liebler kl divergence between two mixture of gaussians the first method is based on matching between the gaussian elementsof the two gaussian mixture density the second method is basedon the unscented transform the proposed method are utilized forimage retrieval task continuous probabilistic image modelingbased on mixture of gaussians together with kl measure for imagesimilarity can be used for image retrieval task with remarkableperformance the efficiency and the performance of the klapproximation method proposed are demonstrated on both simulateddata and real image datasets the experimental result indicatethat our proposed approximation outperform previously suggestedmethods 
we propose a face difference model that decomposesface difference into three component intrinsicdifference transformation difference and noise usingthe face difference model and a detailed subspace analysison the three component we develop a unified frameworkfor subspace analysis using this framework we discoverthe inherent relationship among different subspacemethods and their unique contribution to the extractionof discriminating information from the face difference this eventually lead to the construction of a dparameter space that us three subspace dimension asaxis within this parameter space we develop a unifiedsubspace analysis method that achieves better recognitionperformance than the standard subspace method on over face image from the feret database 
the search for finite state controller for partially observable markov decision process pomdps is often based on approach like gradient ascent attractive because of their relatively low computational cost in this paper we illustrate a basic problem with gradient based method applied to pomdps where the sequential nature of the decision problem is at issue and propose a new stochastic local search method a an alternative the heuristic used in our procedure mimic the sequential reasoning inherent in optimal dynamic programming dp approach we show that our algorithm consistently find higher quality controller than gradient ascent and is competitive with and for some problem superior to other state of the art controller and dp based algorithm on large scale pomdps 
we describe a neuromorphic chip that utilizes transistor heterogeneity introduced by the fabrication process to generate orientation map similar to those imaged in vivo our model consists of a recurrent network of excitatory and inhibitory cell in parallel with a push pull stage similar to a previous model the recurrent network display hotspot of activity that give rise to visual feature map unlike previous work however the map for orientation doe not depend on the sign of contrast instead signindependent cell driven by both on and off channel anchor the map while push pull interaction give rise to sign preserving cell these two group of orientation selective cell are similar to complex and simple cell observed in v orientation m aps neuron in visual area and v and v are selectively tuned for a number of visual feature the most pronounced feature being orientation orientation preference of individual cell varies across the two dimensional surface of the cortex in a stereotyped manner a revealed by electrophysiology and optical imaging study the origin of these preferred orientation po map is debated but experiment demonstrate that they exist in the absence of visual experience to the dismay of advocate of hebbian learning these result suggest that the initial appearance of po map rely on neural mechanism oblivious to input correlation here we propose a model that account for observed po map based on innate noise in neuron threshold and synaptic current the network is implemented in silicon where heterogeneity is a ubiquitous a it is in biology 
in this paper we present an incremental transformation framework called incxslt this framework ha been experimented for the xslt language defined at the world wide web consortium for the currently available tool designing the xml content and the transformation sheet is an inefficient a tedious and an error prone experience incremental transformation processor such a incxslt represent a better alternative to help in the design of both the content and the transformation sheet we believe that such framework are a first step toward fully interactive transformation based authoring environment 
when processing raw document in information retrieval ir system a term weighting scheme is used to calculate the importance of each term which occurs in a document however most term weighting scheme assume that a term is independent of the other term term dependency is an indispensable consequence of language use therefore this assumption can make the information of a document being lost in this paper we propose new approach to refine term weight of document using term dependency discovered from a set of document then we evaluate our method with two experiment based on the vector space model and the language model 
a fundamental problem in text data mining is to extract meaningful structure from document stream that arrive continuously over time e mail and news article are two natural example of such stream each characterized by topic that appear grow in intensity for a period of time and then fade away the published literature in a particular research field can be seen to exhibit similar phenomenon over a much longer time scale underlying much of the text mining work in this area is the following intuitive premise that the appearance of a topic in a document stream is signaled by a burst of activity with certain feature rising sharply in frequency a the topic emerges 
high dimensional data that lie on or near a low dimensional manifold can be described by a collection of local linear model such a descri ption however doe not provide a global parameterization of the manifold arguably an important goal of unsupervised learning in this paper we show how to learn a collection of local linear model that solves this more difficult proble m our local linear model are represented by a mixture of factor analyzer and the global coordination of these model is achieved by adding a regularizing term to the standard maximum likelihood objective function the regularizer break a degeneracy in the mixture model s parameter space favoring model who se internal coordinate system are aligned in a consistent way a a result t he internal coordinate change smoothly and continuously a one traverse a connected path on the manifold even when the path cross the domain of many different local model the regularizer take the form of a kullback leibler divergence and illustrates an unexpected application of variational meth od not to perform approximate inference in intractable probabilistic model but to learn more useful internal representation in tractable one 
we present cameo the camera assisted meeting event observer which is a physical awareness system designed for use by an agent based electronic assistant cameo is used to observe formal meeting environment and infer the activity of people attending them 
we abstract out the core search problem of active learning scheme to better understand the extent to which adaptive labeling can improve sample complexity we give various upper and lower bound on the number of label which need to be queried and we prove that a popular greedy active learning rule is approximately a good a any other strategy for minimizing this number of label 
this paper present an approach to bilingual lexicon extraction from comparable corpus and evaluation on cross language information retrieval we explore a bi directional extraction of bilingual terminology primarily from comparable corpus a combined statistic based and linguistics based model to select best translation candidate to phrasal translation is proposed evaluation using a large test collection for japanese english revealed the proposed combination of bi directional comparable corpus bilingual dictionary and transliteration augmented with linguistics based pruning to be highly effective in cross language information retrieval 
the goal of collaborative filtering is to make recommendation for a test user by utilizing the rating information of user who share interest similar to the test user because rating are determined not only by user interest but also the rating habit of user it is important to normalize rating of different user to the same scale in this paper we compare two different normalization strategy for user rating namely the gaussian normalization method and the decoupling normalization method particularly we incorporated these two rating normalization method into two collaborative filtering algorithm and evaluated their effectiveness on the eachmovie dataset the experiment result have shown that the decoupling method for rating normalization is more effective than the gaussian normalization method in improving the performance of collaborative filtering algorithm 
the nip workshop included a feature selection competition organized by the author we provided participant with five datasets from dierent application domain and called for classification result using a minimal number of feature the competition took place over a period of week and attracted research group participant were asked to make on line submission on the validation and test set with performance on the validation set being presented immediately to the participant and performance on the test set presented to the participant at the workshop in total entry were made on the validation set during the development period and entry on all test set for the final competition the winner used a combination of bayesian neural network with ard prior and dirichlet diusion tree other top entry used a variety of method for feature selection which combined filter and or wrapper or embedded method using random forest kernel method or neural network a a classification engine the result of the benchmark including the prediction made by the participant and the feature they selected and the scoring software are publicly available the benchmark is available at www nipsfsc ec soton ac uk for post challenge submission to stimulate further research 
search engine need to evaluate query extremely fast a challenging task given the vast quantity of data being indexed a significant proportion of the query posed to search engine involve phrase in this paper we consider how phrase query can be efficiently supported with low disk overhead previous research ha shown that phrase query can be rapidly evaluated using nextword index but these index are twice a large a conventional inverted file we propose a combination of nextword index with inverted file a a solution to this problem our experiment show that combined use of an auxiliary nextword index and a conventional inverted file allow evaluation of phrase query in half the time required to evaluate such query with an inverted file alone and the space overhead is only of the size of the inverted file further time saving are available with only slight increase in disk requirement 
common sense knowledge can be efficiently collected from non expert over the web in a similar fashion to the open mind family of distributed knowledge capture project we describe the collection of common sense data through the open mind indoor common sense omics website we restrict the domain to indoor home and office environment to obtain dense knowledge the knowledge wa collected through sentence template that were generated dynamically based on previous user input entry were converted into relation and saved into a database we discus the result of this online collaborative effort and describe two application of the collected data to indoor mobile robot we discus active desire selection based on current belief and command and a room labeling application based on probability estimate from the common sense knowledge base 
we examine the utility of speech and lexical feature for predicting student emotion in computer human spoken tutoring dialogue we first annotate student turn for negative neutral positive and mixed emotion we then extract acoustic prosodic feature from the speech signal and lexical item from the transcribed or recognized speech we compare the result of machine learning experiment using these feature alone or in combination to predict various categorization of the annotated student emotion our best result yield a relative improvement in error reduction over a baseline finally we compare our result with emotion prediction in human human tutoring dialogue 
the web contains a wealth of product review but sifting through them is a daunting task ideally an opinion mining tool would process a set of search result for a given item generating a list of product attribute quality feature etc and aggregating opinion about each of them poor mixed good we begin by identifying the unique property of this problem and develop a method for automatically distinguishing between positive and negative review our classifier draw on information retrieval technique for feature extraction and scoring and the result for various metric and heuristic vary depending on the testing situation the best method work a well a or better than traditional machine learning when operating on individual sentence collected from web search performance is limited due to noise and ambiguity but in the context of a complete web based tool and aided by a simple method for grouping sentence into attribute the result are qualitatively quite useful 
high dimensional data pose a severe challenge for data mining feature selection is a frequently used technique in pre processing high dimensional data for successful data mining traditionally feature selection is focused on removing irrelevant feature however for high dimensional data removing redundant feature is equally critical in this paper we provide a study of feature redundancy in high dimensional data and propose a novel correlation based approach to feature selection within the filter model the extensive empirical study using real world data show that the proposed approach is efficient and effective in removing redundant and irrelevant feature 
we propose probabilistic generative model called parametric mixture model pmms for multiclass multi labeled text categorization problem conventionally the binary classication approach ha been employed in which whether or not text belongs to a category is judged by the binary classier for every category in contrast our approach can simultaneously detect multiple category of text using pmms we derive ecien t learning and prediction algorithm for pmms we also empirically show that our method could signican tly outperform the conventional binary method when applied to multi labeled text categorization using real world wide web page 
delivering web page to mobile phone or personal digital assistant ha become possible with the latest wireless technology however mobile device have very small screen size and memory capacity converting web page for delivery to a mobile device is an exciting new problem in this paper we propose to use a ranking algorithm similar to google s pagerank algorithm to rank the content object within a web page this allows the extraction of only important part of web page for delivery to mobile device experiment show that the new method is effective in experiment on page from randomly selected website the system needed to extract and deliver only of the object in a web page in order to provide of a viewer s desired viewing content this provides significant saving in the wireless traffic and downloading time while providing a satisfactory reading experience on the mobile device 
we study the synthesis of neural coding selective attention and perceptual decision making a hierarchical neural architecture is proposed which implement bayesian integration of noisy sensory input and topdown attentional prior leading to sound perceptual discrimination the model offer an explicit explanation for the experimentally observed modulation that prior information in one stimulus feature location can have on an independent feature orientation the network s intermediate level of representation instantiate known physiological property of visual cortical neuron the model also illustrates a possible reconciliation of cortical and neuromodulatory representation of uncertainty 
cluster analysis is a fundamental problem and technique in many area related to machine learning in this paper we consider rearrangement clustering which is the problem of finding set of object that share common or similar feature by arranging the row object of a matrix specifying object feature in such a way that adjacent object are similar to each other based on a similarity measure of the feature so a to maximize the overall similarity based on formulating this problem a the traveling salesman problem tsp we develop a new tsp based optimal clustering algorithm called tspcluster we overcome a flaw that is inherent in previous approach by relaxing restriction on dissimilarity between cluster our new algorithm ha three important feature finding the optimal k cluster for a given k automatically detecting cluster border and ascertaining a set of most viable clustering result that make good balance among maximizing the overall similarity within cluster and dissimilarity between cluster we apply tspcluster to cluster and display gene of flowering plant arabidopsis which are regulated under various abiotic stress condition we compare tspcluster to the bond energy algorithm and two existing clustering algorithm our tspcluster code is available at climer zhang 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
today most large company maintain virtual private network vpns to connect their remote location into a single secure network vpns can be quite large covering more than location and in most case use standard internet protocol and service such vpns are implemented using a diverse set of technology such a frame relay mpls or ipsec to achieve the goal of privacy and performance isolation from the public internet using vpns to distribute live content ha recently received tremendous interest for example a vpn could be used to broadcast a ceo employee town hall meeting to distribute this type of content economically without overloading the network the deployment of streaming cache or splitter is most likely required in this paper we address the problem of optimally placing such streaming splitter or cache to broadcast to a given set of vpn endpoint under the constraint typically found within a vpn in particular we introduce an efficient algorithm with complexity o v v being the number of router in the vpn this guarantee the optimal cache placement if interception is used for redirection we prove that the general problem is np hard and introduce multiple heuristic for efficient and robust cache placement suitable under different constraint at the expense of increased implementation complexity each heuristic solution provides additional saving in the number of cache required we evaluate proposed solution using extensive simulation in particular we show our flow based solution is very close to the optimal 
we present technique for improving the speed of robust motion estimation based on random sampling of image feature starting from torr and zisserman s mlesac algorithm we address some of the problem posed from both practical and theoretical standpoint and in doing so allow the random search to be replaced by a guided search guidance of the search is based on readily available information which is usually discarded but can significantly reduce the search time this guided sampling algorithm is further specialised for tracking of multiple motion for which result are presented 
a more and more activity are carried out using computer and computer network the amount of potentially sensitive data stored by business government and other party increase different party may wish to benefit from cooperative use of their data but privacy regulation and other privacy concern may prevent the party from sharing their data privacy preserving data mining provides a solution by creating distributed data mining algorithm in which the underlying data is not revealed in this paper we present a privacy preserving protocol for a particular data mining task learning the bayesian network structure for distributed heterogeneous data in this setting two party owning confidential database wish to learn the structure of bayesian network on the combination of their database without revealing anything about their data to each other we give an efficient and privacy preserving version of the k algorithm to construct the structure of a bayesian network for the party joint data 
the beltrami flow is one of the most effective denoising algorithm in image processing for gray level image we show that the beltrami flow equation can be arranged in a reaction diffusion form this reveals the edge enhancing property of the equation and suggests the application of additive operator split aos method for faster convergence a we show with numerical simulation the aos method result in an unconditionally stable semi implicit linearized difference scheme in d and d the value of the edge indicator function are used from the previous step in scale while the pixel value of the next step are used to approximate the flow the optimum ratio between the reaction and diffusion counterpart of the governing pde is studied in order to achieve a better quality of segmentation the computational time decrease by a factor of ten a compared to the explicit scheme for d color image the beltrami flow equation are coupled and do not yield readily to the aos technique however in the proximity of an edge the cross product of color gradient nearly vanish and the coupling becomes weak the principal direction of the edge indicator matrix are normal and tangent to the edge replacing the action of the matrix on the gradient vector by an action of it eigenvalue we reduce the color problemto the gray level case with a reasonable accuracy the scalar edge indicator function for the color case becomes essentially the same a that for the gray level image and the fast implicit technique is implemented 
principal component analysis ha proven to be useful for understanding geometric variability in population of parameterized object the statistical framework is well understood when the parameter of the object are element of a euclidean vector space this is certainly the case when the object are described via landmark or a a dense collection of boundary point we have been developing representation of geometry based on the medial axis description or m rep although this description ha proven to be effective the medial parameter are not naturally element of a euclidean space in this paper we show that medial description are in fact element of a lie group we develop methodology based on lie group for the statistical analysis of medially defined anatomical object 
traditional concatenative speech synthesis system use a number of heuristic to define the target and concatenation cost essential for the design of the unit selection component in contrast to these approach we introduce a general statistical modeling framework for unit selection inspired by automatic speech recognition given appropriate data technique based on that framework can result in a more accurate unit selection thereby improving the general quality of a speech synthesizer they can also lead to a more modular and a substantially more efficient system we present a new unit selection system based on statistical modeling to overcome the original absence of data we use an existing high quality unit selection system to generate a corpus of unit sequence we show that the concatenation cost can be accurately estimated from this corpus using a statistical n gram language model over unit we used weighted automaton and transducer for the representation of the component of the system and designed a new and more efficient composition algorithm making use of string potential for their combination the resulting statistical unit selection is shown to be about time faster than the last release of the at t natural voice product while preserving the same quality and offer much flexibility for the use and integration of new and more complex component 
abstract in this paper we identify the main requisite ofan infrastructure for the integrated management ofprofile data and propose a high level description ofits implementation the main goal of our systemis to offer to mobile user content targeted to theirneeds using a presentation suited to their device 
in this paper we pose a novel research problem for machine learning that involves constructing a process model from continuous data we claim that casting learned knowledge in term of process with associated equation is desirable for scienti c and engineering domain where such notation are commonly used we also argue that existing induction method are not well suited to this task although some technique hold a partial solution in response we describe an approach to learning 
this paper present a search architecture that combine classical search technique with spread activation technique applied to a semantic model of a given domain given an ontology weight are assigned to link based on certain property of the ontology so that they measure the strength of the relation spread activation technique are used to find related concept in the ontology given an initial set of concept and corresponding initial activation value these initial value are obtained from the result of classical search applied to the data associated with the concept in the ontology two test case were implemented with very positive result it wa also observed that the proposed hybrid spread activation combining the symbolic and the sub symbolic approach achieved better result when compared to each of the approach alone 
inside information come in many form knowledge of a corporate takeover a terrorist attack unexpectedly poor earnings the fda s acceptance of a new drug etc anyone who know some piece of soon to break news posse inside information historically insider trading ha been detected after the news is public but this is often too late fraud ha been perpetrated innocent investor have been disadvantaged or terrorist act have been carried out this paper explores early detection of insider trading detection before the news break data mining hold great promise for this emerging application but the problem also pose significant challenge we present the specific problem of insider trading in option market compare decision tree logistic regression and neural net result to result from an expert model and discus insight that knowledge discovery technique shed upon this problem 
this paper address the problem of calibrating camera lensdistortion which can be significant in medium to wide anglelenses while almost all existing nonmetric distortion calibrationmethods need user involvement in one form or another we present anautomatic approach based on the robust the least median of square lmeds estimator our approach is thus le sensitive to erroneousinput data such a image curve that are mistakenly considered asprojections of d linear segment our approach uniquely us fast closed form solution to the distortion coefficient which serveas an initial point for a non linear optimization algorithm tostraighten imaged line moreover we propose a method fordistortion model selection based on geometrical inference successful experiment to evaluate the performance of this approachon synthetic and real data are reported 
the focus of research in text classification ha expanded from simple topic identification to more challenging task such a opinion modality identification unfortunately the latter goal exceed the ability of the traditional bag of word representation approach and a richer more structural representation is required accordingly learning algorithm must be created that can handle the structure observed in text in this paper we propose a boosting algorithm that capture sub structure embedded in text the proposal consists of i decision stump that use subtrees a feature and ii the boosting algorithm which employ the subtree based decision stump a weak learner we also discus the relation between our algorithm and svms with tree kernel two experiment on opinion modality classification confirm that subtree feature are important 
the problem of deriving joint policy for a group of agent that maximize some joint reward function can be modeled a a decentralized partially observable markov decision process pomdp yet despite the growing importance and application of decentralized pomdp model in the multiagents arena few algorithm have been developed for efficiently deriving joint policy for these model this paper present a new class of locally optimal algorithm called joint equilibrium based search for policy jesp we first describe an exhaustive version of jesp and subsequently a novel dynamic programming approach to jesp our complexity analysis reveals the potential for exponential speedup due to the dynamic programming approach these theoretical result are verified via empirical comparison of the two jesp version with each other and with a globally optimal brute force search algorithm finally we prove piece wise linear and convexity pwlc property thus taking step towards developing algorithm for continuous belief state 
the paper explores a very simple agent design method called q decomposition wherein a complex agent is built from simpler subagents each subagent ha it own reward function and run it own reinforcement learning process it supply to a central arbitrator the q value according to it own reward function for each possible action the arbitrator selects an action maximizing the sum of q value from all the subagents this approach ha advantage over design in which subagents recommend action it also ha the property that if each subagent run the sarsa reinforcement learning algorithm to learn it local q function then a globally optimal policy is achieved on the other hand local q learning lead to globally suboptimal behavior in some case this form of agent decomposition allows the local q function to be expressed by muchreduced state and action space these result are illustrated in two domain that require effective coordination of behavior 
recent biological experimental finding have shown that synaptic plasticity depends on the relative timing of the preand postsynaptic spike this determines whether long term potentiation ltp or long term depression ltd is induced this synaptic plasticity ha been called temporally asymmetric hebbian plasticity tah many author have numerically demonstrated that neural network are capable of storing spatiotemporal pattern however the mathematical mechanism of the storage of spatiotemporal pattern is still unknown and the effect of ltd is particularly unknown in this article we employ a simple neural network model and show that interference between ltp and ltd disappears in a sparse coding scheme on the other hand the covariance learning rule is known to be indispensable for the storage of sparse pattern we also show that tah ha the same qualitative effect a the covariance rule when spatiotemporal pattern are embedded in the network 
evidence from psychology suggests that human process definite description that refer to object present in a visual scene incrementally upon hearing them rather than constructing explicit parse tree after the whole sentence wa said which are then used to determine the referent in this paper we describe a real time distributed robotic architecture for human reference resolution that demonstrates various interaction of auditory visual and semantic processing component hypothesized to underlie human process 
this paper describes an incremental parsing approach where parameter are estimated using a variant of the perceptron algorithm a beam search algorithm is used during both training and decoding phase of the method the perceptron approach wa implemented with the same feature set a that of an existing generative model roark a and experimental result show that it give competitive performance to the generative model on parsing the penn treebank we demonstrate that training a perceptron model to combine with the generative model during search provides a percent f measure improvement over the generative model alone to percent 
handling massive datasets is a difficult problem not only due to prohibitively large number of entry but in some case also due to the very high dimensionality of the data often severe feature selection is performed to limit the number of attribute to a manageable size which unfortunately can lead to a loss of useful information feature space reduction may well be necessary for many stand alone classifier but recent advance in the area of ensemble classifier technique indicate that overall accurate classifier aggregate can be learned even if each individual classifier operates on incomplete feature view training data i e such where certain input attribute are excluded in fact by using only small random subset of feature to build individual component classifier surprisingly accurate and robust model can be created in this work we demonstrate how these type of architecture effectively reduce the feature space for submodels and group of sub model which lends itself to efficient sequential and or parallel implementation experiment with a randomized version of adaboost are used to support our argument using the text classification task a an example 
structural information such a layout and look and feel ha been extensively used in the literatuce for extraction of interesting or relevant data efficient storage and query optimization traditionally tree model such a dom tree have been used to represent structural information especially in the case of html and xml document however computation of structural similarity between document based on the tree model is computationally expensive in this paper we propose an alternative scheme for representing the structural information of document based on the path contained in the corresponding tree model since the model includes partial information about parent child and sibling it allows u to define a new family of meaningful and at the same time computationally simple structural similarity measure our experimental result based on the sigmod xml data set a well a html document collection from ibm com dell com and amazon com show that the representation is powerful enough to produce good cluster of structurally similar page 
the paper study two type of event that often overload web site to a point when their service are degraded or disrupted entirely flash event fe and denial of service attack do the former are created by legitimate request and the latter contain malicious request whose goal is to subvert the normal operation of the site we study the property of both type of event with a special attention to characteristic that distinguish the two identifying these characteristic allows a formulation of a strategy for web site to quickly discard malicious request we also show that some content distribution network cdns may not provide the desired level of protection to web site against flash event we therefore propose an enhancement to cdns that offer better protection and use trace driven simulation to study the effect of our enhancement on cdns and web site 
ensemble learning scheme such a adaboost and bagging enhance the performance of a single classifier by combining prediction from multiple classifier of the same type the prediction from an ensemble of diverse classifier can be combined in related way e g by voting or simply by selecting the best classifier via cross validation a technique widely used in machine learning however since no ensemble scheme is always the best choice a deeper insight into the structure of meaningful approach to combine prediction is needed to achieve further progress in this paper we offer an operational reformulation of common ensemble learning scheme voting selection by crossvalidation x val grading and bagging a a stacking scheme with appropriate parameter setting thus from a theoretical point of view all these scheme can be reduced to stacking with an appropriate combination method this result is an important step towards a general theoretical framework for the field of ensemble learning 
topic segmentation can be used a a preprocessing step in numerous natural language processing application in this short paper we will discus how we adapted our segmentation algorithm for automatic summarization 
eigenvoice speaker adaptation ha been shown to be effective when only a small amount of adaptation data is available at the heart of the method is principal component analysis pca employed to find the most important eigenvoices in this paper we postulate that nonlinear pca in particular kernel pca may be even more effective one major challenge is to map the feature space eigenvoices back to the observation space so that the state observation likelihood can be computed during the estimation of eigenvoice weight and subsequent decoding our solution is to compute kernel pca using composite kernel and we will call our new method kernel eigenvoice speaker adaptation on the tidigits corpus we found that compared with a speaker independent model our kernel eigenvoice adaptation method can reduce the word error rate by while the standard eigenvoice approach can only match the performance of the speaker independent model 
multilingual application frequently involve dealing with proper name but name are often missing in bilingual lexicon this problem is exacerbated for application involving translation between latin scripted language and asian language such a chinese japanese and korean cjk where simple string copying is not a solution we present a novel approach for generating the ideographic representation of a cjk name written in a latin script the proposed approach involves first identifying the origin of the name and then back transliterating the name to all possible chinese character using language specific mapping to reduce the massive number of possibility for computation we apply a three tier filtering process by filtering first through a set of attested bigram then through a set of attested term and lastly through the www for a final validation we illustrate the approach with english to japanese back transliteration against test set of japanese given name and surname we have achieved average precision of and respectively 
theoretical and experimental analysis of bagging indicate that it is primarily a variance reduction technique this suggests that bagging should be applied to learning algorithm tuned to minimize bias even at the cost of some increase in variance we test this idea with support vector machine svms by employing out of bag estimate of bias and variance to tune the svms experiment indicate that bagging of low bias svms the lobag algorithm never hurt generalization performance and often improves it compared with well tuned single svms and to bag of individually well tuned svms 
we present a question answering qa system which learns how to detect and rank answer passage by analyzing question and their answer qa pair provided a training data we built our system in only a few person month using off the shelf component a part of speech tagger a shallow parser a lexical network and a few well known supervised learning algorithm in contrast many of the top trec qa system are large group effort using customized ontology question classifier and highly tuned ranking function our ease of deployment arises from using generic trainable algorithm that exploit simple feature extractor on qa pair with trec qa data our system achieves mean reciprocal rank mrr that compare favorably with the best score in recent year and generalizes from one corpus to another our key technique is to recover from the question fragment of what might have been posed a a structured query had a suitable schema been available comprises selector token that are likely to appear almost unchanged in an answer passage the other fragment contains question token which give clue about the answer type and are expected to be replaced in the answer passage by token which specialize or instantiate the desired answer type selector are like constant in where clause in relational query and answer type are like column name we present new algorithm for locating selector and answer type clue and using them in scoring passage with respect to a question 
this paper introduces a framework for modeling and specifying the global behavior of e service composition under this framework peer individual e service communicate through asynchronous message and each peer maintains a queue for incoming message a global watcher keep track of message a they occur we propose and study a central notion of a conversation which is a sequence of class of message observed by the watcher we consider the case where the peer are represented by mealy machine finite state machine with input and output the set of conversation exhibit unexpected behavior for example there exists a composite e service based on mealy peer whose set of conversation is not context free and not regular the set of conversation is always context sensitive one cause for this is the queuing of message we introduce an operator prepone that simulates queue delay from a global perspective and show that the set of conversation of each mealy e service is closed under prepone we illustrate that the global prepone fails to completely capture the queue delay effect and refine prepone to a local version on conversation seen by individual peer on the other hand mealy implementation of a composite e service will always generate conversation whose projection are consistent with individual e service we use projection join to reflect such situation however there are still mealy peer whose set of conversation is not the local prepone and projection join closure of any regular language therefore we propose conversation specification a a formalism to define the conversation allowed by an e service composition we give two technical result concerning the interplay between the local behavior of mealy peer and the global behavior of their composition one result show that for each regular language it local prepone and projection join closure corresponds to the set of conversation by some mealy peer effectively constructed from the second result give a condition on the shape of a composition which guarantee that the set of conversation that can be realized is the local prepone and projection join closure of a regular language 
we develop a family of upper and lower bound on the worst case expected kl loss for estimating a discrete distribution on a fin ite numberm of point given n i i d sample our upper bound are approximationtheoretic similar to recent bound for estimating discret e entropy the lower bound are bayesian based on average of the kl loss under dirichlet distribution the upper bound are convex in their parameter and thus can be minimized by descent method to provide estimator with low worst case error the lower bound are indexed by a one dimensional parameter and are thus easily maximized asymptotic analysis of the bound demonstrates the uniform kl consistency of a wide class of estimator a c n m no matter how slowly and show that no estimator is consistent for c bounded in contrast to entropy estimation moreover the bound are asymptotically tight a c or and are shown numerically to be tight within a factor of two for all c finally in the sparse data limit c we find that the dirichlet bayes add constant estimator with parameter scaling like clog c optimizes both the upper and lower bound suggesting an optimal choice of the add constant parameter in this regime 
multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentence in a document centrality is typically defined in term of the presence of particular important word or in term of similarity to a centroid pseudo sentence we are now considering an approach for computing sentence importance based on the concept of eigenvector centrality prestige that we call lexpagerank in this model a sentence connectivity matrix is constructed based on cosine similarity if the cosine similarity between two sentence exceeds a particular predefined threshold a corresponding edge is added to the connectivity matrix we provide an evaluation of our method on duc data the result show that our approach outperforms centroid based summarization and is quite successful compared to other summarization system 
we describe a pattern acquisition algorithm that learns in an unsupervised fashion a streamlined representation of linguistic structure from a plain natural language corpus this paper address the issue of learning structured knowledge from a large scale natural language data set and of generalization to unseen text the implemented algorithm represents sentence a path on a graph whose vertex are word or part of word significant pattern determined by recursive context sensitive statistical inference form new vertex linguistic construction are represented by tree composed of significant pattern and their associated equivalence class an input module allows the algorithm to be subjected to a standard test of english a a second language esl proficiency the result are encouraging the model attains a level of performance considered to be intermediate for th grade student despite having been trained on a corpus childes containing transcribed speech of parent directed to small child 
knowledge about local invariance with respect to given pattern transformation can greatly improve the accuracy of classification previous approach are either based on regularisation or on the generation of virtual transformed example we develop a new framework for learning linear classifier under known transformation based on semidefinite programming we present a new learning algorithm the semidefinite programming machine sdpm which is able to find a maximum margin hyperplane when the training example are polynomial trajectory instead of single point the solution is found to be sparse in dual variable and allows to identify those point on the trajectory with minimal real valued output a virtual support vector extension to segment of trajectory to more than one transformation parameter and to learning with kernel are discussed in experiment we use a taylor expansion to locally approximate rotational invariance in pixel image from usps and find improvement over known method 
we describe a three dimensional geometric hand model suitable for visual tracking application the kinematic constraint implied by the model s joint have a probabilistic structure which is well described by a graphical model inference in this model is complicated by the hand s many degree of freedom a well a multimodal likelihood caused by ambiguous image measurement we use nonparametric belief propagation nbp to develop a tracking algorithm which exploit the graph s structure to control complexity while avoiding costly dis cretization while kinematic constraint naturally have a local structur e selfocclusions created by the imaging process lead to complex interpendencies in color and edge based likelihood function however we show that local structure may be recovered by introducing binary hidden variable describing the occlusion state of each pixel we augment the nbp algorithm to infer these occlusion variable in a distribut ed fashion and then analytically marginalize over them to produce hand position estimate which properly account for occlusion event we provide simulation showing that nbp may be used to refine inaccurate model i nitializations a well a track hand motion through extended image sequence 
the implicit query iq prototype is a system which automatically generates context sensitive search based on a user s current computing activity in the demo we show iq running when user are reading or composing email query are automatically generated by analyzing the email message and result are presented in a small pane adjacent to the current window to provide peripheral awareness of related information 
the standard approach to the classification of object is to consider the example a independent and identically distributed iid in many real world setting however this assumption is not valid because a topographical relationship exists between the object in this contribution we consider the special case of image segmentation where the object are pixel and where the underlying topography is a d regular rectangular grid we introduce a classification method which not only us measured vectorial feature information but also the label configuration within a topographic neighborhood due to the resulting dependence between the label of neighboring pixel a collective classification of a set of pixel becomes necessary we propose a new method called topographic support vector machine tsvm which is based on a topographic kernel and a self consistent solution to the label assignment shown to be equivalent to a recurrent neural network the performance of the algorithm is compared to a conventional svm on a cell image segmentation task 
we describe a novel method for simultaneously detecting face and estimating their pose in real time the method employ a convolutional network to map image of face to point on a lowdimensional manifold parametrized by pose and image of non face to point far away from that manifold given an image detecting a face and estimating it pose is viewed a minimizing an energy function with respect to the face non face binary variable and the continuous pose parameter the system is trained to minimize a loss function that drive correct combination of label and pose to be associated with lower energy value than incorrect one the system is designed to handle very large range of pose without retraining the performance of the system wa tested on three standard data set for frontal view rotated face and profile is comparable to previous system that are designed to handle a single one of these data set we show that a system trained simuiltaneously for detection and pose estimation is more accurate on both task than similar system trained for each task separately 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
classification algorithm typically induce population wide model that are trained to perform well on average on expected future instance we introduce a bayesian framework for learning instance specific model from data that are optimized to predict well for a particular instance based on this framework we present a lazy instance specific algorithm called isa that performs selective model averaging over a restricted class of bayesian network on experimental evaluation this algorithm show superior performance over model selection we intend to apply such instance specific algorithm to improve the performance of patient specific predictive model induced from medical data 
xml is poised to take the world wide web to the next level of innovation xml data large or small with or without associated schema will be exchanged between increasing number of application running on diverse device efficient storage and transportation of such data is an important issue we have designed a system called millau and a series of algorithm for efficient encoding and representation of xml structure in this paper we describe some of the newer algorithm and apis in our system for compression of xml structure and data our compression algorithm in addition to separating structure and text for compression take advantage of the associated schema if available in compressing the structure we also quantify xml document and their schema with the purpose of defining a decision logic to apply the appropriate compression algorithm for a document or a set of document following a particular schema our system also defines a programming model corresponding to xml document object model and simple api for xml stream and document in compressed form our experiment have shown significant performance gain of our algorithm and apis we describe some of these result in this paper we also describe some web application based on our system 
most information extraction ie system treat separate potential extraction a independent however in many case considering influence between different potential extraction could improve overall accuracy statistical method based on undirected graphical model such a conditional random field crfs have been shown to be an effective approach to learning accurate ie system we present a new ie method that employ relational markov network a generalization of crfs which can represent arbitrary dependency between extraction this allows for collective information extraction that exploit the mutual influence between possible extraction experiment on learning to extract protein name from biomedical text demonstrate the advantage of this approach 
a fundamental problem in business and other application is ranking item with respect to some notion of profit based on historical transaction the difficulty is that the profit of one item not only come from it own sale but also from it influence on the sale of other item i e the cross selling effect in this paper we draw an analogy between this influence and the mutual reinforcement of hub authority web page based on this analogy we present a novel approach to the item ranking problem we apply this ranking approach to solve two selection problem in size constrained selection the maximum number of item that can be selected is fixed in cost constrained selection there is no maximum number of item to be selected but there is some cost associated with the selection of each item in both case the question is what item should be selected to maximize the profit empirically we show that this method find profitable item in the presence of cross selling effect 
the detection of prosodic characteristic is an important aspect of both speech synthesis and speech recognition correct placement of pitch accent aid in more natural sounding speech while automatic detection of accent can contribute to better word level recognition and better textual understanding in this paper we investigate probabilistic contextual and phonological factor that influence pitch accent placement in natural conversational speech in a sequence labeling setting we introduce conditional random field crfs to pitch accent prediction task in order to incorporate these factor efficiently in a sequence model we demonstrate the usefulness and the incremental effect of these factor in a sequence model by performing experiment on hand labeled data from the switchboard corpus our model outperforms the baseline and previous model of pitch accent prediction on the switch board corpus 
existing axis scaling and dimensionality method focus on preserving structure usually determined via the euclidean distance in other word they inherently assume that the euclidean distance is already correct we instead propose a novel nonlinear approach driven by an information theoretic viewpoint which we show is also strongly linked to intrinsic dimensionality or degree of freedom and uniformity nonlinear transformation based on common probability distribution combined with information driven selection simultaneously reduce the number of dimension required and increase the value of those we retain experiment on real data confirm that this approach reveals correlation find novel attribute and scale well 
in many reinforcement learning application the set of possible action can be partitioned by the programmer into subset of similar action this paper present a technique for exploiting this form of prior information to speed up model based reinforcement learning we call it an action renement method because it treat each subset of similar action a a single abstract action early in the learning process and then later renes the abstract action into individual action a more experience is gathered our method estimate the transition probability p s j a for an action a by combining the result of execution of action a with execution of other action in the same subset of similar action this is a form of smoothing of the probability estimate that trade increased bias for reduced variance the paper derives a formula for optimal smoothing which show that the degree of smoothing should decrease a the amount of data increase experiment show that probability smoothing is better than two simpler action renement method on a synthetic maze problem action renement is most useful in problem such a robotics where training experience are expensive 
this paper present a novel approach to sign language recognition that provides extremely high classication rate on minimal training data key to this approach is a stage classication procedure where an initial classication stage extract a high level description of hand shape and motion this high level description is based upon sign linguistics and describes action at a conceptual level easily understood by human moreover such a description broadly generalises temporal activity naturally overcoming variability of people and environment a second stage of classication is then used to model the temporal transition of individual sign using a classier bank of markov chain combined with independent component analysis we demonstrate classication rate a high a for a lexicon of word using only single instance training outperforming previous approach where thousand of training example are required 
recent algorithm like rtdp and lao combine the strength of heuristic search h and dynamic programming dp method by exploiting knowledge of the initial state and an admissible heuristic function for producing optimal policy without evaluating the entire space in this paper we introduce and analyze three new h dp algorithm a first general algorithm schema that is a simple loop in which inconsistent reachable state i e with residual greater than a given c are found and updated until no such state are found and serf to make explicit the basic idea underlying h dp algorithm leaving other commitment aside a second algorithm that build on the first and add a labeling mechanism for detecting solved state based on tarjan s strongly connected component procedure which is very competitive with existing approach and a third algorithm that approximates the latter by enforcing the consistency of the value function over the likely reachable state only and lead to great time and memory saving with no much apparent loss in quality when transition have probability that differ greatly in value 
a general classification framework called boostingchain is proposed for learning boosting cascade in thisframework a chain structure is introduced to integratehistorical knowledge into successive boosting learning moreover a linear optimization scheme is proposed toaddress the problem of redundancy in boosting learningand threshold adjusting in cascade coupling by thismeans the resulting classifier consists of fewer weakclassifiers yet achieves lower error rate than boostingcascade in both training and test experimentalcomparisons of boosting chain and boosting cascade areprovided through a face detection problem thepromising result clearly demonstrate the effectivenessmade by boosting chain 
we propose a two class classification model for grouping human segmented natural image are used a positive example negative example of grouping are constructed by randomly matching human segmentation and image in a preprocessing stage an image is over segmented into super pixel we define a variety of feature derived from the classical gestalt cue including contour texture brightness and good continuation information theoretic analysis is applied to evaluate the power of these grouping cue we train a linear classifier to combine these feature to demonstrate the power of the classification model a simple algorithm is used to randomly search for good segmentation result are shown on a wide range of image 
an advising agent a coach provides advice to other agent about how to act in this paper we contribute an advice generation method using observation of agent acting in an environment given an abstract state definition and partially specified abstract action the algorithm extract a markov chain infers a markov decision process and then solves the mdp given an arbitrary reward signal to generate advice we evaluate our work in a simulated robot soccer environment and experimental result show improved agent performance when using the advice generated from the mdp for both a sub task and the full soccer game 
a new large margin classifier named maxi min margin machine m is proposed in this paper this new classifier is constructed based on both a local and a global view of data while the most popular large margin classifier support vector machine svm and the recently proposed important model minimax probability machine mpm consider data only either locally or globally this new model is theoretically important in the sense that svm and mpm can both be considered a it special case furthermore the optimization of m can be cast a a sequential conic programming problem which can be solved efficiently we describe the m model definition provide a clear geometrical interpretation present theoretical justification propose efficient solving method and perform a series of evaluation on both synthetic data set and real world benchmark data set it comparison with svm and mpm also demonstrates the advantage of our new model 
clarity in semantics and a rich formalization of this semantics are important requirement for ontology designed to be deployed in large scale open distributed system such a the envisioned semantic web this is especially important for the description of web service which should enable complex task involving multiple agent a one of the first initiative of the semantic webcommunity for describing web service owl s attracts a lot of interest even though it is still under development we identify problematic aspect of owl s and suggest enhancement through alignment to a foundational ontology another contribution of ourwork is the core ontology of service that try to fill the epistemological gap between the foundational ontology and owl s it can be reused to align other web service description language a well finally we demonstrate the applicability of our work byaligning owl s standard example called congobuy 
dimensionality reduction technique seek to representa set of image a a set of point in a low dimensionalspace here we explore a video representation thatconsiders a video a two part a space of possibleimages and a trajectory through that space the non lineardimensionality reduction technique of isomap give for many interesting scene a very low dimensionalrepresentation of the space of possible image analysis of the shape of the video trajectory throughthese image space give new tool for video analysis experiment with natural video sequence illustratemethods for the very different tasts of classifyingvideo clip and temporal super resolution 
this paper is about the evolutionary design of multi agent system an important part of recent research in this domain ha been focusing on collaborative revolutionary method we expose possible drawback of these method and show that for a non trivial problem called the blind mouse problem a classical ga approach in which whole population are evaluated selected and crossed together with a few tweak find an elegant and non intuitive solution more efficiently than cooperative coevolution the difference in efficiency grows with the number of agent within the simulation we propose an explanation for this poorer performance of cooperative coevolution based on the intrinsic fragility of the evaluation process this explanation is supported by theoretical and experimental argument 
because of practical limit in characterizing the safety profile of therapeutic product prior to marketing manufacturer and regulatory agency perform post marketing surveillance based on the collection of adverse reaction report pharmacovigilance the resulting database while rich in real world information are notoriously difficult to analyze using traditional technique each report may involve multiple medicine symptom and demographic factor and there is no easily linked information on drug exposure in the reporting population kdd technique such a association finding are well matched to the problem but are difficult for medical staff to apply and interpret to deploy kdd effectively for pharmacovigilance lincoln technology and glaxosmithkline collaborated to create a webbased safety data mining web environment the analytical core is a high performance implementation of the mgps multi item gamma poisson shrinker algorithm described previously by dumouchel and pregibon with several significant extension and enhancement the environment offer an interface for specifying data mining run a batch execution facility tabular and graphical method for exploring association and drilldown to case detail substantial work wa involved in preparing the raw adverse event data for mining including harmonization of drug name and removal of duplicate report the environment can be used to explore both drug event and multi way association interaction syndrome it ha been used to study age gender effect to predict the safety profile of proposed combination drug and to separate contribution of individual drug to safety problem in polytherapy situation 
we present a method for unsupervised learning of classesof motion in video we project optical flow field to a complete orthogonal a priori set of basis function in a probabilisticfashion which improves the estimation of the projectionsby incorporating uncertainty in the flow we thencluster the projection using a mixture of feature weightedgaussians over optical flow field the resulting modelextracts a concise probabilistic description of the majorclasses of optical flow present the method is demonstratedon a video of a person s facial expression 
computation without stable state is a computing paradigm different from turing s and ha been demonstrated for various type of simulated neural network this publication transfer this to a hardware implemented neural network result of a software implementation are reproduced showing that the performance peak when the network exhibit dynamic at the edge of chaos the liquid computing approach seems well suited for operating analog computing device such a the used vlsi neural network 
heterogeneous type of gene expression may provide a better insight into the biological role of gene interaction with the environment disease development and drug effect at the molecular level in this paper for both exploring and prediction purpose a time lagged recurrent neural network with trajectory learning is proposed for identifying and classifying the gene functional pattern from the heterogeneous nonlinear time series microarray experiment the proposed procedure identify gene functional pattern from the dynamic of a state trajectory learned in the heterogeneous time series and the gradient information over time also the trajectory learning with back propagation through time algorithm can recognize gene expression pattern vary over time this may reveal much more information about the regulatory network underlying gene expression the analyzed data were extracted from spotted dna microarrays in the budding yeast expression measurement produced by eisen et al the gene matrix contained experiment over a variety of heterogeneous experiment condition the number of recognized gene pattern in our study ranged from two to ten and were divided into three case optimal network architecture with different memory structure were selected based on akaike and bayesian information statistical criterion using two way factorial design the optimal model performance wa compared to other popular gene classification algorithm such a nearest neighbor support vector machine and self organized map the reliability of the performance wa verified with multiple iterated run 
this paper applies fast sparse multidimensional scaling md to a large graph of music similarity with k vertex that represent artist album and track and m edge that represent similarity between those entity once vertex are assigned location in a euclidean space the location can be used to browse music and to generate playlist md on very large sparse graph can be effectively performed by a family of algorithm called rectangular dijsktra rd md algorithm these rd algorithm operate on a dense rectangular slice of the distance matrix created by calling dijsktra a constant number of time two rd algorithm are compared landmark md which us the nystrm approximation to perform md and a new algorithm called fast sparse embedding which us fastmap these algorithm compare favorably to laplacian eigenmaps both in term of speed and embedding quality 
robust estimator such a least median of squared lmeds residual m estimator the least trimmed square lts etc havebeen employed to estimate optical flow from image sequence inrecent year however these robust estimator have a breakdownpoint of no more than in this paper we propose a novel robustestimator called variable bandwidth quick maximum density powerestimator vbqmdpe which can tolerate more than outlier weapply the novel proposed estimator to robust optical flowestimation our method yield better result than most otherrecently proposed method and it ha the potential to betterhandle multiple motion effect 
this paper describes an approach to recovering surface model ofcomplex scene from the quasi sparse data returned by a featurebased stereo system the method can be used to merge stereo resultsobtained from different viewpoint into a single coherent surfacemesh the technique proceeds by exploiting the free space theoremwhich provides a principled mechanism for reasoning about thestructure of the scene based on quasi sparse correspondence inmultiple image effective method for overcoming the difficultiesposed by missing feature and outlier are discussed resultsobtained by applying this approach to actual image are presented 
this paper show how finite approximation of long distance dependency ldd resolution can be obtained automatically for wide coverage robust probabilistic lexical functional grammar lfg resource acquired from treebanks we extract lfg subcategorisation frame and path linking ldd reentrancies from f structure generated automatically for the penn ii treebank tree and use them in an ldd resolution algorithm to parse new text unlike collins johnson in our approach resolution of ldds is done at f structure attribute value structure representation of basic predicate argument or dependency structure without empty production trace and coindexation in cfg parse tree currently our best automatically induced grammar achieve f score for f structure parsing section of the wsj part of the penn ii treebank and evaluating against the dcu and against the parc dependency bank king et al performing at the same or a slightly better level than state of the art hand crafted grammar kaplan et al 
we present a systematic comparison of machine learning method applied to the problem of fully automatic recognition of facial expression including adaboost support ve ctor machine and linear discriminant analysis each video frame is first scanned in real time to detect upright fronta l face the face found are scaled into image patch of equal size and sent downstream for further processing gabor energy filter are applied at the scaled image patch followed by a recognition engine that code facial expression into dimension in real time neutral anger disgust fear joy sadness surprise we report result on a series of experiment comparing spatial frequency range feature selection technique and recognition engine be st result were obtained by selecting a subset of gabor filter using adaboost and then training support vector machine on the output of the filter selected by adaboost the generalization performance to new subject for a way forced choice wa and correct on two publicly available datasets the best performance reported so far on these datasets surprisingly registration of internal facial f eatures wa not necessary even though the face detector doe not provide precisely registered image the output of the classifier change smoothly a a function of time and thus can be used for unobtrusive motion capture we developed an end to end system that provides facial expression code at frame per second and animates a computer generated character in real time 
this paper proposes a novel decision tree for a data set with time series attribute our time series tree ha a value i e a time sequence of a time series attribute in it internal node and split example based on dissimilarity between a pair of time sequence our method selects for a split test a time sequence which exists in data by exhaustive search based on class and shape information experimental result confirm that our induction method construct comprehensive and accurate decision tree moreover a medical application show that our time series tree is promising for knowledge discovery 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
reinforcement learning and q learning inparticular encounter two major problemswhen dealing with large state space first learning the q function in tabular form maybe infeasible because of the excessive amountof memory needed to store the table andbecause the q function only converges aftereach state ha been visited multiple time 
instance selection and feature selection are two orthogonal method for reducing the amount and complexity of data feature selection aim at the reduction of redundant feature in a dataset whereas instance selection aim at the reduction of the number of instance so far these two method have mostly been considered in isolation in this paper we present a new algorithm which we call fis feature and instance selection that target both problem simultaneously in the context of text classificationour experiment on the reuters and newsgroups datasets show that fis considerably reduces both the number of feature and the number of instance the accuracy of a range of classifier including na ve bayes tan and lb considerably improves when using the fis preprocessed datasets matching and exceeding that of support vector machine which is currently considered to be one of the best text classification method in all case the result are much better compared to mutual information based feature selection the training and classification speed of all classifier is also greatly improved 
this paper present a discussion of the theoretical complexity of plan recognition on the basis of an analysis of the number of explanation that any complete plan recognition algorithm must consider given various property of the plan library on the basis of these result it point out property of plan library that make them computationally expensive 
learning in many multi agent setting is in herently repeated play this call into ques tion the naive application of single play nash equilibrium in multi agent learning and sug gests instead the application of give and take principle of bargaining we modify and analyze a satis cing algorithm based on karandikar et al that is compat ible with the bargaining perspective this algorithm is a form of relaxation search that converges to a satis cing equilibrium without knowledge of game payofis or other agent action we then develop an m action n player social dilemma that encodes the key element of the prisoner s dilemma this game is instructive because it characterizes social dilemma with more than two agent and more than two choice we show how several difierent multi agent learning algo rithms behave in this social dilemma and demonstrate that the satis cing algorithm converges with high probability to a pareto e cient solution in self play and to the single play nash equilibrium against sel sh agent finally we present theoretical result that characterize the behavior of the algorithm 
today there is a plethora of data accessible via the internet the web ha greatly simplified the process of searching for accessing and sharing information however a considerable amount of internet distributed data still go unnoticed and unutilized particularly in the case of frequently updated internet distributed database in this paper we give an overview of webformulate a web based visual continual query system that address the problem associated with formulating temporal ad hoc analysis over network of heterogeneous frequently updated data source the main distinction between this system and existing internet facility to retrieve information and assimilate it into computation is that webformulate provides the necessary facility to perform continual query developing and maintaining dynamic link such that web based computation and report automatically maintain themselves a further distinction is that this system is specifically designed for user of spreadsheet level ability rather than professional programmer 
introduction a mobile robot acting in the world is faced with a large amount of sensory data and uncertainty in it action outcome indeed almost all interesting sequential decision making domain involve large state space and large stochastic action set we investigate a way to act intelligently a quickly a possible in domain where finding a complete policy would take a hopelessly long time this approach relational envelope based planning rebp tackle large noisy problem along two ax first describing a domain a a relational mdp instead of a an atomic or propositionally factored mdp allows problem structure and dynamic to be captured compactly with a small set of probabilistic relational rule second an envelope based approach to planning let an agent begin acting quickly within a restricted part of the full state space and to judiciously expand it envelope a resource permit the case for both probability and logical structure quickly generating generating usable plan when the world abounds with uncertainty is an important and difficult enterprise consider the classic block world domain the number of way to make a stack of a certain height grows exponentially with the number of block on the table and if the outcome of action are uncertain the task becomes even more daunting we want planning technique that can deal with large state space and large stochastic action set since most compelling realistic domain have these characteristic we are investigating a method for planning in very large domain by using expressive rule to restrict attention to high utility subset of the state space 
in a multi view problem the feature of thedomain can be partitioned into disjoint subset view that are sufficient to learn the target concept 
text categorization algorithm usually represent document a bag of word and consequently have to deal with huge number of feature most previous study found that the majority of these feature are relevant for classification and that the performance of text categorization with support vector machine peak when no feature selection is performed we describe a class of text categorization problem that are characterized with many redundant feature even though most of these feature are relevant the underlying concept can be concisely captured using only a few feature while keeping all of them ha substantially detrimental effect on categorization accuracy we develop a novel measure that capture feature redundancy and use it to analyze a large collection of datasets we show that for problem plagued with numerous redundant feature the performance of c is significantly superior to that of svm while aggressive feature selection allows svm to beat c by a narrow margin 
bundle ajustment is used to obtain accurate visual reconstructionsby minimizing the reprojection error the coordinateframe ambiguity or more generality the gauge freedom ha been dealt with in different manner it ha oftenbeen reported that standard bundle adjustment algorithmswere not gauge invariant two iteration within differentgauges can lead to geometrically very different result surprisingly most algorithm do not exploit gauge freedom toimprove performance we consider this issue we analyzetheoretically the impact of the gauge on standard algorithm we show that a sufficiently general damping matrixin levenberg marquardt iteration can be used to implicitlyreproduce a gauge transformation we show that if thedamping matrix is chosen such that the decrease in the reprojectionerror is maximized then the iteration is gauge invariant experimental result on simulated and real data showthat our gauge invariant bundle adjustment algorithm outperformsexisting one in term of stability 
extensive effort have been devoted to developing efficient algorithm for mining frequent pattern however frequent pattern mining remains a time consuming process especially for very large datasets it is therefore desirable to adopt a mining once and using many time strategy unfortunately there ha been little work reported on managing and organizing a large set of pattern for future use in this paper we propose a disk based data structure cfp tree condensed frequent pattern tree for organizing frequent pattern discovered from transactional database in addition to an efficient algorithm for cfp tree construction we also developed algorithm to efficiently support two important type of query namely query with minimum support constraint and query with item constraint against the stored pattern a these two type of query are basic building block for complex frequent pattern related mining task comprehensive experimental study ha been conducted to demonstrate the effectiveness of cfp tree and efficiency of related algorithm 
we describe an open source java toolkit of method for matching name and record we summarize result obtained from using various string distance metric on the task of matching entity name these metric include distance function proposed by several different community such a edit distance metric fast heuristic string comparators token based distance metric and hybrid method we then describe an extension to the toolkit which allows record to be compared we discus some issue involved in performing a similar comparision for record matching technique and finally present result for some baseline record matching algorithm that aggregate string comparison between field 
this paper applies machine learning technique to acquiring aspect of the meaning of discourse marker three subtasks of acquiring the meaning of a discourse marker are considered learning it polarity veridicality and type i e causal temporal or additive accuracy of over is achieved for all three task well above the baseline 
we consider the problem of geometrical surface reconstruction from one or several image using learned shape model while human can effortlessly retrieve d shape information this inverse problem ha turned out to be difcult to perform automatically we introduce a framework based on level set surface reconstruction and shape model for achieving this goal through this merging we obtain an efcient and robust method for reconstructing surface of an object category of interest the shape model includes surface cue such a point curve and silhouette feature based on idea from active shape model we show how both the geometry and the appearance of these feature can be modelled consistently in a multi view context the complete surface is obtained by evolving a level set driven by a pde which try to t the surface to the inferred d feature in addition an a priori d surface model is used to regularize the solution in particular where surface feature are sparse experiment are demonstrated on a database of real face image 
when automatically extracting information from the world wide web most established method focus on spotting single html document however the problem of spotting complete web site is not handled adequately yet in spite of it importance for various application therefore this paper discus the classification of complete web site first we point out the main difference to page classification by discussing a very intuitive approach and it weakness this approach treat a web site a one large html document and applies the well known method for page classification next we show how accuracy can be improved by employing a preprocessing step which assigns an occurring web page to it most likely topic the determined topic now represent the information the web site contains and can be used to classify it more accurately we accomplish this by following two direction first we apply well established classification algorithm to a feature space of occurring topic the second direction treat a site a a tree of occurring topic and us a markov tree model for further classification to improve the efficiency of this approach we additionally introduce a powerful pruning method reducing the number of considered web page our experiment show the superiority of the markov tree approach regarding classification accuracy in particular we demonstrate that the use of our pruning method not only reduces the processing time but also improves the classification accuracy 
supervised learning method for wsd yield better performance than unsupervised method yet the availability of clean training data for the former is still a severe challenge in this paper we present an unsupervised bootstrapping approach for wsd which exploit huge amount of automatically generated noisy data for training within a supervised learning framework the method is evaluated using the noun in the english lexical sample task of senseval our algorithm doe a well a supervised algorithm on of this test set which is an improvement of absolute over state of the art bootstrapping wsd algorithm we identify seven different factor that impact the performance of our system 
hierarchy provide a mean of organizing summarizing and accessing information we describe a method for automatically generating hierarchy from small collection of text and then apply this technique to summarizing the document retrieved by a search engine 
in this paper we present a method for the semantic tagging of word chunk extracted from a written transcription of conversation this work is part of an ongoing project for an information extraction system in the field of maritime search and rescue sar our purpose is to automatically annotate part of text with concept from a sar ontology our approach combine two knowledge source a sar ontology and the wordsmyth dictionary thesaurus and it us a similarity measure for the classification evaluation is carried out by comparing the output of the system with key answer of predefined extraction template 
we present and empirically analyze a machine learning approach for detecting intrusion on individual computer our winnow based algorithm continually monitor user and system behavior recording such property a the number of byte transferred over the last second the program that currently are running and the load on the cpu in all hundred of measurement are made and analyzed each second using this data our algorithm creates a model that represents each particular computer s range of normal behavior parameter that determine when an alarm should be raised due to abnormal activity are set on a per computer basis based on an analysis of training data a major issue in intrusion detection system is the need for very low false alarm rate our empirical result suggest that it is possible to obtain high intrusion detection rate and low false alarm rate le than one per day per computer without stealing too many cpu cycle le than we also report which system measurement are the most valuable in term of detecting intrusion a surprisingly large number of different measurement prove significantly useful 
we present a semantic web application that we callcs aktive space the application exploit a wide range of semantically heterogeneousand distributed content relating to computer science research in theuk this content is gathered on a continuous basis using a variety of method including harvesting and scraping a well a adopting a range model for content acquisition the content currently comprises aroundten million rdf triple and we have developed storage retrieval andmaintenance method to support it management the content is mediated through an ontology constructed for the application domainand incorporates component from other published ontology c aktive spacesupports the exploration of pattern and implication inherent in the content and exploit a variety of visualisation and multi dimensional representation knowledge service supported in the applicationinclude investigating community of practice who is working researching or publishing with whom this work illustrates a number ofsubstantial challenge for the semantic web these include problem of referential integrity tractable inference and interaction support wereview our approach to these issue and discus relevant related work 
web search engine employ multiple so called crawler to maintain local copy of web page but these web page are frequently updated by their owner and therefore the crawler must regularly revisit the web page to maintain the freshness of their local copy in this paper we propose a two part scheme to optimize this crawling process one goal might be the minimization of the average level of staleness over all web page and the scheme we propose can solve this problem alternatively the same basic scheme could be used to minimize a possibly more important search engine embarrassment level metric the frequency with which a client make a search engine query and then click on a returned url only to find that the result is incorrect the first part our scheme determines the nearly optimal crawling frequency a well a the theoretically optimal time to crawl each web page it doe so within an extremely general stochastic framework one which support a wide range of complex update pattern found in practice it us technique from probability theory and the theory of resource allocation problem which are highly computationally efficient crucial for practicality because the size of the problem in the web environment is immense the second part employ these crawling frequency and ideal crawl time a input and creates an optimal achievable schedule for the crawler our solution based on network flow theory is exact a well a highly efficient an analysis of the update pattern from a highly accessed and highly dynamic web site is used to gain some insight into the property of page update in practice then based on this analysis we perform a set of detailed simulation experiment to demonstrate the quality and speed of our approach 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this paper discus building complex classifier from a singlelabeled example and vast number of unlabeled observation set eachderived from observation of a single processor object when datacan be measured by observation it is often plentiful and it isoften possible to make more than one observation of the state of aprocess or object this paper discus how to exploit thevariability across such set of observation of the same object toestimate class label for unlabeled example given a minimal numberof labeled example in contrast to similar semi supervisedclassification procedure that define the likelihood that twoobservations share a label a a function of the embedded distancebetween the two observation this method us the naive bayesestimate of how often the two observation did result from the sameobserved process exploiting this additional source of informationin an iterative estimation procedure can generalize complexclassification model from single labeled observation someexamples involving classification of tracked object in alow dimensional feature space given thousand of unlabeledobservation set are used to illustrate the effectiveness of thismethod 
part number are widely used within an enterprise throughout the manufacturing process the point of entry of such part number into this process is normally via a bill of material or bom sent by a contact manufacturer or supplier each line of the bom provides information about one part such a the supplier part number the bom receiver s corresponding internal part number an unstructured textual part description the supplier name etc however in a substantial number of case the bom receiver s internal part number is absent hence before this part can be incorporated into the receiver s manufacturing process it ha to be mapped to an internal part of the bom receiver based on the information of the part in the bom historically this mapping process ha been done manually which is a highly time consuming labor intensive and error prone process this paper describes a system for automating the mapping of bom part number the system us a two step modeling and mapping approach first the system us historical bom data receiver s part specification data and receiver s part taxonomic data along with domain knowledge to automatically learn classification model for mapping a given bom part description to successively lower level of the receiver s part taxonomy to reduce the set of potential internal part to which the bom part could map to then information about various part parameter is extracted from the bom part description and compared to the specification data of the potential internal part to choose the final mapped internal part mapping done by the system are very accurate and the system is currently being deployed within ibm for mapping boms received by the corporate procurement manufacturing division 
the microeconomic framework for data mining assumes that an enterprise chooses a decision maximizing the overall utility over all customer where the contribution of a customer is a function of the data available on that customer in catalog segmentation the enterprise want to design k product catalog of size r that maximize the overall number of catalog product purchased however there are many application where a customer once attracted to an enterprise would purchase more product beyond the one contained in the catalog therefore in this paper we investigate an alternative problem formulation that we call customer oriented catalog segmentation where the overall utility is measured by the number of customer that have at least a specified minimum interest t in the catalog we formally introduce the customer oriented catalog segmentation problem and discus it complexity then we investigate two different paradigm to design efficient approximate algorithm for the customer oriented catalog segmentation problem greedy deterministic and randomized algorithm since greedy algorithm may be trapped in a local optimum and randomized algorithm crucially depend on a reasonable initial solution we explore a combination of these two paradigm our experimental evaluation on synthetic and real data demonstrates that the new algorithm yield catalog of significantly higher utility compared to classical catalog segmentation algorithm 
multi view learner reduce the need for labeled data by exploiting disjoint sub set of feature view each of which is sufficient for learning such algorithm assume that each view is a strong view i e perfect learning is possible in each view we extend the multi view framework by introducing a novel algorithm aggressive co testing that exploit both strong and weak view in a weak view one can learn a concept that is strictly more general or specific than the target concept aggressive co testing us the weak view both for detecting the most informative example in the domain and for improving the accuracy of the prediction in a case study on wrapper induction task our algorithm requires significantly fewer labeled example than existing state of the art approach 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
this paper extends previous work on the skewing algorithm a promising approach that allows greedy decision tree induction algorithm to handle problematic function such a parity function with a lower run time penalty than lookahead a deficiency of the previously proposed algorithm is it inability to scale up to high dimensional problem in this paper we describe a modified algorithm that scale better with increasing number of variable we present experiment with randomly generated boolean function that evaluate the algorithm s response to increasing dimension we also evaluate the algorithm on a challenging real world biomedical problem that of sh domain binding our result indicate that our algorithm almost always outperforms an information gain based decision tree learner 
phrase level translation model are effective in improving translation quality by addressing the problem of local re ordering across language boundary method that attempt to fundamentally modify the traditional ibm translation model to incorporate phrase typically do so at a prohibitive computational cost we present a technique that begin with improved ibm model to create phrase level knowledge source that effectively represent local a well a global phrasal context our method is robust to noisy alignment at both the sentence and corpus level delivering high quality phrase level translation pair that contribute to significant improvement in translation quality a measured by the bleu metric over word based lexica a well a a competing alignment based method 
this paper present a transaction time http server called ttapache that support document versioning a document often consists of a main file formatted in html or xml and several included file such a image and stylesheets a change to any of the file associated with a document creates a new version of that document to construct a document version history snapshot of the document s file are obtained over time transaction time are associated with each file version to record the version s lifetime the transaction time is the system time of the edit that created the version accounting for transaction time is essential to supporting audit query that delve into past document version and differential query that pinpoint difference between two version ttapache performs automatic versioning when a document is read thereby removing the burden of versioning from document author since some version may be created but never read ttapache distinguishes between known and assumed version of a document ttapache ha a simple query language to retrieve desired version a browser can request a specific version or the entire history of a document query can also rewrite link and reference to point to current or past version over time the version history of a document continually grows to free space some version can be vacuumed vacuuming a version however change the semantics of request for that version this paper present several policy for vacuuming version and strategy for accounting for vacuumed version in query 
speech dereverberation is desirable with a view to achieving for example robust speech recognition in the real world however it is still a challenging problem especially when using a single microphone although blind equalization technique have been exploited they cannot deal with speech signal appropriately because their assumption are not satisfied by speech signal we propose a new dereverberation principle based on an inherent property of speech signal namely quasi periodicity the present method learn the dereverberation filter from a lot of speech data with no prior knowledge of the data and can achieve high quality speech dereverberation especially when the reverberation time is long 
most machine learning researcher perform quantitative experiment to estimate generalization error and compare the performance of different algorithm in particular their proposed algorithm in order to be able to draw statistically convincing conclusion it is important to estimate the uncertainty of such estimate this paper study the very commonly used k fold cross validation estimator of generalization performance the main theorem show that there exists no universal valid under all distribution unbiased estimator of the variance of k fold cross validation the analysis that accompanies this result is based on the eigen decomposition of the covariance matrix of error which ha only three different eigenvalue corresponding to three degree of freedom of the matrix and three component of the total variance this analysis help to better understand the nature of the problem and how it can make naive estimator that don t take into account the error correlation due to the overlap between training and test set grossly underestimate variance this is confirmed by numerical experiment in which the three component of the variance are compared when the difficulty of the learning problem and the number of fold are varied 
we investigate how to learn a kernel matrix for high dimensional data that lie on or near a low dimensional manifold noting that the kernel matrix implicitly map the data into a nonlinear feature space we show how to discover a mapping that unfolds the underlying manifold from which the data wa sampled the kernel matrix is constructed by maximizing the variance in feature space subject to local constraint that preserve the angle and distance between nearest neighbor the main optimization involves an instance of semidefinite programming a fundamentally different computation than previous algorithm for manifold learning such a isomap and locally linear embedding the optimized kernel perform better than polynomial and gaussian kernel for problem in manifold learning but worse for problem in large margin classification we explain these result in term of the geometric property of different kernel and comment on various interpretation of other manifold learning algorithm a kernel method 
there ha been considerable success in automated reconstruction for image sequence where small baseline algorithm can be used to establish match across a number of image in contrast in the case of widely separated view method have generally been restricted to two or three view in this paper we investigate the problem of establishing relative viewpoint given a large number of image where no ordering information is provided a typical application would be where image are obtained from different source or at different time both the viewpoint position orientation scale and lighting condition may vary significantly over the data set such a problem is not fundamentally amenable to exhaustive pair wise and triplet wide baseline matching because this would be prohibitively expensive a the number of view increase instead we investiate how a combination of image invariant covariants and multiple view relation can be used in concord to enable efficient multiple view matching the result is a matching algorithm which is linear in the number of view the method are illustrated on several real image data set the output enables an image based technique for navigating in a d scene moving from one image to whichever image is the next most appropriate 
the aim of this paper is to present a system that us both inducted hypothesis and expert knowledge for recognition of object within digital image classification of pixel by spectral and spatial feature is learned via example recognition of object within resulting class image is performed by a novel method of search that take advice on object property and enables error in object recognition to be systematically isolated and rectified guided by the system a user selects whether error correction is machine learned or user defined the adaptability of mixed initiative error correction lead to wide applicability the system is demonstrated using image from a robot soccer domain 
in many real world application active selection of training example can significantly reduce the number of labelled training example to learn a classification function different strategy in the field of support vector machine have been proposed that iteratively select a single new example from a set of unlabelled example query the corresponding class label and then perform retraining of the current classifier however to reduce computational time for training it might be necessary to select batch of new training example instead of single example strategy for single example can be extended straightforwardly to select batch by choosing the h example that get the highest value for the individual selection criterion we present a new approach that is especially designed to construct batch and incorporates a diversity measure it ha low computational requirement making it feasible for large scale problem with several thousand of example experimental result indicate that this approach provides a faster method to attain a level of generalization accuracy in term of the number of labelled example 
the link between genetic algorithm and population based markov chain monte carlo mcmc method are explored genetic algorithm gas are well known for their capability to optimize function of discretevalued variable but the mcmc interpretation allows ga variant to be used for sampling discrete space e g in bayesian inference for machine learning the ga crossover and mutation operator are modied to provide valid mcmc sample and a new exclusive or operator is introduced a an alternative way to recombine population member this is shown to improve sampling performance in a medical diagnostic problem domain the sampler can also be used within simulated annealing to provide a global optimizer that is similar to a ga in structure but ha known convergence property 
word sense discrimination is an unsupervised clustering problem which seek to discover which instance of a word s are used in the same meaning this is done strictly based on information found in raw corpus without using any sense tagged text or other existing knowledge source our particular focus is to systematically compare the efficacy of a range of lexical feature context representation and clustering algorithm when applied to this problem 
the minimax probability machine mpm considers a binary classiflcation problem where mean and covariance matrix of each class are assumed to be known without making any further distributional assumption the mpm minimizes the worst case probability fi of misclassiflcation of future data point however the validity of the upper bound fi depends on the accuracy of the estimate of the real but unknown mean and covariance first we show how to make this minimax approach robust against certain estimation error for unknown but bounded mean and covariance matrix we guarantee a robust upper bound secondly the robust minimax approach for supervised learning is extended in a very natural way to the unsupervised learning problem of quantile estimation computing a minimal region in input space where at least a fraction fi of the total probability mass life mercer kernel can be exploited in this setting to obtain nonlinear region positive empirical result are obtained when comparing this approach to single class svm and a class svm approach 
w c round and g q zhang have recently proposed to study a form of resolution on algebraic domain round and zhang this framework allows reasoning with knowledge which is hierarchically structured and form a suitable domain more precisely a coherent algebraic cpo a studied in domain theory in this paper we give condition under which a resolution theorem in a form underlying resolution based logic programming system can be obtained the investigation bear potential for engineering new knowledge representation and reasoning system on a firm domain theoretic background 
qualitative assessment of scientific computation is an emerging application area that applies a data driven approach to characterize at a high level phenomenon including conditioning of matrix sensitivity to various type of error propagation and algorithmic convergence behavior this paper develops a spatial aggregation approach that formalizes such analysis in term of model selection utilizing spatial structure extracted from matrix perturbation datasets we focus in particular on the characterization of matrix eigenstructure both analyzing sensitivity of computation with spectral portrait and determining eigenvalue multiplicity with jordan portrait our approach employ spatial reasoning to overcome noise and sparsity by detecting mutually reinforcing interpretation and to guide subsequent data sampling it enables quantitative evaluation of property of a scientific computation in term of confidence in a model explainable in term of the sampled data and domain knowledge about the underlying mathematical structure not only is our methodology more rigorous than the common approach of visual inspection but it also is often substantially more efficient due to well defined stopping criterion result show that the mechanism efficiently sample perturbation space and successfully uncovers high level property of matrix 
in this paper we describe an approach to combining text and visual feature from mpeg description of video a video retrieval process is aligned to a text retrieval process based on the tf idf vector space model via clustering of low level visual feature our assumption is that shot within the same cluster are not only similar visually but also semantically to a certain extent our experiment on the trecvid and trecvid collection show that adding extra meaning to a shot based on the shot from the same cluster is useful when each video in a collection contains a high proportion of similar shot for example in documentary 
oversubscribed scheduling problem require removing or partially satisfying task when enough resource are not available for a particular oversubscribed problem air force satellite control network scheduling we find that the best approach make long leap in the search space we find this is in part due to large plateau in the search space algorithm moving only one task at a time are impractical both a genetic algorithm and squeaky wheel optimization swo make long leap in the search space and produce good solution almost time faster than local search greedy initialization is shown to be critical to good performance but is not a important a directed leap when using fewer than evaluation swo show superior performance with evaluation a genetic algorithm using a population seeded with greedy solution further improves on the swo result 
given the experimental nature of information retrieval progress critically depends on analyzing the error made by existing retrieval approach and understanding their limitation our research explores various hypothesized reason for hard topic in trec ad hoc task and show that the bad performance is partially due to the existence of highly distracting sub collection that can dominate the overall performance 
we present novel simple appearance and shape model that we callepitomes the epitome of an image is it miniature condensedversion containing the essence of the textural and shape propertiesof the image a opposed to previously used simple image model such a template or basis function the size of the epitome isconsiderably smaller than the size of the image or object itrepresents but the epitome still contains most constitute elementsneeded to reconstruct the image fig a collection of imagesoften share an epitome e g when image are a few consecutiveframes from a video sequence or when they are photograph ofsimilar object a particular image in a collection is defined byits epitome and a smooth mapping from the epitome to the imagepixels when the epitomic representation is used within ahierarchical generative model appropriate inference algorithm canbe derived to extract the epitome from a single image or acollection of image and at the same time perform various inferencetasks such a image segmentation motion estimation objectremoval and super resolution 
member of multi robot team may need to collaborate to accomplish a task due to difference in capability this paper describes an extension of the alliance architecture that enables agent recruitment within a decentralized uav ugv robot team without task preemption but us a formal model of emotion and handle heterogeneity affective computing allows recruitment to be robust under loss of communication between agent and minimizes the number of message passed data from simulation show that the affective strategy succeeds with a random message loss rate up to and requires fewer message to be sent compared to greedy and random and that of these affective scale best with team size comparison of broadcast to unicast messaging are also made in simulation 
this paper address three question is it useful to attempt to learn a bayesian network structure with hundred of thousand of node how should such structure search proceed practically the third question arises out of our approach to the second how can frequent set agrawal et al which are extremely popular in the area of descriptive data mining be turned into a probabilistic model large sparse datasets with hundred of thousand of record and attribute appear in social network warehousing supermarket transaction and web log the complexity of structural search made learning of factored probabilistic model on such datasets unfeasible we propose to use frequent set to significantly speed up the structural search unlike previous approach we not only cache n way sufficient statistic but also exploit their local structure we also present an empirical evaluation of our algorithm applied to several massive datasets 
linear discriminant analysis lda is a well known method for feature extraction and dimension reduction it ha been used widely in many application such a face recognition recently a novel lda algorithm based on qr decomposition namely lda qr ha been proposed which is competitive in term of classification accuracy with other lda algorithm but it ha much lower cost in time and space however lda qr is based on linear projection which may not be suitable for data with nonlinear structure this paper first proposes an algorithm called kda qr which extends the lda qr algorithm to deal with nonlinear data by using the kernel operator then an efficient approximation of kda qr called akda qr is proposed experiment on face image data show that the classification accuracy of both kda qr and akda qr are competitive with generalized discriminant analysis gda a general kernel discriminant analysis algorithm while akda qr ha much lower time and space cost 
we investigate how random projection can best be used for clustering high dimensional data random projection ha been shown to have promising theoretical property in practice however we nd that it result in highly unstable clustering performance our solution is to use random projection in a cluster ensemble approach empirical result show that the proposed approach achieves better and more robust clustering performance compared to not only single run of random projection clustering but also clustering with pca a traditional data reduction method for high dimensional data to gain insight into the performance improvement obtained by our ensemble method we analyze and identify the influence of the quality and the diversity of the individual clustering solution on the nal ensemble performance 
xml repository are now a widespread mean for storing and exchanging information on the web a these repository become increasingly used in dynamic application such a e commerce there is a rapidly growing need for a mechanism to incorporate reactive functionality in an xml setting event condition action eca rule are a technology from active database and are a natural method for supporting suchfunctionality eca rule can be used for activity such a automatically enforcing document constraint maintaining repository statistic and facilitating publish subscribe application an important question associated with the use of a eca rule is how to statically predict their run time behaviour in this paper we define a language for eca rule on xml repository we then investigate method for analysing the behaviour of a set of eca rule a task which ha added complexity in this xml setting compared with conventional active database 
decision tree are commonly used for classification we propose to use decision tree not just for classification but also for the wider purpose of knowledge discovery because visualizing the decision tree can reveal much valuable information in the data we introduce paintingclass a system for interactive construction visualization and exploration of decision tree paintingclass provides an intuitive layout and convenient navigation of the decision tree paintingclass also provides the user the mean to interactively construct the decision tree each node in the decision tree is displayed a a visual projection of the data through actual example and comparison with other classification method we show that the user can effectively use paintingclass to construct a decision tree and explore the decision tree to gain additional knowledge 
estimating the parameter of a pencil of line is addressed a statistical model for the measurement is developed from which the cramer rao lower bound is determined an estimator is derived and it performance is simulated and compared to the bound the estimator is shown to be asymptotically efficient and superior to the classical least square algorithm 
abstract standard approach to object detection focus on local patch of theimage and try to classify them a background or not we propose touse the scene context image a a whole a an extra source of global information to help resolve local ambiguity we present a conditionalrandom field for jointly solving the task of object detection and sceneclassification 
the instability problem of decision tree classification algorithm is that small change in input training sample may cause dramatically large change in output classification rule different rule generated from almost the same training sample are against human intuition and complicate the process of decision making in this paper we present fundamental theorem for the instability problem of decision tree classifier the first theorem give the relationship between a data change and the resulting tree structure change i e split change the second theorem instability theorem provides the cause of the instability problem based on the two theorem algorithmic improvement can be made to lessen the instability problem empirical result illustrate the theorem statement the tree constructed by the proposed algorithm are more stable noise tolerant informative expressive and concise our proposed sensitivity measure can be used a a metric to evaluate the stability of splitting predicate the tree sensitivity is an indicator of the confidence level in rule and the effective lifetime of rule 
chinese part of speech po tagging assigns one po tag to each word in a chinese sentence however since word are not demarcated in a chinese sentence chinese po tagging requires word segmentation a a prerequisite we could perform chinese po tagging strictly after word segmentation one at a time approach or perform both word segmentation and po tagging in a combined single step simultaneously all atonce approach also we could choose to assign po tag on a word by word basis making use of word feature in the surrounding context word based or on a character by character basis with character feature character based this paper present an in depth study on such issue of processing architecture and feature representation for chinese po tagging within a maximum entropy framework we found that while the all at once characterbased approach is the best the one at a time character based approach is a worthwhile compromise performing only slightly worse in term of accuracy but taking shorter time to train and run a part of our investigation we also built a state of the art chinese word segmenter which outperforms the best sighan word segmenters in the closed track on out of test corpus 
given a parallel parsed corpus statistical treeto tree alignment attempt to match node in the syntactic tree for a given sentence in two language we train a probabilistic tree transduction model on a large automatically parsed chinese english corpus and evaluate result against human annotated word level alignment we find that a constituent based model performs better than a similar probability model trained on the same tree converted to a dependency representation 
we present a new method and system for performing the new event detection task i e in one or multiple stream of news story all story on a previously unseen new event are marked the method is based on an incremental tf idf model our extension include generation of source specific model similarity score normalization based on document specific average similarity score normalization based on source pair specific average term reweighting based on inverse event frequency and segmentation of the document we also report on extension that did not improve result the system performs very well on tdt and tdt test data and scored second in the tdt evaluation 
the multiple instance learning mil model ha been very successful in application area such a drug discovery and content based image retrieval recently a generalization of this model and an algorithm for this generalization were introduced showing significant advantage over the conventional mil model in certain application area unfortunately this algorithm is inherently inefficient preventing scaling to high dimension we reformulate this algorithm using a kernel for a support vector machine reducing it time complexity from exponential to polynomial computing the kernel is equivalent to counting the number of axis parallel box in a discrete bounded space that contain at least one point from each of two multisets p and q we show that this problem is p complete but then give a fully polynomial randomized approximation scheme fpras for it finally we empirically evaluate our kernel 
viral marketing take advantage of network of influence among customer to inexpensively achieve large change in behavior our research seek to put it on a firmer footing by mining these network from data building probabilistic model of them and using these model to choose the best viral marketing plan knowledge sharing site where customer review product and advise each other are a fertile source for this type of data mining in this paper we extend our previous technique achieving a large reduction in computational cost and apply them to data from a knowledge sharing site we optimize the amount of marketing fund spent on each customer rather than just making a binary decision on whether to market to him we take into account the fact that knowledge of the network is partial and that gathering that knowledge can itself have a cost our result show the robustness and utility of our approach 
constraint based mining of itemsets for question such a find all frequent itemsets where the total price is at least ha received much attention recently two class of constraint monotone and antimonotone have been identified a very useful there are algorithm that efficiently take advantage of either one of these two class but no previous algorithm can efficiently handle both type of constraint simultaneously in this paper we present the first algorithm called dualminer that us both monotone and antimonotone constraint to prune it search space we complement a theoretical analysis and proof of correctness of dualminer with an experimental study that show the efficacy of dualminer compared to previous work 
abstract in dynamic programming convergence of algorithm such a value iteration or policy iteration result in discounted problemsfrom a contraction property of the back up operator guaranteeing convergence to it fixedpoint when approximation is considered known result in approximate policy iteration provide bound on the closeness to optimality of the approximate value function obtained by successive policy improvement step a a function of the maximum norm of value determination error during policy evaluation step unfortunately such result have limited practical range since most function approximators such a linear regression select the best fit in a given class of parameterized function by minimizing some weighted quadratic norm in this paper we provide error bound for approximate policy iteration using quadratic norm and illustrate those result in the case of feature based linear function approximation 
latent dirichlet allocation lda is a fully generative approach to language modelling which overcomes the inconsistent generative semantics of probabilistic latent semantic indexing plsi this paper show that plsi is a maximum a posteriori estimated lda model under a uniform dirichlet prior therefore the perceived shortcoming of plsi can be resolved and elucidated within the lda framework 
in peer to peer network finding the appropriate answer for an information request such a the answer to a query for rdf s data depends on selecting the right peer in the network we hereinvestigate how social metaphor can be exploited effectively andefficiently to solve this task to this end we define a method for query routing remindin that let i peer observewhich query are successfully answered by other peer ii memorizes this observation and iii subsequently us this information in order to select peer to forward request to remindin ha been implemented for the swap peer to peer platformas well a for a simulation environment we have used the simulation environment in order to investigate how successfulvariations of remindin are and how they compare to baseline strategy in term of number of message forwarded in the networkand statement appropriately retrieved 
collective choice setting are the heart of society game theory provides a basis for engineering the incentive into the interaction mechanism e g rule of an election or auction so that a desirable system wide outcome e g president resource allocation or task allocation is chosen even though every agent act based on self interest however there are a host of computer science issue not traditionally addressed in game theory that have to be addressed in order to make mechanism work in the real world those computing communication and privacy issue are deeply intertwined with the economic incentive issue for example the fact that agent have limited computational capability to determine their own and others preference ruin the incentive property of established auction mechanism and give rise to new issue on the positive side computational complexity can be used a a barrier to strategic behavior in setting where economic mechanism design fall short novel computational approach also enable new economic institution for example market clearing technology with specialized search algorithm is enabling a form of interaction that i call expressive competition a another example selective incremental preference elicitation can determine the optimal outcome while requiring the agent to determine and reveal only a small portion of their preference furthermore automated mechanism design can yield better mechanism than the best known to date 
an under explored question in cross language information retrieval clir is to what degree the performance of clir method depends on the availability of high quality translation resource for particular domain to address this issue we evaluate several competitive clir method with different training corpus on test document in the medical domain our result show severe performance degradation when using a general purpose training corpus or a commercial machine translation system systran versus a domain specific training corpus a related unexplored question is whether we can improve clir performance by systematically analyzing training resource and optimally matching them to target collection we start exploring this problem by suggesting a simple criterion for automatically matching training resource to target corpus by using cosine similarity between training and target corpus a resource weight we obtained an average of improvement over using all resource with no weight the same metric yield of the performance obtained when an oracle chooses the optimal resource every time 
hill climbing search is the most commonly used search algorithm in ilp system because it permit the generation of theory in short running time however a well known drawback of this greedy search strategy is it myopia macro operator or macro for short a recently proposed technique to reduce the search space explored by exhaustive search can also be argued to reduce the myopia of hill climbing search by automatically performing a variable depth look ahead in the search space surprisingly macro have not been employed in a greedy learner in this paper we integrate macro into a hill climbing learner in a detailed comparative study in several domain we show that indeed a hill climbing learner using macro performs significantly better than current state of the art system involving other technique for reducing myopia such a fixed depth look ahead template based look ahead beam search or determinate literal in addition macro in contrast to some of the other approach can be computed fully automatically and do not require user involvement nor special domain property such a determinacy 
question classification is very important for question answering this paper present our research work on automatic question classification through machine learning approach we have experimented with five machine learning algorithm nearest neighbor nn naive bayes nb decision tree dt sparse network of winnow snow and support vector machine svm using two kind of feature bag of word and bag of ngrams the experiment result show that with only surface text feature the svm outperforms the other four method for this task further we propose to use a special kernel function called the tree kernel to enable the svm to take advantage of the syntactic structure of question we describe how the tree kernel can be computed efficiently by dynamic programming the performance of our approach is promising when tested on the question from the trec qa track 
entropy type measure for the heterogeneity of cluster have been used for a long time this paper study the entropy based criterion in clustering categorical data it first show that the entropy based criterion can be derived in the formal framework of probabilistic clustering model and establishes the connection between the criterion and the approach based on dissimilarity co efficients an iterative monte carlo procedure is then presented to search for the partition minimizing the criterion experiment are conducted to show the effectiveness of the proposed procedure 
current approach to information retrieval rely on the creativity of individual to develop new algorithm in this investigation the use of genetic algorithm ga and genetic programming gp to learn ir algorithm is examined document structure weighting is a technique whereby different part of a document title abstract etc contribute unevenly to the overall document weight during ranking near optimal weight can be learned with a ga doing so show a statistically significant relative improvement in map for vector space inner product and croft s probabilistic ranking but no improvement for bm two application of this approach are suggested offline learning and relevance feedback in a second set of experiment a new ranking function wa learned using gp this new function yield a statistically significant relative improvement on unseen query tested on the training document portability test to different collection not used in training demonstrate the performance of the new function exceeds vector space and probability and slightly exceeds bm learning weight for this new function is proposed the application of genetic learning to stemming and thesaurus construction is discussed stemming rule such a those of the porter algorithm are candidate for gp learning whereas synonym set are candidate for ga learning 
in this poster we describe an investigation of topic similarity measure we elicit assessment on the similarity of pair of topic from subject and use these a a benchmark to ass how well each measure performs the measure have the potential to form the basis of a predictive technique for adaptive search system the result of our evaluation show that measure based on the level of correlation between topic concord most with general subject perception of search topic similarity 
abstract convergence for iterative reinforcement learning algorithm liketd depends on the sampling strategy for the transition however in practical application it is convenient to take transitiondata from arbitrary source without losing convergence in thispaper we investigate the problem of repeated synchronous updatesbased on axed set of transition our main theorem yield su cient condition of convergence for combination of reinforcementlearning algorithm and linear 
abstract given any generative classier based on an inexact density model we can dene a discriminative counterpart that reduces it asymptotic error rate we introduce a family of classiers that interpolate the two approach thus providing a new way to compare them and giving an estimation procedure whose classication performance is well balanced between the bias of generative classiers and the variance of discriminative one we show that an intermediate trade o between the two strategy is often preferable both theoretically and in experiment on real data 
we present a prototype natural language problem solving application for a financial service call center developed a part of the amiti s multilingual human computer dialogue project our automated dialogue system based on empirical evidence from real call center conversation feature a data driven approach that allows for mixed system customer initiative and spontaneous conversation preliminary evaluation result indicate efficient dialogue and high user satisfaction with performance comparable to or better than that of current conversational travel information system 
we consider supervised learning in the presence of very many irrelevant feature and study two different regularization method for preventing overfitting focusing on logistic regression we show that using l regularization of the parameter the sample complexity i e the number of training example required to learn well grows only logarithmically in the number of irrelevant feature this logarithmic rate match the best known bound for feature selection and indicates that l regularized logistic regression can be effective even if there are exponentially many irrelevant feature a there are training example we also give a lower bound showing that any rotationally invariant algorithm including logistic regression with l regularization svms and neural network trained by backpropagation ha a worst case sample complexity that grows at least linearly in the number of irrelevant feature 
a new calibration algorithm for multi camera systemsusing a planar reference pattern is proposed the algorithmis an extension of sturm maybank zhang style plane basedcalibration technique for use with multiple camera rigiddisplacements between the camera are recovered a well asthe intrinsic parameter only by capturing with the camerasa model plane with known reference point placed at threeor more location thus the algorithm yield a simple calibrationmeans for stereo vision system with an arbitrarynumber of camera while maintaining the handiness andflexibility of the original method the algorithm is based onfactorization of homography matrix between the modeland image plane into the camera and plane parameter to compensate for the indetermination of scaling factor each homography matrix is rescaled by a double eigenvalueof a planar homology defined by two view and two modelplanes the obtained parameter are finally refined by anon linear maximum likelihood estimation mle process the validity of the proposed technique wa verified throughsimulation and experiment with real data 
this paper a security property that we believe ha boththese desired property we are currently demonstrating that the securitycan be proved byshowing that it hold of a system under development thegoal of this paper is to show that the security policy can be used whichwe demonstrate two way first we introduce some theorem similar towhat others have used to describe a separation kernel and prove that ourspecication implies theirs second we formalize an example applicationthat 
this study is the first to evaluate the performance benefit of using the recently proposed tcp splice kernel service in web proxy server previous study show that splicing client and server tcp connection in the ip layer improves the throughput of proxy server like firewall and content router by reducing the data transfer overhead in a web proxy server data transfer overhead represent a relatively large fraction of the request processing overhead in particular when content is not cacheable or the proxy cache is memory based the study is conducted with a socket level implementation of tcp splice compared to ip level implementation socket level implementation make possible the splicing of connection with different tcp characteristic and improve response time by reducing recovery delay after a packet loss the experimental evaluation is focused on http request type for which the proxy can fully exploit the tcp splice service which are the request for non cacheabl content and ssl tunneling the experimental testbed includes an emulated wan environment and benchmark application for http web client web server and web proxy running on aix r machine our experiment demonstrate that tcp splice enables reduction in cpu utilization of of the cpu depending on file size and request rate larger relative reduction are observed when tunneling ssl connection in particular for small file transfer response time are also reduced by up to sec 
we introduce population based markov chain monte carlo sampling algorithm that use proposal density obtained by a novel method direct search optimization technique downhill simplex method and differential evolution operate in real valued space using a population of state vector and geometric operation to generate proposal similar geometric proposal are used here for mcmc sampling but are modified to meet the strict requirement for unbiased sampling of the target density we 
this paper proposes a hybrid of hand crafted rule and a machine learning method for chunking korean in the partially free word order language such a korean and japanese a small number of rule dominate the performance due to their well developed postposition and ending thus the proposed method is primarily based on the rule and then the residual error are corrected by adopting a memory based machine learning method since the memory based learning is an efficient method to handle exception in natural language processing it is good at checking whether the estimate are exceptional case of the rule and revising them an evaluation of the method yield the improvement in f score over the rule or various machine learning method alone 
significant plasticity in sensory cortical representation can be driven in mature animal either by behavioural task that pair sensory stimulus with reinforcement or by electrophysiological experiment that pair sensory input with direct stimulation of neuromodulatory nucleus but usually not by sensory stimulus presented alone biologically motivated theory of representational learning however have tended to focus on unsupervised mechanism which may play a significant role on evolutionary or developmental timescales but which neglect this essential role of reinforcement in adult plasticity by contrast theoretical reinforcement learning ha generally dealt with the acquisition of optimal policy for action in an uncertain world rather than with the concurrent shaping of sensory representation this paper develops a framework for representational learning which build on the relative success of unsupervised generativemodelling account of cortical encoding to incorporate the effect of reinforcement in a biologically plausible way a remarkable feature of the brain is it ability to adapt to and learn from experience this learning ha measurable physiological correlate in term of change in the stimulusresponse property of individual neuron in the sensory system of the brain a well a in many other area while passive exposure to sensory stimulus can have profound effect on the developing sensory cortex significant plasticity in mature animal tends to be observed only in situation where sensory stimulus are associated with either behavioural or electrical reinforcement considerable theoretical attention ha been paid to unsupervised learning of representation adapted to natural sensory statistic and to the learning of optimal policy of action for decision process however relatively little work particularly of a biological bent ha sought to understand the impact of reinforcement task on representation to be complete understanding of sensory plasticity must come at two different level at a mechanistic level it is important to understand how synapsis are modified and how synaptic modification can lead to observed change in the response property of cell numerous experiment and model have addressed these question of how sensory plastic 
in recent year statistical language model are being proposed a alternative to the vector space model viewing document a language sample introduces the issue of defining a joint probability distribution over the term the present paper model a document a the result of a markov process it argues that this process is ergodic which is theoretically plausible and easy to verify in practice the theoretical result is that the joint distribution can be easily obtained this can also be applied for search resolution other than the document level we verified this in an experiment on query expansion demonstrating both the validity and the practicability of the method this hold a promise for general language model 
gaussian process regression allows a simple analytical treatment of exact bayesian inference and ha been found to provide good performance yet scale badly with the number of training data in this paper we compare several approach towards scaling gaussian process regression to large data set the subset of representers method the reduced rank approximation online gaussian process and the bayesian committee machine furthermore we provide theoretical insight into some of our experimental result we found that subset of representers method can give good and particularly fast prediction for data set with high and medium noise level on complex low noise data set the bayesian committee machine achieves significantly better accuracy yet at a higher computational cost 
in this paper we aim to recover the d shape of a human face using a single image we use a combination of symmetric shape from shading by zhao and chellappa and statistical approach for facial shape reconstruction by atick griffin and redlich given a single frontal image of a human face under a known directional illumination from a side we represent the solution a a linear combination of basis shape and recover the coefficient using a symmetry constraint on a facial shape and albedo by solving a single least square system of equation our algorithm provides a closed form solution which satisfies both symmetry and statistical constraint in the best possible way our procedure take only a few second account for varying facial albedo and is simpler than the previous method in the special case of horizontal illuminant direction our algorithm run even a fast a matrix vector multiplication 
privacy preserving data mining ha concentrated on obtaining valid result when the input data is private an extreme example is secure multiparty computation based method where only the result are revealed however this still leaf a potential privacy breach do the result themselves violate privacy this paper explores this issue developing a framework under which this question can be addressed metric are proposed along with analysis that those metric are consistent in the face of apparent problem 
a technical infrastructure for storing querying and managing rdfdata is a key element in the current semantic web development system like jena sesame or the ic forth rdf suite are widelyused for building semantic web application currently none ofthese system support the integrated querying of distributed rdf repository we consider this a major shortcoming since the semanticweb is distributed by nature in this paper we present an architecture for querying distributed rdf repository by extending the existing sesame system we discus the implication of our architectureand propose an index structure a well a algorithm forquery processing and optimization in such a distributed context 
current web search engine generally impose link analysis based re ranking on web page retrieval however the same technique when applied directly to small web search such a intranet and site search cannot achieve the same performance because their link structure are different from the global web in this paper we propose an approach to constructing implicit link by mining user access pattern and then apply a modified pagerank algorithm to re rank web page for small web search our experimental result indicate that the proposed method outperforms content based method by explicit link based pagerank by and directhit by respectively 
a data stream is a massive unbounded sequence of data element continuously generated at a rapid rate consequently the knowledge embedded in a data stream is more likely to be changed a time go by identifying the recent change of a data stream specially for an online data stream can provide valuable information for the analysis of the data stream in addition monitoring the continuous variation of a data stream enables to find the gradual change of embedded knowledge however most of mining algorithm over a data stream do not differentiate the information of recently generated transaction from the obsolete information of old transaction which may be no longer useful or possibly invalid at present this paper proposes a data mining method for finding recent frequent itemsets adaptively over an online data stream the effect of old transaction on the mining result of the data steam is diminished by decaying the old occurrence of each itemset a time go by furthermore several optimization technique are devised to minimize processing time a well a main memory usage finally the proposed method is analyzed by a series of experiment 
this paper present an algorithm to generate possible variant for biomedical term the algorithm give each variant it generation probability representing it plausibility which is potentially useful for query and dictionary expansion the probabilistic rule for generating variant are automatically learned from raw text using an existing abbreviation extraction technique our method therefore requires no linguistic knowledge or labor intensive natural language resource we conducted an experiment using medline abstract for rule induction and abstract for testing the result indicate that our method will significantly increase the number of retrieved document for long biomedical term 
a typical neuron in visual cortex receives most input from other cortical neuron with a roughly similar stimulus preference doe this arrangement of input allow efficient readout of sensory information by the target cortical neuron we address this issue by using simple modelling of neuronal population activity and information theoretic tool we find that efficient synaptic information transmission requires that the tuning curve of the afferent neuron is approximately a wide a the spread of stimulus preference of the afferent neuron reaching the target neuron by meta analysis of neurophysiological data we found that this is the case for cortico cortical input to neuron in visual cortex we suggest that the organization of v cortico cortical synaptic input allows optimal information transmission 
automated verification is one of the most successful application of automated reasoning in computer science in automated verification one us algorithmic technique to establish the correctness of the design with respect to a given property automated verification is based on a small number of key algorithmic idea tying together graph theory automaton theory and logic in this self contained talk i will describe how this holy trinity gave rise to automated verification tool and mention some application to planning 
the problem of selecting a subset of relevant feature in apotentially overwhelming quantity of data is classic and found inmany branch of science including example in computer vision text processing and more recently bio informatics are abundant inthis work we present a definition of relevancy based on spectralproperties of the affinity or laplacian of the feature measurement matrix the feature selection process is then based ona continuous ranking of the feature defined by a least squaresoptimization process a remarkable property of the featurerelevance function is that sparse solution for the ranking valuesnaturally emerge a a result of a biased non negativity of a keymatrix in the process a a result a simple least squaresoptimization process converges onto a sparse solution i e aselection of a subset of feature which form a local maximum overthe relevance function the feature selection algorithm can beembedded in both unsupervised and supervised inference problem andempirical evidence show that the feature selection typicallyachieve high accuracy even when only a small fraction of thefeatures are relevant 
a a large and complex application platform the world wide web is capable of delivering a broad range of sophisticated application however many web application go through rapid development phase with extremely short turnaround time making it difficult to eliminate vulnerability here we analyze the design of web application security assessment mechanism in order to identify poor coding practice that render web application vulnerable to attack such a sql injection and cross site scripting we describe the use of a number of software testing technique including dynamic analysis black box testing fault injection and behavior monitoring and suggest mechanism for applying these technique to web application real world situation are used to test a tool we named the web application vulnerability and error scanner wave an open source project available at http wave sourceforge net and to compare it with other tool our result show that wave is a feasible platform for assessing web application security 
the world wide web is opening up access to document and data for scholar however it ha not yet impacted on one of the primary activity in research assessing new finding in the light of current knowledge and debating it with colleague the claimaker system us a directed graph model with similarity to hypertext in which new idea are published a node which other contributor can build on or challenge in a variety of way by linking to them node and link have semantic structure to facilitate the provision of specialist service for interrogating and visualizing the emerging network by way of example this paper is grounded in a claimaker model to illustrate how new claim can be described in this structured way 
using a markov chain perspective of spectral clustering we present an algorithm to automatically find the number of stable cluster in a dataset the markov chain s behaviour is characterized by the spectral property of the matrix of transition probability from which we derive eigenflows along with their halflives an eigenflow describes the flow of probability mass due to the markov chain and it is characterized by it eigenvalue or equivalently by the halflife of it decay a the markov chain is iterated a ideal stable cluster is one with zero eigenflow and infinite half life the key insight in this paper is that bottleneck between weakly coupled cluster can be identified by computing the sensitivity of the eigenflow s halflife to variation in the edge weight we propose a novel eigencuts algorithm to perform clustering that remove these identified bottleneck in an iterative fashion 
psychophysical data suggest that temporal modulation of stimulus amplitude envelope play a prominent role in the perceptual segregation of concurrent sound in particular the detection of an unmodulated signal can be significantly improved by adding amplitude modulation to the spectral envelope of a competing masking noise this perceptual phenomenon is known a comodulation masking release cmr despite the obvious influence of temporal structure on the perception of complex auditory scene the physiological mechanism that contribute to cmr and auditory streaming are not well known a recent physiological study by nelken and colleague ha demonstrated an enhanced cortical representation of auditory signal in modulated noise our study evaluates these cmr like response pattern from the perspective of a hypothetical auditory edge detection neuron it is shown that this simple neural model for the detection of amplitude transient can reproduce not only the physiological data of nelken et al but also in light of previous result a variety of physiological and psychoacoustical phenomenon that are related to the perceptual segregation of concurrent sound 
we are creating an environment for investigating the role of advanced ai in interactive story based computer game this environment is based on the unreal tournament ut game engine and the soar ai engine unreal provides a d virtual environment while soar provides a flexible architecture for developing complex ai character this paper describes our progress to date starting with our game haunt which is designed so that complex ai character will be critical to the success or failure of the game it address design issue with constructing a plot for an interactive storytelling environment creating synthetic character for that environment and using a story director agent to tell the story with those character 
in the analysis of natural image gaussian scale mixture gsm have been used to account for the statistic of lter response and to inspire hierarchical cortical representational learning scheme gsms pose a critical assignment problem working out which lter response were generated by a common multiplicative factor we present a new approach to solving this assignment problem through a probabilistic extension to the basic gsm and show how to perform inference in the model using gibbs sampling we demonstrate the efcac y of the approach on both synthetic and image data understanding the statistical structure of natural image is an important goal for visual neuroscience neural representation in early cortical area decompose image and likely other sensory input in a way that is sensitive to sophisticated aspect of their probabilistic structure this structure also play a key role in method for image processing and coding a striking aspect of natural image that ha reections in both top down and bottom up modeling is coordination across nearby location scale and orientation from a topdown perspective this structure ha been modeled using what is known a a gaussian scale mixture model gsm gsms involve a multi dimensional gaussian each dimension of which capture local structure a in a linear lter multiplied by a spatialized collection of common hidden scale variable or mixer variable which capture the coordination gsms have wide implication in theory of cortical receptive eld development eg the comprehensive bubble framework of hyv arinen the mixer variable provide the top down account of two bottom up characteristic of natural image statistic namely the bowtie statistical dependency and the fact that the marginal distribution of receptive eld lik e lters have high kurtosis in hindsight these idea also bear a close relationship with ruderman and bialek s multiplicative bottom up image analysis framework and statistical model for divisive gain control coordinated structure ha also been addressed in other image work and in other domain such a speech and nance 
we propose a fast iterative classification algorithm for kernel fisher discriminant kfd using heterogeneous kernel model in contrast with the standard kfd that requires the user to predefine a kernel function we incorporate the task of choosing an appropriate kernel into the optimization problem to be solved the choice of kernel is defined a a linear combination of kernel belonging to a potentially large family of different positive semidefinite kernel the complexity of our algorithm doe not increase significantly with respect to the number of kernel on the kernel family experiment on several benchmark datasets demonstrate that generalization performance of the proposed algorithm is not significantly different from that achieved by the standard kfd in which the kernel parameter have been tuned using cross validation we also present result on a real life colon cancer dataset that demonstrate the efficiency of the proposed method 
we introduce a metagrammar which allows u to automatically generate from a single and compact metagrammar hierarchy parallel lexical functional grammar lfg and tree adjoining grammar tag for french and for english the grammar writer specifies in compact manner syntactic property that are potentially framework and to some extent language independent such a subcategorization valency alternation and realization of syntactic function from which grammar for several framework and language are automatically generated offline 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
we introduce nashprop an iterative and local message passing algorithm for computing nash equilibrium in multi player game represented by arbitrary undirected graph we provide a formal analysis and experimental evidence demonstrating that nashprop performs well on large graphical game with many loop often converging in just a dozen iteration on graph with hundred of node nashprop generalizes the tree algorithm of kearns et al and can be viewed a similar in spirit to belief propagation in probabilistic inference and thus complement the recent work of vickrey and koller who explored a junction tree approach thus a for probabilistic inference we have at least two promising general purpose approach to equilibrium computation in graph 
we propose a spectral partitioning approach for large scaleoptimization problem specifically structure from motion in structure from motion partitioning method reduce theproblem into smaller and better conditioned subproblemswhich can be efficiently optimized our partitioning methoduses only the hessian of the reprojection error and it eigenvector we show that partitioned system that preserve theeigenvectors corresponding to small eigenvalue result inlower residual error when optimized we create partitionsby clustering the entry of the eigenvectors of the hessiancorresponding to small eigenvalue this is a more generaltechnique than relying on domain knowledge and heuristicssuch a bottom up structure from motion approach simultaneously it take advantage of more information thangeneric matrix partitioning algorithm 
in this paper we propose a unified framework for automatic evaluation of nlp application using n gram co occurrence statistic the automatic evaluation metric proposed to date for machine translation and automatic summarization are particular instance from the family of metric we propose we show that different member of the same family of metric explain best the variation obtained with human evaluation according to the application being evaluated machine translation automatic summarization and automatic question answering and the evaluation guideline used by human for evaluating such application 
recent experiment have indicated the possibility to use the brain electrical activity to directly control the movement of robotics or prosthetic device in this paper we report result with a portable non invasive brain computer interface that make possible the continuous control of a mobile robot in a house like environment the interface us surface electrode to measure electroencephalogram eeg signal from which a statistical classifier recognizes different mental state until now brain actuated control of robot ha relied on invasive approach requiring surgical implantation of electrode since eeg based system have been considered too slow for controlling rapid and complex sequence of movement here we show that after a few day of training two human subject successfully moved a robot between several room by mental control only furthermore mental control wa only marginally worse than manual control on the same task 
this paper present an action selection framework based on an assemblage of self organizing neural network called cooperative extended kohonen map this framework encapsulates two feature that significantly enhance a robot s action selection capability self organization in the continuous state and action space to provide smooth efficient and fine motion control action selection via the cooperation and competition of extended kohonen map to achieve more complex motion task qualitative and quantitative comparison for singleand multi robot task show our framework can provide better action selection than do potential field method 
the decision function constructed by support vector machine svm s usually depend only on a subset of the training set the so called support vector we derive asymptotically sharp lower and upper bound on the number of support vector for several standard type of svm s in particular we show for the gaussian rbf kernel that the fraction of support vector tends to twice the bayes risk for the l svm to the probability of noise for the l svm and to for the l svm 
we describe the ra scanner a novel system for the examination of patient suffering from rheumatoid arthritis the ra scanner is based on a novel laser based imaging technique which is sensitive to the optical characteristic of finger joint tissue based on the laser image finger joint are classified according to whether the inflammatory status ha improved or worsened to perform the classification task various linear and kernel based system were implemented and their performance were compared special emphasis wa put on measure to reliably perform parameter tuning and evaluation since only a very small data set wa available based on the result presented in this paper it wa concluded that the ra scanner permit a reliable classification of pathological finger joint thus paving the way for a further development from prototype to product stage 
many application of natural language processing technology involve analyzing text that concern the psychological state and process of people including their belief goal prediction explanation and plan in this paper we describe our effort to create a robust large scale lexical semantic resource for the recognition and classification of expression of commonsense psychology in english text we achieve high level of precision and recall by hand authoring set of local grammar for commonsense psychology concept and show that this approach can achieve classification performance greater than that obtained by using machine learning technique we demonstrate the utility of this resource for large scale corpus analysis by identifying reference to adversarial and competitive goal in political speech throughout u s history 
this paper describes an automatic content indexing system for news program with a special emphasis on it segmentation process the process can successfully segment an entire news program into topic centered news story the primary tool is a linguistic topic segmentation algorithm experiment show that the resulting speech based segment are fairly accurate and scene change point supplied by an external video processor can be of help in improving segmentation effectiveness 
standard approach to object detection focus on local patch of the image and try to classify them a background or not we propose to use the scene context image a a whole a an extra source of global information to help resolve local ambiguity we present a conditional random field for jointly solving the task of object detection and scene classification 
we analyzes category score algorithm for k nn classifier found in the literature including majority voting algorithm mva simple sum algorithm ssa mva and ssa are two mainly used algorithm to estimate score for candidate category in k nn classifier system based on the hypothesis that utilization of internal relation between document and category could improve system performance two new weighting score model concept based weighting cbw score model and term independence based weighting ibw score model are proposed our experimental result confirm our hypothesis and show that in the term of precision average ibw and cbw are better than the other score model while ssa is higher than mva according to macro average f cbw performs best rocchio based algorithm rba always performs worst 
wordnet similarity is a freely available software package that make it possible to measure the semantic similarity or relatedness between a pair of concept or word sens it provides six measure of similarity and three measure of relatedness all of which are based on the lexical database wordnet these measure are implemented a perl module which take a input two concept and return a numeric value that represents the degree to which they are similar or related 
starting from first principle we re visit the statistical approach and study two form of the bayes decision rule the common rule for minimizing the number of string error and a novel rule for minimizing the number of symbol error the bayes decision rule for minimizing the number of string error is widely used e g in speech recognition po tagging and machine translation but it justification is rarely questioned to minimize the number of symbol error a is more suitable for a task like po tagging we show that another form of the bayes decision rule can be derived the major purpose of this paper is to show that the form of the bayes decision rule should not be taken for granted a it is done in virtually all statistical nlp work but should be adapted to the error measure being used we present first experimental result for po tagging task 
many tool have been developed to help user query extract and integrate data from web page generated dynamically from database i e from the hidden web a key prerequisite for such tool is to obtain the schema of the attribute of the retrieved data in this paper we describe a system called dela which reconstructs part of a hidden back end web database it doe this by sending query through html form automatically generating regular expression wrapper to extract data object from the result page and restoring the retrieved data into an annotated labelled table the whole process need no human involvement and prof to be fast le than one minute for wrapper induction for each site and accurate over correctness for data extraction and around correctness for label assignment 
we present earchivarius an interactive system for accessing collection of electronic mail the system combine search clustering visualization and time based visualization of email message and people who send or received the message 
we propose a unified approach for summarization based on the analysis of video structure and video highlight our approach emphasizes both the content balance and perceptual quality of a summary normalized cut algorithm is employed to globally and optimally partition a video into cluster a motion attention model based on human perception is employed to compute the perceptual quality of shot and cluster the cluster together with the computed attention value form a temporal graph similar to markov chain that inherently describes the evolution and perceptual importance of video cluster in our application the flow of a temporal graph is utilized to group similar cluster into scene while the attention value are used a guideline to select appropriate subshots in scene for summarization 
we prove generalization error bound for predicting entry in a partially observed matrix by approximating the observed entry with a low rank matrix to do so we bound the number of sign configuration of lowrank matrix using a result about realizable oriented matroids 
a new measure anchormap is introduced to evaluate how close two document retrieval ranking are to each other it is shown that anchormap score when run on a set of initial ranked document list from different system are very highly correlated with categorization of topic a easy or hard and separately are highly correlated with those topic on which blind feedback work in another experiment anchormap is used to compare the initial ranked document list from a single system against the ranked document list from that system after blind feedback again high anchormap value are highly correlated with both topic difficulty and successful application of blind feedback both experiment are example of using property of a topic which are independent of relevance information to predict the actual performance of ir system on the topic initial experiment to attempt to improve retrieval performance based upon anchormap failed the cause for failure are discussed 
current ontological specification for semantically describing property of web service are limited to their static interface description normally for proving property of service composition mapping input output parameter and specifying the pre post condition are found to be sufficient however these property are assertion only on the initial and final state of the service respectively they do not help in specifying verifying ongoing behaviour of an individual service or a composed system we propose a framework for enriching semantic service description with two compositional assertion assumption and commitment that facilitate reasoning about service composition and verification of their integration the technique is based on interval temporal logic itl a sound formalism for specifying and proving temporal property of system our approach utilizes the recently proposed semantic web rule language 
the performance of many image analysis task depend on the image resolution at which they are applied traditionally resolution selection method rely on spatial derivative of image intensity differential measurement however are sensitive to noise and are local they cannot characterize pattern such a texture which are defined over extensive image region in this work we present a novel tool for resolution selection that considers sufficiently large image region and is robust to noise it is based on the generalized entropy of the histogram of an image at multiple resolution we first examine in general the variation of histogram entropy with image resolution then we examine the sensitivity of this variation for shape and texture in an image finally we discus the significance of resolution of maximum histogram entropy it is shown that computing feature at these resolution increase the discriminability between image it is also shown that maximum histogram entropy value can be used to improve optical flow estimate for block based algorithm in image sequence with a changing zoom factor 
we propose a probabilistic generative account of configura l learning phenomenon in classical conditioning configural learning e xperiments probe how animal discriminate and generalize between pattern of simultaneously presented stimulus such a tone and light that are differentially predictive of reinforcement previous model of these issue have been successful more on a phenomenological than an explanatory level they reproduce experimental finding but lacking fo rmal foundation provide scant basis for understanding why animal behave a they do we present a theory that clarifies seemingly arbitrary a pects of previous model while also capturing a broader set of data key pattern of data e g concerning animal readiness to distinguish pattern with varying degree of overlap are shown to follow from statistical inference 
abstract in slow feature analysis sfa it ha been demonstrated thathigh order invariant property can be extracted by projecting inputsinto a nonlinear space and computing the slowest changingfeatures in this space this ha been proposed a a simple generalmodel for learning nonlinear invariance in the visual system however this method is highly constrained by the curse of dimensionalitywhich limit it to simple theoretical simulation this paperdemonstrates that by using a 
device mismatch in vlsi degrades the accuracy of analog arithmetic circuit and lower the learning performance of large scale neural network implemented in this technology we show compact low power on chip calibration technique that compensate for device mismatch our technique enable large scale analog vlsi neural network with learning performance on the order of bit we demonstrate our technique on a synapse linear perceptron learning with the least mean square lm algorithm and fabricated in a m cmos process 
in this paper we present a generative model for textured motionphenomena such a falling snow wavy river and dancing grass etc firstly we represent an image a a linear superposition of imagebases selected from a generic and over complete dictionary thedictionary contains gabor base for point particle element andfourier base for wave element these base compete to explain theinput image the transform from a raw image to a base or a tokenrepresentation lead to large dimension reduction secondly weintroduce a unified motion equation to characterize the motion ofthese base and the interaction between wave and particle e g a ball floating on water we use statistical learning algorithm toidentify the structure of moving object and their trajectoriesautomatically then novel sequence can be synthesized easily fromthe motion and image model thirdly we replace the dictionary ofgabor and fourier base with symbolic sketch also base withthe same image and motion model we can render realistic andstylish cartoon animation in our view cartoon and sketch aresymbolic visualization of the inner representation for visualperception the success of the cartoon animation in turn suggeststhat our image and motion model capture the essence of visualperception of textured motion 
we investigate algebraic logical and geometric property of concept recognized by various class of probabilistic classifier for this we introduce a natural hierarchy of probabilistic classifier the lowest level of which comprises the naive bayesian classifier we show that the expressivity of classifier on the different level in the hierarchy is characterized algebraically by separability with polynomial of different degree a consequence of this result is that every linearly separable concept can be recognized by a naive bayesian classifier we contrast this result with negative result about the naive bayesian classifier previously reported in the literature and point out that these result only pertain to specific learning scenario for naive bayesian classifier we also present some logical and geometric characterization of linearly separable concept thus providing additional intuitive insight into what concept are recognizable by naive bayesian classifier 
reflecting the rapid growth in the utilization of large test collection for information retrieval since the s extensive comparative experiment have been performed to explore the effectiveness of various retrieval model however most collection were intended for retrieving newspaper article and technical abstract in this paper we describe the process of producing a test collection for patent retrieval the ntcir patent retrieval collection which includes two year of japanese patent application and topic produced by professional patent searcher we also report experimental result obtained by using this collection to re examine the effectiveness of existing retrieval model in the context of patent retrieval the relative superiority among existing retrieval model did not significantly differ depending on the document genre that is patent and newspaper article issue related to patent retrieval are also discussed 
this paper address the problem of automatically structuring heterogenous document collection by using clustering method in contrast to traditional clustering we study restrictive method and ensemble based meta method that may decide to leave out some document rather than assigning them to inappropriate cluster with low confidence these technique result in higher cluster purity better overall accuracy and make unsupervised self organization more robust our comprehensive experimental study on three different real world data collection demonstrate these benefit the proposed method seem particularly suitable for automatically substructuring personal email folder or personal web directory that are populated by focused crawler and they can be combined with supervised classification technique 
a standard intuition underlying traditional account of belief change is the principle of minimal change in this paper we introduce a novel account of belief change in which the agent s belief state is modified minimally to incorporate exactly the new information thus a revision by p q will result in a new belief state in which p q is believed but a stronger proposition such a p q is not regardless of the initial form of the belief state 
this paper present a novel method for detecting vehicle asobstacles in various road scene using a single on board camera vehicle are detected by testing whether the motion of a set ofthree horizontal line segment which are always on the vehicle satisfies the motion constraint of the ground plane or that of thesurface plane of the vehicle the motion constraint of each planeis derived from the projective invariant combined with thevanishing line of the plane that is a prior knowledge of roadscenes the proposed method is implemented into a newly developedon board lsi experimental result for real road scene undervarious condition show the effectiveness of the proposed method 
collaborative filtering ha been very successful in both research and application such a information filtering and e commerce the k nearest neighbor knn method is a popular way for it realization it key technique is to find k nearest neighbor for a given user to predict his interest however this method suffers from two fundamental problem sparsity and scalability in this paper we present our solution for these two problem we adopt two technique a matrix conversion method for similarity measure and an instance selection method and then we present an improved collaborative filtering algorithm based on these two method in contrast with existing collaborative algorithm our method show it satisfactory accuracy and performance 
in this paper sparse representation factorization of a data matrix is first discussed an overcomplete basis matrix is estimated by using the k mean method we have proved that for the estimated overcomplete basis matrix the sparse solution coefficient matrix with minimum l norm is unique with probability of one which can be obtained using a linear programming algorithm the comparison of the l norm solution and the l norm solution are also presented which can be used in recoverability analysis of blind source separation bs next we apply the sparse matrix factorization approach to bs in the overcomplete case generally if the source are not sufficiently sparse we perform blind separation in the time frequency domain after preprocessing the observed data using the wavelet packet transformation third an eeg experimental data analysis example is presented to illustrate the usefulness of the proposed approach and demonstrate it performance two almost independent component obtained by the sparse representation method are selected for phase synchronization analysis and their period of significant phase synchronization are found which are related to task finally concluding remark review the approach and state area that require further study 
the problem of tracking a varying number of non rigid object ha two major diculties first the observation model and target distribution can be highly non linear and non gaussian second the presence of a large varying number of object creates complex interaction with overlap and ambiguity to surmount these diculties we introduce a vision system that is capable of learning detecting and tracking the object of interest the system is demonstrated in the context of tracking hockey player using video sequence our approach combine the strength of two successful algorithm mixture particle lters and adaboost the mixture particle lter is ideally suited to multi target tracking a it assigns a mixture component to each player the crucial design issue in mixture particle lters are the choice of the proposal distribution and the treatment of object leaving and entering the scene here we construct the proposal distribution using a mixture model that incorporates information from the dynamic model of each player and the detection hypothesis generated by adaboost the learned adaboost proposal distribution allows u to quickly detect player entering the scene while the ltering process enables u to keep track of the individual player the result of interleaving adaboost with mixture particle lters is a simple yet powerful and fully automatic multiple object tracking system 
in the semantic web architecture web ontology language arebuilt on top of rdf s however serious difficulty have arisen when trying to layer expressive ontology language like owl on top of rdf schema although these problem can be avoided owl andthe whole semantic web architecture becomes much more complex than it should be in this paper a possible simplification of thesemantic web architecture is suggested which ha several import antadvantages with respect to the layering currently accepted by the w c ontology working group 
cost sensitive learning address the issue of classification in the presence of varying cost associated with different type of misclassification in this paper we present a method for solving multi class cost sensitive learning problem using any binary classification algorithm this algorithm is derived using hree key idea iterative weighting expanding data space and gradient boosting with stochastic ensemble we establish some theoretical guarantee concerning the performance of this method in particular we show that a certain variant posse the boosting property given a form of weak learning assumption on the component binary classifier we also empirically evaluate the performance of the proposed method using benchmark data set and verify that our method generally achieves better result than representative method for cost sensitive learning in term of predictive performance cost minimization and in many case computational efficiency 
this paper is about non approximate acceleration of high dimensional nonparametric operation such a k nearest neighbor classifier and the prediction phase of support vector machine classifier we a ttempt to exploit the fact that even if we want exact answer to nonparametric query we usually do not need to explicitly find the datapoi nt close to the query but merely need to ask question about the property about that set of datapoints this offer a small amount of computational leeway and we investigate how much that leeway can be exploited for clarity this paper concentrate on pure k nn classification and the prediction phase of svms we introduce new ball tree algorithm that on real world datasets give acceleration of fold up to fold compared against highly optimized traditional ball tree based k nn these result include datasets with up to dimension and record and show non trivial speedup while giving exact answer 
content based music genre classification is a fundamental component of music information retrieval system and ha been gaining importance and enjoying a growing amount of attention with the emergence of digital music on the internet currently little work ha been done on automatic music genre classification and in addition the reported classification accuracy are relatively low this paper proposes a new feature extraction method for music genre classification dwchs dwchs stand for daubechies wavelet coefficient histogram dwchs capture the local and global information of music signal simultaneously by computing histogram on their daubechies wavelet coefficient effectiveness of this new feature and of previously studied feature are compared using various machine learning classification algorithm including support vector machine and linear discriminant analysis it is demonstrated that the use of dwchs significantly improves the accuracy of music genre classification 
recent algorithm provide powerful solution to the problem of determining cost minimizing or revenue maximizing allocation of item in combinatorial auction however in many setting criterion other than cost e g the number of winner the delivery date of item etc are also relevant in judging the quality of an allocation furthermore the bid taker is usually uncertain about her preference regarding tradeoff between cost and nonprice feature we describe new method that allow the bid taker to determine approximately optimal allocation despite this these method rely on the notion of minimax regret to guide the elicitation of preference from the bid taker and to measure the quality of an allocation in the presence of utility function uncertainty computational experiment demonstrate the practicality of minimax computation and the efficacy of our elicitation technique 
abstract specular reflection present di culties for many area of computer vision such a stereo and segmentation to separate specu lar and difiuse reflection component previous approach generally re quire accurate segmentation regionally uniform reflectance or structured lighting to overcome these limiting assumption we propose a method based on color analysis and multibaseline stereo that simultaneously e timates the separation and the true depth of specular reflection first pixel with a specular component are detected by a novel form of color histogram difierencing that utilizes the epipolar constraint this process us relevant data from all the stereo image for robustness and ad dress the problem of color occlusion based on the lambertian model of difiuse reflectance stereo correspondence is then employed to compute for specular pixel their corresponding difiuse component in other view the result of color based detection aid the stereo correspondence which determines both separation and true depth of specular pixel our ap proach integrates color analysis and multibaseline stereo in a synergistic manner to yield accurate separation and depth a demonstrated by our result on synthetic and real image sequence 
estimation of camera motion and structure of rigid object in the d world from multiple camera image by bundle adjustment is often performed by iterative minimization method due to their low computational effort these method need a robust initialization in order to converge to the global minimum in this paper a new criterion for keyframe selection is presented while state of the art criterion just avoid degenerated camera motion configuration the proposed criterion selects the keyframe pairing with the lowest expected estimation error of initial camera motion and object structure the presented result show that the convergence probability of bundle adjustment is significantly improved with the new criterion compared to the state of the art approach 
this paper describes discriminative language modeling for a large vocabulary speech recognition task we contrast two parameter estimation method the perceptron algorithm and a method based on conditional random field crfs the model are encoded a deterministic weighted finite state automaton and are applied by intersecting the automaton with word lattice that are the output from a baseline recognizer the perceptron algorithm ha the benefit of automatically selecting a relatively small feature set in just a couple of pass over the training data however using the feature set output from the perceptron algorithm initialized with their weight crf training provides an additional reduction in word error rate for a total absolute reduction from the baseline of 
integrity constraint are an essential part of modern schema definition language they are useful for semantic specification update consistency control query optimization etc in this paper we propose ucm a model of integrity constraint for xml that is both simple and expressive because it relies on a single notion of key and foreign key the ucm model is easy to use and make formal reasoning possible because it relies on a powerful type system the ucm model is expressive capturing in a single framework the constraint found in relational database object oriented schema and xml document type definition we study the problem of consistency of ucm constraint the interaction between constraint and subtyping and algorithm for implementing these constraint 
we will present a novel two step fuzzy translation technique for cross lingual spelling variant in the first stage transformation rule are applied to source word to render them more similar to their target language equivalent the rule are generated automatically using translation dictionary a source data in the second stage the intermediate form obtained in the first stage are translated into a target language using fuzzy matching the effectiveness of the technique wa evaluated empirically using five source language and english a a target language the target word list contained english word with the correct equivalent for the source word among them the source word were translated using the two step fuzzy translation technique and the result were compared with those of plain fuzzy matching based translation the combined technique performed better sometimes considerably better than fuzzy matching alone 
this paper describes progress towards a general framework for incorporating multimodal cue into a trainable system for automatically annotating user defined semantic concept in broadcast video model of arbitrary concept are constructed by building classifier in a score space defined by a pre deployed set of multimodal model result show annotation for user defined concept both in and outside the pre deployed set is competitive with our best video only model on the trec video corpus an interesting side result show speech only model give performance comparable to our best video only model for detecting visual concept such a outdoors face and cityscape 
an open problem in reinforcement learning is discovering hierarchical structure hexq an algorithm which automatically attempt to decompose and solve a model free factored mdp hierarchically is described by searching for aliased markov sub space region based on the state variable the algorithm us temporal and state abstraction to construct a hierarchy of interlinked smaller mdps 
many technique for complex speech processing such a denoising and deconvolution time frequency warping multiple speaker separation and multiple microphone analysis operate on sequence of short time power spectrum spectrogram a representation which is often well suited to these task however a significant problem with algorithm that manipulate spectrogram is that the output spectrogram doe not include a phase component which is needed to create a time domain signal that ha good perceptual quality here we describe a generative model of time domain speech signal and their spectrogram and show how an efficient optimizer can be used to find the maximum a posteriori speech signal given the spectrogram in contrast to technique that alternate between estimating the phase and a spectrally consistent signal our technique directly infers the speech signal thus jointly optimizing the phase and a spectrally consistent signal we compare our technique with a standard method using signal to noise ratio but we also provide audio file on the web for the purpose of demonstrating the improvement in perceptual quality that our technique offer 
this paper report on the user centered design methodology and technique used for the elicitation of user requirement and how these requirement informed the first phase of the user interface design for a cross language information retrieval system we describe a set of factor involved in analysis of the data collected and finally discus the implication for user interface design based on the finding 
in this paper we present a learning approach to the scenario template task of information extraction where information filling one template could come from multiple sentence when tested on the muc task our learning approach achieves accuracy competitive to the best of the muc system which were all built with manually engineered rule our analysis reveals that our use of full parsing and state of the art learning algorithm have contributed to the good performance to our knowledge this is the first research to have demonstrated that a learning approach to the full scale information extraction task could achieve performance rivaling that of the knowledge engineering approach 
current description logic reasoning system provide only limited support for debugging logically erroneous knowledge base in this paper we propose new non standard reasoning service which we designed and implemented to pinpoint logical contradiction when developing the medical terminology dice we provide complete algorithm for unfoldable acc tboxes based on minimisation of axiom using boolean method for minimal unsatisfiability presening sub tboxes and an incomplete bottom up method for generalised incoherence preserving terminology 
cortical synaptic plasticity depends on the relative timing of preand postsynaptic spike and also on the temporal pattern of presynaptic spike and of postsynaptic spike we study the hypothesis that cortical synaptic plasticity doe not associate individual spike but rather whole firing episode and depends only on when these episode start and how long they last but a little a possible on the timing of individual spike here we present the mathematical background for such a study standard method from hidden markov model are used to define what firing episode are estimating the probability of being in such an episode requires not only the knowledge of past spike but also of future spike we show how to construct a causal learning rule which depends only on past spike but associate preand postsynaptic firing episode a if it also knew future spike we also show that this learning rule agrees with some feature of synaptic plasticity in superficial layer of rat visual cortex froemke and dan nature 
directed graphical model with one layer of observed random variable and one or more layer of hidden random variable have been the dominant modelling paradigm in many research field although this approach ha met with considerable success the causal semantics of these model can make it difficult to infer the posterior distribution over the hidden variable in this paper we propose an alternative two layer model based on exponential family distribution and the semantics of undirected model inference in these exponential family harmonium is fast while learning is performed by minimizing contrastive divergence a member of this family is then studied a an alternative probabilistic model for latent semantic indexing in experiment it is shown that they perform well on document retrieval task and provide an elegant solution to searching with keywords 
abstract to human computer interaction and force u to think innew way about how computer could be used in daily life face to face communication is a real time process operatingat a a time scale in the order of millisecond thelevel of uncertainty at this time scale is considerable makingit necessary for human and machine to rely on sensoryrich perceptual primitive rather than slow symbolic inferenceprocesses in this paper we present progress on onesuch perceptual primitive the 
xml ha become one of the core technology for contemporary business application especially web based application to facilitate processing of diverse xml data we propose an extensible integrated xml processing architecture the xml virtual machine xvm which connects xml data with their behavior at the same time the xvm is also a framework for developing and deploying xml based application using component based technique the xvm support arbitrary granularity and provides a high degree of modularity and reusability xvm component are dynamically loaded and composed during xml data processing using the xvm both client side and server side xml application can be developed and deployed in an integrated way we also present an xml application container built on top of the xvm along with several sample application to demonstrate the applicability of the xvm framework 
a user acquire or gain access to an increasingly diverse range of web access client web application are adapting their user interface to support multiple modality on multiple client type user experience can be enhanced by client with differing capability combining to provide a distributed user interface to application indeed user will be frustrated if their interaction with application is limited to one client at a time this paper discus the requirement for coordinating web interaction across an aggregation of client we present a framework for multi device browsing that provides both coordinated navigation between web resource and coordinated interaction between variant or representation of those resource once instantiated in the client the framework protects the application from some of the complexity of client aggregation we show how a small number of enhancement to the xforms and xml event vocabulary can facilitate coordination between client and provide an appropriate level of control to application we also describe a novel proxy which consolidates http request from aggregation of client and reduces the burden that multi client browsing place on the application 
the problem of denoising image is one of the most important and widely studied problem in image processing and computer vision various image filtering strategy based on linear system statistic information theory and variational calculus have been effective but invariably make strong assumption about the property of the signal and or noise therefore they lack the generality to be easily applied to new application or diverse image collection this paper describes a novel unsupervised information theoretic adaptive filter uinta that improves the predictability of pixel intensity from their neighborhood by decreasing the joint entropy between them in this way uinta automatically discovers the statistical property of the signal and can thereby reduce noise in a wide spectrum of image and application the paper describes the formulation required to minimize the joint entropy measure present several important practical consideration in estimating image region statistic and then present a series of result and comparison on both real and synthetic data 
a linear model tree is a decision tree with a linear functional model in each leaf previous model tree induction algorithm have operated on the entire training set however there are many situation when an incremental learner is advantageous in this paper we demonstrate that model tree can be induced incrementally using an algorithm that scale linearly with the number of example an incremental node splitting rule is presented together with incremental method for stopping the growth of the tree and pruning empirical testing in three domain where the emphasis is on learning a dynamic model of the environment show that the algorithm can learn a more accurate approximation from fewer example than other incremental method in addition the induced model are smaller and the learner requires le prior knowledge about the domain 
speculative execution of information gathering plan can dramatically reduce the effect of source i o latency on overall performance however the utility of speculation is closely tied to how accurately data value are predicted at runtime caching 
mark recapture model have for many year been used to estimate the unknown size of animal and bird population in this article we adapt a finite mixture mark recapture model in order to estimate the number of active telephone line in the usa the idea is to use the calling pattern of line that are observed on the long distance network to estimate the number of line that do not appear on the network we present a bayesian approach and use markov chain monte carlo method to obtain inference from the posterior distribution of the model parameter at the state level our result are in fairly good agreement with recent published report on line count for line that are easily classified a business or residence the estimate have low variance when the classification is unknown the variability increase considerably result are insensitive to change in the prior distribution we discus the significant computational and data mining challenge caused by the scale of the data approximately million call detail record per day observed over a number of week 
the tangential neuron in the fly brain are sensitive to the typical optic flow pattern generated during self motion in this study we examine whether a simplified linear model of these neuron can be used to estimate self motion from the optic flow we present a theory for the construction of an estimator consisting of a linear combination of optic flow vector that incorporates prior knowledge both about the distance distribution of the environment and about the noise and self motion statistic of the sensor the estimator is tested on a gantry carrying an omnidirectional vision sensor the experiment show that the proposed approach lead to accurate and robust estimate of rotation rate whereas translation estimate turn out to be le reliable a recent study ha shown that a simplified computational model of the tangential neuron a a weighted sum of flow measurement wa able to reproduce the observed response field the weight were chosen according to an optimality principle which minimizes the output variance of the model caused by noise and distance variability between different scene the question on how the output of such processing unit could be used for self motion estimation wa left open however in this paper we want to fill a part of this gap by presenting a classical linear estimation approach that extends a special case of the previous model to the complete self motion problem we again use linear combination of local flow measurement but instead of prescribing a fixed motion axis and minimizing the output variance we require that the quadratic error in the estimated self motion parameter be a small a possible from this 
the area under the roc curve auc ha been advocated a an evaluation criterion for the bipartite ranking problem we study large deviation property of the auc in particular we derive a distribution free large deviation bound for the auc which serf to bound the expected accuracy of a ranking function in term of it empirical auc on an independent test sequence a comparison of our result with a corresponding large deviation result for the classification error rate suggests that the test sample size required to obtain an accurate estimate of the expected accuracy of a ranking function with confidence is larger than that required to obtain an accurate estimate of the expected error rate of a classification function with the same confidence a simple application of the union bound allows the large deviation bound to be extended to learned ranking function chosen from finite function class 
this paper introduces a rule based context dependent word clustering method with the rule derived from various domain database and the word text orthographic property besides significant dimensionality reduction our experiment show that such rule based word clustering improves by the overall accuracy of extracting bibliographic field from reference and by on average the class specific performance on the line classification of document header 
the problem of approximating the product of several gaussian mixture distribution arises in a number of context including the nonparametric belief propagation nbp inference algorithm and the training of product of expert model this paper develops two multiscale algorithm for sampling from a product of gaussian mixture and compare their performance to existing method the first is a multiscale variant of previously proposed monte carlo technique with comparable theoretical guarantee but improved empirical convergence rate the second make use of approximate kernel density evaluation method to construct a fast approximate sampler which is guaranteed to sample point to within a tunable parameter of their true probability we compare both multiscale sampler on a set of computational example motivated by nbp demonstrating significant improvement over existing method 
reinforcement learning ha been used for training game playing agent the value function for a complex game must be approximated with a continuous function because the number of state becomes too large to enumerate temporal difference learning with self play is one method successfully used to derive the value approximation function coevolution of the value function is also claimed to yield good result this paper report on a direct comparison between an agent trained to play gin rummy using temporal difference learning and the same agent trained with co evolution coevolution produced superior result 
this paper introduces an approach for identifying predictive structure in relational data using the multiple instance framework by a predictive structure we mean a structure that can explain a given labeling of the data and can predict label of unseen data multiple instance learning ha previously only been applied to flat or propositional data and we present a modification to the framework that allows multiple instance technique to be used on relational data we present experimental result using a relational modification of the diverse density method maron maron lozano perez and of a method based on the chi squared statistic mcgovern jensen we demonstrate that multipleinstance learning can be used to identify predictive structure on both a small illustrative data set and the internet movie database we compare the classification result to a k nearest neighbor approach 
in this paper we present a family of algorithm that can simultaneously align and cluster set of multidimensional curve defined on a discrete time grid our approach us the expectation maximization em algorithm to recover both the mean curve shape for each cluster and the most likely shift offset and cluster membership for each curve we demonstrate how bayesian estimation method can improve the result for small sample size by enforcing smoothness in the cluster mean curve we evaluate the methodology on two real world data set time course gene expression data and storm trajectory data experimental result show that model that incorporate curve alignment systematically provide improvement in predictive power and within cluster variance on test data set the proposed approach provides a non parametric computationally efficient and robust methodology for clustering broad class of curve data 
we propose an answer to the what is ai question namely that al is really or at least really ought in significant part to be psychometric ai pai along the way we set out and rebut five objection to pai describe peri a robot in our lab who exemplifies pai and briefly treat the future of psychometric ai first by pointing toward some promising pai based application and then by raising some of the big philosophical question the success of psychometric ai will raise 
the web ha established itself a the dominant medium for doing electronic commerce realizing that it global reach provides significant market and business opportunity service provider both large and small are advertising their service on the web a number of them operate their own web site promoting their service at length while others are merely listed in a referral site aggregating all of the provider into a queriable service directory make it easy for customer to locate the one most suited for his her need yellowpager is a tool for creating service directory by mining web source service directory created by yellowpager have several merit compared to those generated by existing practice which typically require participation by service provider e g verizon s superyellowpages com firstly the information content will be rich secondly since the process is automated and repeatable the content can always be kept current finally the same process can be readily adapted to different domain yellowpager build service directory by mining the web through a combination of keyword based search engine web agent text classifier and novel extraction algorithm the extraction is driven by a service ontology consisting of a taxonomy of service concept and their associated attribute such a name and address and type description for the attribute in addition the ontology also associate an extractor function with each attribute applying the function to a web page will identify all the occurrence of the attribute in that page yellowpager s mining algorithm consists of a training step followed by classification and extraction step in the training step a classifier is trained to identify web page relevant to the service of interest the classification step proceeds by doing a search for the particular service of interest using a keyword based web search engine and retrieves all the matching web page from these page the relevant one are identified using the classifier the final step is extraction of attribute value associated with the service from these page each web page is parsed into a dom tree and the extractor function are applied all of the attribute corresponding to a service provider are then correctly aggregated this can pose difficulty especially in the presence of multiple service provider in a page using a novel concept of scoring and conflict resolution to prevent erroneous association of attribute with service provider entity in the page the algorithm aggregate all the attribute occurrence correctly the extractor function may not be complete in the sense that it cannot always identify all the attribute in a page by exploiting the regularity of the sequence in which attribute occurr in referral page the mining algorithm automatically learns generalized pattern to locate attribute that the extractor function miss the distinguishing aspect of yellowpager s extraction algorithm are i it is unsupervised and ii the attribute value in the page are extracted independent of any page specific relationship that may exist among the markup tag yellowpager ha been used by a large pet food producer to build a directory of veterinarian service provider in the united state the resulting database wa found to be much larger and richer than that found in vetquest vetworld and the super yellow page yellowpager is implemented in java and is interfaced to rainbow a library utility in c that is used for classification the tool will demonstrate the creation of a service directory for any service domain by mining web source 
we investigate how stack filter function class like weighted order statistic can be applied to classification problem this lead to a new design criterion for linear classifier when input are binary valued and weight are positive we present a rank based measure of margin that is directly optimized a a standard linear program and investigate it relationship to regularization our approach can robustly combine large number of base hypothesis and ha similar performance to other type of regularization 
dominant set are a new graph theoretic concept that ha proven to be relevant in pairwise data clustering problem such a image segmentation they generalize the notion of a maximal clique to edgeweighted graph and have intriguing non trivial connection to continuous quadratic optimization and spectral based grouping we address the problem of grouping out of sample example after the clustering process ha taken place this may serve either to drastically reduce the computational burden associated to the processing of very large data set or to efficiently deal with dynamic situation whereby data set need to be updated continually we show that the very notion of a dominant set offer a simple and efficient way of doing this numerical experiment on various grouping problem show the effectiveness of the approach 
controlling the sensing of an environment by an agent ha been accepted a necessary for effective operation within most practical domain usually however agent operate in partially observable domain where not all parameter of interest are accessible to direct sensing in such circumstance sensing action must be chosen for what they will reveal indirectly through an axiomatized model of the domain causal structure including ramification this article show how sensing can be chosen so a to acquire and use indirectly obtained information to meet goal not otherwise possible classical logic event calculus is extended with both a knowledge formalism and causal ramification and is used to show how inferring unknown information about a domain lead to conditional sensing action 
indexing and ranking are two key factor for efficient and effective xml information retrieval inappropriate indexing may result in false negative and false positive and improper ranking may lead to low precision in this paper we propose a configurable xml information retrieval system in which user can configure appropriate index type for xml tag and text content based on user index configuration the system transforms xml structure into a compact tree representation ctree and index xml text content to support xml ranking we propose the concept of weighted term frequency and inverted element frequency where the weight of a term depends on it frequency and location within an xml element a well a it popularity among similar element in an xml dataset we evaluate the effectiveness of our system through extensive experiment on the inex dataset and content and structure ca topic the experimental result reveal that our system ha significantly high precision at low recall region and achieves the highest average precision a compared with official inex submission using the strict evaluation metric 
we interpret non negative matrix factorization geometrically a the problem of finding a simplicial cone which contains a cloud of data point and which is contained in the positive orthant we show that under certain condition basically requiring that some of the data are spread across the face of the positive orthant there is a unique such simplicial cone we give example of synthetic image articulation database which obey these condition these require separated support and factorial sampling for such database there is a generative model in term of part and nmf correctly identifies the part we show that our theoretical result are predictive of the performance of published nmf code by running the published algorithm on one of our synthetic image articulation database 
we describe a new family of topic ranking algorithm for multi labeled document the motivation for the algorithm stem from recent advance in online learning algorithm the algorithm we present are simple to implement and are time and memory efficient we evaluate the algorithm on the reuters corpus and the new corpus released by reuters in on both corpus the algorithm we present outperform adaptation to topic ranking of rocchio s algorithm and the perceptron algorithm we also outline the formal analysis of the algorithm in the mistake bound model to our knowledge this work is the first to report performance result with the entire new reuters corpus 
this paper present how the development of a polynomial ordering andthe verification of it property can be fit in the framework of acl 
the main conclusion from the metric based evaluation of video retrieval system at trec s video track is that non interactive image retrieval from general collection using visual information only is not yet feasible we show how a detailed analysis of retrieval result looking beyond mean average precision map score on topical relevance give significant insight in the main problem with the visual part of the retrieval model under study such an analytical approach prof an important addition to standard evaluation measure 
we consider object recognition a the process of attaching meaningful label to specic region of an image and propose a model that learns spatial relationship between object given a set of image and their associated text e g keywords caption description the objective is to segment an image in either a crude or sophisticated fashion then to nd the proper association between word and region previous model are limited by the scope of the representation in particular they fail to exploit spatial context in the image and word we develop a more expressive model that take this into account we formulate a spatially consistent probabilistic mapping between continuous image feature vector and the supplied word token by learning both word to region association and object relation the proposed model augments scene segmentation due to smoothing implicit in spatial consistency context introduces cycle to the undirected graph so we cannot rely on a straightforward implementation of the em algorithm for estimating the model parameter and density of the unknown alignment variable instead we develop an approximate em algorithm that us loopy belief propagation in the inference step and iterative scaling on the pseudo likelihood approximation in the parameter update step the experiment indicate that our approximate inference and learning algorithm converges to good local solution experiment on a diverse array of image show that spatial context considerably improves the accuracy of object recognition most signican tly spatial context combined with a nonlinear discrete object representation allows our model to cope well with over segmented scene 
we describe a model for creating word to word and phrase to phrase alignment between document and their human written abstract such alignment are critical for the development of statistical summarization system that can be trained on large corpus of document abstract pair our model which is based on a novel phrase based hmm outperforms both the cut paste alignment model jing and model developed in the context of machine translation brown et al 
we investigate a number of simple method for improving the word alignment accuracy of ibm model we demonstrate reduction in alignment error rate of approximately resulting from giving extra weight to the probability of alignment to the null word smoothing probability estimate for rare word and using a simple heuristic estimation method to initialize or replace em training of model parameter 
web community are web virtual broadcasting space where people can freely discus anything while such community function a discussion board they have even greater value a large repository of archived information in order to unlock the value of this resource we need an effective mean for searching archived discussion thread unfortunately the technique that have proven successful for searching document collection and the web are not ideally suited to the task of searching archived community discussion in this paper we explore the problem of creating an effective ranking function to predict the most relevant message to query in community search we extract a set of predictive feature from the thread tree of newsgroup message a well a feature of message author and lexical distribution within a message thread our final result indicate that when using linear regression with this feature set our search system achieved a performance improvement compared to our baseline system 
abstract in this paper we show that it is possible to model sensory impressionsof consumer about beef meat this is not a straightforward task thereason is that when we are aiming to induce a function that map objectdescriptions into rating we must consider that consumer rating arejust a way to express their preference about the product presented inthe same testing session therefore we had to use a special purposesvm polynomial kernel the training data set used collect the 
like many purely data driven machine learning method support vector machine svm classifier are learned exclusively from the evidence presented in the training dataset thus a larger training dataset is required for better performance in some application there might be human knowledge available that in principle could compensate for the lack of data in this paper we propose a simple generalization of svm weighted margin svm wmsvms that permit the incorporation of prior knowledge we show that sequential minimal optimization can be used in training wmsvm we discus the issue of incorporating prior knowledge using this rather general formulation the experimental result show that the proposed method of incorporating prior knowledge is effective 
in this work we describe a novel statistical video representationand modeling scheme unsupervised clusteringvia gaussian mixture modeling extract coherent spacetimeregions in feature space and corresponding coherentsegments video region in the video content a key featureof the system is the analysis of video input a a singleentity a opposed to a sequence of separate frame spaceand time are treated uniformly the extracted space timeregions allow for the detection and 
in this paper we present a method that integrates cue from shading shadow and specular reflection for estimating directional illumination in a textured scene texture pose a problem for lighting estimation since texture edge can be mistaken for change in illumination condition and unknown variation in albedo make reflectance model fitting impractical unlike previous work which all assume known or uniform reflectance our method can deal with the effect of texture by capitalizing on physical consistency that exist among the lighting cue since scene texture do not exhibit such coherence we use this property to minimize the influence of texture on illumination direction estimation for the recovered light source direction a technique for estimating their intensity in the presence of texture is also proposed 
this paper discus the specific of planning in multiagent environment it present the formal framework mapl maple for describing multiagent planning domain mapl allows to describe both qualitative and quantitative temporal relation among event thus subsuming the temporal model of both pddl and pop other feature are different level of control over action modeling of agent ignorance of fact and plan synchronization with communicative action for single agent planning in multi agent domain we present a novel forward search algorithm synthesizing mapl s partially ordered temporal plan finally we present a general distributed algorithm scheme for solving mapl problem with several coordinating planner these different contribution are intended a a step towards a simple yet expressive standard for the description of multiagent planning domain and algorithm such a standard could in the future allow cross evaluation of multi agent planning algorithm on standardized benchmark 
if two translation system differ differ in performance on a test set can we trust that this indicates a difference in true system quality to answer this question we describe bootstrap resampling method to compute statistical significance of test result and validate them on the concrete example of the bleu score even for small test size of only sentence our method may give u assurance that test result difference are real 
feature selection a a preprocessing step to machine learning ha been eective in reducing dimensionality removing irrelevant data increasing learning accuracy and improving comprehensibility however the recent increase of dimensionality of data pose a severe challenge to many existing feature selection method with respect to eciency and eectiveness in this work we introduce a novel concept predominant correlation and propose a fast filter method which can identify relevant feature a well a redundancy among relevant feature without pairwise correlation analysis the eciency and eectiveness of our method is demonstrated through extensive comparison with other method using real world data of high dimensionality 
mobile device have already been widely used to access the web however because most available web page are designed for desktop pc in mind it is inconvenient to browse these large web page on a mobile device with a small screen in this paper we propose a new browsing convention to facilitate navigation and reading on a small form factor device a web page is organized into a two level hierarchy with a thumbnail representation at the top level for providing a global view and index to a set of sub page at the bottom level for detail information a page adaptation technique is also developed to analyze the structure of an existing web page and split it into small and logically related unit that fit into the screen of a mobile device for a web page not suitable for splitting auto positioning or scrolling by block is used to assist the browsing a an alterative our experimental result show that our proposed browsing convention and developed page adaptation scheme greatly improve the user s browsing experience on a device with a small display 
this paper provides a blueprint for constructing collaborative and distributed knowledge discovery system within grid based computing environment the need for such system is driven by the quest for sharing knowledge information and computing resource within the boundary of single large distributed organisation or within complex virtual organisation vo created to tackle specific project the proposed architecture is built on top of a resource federation management layer and is composed of a set of different resource we show how this architecture will behave during a typical kdd process design and deployment how it enables the execution of complex and distributed data mining task with high performance and how it provides a community of e scientist with mean to collaborate retrieve and reuse both kdd algorithm discovery process and knowledge in a visual analytical environment 
clustering time series is a problem that ha application in a wide variety of field and ha recently attracted a large amount of research in this paper we focus on clustering data derived from autoregressive moving average arma model using k mean and k medoids algorithm with the euclidean distance between estimated model parameter we justify our choice of clustering technique and distance metric by reproducing result obtained in related research our research aim is to ass the affect of discretising data into binary sequence of above and below the median a process known a clipping on the clustering of time series it is known that the fitted ar parameter of clipped data tend asymptotically to the parameter for unclipped data we exploit this result to demonstrate that for long series the clustering accuracy when using clipped data from the class of arma model is not significantly different to that achieved with unclipped data next we show that if the data contains outlier then using clipped data produce significantly better clustering we then demonstrate that using clipped series requires much le memory and operation such a distance calculation can be much faster finally we demonstrate these advantage on three real world data set 
this paper describes a method for learning the countability preference of english noun from raw text corpus the method map the corpus attested lexico syntactic property of each noun onto a feature vector and us a suite of memory based classifier to predict membership in countability class we were able to assign countability to english noun with a precision of 
ordering information is a critical task for natural language generation application in this paper we propose an approach to information ordering that is particularly suited for text to text generation we describe a model that learns constraint on sentence order from a corpus of domain specific text and an algorithm that yield the most likely order among several alternative we evaluate the automatically generated ordering against authored text from our corpus and against human subject that are asked to mimic the model s task we also ass the appropriateness of such a model for multidocument summarization 
we present a linguistically motivated algorithm for reconstructing nonlocal dependency in broad coverage context free parse tree derived from treebanks we use an algorithm based on loglinear classifier to augment and reshape context free tree so a to reintroduce underlying nonlocal dependency lost in the context free approximation we find that our algorithm compare favorably with prior work on english using an existing evaluation metric and also introduce and argue for a new dependency based evaluation metric by this new evaluation metric our algorithm achieves error reduction on gold standard input tree and error reduction on state of the art machine parsed input tree when compared with the best previous work we also present the first result on non local dependency reconstruction for a language other than english comparing performance on english and german our new evaluation metric quantitatively corroborates the intuition that in a language with freer word order the surface dependency in context free parse tree are a poorer approximation to underlying dependency structure 
several technique have been developed for recovering reflectanceproperties of real surface under unknown illumination condition however in most case those technique assume that the lightsources are located at inifinity which cannot be applied to forexample photometric modeling of indoor environment in thispaper we propose two method to estimate the surface reflectanceproperty of an object a well a the position of a light sourcefrom a single image without the distant illumination assumption given a color image of an object with specular reflection a aninput the first method estimate the light source position byfitting to the lambertian diffuse component while separating thespecular and diffuse component by using an iterative relaxationscheme moreover we extend the above method by using a singlespecular image a an input thus removing it constraint on thediffuse reflectance property and the number of light source thismethod simultaneously recovers the reflectance property and thelight source position by optimizing the linearity of alog transformed torrance sparrow model by estimating the object sreflectance property and the light source position we can freelygenerate synthetic image of the target object under arbitrarysource direction and source surface distance 
in the authorship verification problem we are given example of the writing of a single author and are asked to determine if given long text were or were not written by this author we present a new learning based method for adducing the depth of difference between two example set and offer evidence that this method solves the authorship verification problem with very high accuracy the underlying idea is to test the rate of degradation of the accuracy of learned model a the best feature are iteratively dropped from the learning process 
the distributional principle according to which morpheme that occur in identical context belong in some sense to the same category ha been advanced a a mean for extracting syntactic structure from corpus data we extend this principle by applying it recursively and by using mutual information for estimating category coherence the resulting model learns in an unsupervised fashion highly structured distributed representation of syntactic knowledge from corpus it also exhibit promising behavior in task usually thought to require representation anchored in a grammar such a systematicity 
co clustering is a powerful data mining technique with varied application such a text clustering microarray analysis and recommender system recently an information theoretic co clustering approach applicable to empirical joint probability distribution wa proposed in many situation co clustering of more general matrix is desired in this paper we present a substantially generalized co clustering framework wherein any bregman divergence can be used in the objective function and various conditional expectation based constraint can be considered based on the statistic that need to be preserved analysis of the co clustering problem lead to the minimum bregman information principle which generalizes the maximum entropy principle and yield an elegant meta algorithm that is guaranteed to achieve local optimality our methodology yield new algorithm and also encompasses several previously known clustering and co clustering algorithm based on alternate minimization 
exploiting unannotated natural language data is hard largely because unsupervised parameter estimation is hard we describe deterministic annealing rose et al a an appealing alternative to the expectation maximization algorithm dempster et al seeking to avoid search error da begin by globally maximizing an easy concave function and maintains a local maximum a it gradually morphs the function into the desired non concave likelihood function applying da to parsing and tagging model is shown to be straightforward significant improvement over em are shown on a part of speech tagging task we describe a variant skewed da which can incorporate a good initializer when it is available and show significant improvement over em on a grammar induction task 
this demonstration will describe how timber a native xml database system ha been extended with the capability to answer xml style structured query e g xquery with embedded ir style keyword based non boolean condition with the original structured query processing engine and the ir extension built into the system timber is well suited for efficiently and effectively processing query with both structural and textual content constraint 
we introduce a novel active learning scenario in which a user want to work with a learning algorithm to identify useful anomaly these are distinguished from the traditional statistical denition of anomaly a outlier or merely ill modeled point our distinction is that the usefulness of anomaly is categorized subjectively by the user we make two additional assumption first there exist extremely few useful anomaly to be hunted down within a massive dataset second both useful and useless anomaly may sometimes exist within tiny class of similar anomaly the challenge is thus to identify rare category record in an unlabeled noisy set with help in the form of class label from a human expert who ha a small budget of datapoints that they are prepared to categorize we propose a technique to meet this challenge which assumes a mixture model t to the data but otherwise make no assumption on the particular form of the mixture component this property promise wide applicability in real life scenario and for various statistical model we give an overview of several alternative method highlighting their strength and weakness and conclude with a detailed empirical analysis we show that our method can quickly zoom in on an anomaly set containing a few ten of point in a dataset of hundred of thousand 
motivation modern sequencing technology now permit the sequencing of entire genome leading to thousand of new gene sequence in need of detailed annotation it is too time consuming to predict the property of each protein sequence manually and to organize the result of many prediction tool by hand the prediction process must be automated so the prediction can be automatically organized but the prediction must also be transparent that is the rationale for each prediction should be easily examinable by anyone that wish to use the prediction result proteome analyst pa is a web based system for predicting the property of each protein in a proteome pa ha three interesting feature first it provides a single web based tool that allows the user to select a wide range of analytic tool and automatically apply them to each protein in a proteome in essence pa provides one stop automatic high throughput analysis second pa ha the ability to explain it prediction to user pa is based on established machine learning technique but make every prediction transparent to it user third pa allows user to easily create their own transparent custom predictor without programming availability http www c ualberta ca bioinfo pa supplementary information http www c ualberta ca bioinfo pa walkthrough http www c ualberta ca bioinfo pa experiment contact bioinfo c ualberta ca 
in this paper we introduce an efficient replanning algorithm for nondeterministic domain namely what we believe to be the first incremental heuristic minimax search algorithm we apply it to the dynamic discretization of continuous domain resulting in an efficient implementation of the parti game reinforcement learning algorithm for control in high dimensional domain 
many real world classification task involve the prediction of multiple inter dependent class label a prototypical case of this sort deal with prediction of a sequence of label for a sequence of observation such problem arise naturally in the context of annotating and segmenting observation sequence this paper generalizes gaussian process classification to predict multiple label by taking dependency between neighboring label into account our approach is motivated by the desire to retain rigorous probabilistic semantics while overcoming limitation of parametric method like conditional random field which exhibit conceptual and computational difficulty in high dimensional input space experiment on named entity recognition and pitch accent prediction task demonstrate the competitiveness of our approach 
the intelligent tutoring system autotutor us latent semantic analysis to evaluate student answer to the tutor s question by comparing a student s answer to a set of expected answer the system determines how much information is covered and how to continue the tutorial despite the success of lsa in tutoring conversation the system sometimes ha difficulty determining at an early stage whether or not an expectation is covered a new lsa algorithm significantly improves the precision of autotutor s natural language understanding and can be applied to other natural language understanding application 
we consider the problem of computing low rank approximation of matrix the novelty of our approach is that the low rank approximation are on a sequence of matrix unlike the problem of low rank approximation of a single matrix which wa well studied in the past the proposed algorithm in this paper doe not admit a closed form solution in general we did extensive experiment on face image data to evaluate the effectiveness of the proposed algorithm and compare the computed low rank approximation with those obtained from traditional singular value decomposition based method 
this paper investigates the level of metadata accuracy required for image filter to be valuable to user access to large digital image and video collection is hampered by ambiguous and incomplete metadata attributed to imagery though improvement are constantly made in the automatic derivation of semantic feature concept such a indoor outdoor face and cityscape it is unclear how good these improvement should be and under what circumstance they are effective this paper explores the relationship between metadata accuracy and effectiveness of retrieval using an amateur photo collection documentary video and news video the accuracy of the feature classification is varied from performance typical of automated classification today to ideal performance taken from manually generated truth data result establish an accuracy threshold at which semantic feature can be useful and empirically quantify the collection size when filtering first show it effectiveness 
a challenging problem for spoken dialog system is the design of utterance generation module that are fast flexible and general yet produce high quality output in particular domain a promising approach is trainable generation which us general purpose linguistic knowledge automatically adapted to the application domain this paper present a trainable sentence planner for the match dialog system we show that trainable sentence planning can produce output comparable to that of match s template based generator even for quite complex information presentation 
coalition formation is a key problem in automated negotiation among self interested agent and other electronic commerce application a coalition of agent can sometimes accomplish thing that the individual agent cannot or can do thing more efficiently however motivating the agent to abide to a solution requires careful analysis only some of the solution are stable in the sense that no group of agent is motivated to break off and form a new coalition this constraint ha been studied extensively in cooperative game theory however the computational question around this constraint have received le attention when it come to coalition formation among software agent that represent real world party these question become increasingly explicit in this paper we define a concise general representation for game in characteristic form that relies on superadditivity and show that it allows for efficient checking of whether a given outcome is in the core we then show that determining whether the core is nonempty is np complete both with and without transferable utility we demonstrate that what make the problem hard in both case is determining the collaborative possibility the set of outcome possible for the grand coalition by showing that if these are given the problem becomes tractable in both case however we then demonstrate that for a hybrid version of the problem where utility transfer is possible only within the grand coalition the problem remains np complete even when the collaborative possibility are given 
this paper present a new approach to imaging thatsignificantly enhances the dynamic range of a camera the key idea is to adapt the exposure of each pixel onthe image detector based on the radiance value of thecorresponding scene point this adaptation is done inthe optical domain that is during image formation inpractice this is achieved using a spatial light modulatorwhose transmittance can be varied with high resolutionover space and time a real time control algorithm isdeveloped that us acquired image to automaticallyadjust the transmittance function of the spatial modulator each captured image and it corresponding transmittance function are used to compute a very high dynamic range image that is linear in scene radiance we have implemented a video rate adaptive dynamicrange camera that consists of a color ccd detector anda controllable liquid crystal light modulator experiment have been conducted in scenario with complexand harsh lighting condition the result indicate thatadaptive imaging can have a significant impact on visionapplications such a monitoring tracking recognition and navigation 
the family of vickrey clarke grove vcg mechanism is arguably the most celebrated achievement in truthful mechanism design however vcg mechanism have their limitation they only apply to optimization problem with a utilitarian objective function and their output should optimize the objective function for many optimization problem finding the optimal output is computationally intractable if we apply vcg mechanism to polynomialtime algorithm that approximate the optimal solution the resulting mechanism may no longer be truthful in light of these limitation it is useful to study whether we can design a truthful non vcg payment scheme that is computationally tractable for a given output method o in this paper we focus our attention on binary demand game in which the agent only available action are to take part in the a game or not to for these problem we prove that a truthful mechanism m o p exists with proper payment method p if and only if o satisfies a certain monotone property we also provide several general algorithm to compute the payment efficiently for various type of output in particular we show how a truthful payment can be computed through or and combination round based combination and some more complex combination of output from subgames 
in traditional peer to peer search network operation focus on properly labeled file such a music or video and the actual search is often limited to text tag the explosive growth of available multimedia document in recent year call for more flexible search capability namely search by content most content based search algorithm are computationally intensive making them inappropriate for a peer to peer environment in this paper we discus a content based music retrieval algorithm that can be decomposed and parallelized efficiently we present a peer to peer architecture for such a system that make use of spare resource among subscriber with protocol that dynamically redistribute load in order to maximize throughput and minimize inconvenience to subscriber our framework can be extended beyond the music retrieval domain and adapted to other scenario where resource pooling is desired a long a the underlying algorithm satisfies certain condition 
this paper present semframe a system that induces frame semantic verb class from wordnet and ldoce semantic frame are thought to have significant potential in resolving the paraphrase problem challenging many language based application when compared to the handcrafted framenet semframe achieves it best recall precision balance with recall based on semframe s coverage of framenet frame and precision based on semframe verb semantic relatedness to frame evoking verb the next best performing semantic verb class achieve recall and precision 
in this paper we focus on methodology of finding a classifier with a minimal cost in presence of additional performance constraint rocch analysis where accuracy and cost are intertwined in the solution space wa a revolutionary tool for two class problem we propose an alternative formulation a an optimization problem commonly used in operation research this approach extends the rocch analysis to allow for locating optimal solution while outside constraint are present similarly to the rocch analysis we combine cost and class distribution while defining the objective function rather than focusing on slope of the edge in the convex hull of the solution space however we treat cost a an objective function to be minimized over the solution space by selecting the best performing classifier s one or more vertex in the solution space the linear programming framework provides a theoretical and computational methodology for finding the vertex classifier which minimizes the objective function 
knapsack constraint are a key modeling structure in discrete optimization and form the core of many real life problem formulation only recently a cost based filtering algorithm for knapsack constraint wa published that is based on some previously developed approximation algorithm for the knapsack problem in this paper we provide an empirical evaluation of approximated consistency for knapsack constraint by applying it to the market split problem and the automatic recording problem 
the general motor variation reduction adviser is a knowledge system built on case based reasoning principle that is currently in use in a dozen general motor assembly center this paper review the overall characteristic of the system and then focus on various ai element critical to support it deployment to a production system a key ai enabler is ontology guided search using domain specific ontology 
video based handwritten character recognition vcr system is a new type of character recognitionsystem with many unique advantage over on linecharacter recognition system it main problem is toeffectively extract stroke dynamic information fromvideo data for character recognition in this paper wepropose a new stroke extraction algorithm throughdynamic stroke information analysis for a vcr system the experimental result on over video charactersequences show that our system can extract the chinesecharacter stroke dynamic information similar to an on line system 
in this paper we introduce a set of novel distance metric that use model based representation for trajectory we determine the similarity of trajectory using the conformity of the corresponding hmm model these metric enable the comparison of trajectory without any limitation of the conventional measure they accurately identify the coordinate orientation and speed affinity the proposed hmm based distance metric can be used not only for ground truth comparison but for clustering a well our experiment prove that they have superior discriminative property 
pan tilt camera are often used a component of wide areasurveillance system it is necessary to calibrate these camera inrelation to one another in order to obtain a consistentrepresentation of the entire space existing method forcalibrating pan tilt camera have assumed an idealized model ofcamera mechanic in addition most method have been calibratedusing only asmall range of camera motion this paper present amethod for calibrating pan tilt camera that introduces a morecomplete model of camera motion pan and tilt rotation are modeledas occurring around arbitrary ax in space in addition the widearea surveillance system itself is used to build a large virtualcalibration object resulting in better calibration than would bepossible with a single small calibration target finally theproposed enhancement are validated experimentally withcomparisons showing the improvement provided over more traditionalmethods 
in this paper we study the problem of applying data mining to facilitate the investigation of money laundering crime mlcs we have identified a new paradigm of problem that of automatic community generation based on uni party data the data in which there is no direct or explicit link information available consequently we have proposed a new methodology for link discovery based on correlation analysis ldca we have used mlc group model generation a an exemplary application of this problem paradigm and have focused on this application to develop a specific method of automatic mlc group model generation based on timeline analysis using the ldca methodology called coral a prototype of coral method ha been implemented and preliminary testing and evaluation based on a real mlc case data are reported the contribution of this work are identification of the uni party data community generation problem paradigm proposal of a new methodology ldca to solve for problem in this paradigm formulation of the mlc group model generation problem a an example of this paradigm application of the ldca methodology in developing a specific solution coral to the mlc group model generation problem and development evaluation and testing of the coral prototype in a real mlc case data 
we present a novel bayesian approach to the problem of value function estimation in continuous state space we deflne a probabilistic generative model for the value function by imposing a gaussian prior over value function and assuming a gaussian noise model due to the gaussian nature of the random process involved the posterior distribution of the value function is also gaussian and is therefore described entirely by it mean and covariance we derive exact expression for the posterior process moment and utilizing an e cient sequential sparsiflcation method we describe an on line algorithm for learning them we demonstrate the operation of the algorithm on a dimensional continuous spatial navigation domain 
in this paper we describe a news story gisting system that generates a word short summary of a news story this system us a machine learning technique to combine linguistic statistical and positional information in order to generate an appropriate summary we also present the result of an automatic evaluation of this system with respect to the performance of other baseline summarisers using the new rouge evaluation metric 
this paper describes a pedestrian detection system that integratesimage intensity information with motion information we use a detection style algorithm that scan a detectorover two consecutive frame of a video sequence thedetector is trained using adaboost to take advantage ofboth motion and appearance information to detect a walkingperson past approach have built detector based onmotion information or detector based on appearance information but ours is the first to combine both source ofinformation in a single detector the implementation describedruns at about frame second detects pedestriansat very small scale a small a x pixel and ha avery low false positive rate our approach build on the detection work of viola andjones novel contribution of this paper include i developmentof a representation of image motion which is extremelyefficient and ii implementation of a state of theart pedestrian detection system which operates on low resolutionimages under difficult condition such a rain andsnow 
this paper describes a pedestrian detection system that integrates image intensity information with motion information we use a detection style algorithm that scan a detector over two consecutive frame of a video sequence the detector is trained using adaboost to take advantage of both motion and appearance information to detect a walking person past approach have built detector based on motion information or detector based on appearance information but ours is the first to combine both source of information in a single detector the implementation described run at about frame second detects pedestrian at very small scale a small a x pixel and ha a very low false positive rate our approach build on the detection work of viola and jones novel contribution of this paper include i development of a representation of image motion which is extremely efficient and ii implementation of a state of the art pedestrian detection system which operates on low resolution image under difficult condition such a rain and snow 
abstract this paper present a kernel method that allows to combine colorand shape information for appearance based object recognition itdoesn t require to dene a new common representation but use thepower of kernel to combine dierent representation together in aneective manner these result are achieved using result of statisticalmechanics of spin glass combined with markov randomeldsvia kernel function experiment show an increase in recognitionrate up to with 
the design of cooperative multi robot system is a highly active research area in robotics two line of research in particular have generated interest the solution of large weakly coupled mdps and the design and implementation of market architecture we propose a new algorithm which join together these two line of research for a class of coupled mdps our algorithm automatically design a market architecture which cause a decentralized multi robot system to converge to a consistent policy we can show that this policy is the same a the one which would be produced by a particular centralized planning algorithm we demonstrate the new algorithm on three simulation example multi robot towing multi robot path planning with a limited fuel resource and coordinating behavior in a game of paint ball 
this paper describes an efficient method for learning the parameter of a gaussian process gp the parameter are learned from multiple task which are assumed to have been drawn independently from the same gp prior an efficient algorithm is obtained by extending the informative vector machine ivm algorithm to handle the multi task learning case the multi task ivm mtivm save computation by greedily selecting the most informative example from the separate task the mt ivm is also shown to be more efficient than random sub sampling on an artificial data set and more effective than the traditional ivm in a speaker dependent phoneme recognition task 
finding effective method for developing an ensemble of model ha been an active research area of large scale data mining in recent year model learned from data are often subject to some degree of uncertainty for a variety of resoans in classification ensemble of model provide a useful mean of averaging out error introduced by individual classifier hence reducing the generalization error of prediction the plurality voting method is often chosen for bagging because of it simplicity of implementation however the plurality approach to model reconciliation is ad hoc there are many other voting method to choose from including the anti plurality method the plurality method with elimination the borda count method and condorcet s method of pairwise comparison any of these could lead to a better method for reconciliation in this paper we analyze the use of these voting method in model reconciliation we present empirical result comparing performance of these voting method when applied in bagging these result include some surprise and among other thing suggest that plurality is not always the best voting method the number of class can affect the performance of voting method and the degree of dataset noise can affect the performance of voting method while it is premature to make final judgment about specific voting method the result of this work raise interesting question and they open the door to the application of voting theory in classification theory 
we present a generative model and stochastic filtering algor ithm for si multaneous tracking of d position and orientation non rigid motion object texture and background texture using a single camera we show that the solution to this problem is formally equivalent to stochastic fil tering of conditionally gaussian process a problem for which well known approach exist we propose an approach based on monte carlo sampling of the nonlinear component of the process object mo tion and exact filtering of the object and background textur e given the sampled motion the smoothness of image sequence in time and space is exploited by using laplace s method to generate proposal distribution for importance sampling the resulting inference algorithm encom pass both optic flow and template based tracking a specia l case and elucidates the condition under which these method are optimal we demonstrate an application of the system to d non rigid face tracking 
we present a language independent and unsupervised algorithm for the segmentation of word into morphs the algorithm is based on a new generative probabilistic model which make use of relevant prior information on the length and frequency distribution of morphs in a language our algorithm is shown to outperform two competing algorithm when evaluated on data from a language with agglutinative morphology finnish and to perform well also on english data 
convex programming involves a convex set f rn and a convex cost function c f r the goal of convex programming is to nd a point in f which minimizes c in online convex programming the convex set is known in advance but in each step of some repeated optimization problem one must select a point inf before seeing the cost function for that step this can be used to model factory production farm production and many other industrial optimization problem where one is unaware of the value of the item produced until they have already been constructed we introduce an algorithm for this domain we also apply this algorithm to repeated game and show that it is really a generalization of innitesimal gradient ascent and the result here imply that generalized innitesimal gradient ascent giga is universally consistent 
abstract the goal of low level vision is to estimate an underlying scene given an observed image real world scene eg albedo or shape can be very complex conventionally requiring high dimensional representation which are hard to estimate and store we propose a low dimensional rep resentation called a scene recipe that relies on the image itself to de scribe the complex scene configuration shape recipe are an example these are the regression coefficient that predict the bandpassed shape from image data we describe the benefit of this representation and show two us illustrating their property we improve stereo shape estimate by learning shape recipe at low resolution and applying them at full resolution shape recipe implicitly contain information about lighting and material and we use them for material segmentation 
this paper present a dependency language model dlm that capture linguistic constraint via a dependency structure i e a set of probabilistic dependency that express the relation between headword of each phrase in a sentence by an acyclic planar undirected graph our contribution are three fold first we incorporate the dependency structure into an n gram language model to capture long distance word dependency second we present an unsupervised learning method that discovers the dependency structure of a sentence using a bootstrapping procedure finally we evaluate the proposed model on a realistic application japanese kana kanji conversion experiment show that the best dlm achieves an error rate reduction over the word trigram model 
there is an evident correlation between semantic distance and creativity in the treatment of metaphor when the tenor and vehicle concept of a metaphor are semantic neighbor the local structure of the taxonomy can be used to constrain any interpretation and thus significantly curtail the breadth of the search space this reliance on taxonomy make wordnet ideally suited to the treatment of local metaphor however the distance involved in creative metaphor mean that the tenor and vehicle rarely belong to the same semantic category and interpretation must involve more than a simple recognition of a common taxonomic parent because creative metaphor effectively involves reconceptualization interpretation must instead exploit the internal relational structure of the tenor and vehicle in this paper we describe how certain key element of the qualia structure of a concept pertaining to it agentive and telic property can be automatically extracted from the sense gloss in wordnet 
this paper address the problem of untangling hidden graph from a set of noisy detection of undirected edge we present a model of the generation of the observed graph that includes degree based structure prior on the hidden graph exact inference in the model is intractable we present an efficien t approximate inference algorithm to compute edge appearance posterior we evaluate our model and algorithm on a biological graph inference problem 
in the wrapper approach for feature selection a popular criterion used is the leave one out estimate of the classification error while being relatively unbiased the leave one out error estimate is nonetheless known to exhibit a large variance which can be detrimental especially for small sample we propose reducing it variance i e smoothing at two level at the first level we smooth the error count using estimate of posterior probability while at the second level we smooth the posterior probability estimate themselves using bayesian estimation with conjugate prior furthermore we propose using the jackknife to reduce the bias inherent in bayesian estimator we then show empirically that smoothing the error estimate give improved performance in feature selection 
making sense of large amount of unlabeled data is hard but this is what we are up against in many real life situation our key observation is that in many real life application additional partial information about the data can be obtained with very little cost for example in video indexing we may want to use the fact that a sequence of face obtained from successive frame in roughly the same location are likely to contain the same unknown individual similarly when querying an image database using a standard retrieval engine user may be asked to partition the retrieved set into category this mode of supervision doe not provide label of data point but rather supply relational information about the label equivalence of data point learning using equivalence relation is different from learning using label and pose new technical challenge existing machine learning technique are not designed to exploit this information we therefore developed a number of novel method which learn from relational data we provide result of our method on a distributed image querying system that work on a large facial image database and on the clustering and retrieval of surveillance data our result show that we can significantly improve the performance of image retrieval by taking advantage of such assumption a temporal continuity in the data significant improvement is also obtained by asking the user of the system take the role of distributed teacher which reduces the need for expensive labeling by paid labor 
this paper is about learning using partial information in the form of equivalence constraint equivalence constraint provide relational information about the label of data point rather than the label themselves our work is motivated by the observation that in many real life application partial information about the data can be obtained with very little cost for example in video indexing we may want to use the fact that a sequence of face obtained from successive frame in roughly the same location is likely to contain the same unknown individual learning using equivalence constraint is different from learning using label and pose new technical challenge in this paper we present three novel method for clustering and classification which use equivalence constraint we provide result of our method on a distributed image querying system that work on a large facial image database and on the clustering and retrieval of surveillance data our result show that we can significantly improve the performance of image retrieval by taking advantage of such assumption a temporal continuity in the data significant improvement is also obtained by making the user of the system take the role of distributed teacher which reduces the need for expensive labeling by paid human labor 
we describe semi markov conditional random field semi cr f a conditionally trained version of semi markov chain intuitively a semicrf on an input sequence x output a segmentation of x in which label are assigned to segment i e subsequence of x rather than to individual element xi of x importantly feature for semi crfs can measure property of segment and transition within a segment can be non markovian in spite of this additional power exact learning and inference algorithm for semi crfs are polynomial time often only a small constant factor slower than conventional crfs in experiment on five named entity recognition problem semi crfs genus lly outperform conventional crfs 
transformation of both the response variable and the predictor is commonly used in fitting regression model however these transformation method do not always provide the maximum linear correlation between the response variable and the predictor especially when there are non linear relationship between predictor and the response such a the medical data set used in this study a spline based transformation method is proposed that is second order smooth continuous and minimizes the mean squared error between the response and each predictor since the computation time for generating this spline is o n the processing time is reasonable with massive data set in contrast to cubic smoothing spline the resulting transformation equation also display a high level of efficiency for scoring data used for predicting health outcome contains an abundance of non linear relationship between predictor and the outcome requiring an algorithm for modeling them accurately thus a transformation that fit an adaptive cubic spline to each of a set of variable is proposed these curve are used a a set of transformation function on the predictor a case study of how the transformed variable can be fed into a simple linear regression model to predict risk outcome is presented the result show significant improvement over the performance of the original variable in both linear and non linear model 
principal component analysis pca is a widely used statistical technique for unsupervised dimension reduction k mean clustering is a commonly used data clustering for performing unsupervised learning task here we prove that principal component are the continuous solution to the discrete cluster membership indicator for k mean clustering new lower bound for k mean objective function are derived which is the total variance minus the eigenvalue of the data covariance matrix these result indicate that unsupervised dimension reduction is closely related to unsupervised learning several implication are discussed on dimension reduction the result provides new insight to the observed effectiveness of pca based data reduction beyond the conventional noise reduction explanation that pca via singular value decomposition provides the best low dimensional linear approximation of the data on learning the result suggests effective technique for k mean data clustering dna gene expression and internet newsgroups are analyzed to illustrate our result experiment indicate that the new bound are within of the optimal value 
this paper present a method to develop a class of variable memory markov model that have higher memory capacity than traditional uniform memory markov model the structure of the variable memory model is induced from a manually annotated corpus through a decision tree learning algorithm a series of comparative experiment show the resulting model outperform uniform memory markov model in a part of speech tagging task 
manually querying search engine in order to accumulate a large bodyof factual information is a tedious error prone process of piecemealsearch search engine retrieve and rank potentially relevantdocuments for human perusal but do not extract fact assessconfidence or fuse information from multiple document this paperintroduces knowitall a system that aim to automate the tedious process ofextracting large collection of fact from the web in an autonomous domain independent and scalable manner the paper describes preliminary experiment in which an instance of knowitall running for four day on a single machine wa able to automatically extract fact knowitall associate a probability with each fact enabling it to trade off precision and recall the paper analyzes knowitall s architecture and report on lesson learned for the design of large scale information extraction system 
we introduce a class of nonstationary covariance function for gaussian process gp regression nonstationary covariance function allow the model to adapt to function whose smoothness varies with the input the class includes a nonstationary version of the mat rn stationary covariance in which the differentiability of the regression function is controlled by a parameter freeing one from fixing the differentiability in advance in experiment the nonstationary gp regression model performs well when the input space is two or three dimension outperforming a neural network model and bayesian free knot spline model but is outperformed in one dimension by a state of the art bayesian free knot spline model the model readily generalizes to non gaussian data use of computational method for speeding gp fitting may allow for implementation of the method on larger datasets 
collaborative filtering aim at learning predictive model of user preference interest or behavior from community data i e a database of available user preference in this paper we describe a new model based algorithm designed for this task which is based on a generalization of probabilistic latent semantic analysis to continuous valued response variable more specifically we assume that the observed user rating can be modeled a a mixture of user community or interest group where user may participate probabilistically in one or more group each community is characterized by a gaussian distribution on the normalized rating for each item the normalization of rating is performed in a user specific manner to account for variation in absolute shift and variance of rating experiment on the eachmovie data set show that the proposed approach compare favorably with other collaborative filtering technique 
this paper describes a system that can build appearance model ofanimals automatically from a video sequence of the relevant animalwith no explicit supervisory information the video sequence neednot have any form of special background animal are modeled a a d kinematic chain of rectangular segment where the number ofsegments and the topology of the chain are unknown the systemdetects possible segment cluster segment whose appearance iscoherent over time and then build a spatial model of such segmentclusters the resulting representation of the spatial configurationof the animal in each frame can be seen either a a track inwhich case the system described should be viewed a a generalizedtracker that is capable of modeling object while tracking them or a the source of an appearance model which can be used to builddetectors for the particular animal this is because knowing avideo sequence is temporally coherent i e that a particularanimal is present through the sequence is a strong supervisorysignal the method is shown to be successful a a tracker on videosequences of real scene showing three different animal for thesame reason it is successful a a tracker the method result indetectors that can be used to find each animal fairly reliablywithin the corel collection of image 
using our question answering system question from the trec evaluation were executed over a series of web data collection with the size of the collection increasing from gigabyte up to nearly a terabyte 
there is enormous amount of multilingual document from various source and possibly from different country describing a single event or a set of related event it is desirable to construct text mining method that can compare and highlight similarity and difference of those multilingual document we discus our ongoing research that seek to model a pair of multilingual document a a weighted bipartite graph with the edge weight computed by mean of machine translation we use spectral method to identify dense subgraphs of the weighted bipartite graph which can be considered a corresponding to sentence that correlate well in textual content we illustrate our approach using english and german text 
in this paper we introduce and study a logic of desire the semantics of our logic is defined by mean of two ordering relation representing preference and normality a in boutilier s logic qdt however the desire are interpreted in a different way in context a i desire b is interpreted a the best among the most normal a b world are preferred to the most normal a b world we study the formal property of these desire illustrate their expressive power on several class of example and position them with respect to previous work in qualitative decision theory 
a commercial web page typically contains many information block apart from the main content block it usually ha such block a navigation panel copyright and privacy notice and advertisement for business purpose and for easy user access we call these block that are not the main content block of the page the noisy block we show that the information contained in these noisy block can seriously harm web data mining eliminating these noise is thus of great importance in this paper we propose a noise elimination technique based on the following observation in a given web site noisy block usually share some common content and presentation style while the main content block of the page are often diverse in their actual content and or presentation style based on this observation we propose a tree structure called style tree to capture the common presentation style and the actual content of the page in a given web site by sampling the page of the site a style tree can be built for the site which we call the site style tree sst we then introduce an information based measure to determine which part of the sst represent noise and which part represent the main content of the site the sst is employed to detect and eliminate noise in any web page of the site by mapping this page to the sst the proposed technique is evaluated with two data mining task web page clustering and classification experimental result show that our noise elimination technique is able to improve the mining result significantly 
we present a principled methodology for filtering news story by formal measure of information novelty and show how the technique can be usedto custom tailor news feed based on information that a user ha already reviewed we review method for analyzing novelty and then describe newsjunkie a system that personalizes news for user by identifying the novelty of story in the context of story they have already reviewed newsjunkie employ novelty analysis algorithm that represent article a word and named entity the algorithm analyze inter andintra document dynamic by considering how information evolves over timefrom article to article a well a within individual article we review the result of a user study undertaken to gauge the value of the approachover legacy time based review of newsfeeds and also to compare the performance of alternate distance metric that are used to estimate the dissimilarity between candidate new article and set of previously reviewed article 
self calibration using pure rotation is a well known technique and ha been shown to be a reliable mean for recovering intrinsic camera parameter however in practice it is virtually impossible to ensure that the camera motion for this type of self calibration is a pure rotation in this paper we present an error analysis of recovered intrinsic camera parameter due to the presence of translation we derived closed form error expression for a single pair of image with nondegenerate motion for multiple rotation for which there are no closed form solution analysis wa done through repeated experiment among others we show that translation independent solution do exist under certain practical condition our analysis can be used to help choose the least error prone approach if multiple approach exist for a given set of condition 
this paper proposes an algorithm to hierarchically cluster document each cluster is actually a cluster of document and an associated cluster of word thus a document word co cluster note that the vector model for document creates the document word matrix of which every co cluster is a submatrix one would intuitively expect a submatrix made up of high value to be a good document cluster with the corresponding word cluster containing it most distinctive feature our algorithm look to exploit this we have defined matrix density and our algorithm basically us matrix density consideration in it working the algorithm is a partitional agglomerative algorithm the partitioning step involves the identification of dense submatrices so that the respective row set partition the row set of the complete matrix the hierarchical agglomerative step involves merging the most similar submatrices until we are down to the required number of cluster if we want a flat clustering or until we have just the single complete matrix left if we are interested in a hierarchical arrangement of document it also generates apt label for each cluster or hierarchy node the similarity measure between cluster that we use here for the merging cleverly us the fact that the cluster here are co cluster and is a key point of difference from existing agglomerative algorithm we will refer to the proposed algorithm a rpsa rowset partitioning and submatrix agglomeration we have compared it a a clustering algorithm with spherical k mean and spectral graph partitioning we have also evaluated some hierarchy generated by the algorithm 
this paper proposes neural mechanism of transcranial magnetic stimulation tm tm can stimulate the brain non invasively through a brief magnetic pulse delivered by a coil placed on the scalp interfering with specific cortical function with a high temporal resolution due to these advantage tm ha been a popular experimental tool in various neuroscience field however the neural mechanism underlying tmsinduced interference are still unknown a theoretical basis for tm ha not been developed this paper provides computational evidence that inhibitory interaction in a neural population not an isolated single neuron play a critical role in yielding the neural interference induced by tm 
in this paper we extend previous result providing a theoretical analysis of a new monte carlo ensemble classifier the framework allows u to characterize the condition under which the ensemble approach can be expected to outperform the single hypothesis classifier moreover we provide a closed form expression for the distribution of the true ensemble accuracy a well a of it mean and variance we then exploit this result in order to analyze the expected error behavior in a particularly interesting case 
support vector machine and other kernel machine offer robust modern machine learning method for nonlinear classification however relative to other alternative such a linear method decision tree and neural network they can be order of magnitude slower at query time unlike existing method that attempt to speedup querytime such a reduced set compression e g burges and anytime bounding e g decoste we propose a new and efficient approach based on treating the kernel machine classifier a a special form of k nearest neighbor our approach improves upon a traditional k nn by determining at query time a good k for each query based on pre query analysis guided by the original robust kernel machine we demonstrate effectiveness on high dimensional benchmark mnist data observing a greater than fold reduction in the number of svs required per query amortized over all pairwise mnist digit classifier with no extra test error in fact it happens to make fewer 
principal component analysis pca is a widely used statistical technique for unsupervised dimension reduction k mean clustering is a commonly used data clustering for performing unsupervised learning task here we prove that principal component are the continuous solution to the discrete cluster membership indicator for k mean clustering new lower bound for k mean objective function are derived which is the total variance minus the eigenvalue of the data covariance matrix these result indicate that unsupervised dimension reduction is closely related to unsupervised learning several implication are discussed on dimension reduction the result provides new insight to the observed effectiveness of pca based data reduction beyond the conventional noise reduction explanation that pca via singular value decomposition provides the best low dimensional linear approximation of the data on learning the result suggests effective technique for k mean data clustering dna gene expression and internet newsgroups are analyzed to illustrate our result experiment indicate that the new bound are within of the optimal value 
a novel feature selection algorithm is presented based on the global minimization of a data dependent generalization error bound feature selection and scaling algorithm often lead to non convex optimization problem which in many previous approach were addressed through gradient descent procedure that can only guarantee convergence to a local minimum we propose an alternative approach whereby the global solution of the non convex optimization problem is derived via an equivalent optimization problem moreover the convex optimization task is reduced to a conic quadratic programming problem for which efficient solver are available highly competitive numerical result on both artificial and real world data set are reported 
our goal is to mesh the symbolic reasoning capability of a cognitive model with the constrained optimization possibility inherent in optimal control we plan to develop and test such a system for several different dynamical model in environment of differing certainty and differing efficiency requirement 
information retrieval system evaluation is complicated by the need for manually assessed relevance judgment large manually built directory on the web open the door to new evaluation procedure by assuming that web page are the known relevant item for query that exactly match their title we use the odp open directory project and looksmart directory for system evaluation we test our approach with a sample from a log of ten million web query and show that such an evaluation is unbiased in term of the directory used stable with respect to the query set selected and correlated with a reasonably large manual evaluation 
we consider the situation in semi supervised learning where the label sampling mechanism stochastically depends on the true response a well a potentially on the feature we suggest a method of moment for estimating this stochastic dependence using the unlabeled data this is potentially useful for two distinct purpose a a an input to a supervised learning procedure which can be used to de bias it result using labeled data only and b a a potentially interesting learning task in itself we present several example to illustrate the practical usefulness of our method 
we present two simple search method for computing a sample nash equilibrium in a normal form game one for player game and one for n player game both algorithm bias the search towards support that are small and balanced and employ a backtracking procedure to efficiently explore these support making use of a new comprehensive testbed we test these algorithm on many class of game and show that they perform well against the state of the art the lemke howson algorithm for player game and simplicial subdivision and govindan wilson for n player game 
most prevalent technique in support vector machine svm feature selection are based on the intuition that the weight of feature that are close to zero are not required for optimal classification in this paper we show that indeed in the sample limit the irrelevant variable in a theoretical and optimal sense will be given zero weight by a linear svm both in the soft and the hard margin case however svm based method have certain theoretical disadvantage too we present example where the linear svm may assign zero weight to strongly relevant variable i e variable required for optimal estimation of the distribution of the target variable and where weakly relevant feature i e feature that are superfluous for optimal feature selection given other feature may get non zero weight we contrast and theoretically compare with markov blanket based feature selection algorithm that do not have such disadvantage in a broad class of distribution and could also be used for causal discovery 
many nlp task rely on accurately estimating word dependency probability p where the word w and w have a particular relationship such a verb object because of the sparseness of count of such dependency smoothing and the ability to use multiple source of knowledge are important challenge for example if the probability p n v of noun n being the subject of verb v is high and v take similar object to v and v is synonymous to v then we want to conclude that p n v should also be reasonably high even when those word did not cooccur in the training data to capture these higher order relationship we propose a markov chain model whose stationary distribution is used to give word probability estimate unlike the manually defined random walk used in some link analysis algorithm we show how to automatically learn a rich set of parameter for the markov chain s transition probability we apply this model to the task of prepositional phrase attachment obtaining an accuracy of 
abstract model of spatial variation in image are central to a large number of low level computer vision problem including segmentation registration and d structure detection often image are represented using parametric model to characterize noise free image variation and additive noise however the noise model may be unknown and para metric model may only be valid on individual segment of the image consequently we model noise using a nonparametric kernel density esti mation framework and use a locally or globally linear parametric model to represent the noise free image pattern this result in a novel ro bust redescending m parameter estimator for the above image model which we call the kernel maximum likelihood estimator kml we also provide a provably convergent iterative algorithm for the resultant optimization problem the estimation framework is empirically validated on synthetic data and applied to the task of range image segmentation 
we present the software architecture of a robotic system for mapping abandoned mine the software is capable of acquiring consistent d map of large mine with many cycle represented a markov random eld d c space map are acquired from local d range scan which are used to identify navigable path using a search our system ha been deployed in three abandoned mine two of which inaccessible to people where it ha acquired map of unprecedented detail and accuracy 
recent work ha examined the estimation of model of stimulus driven neural activity in which some linear filtering process is followed by a nonlinear probabilistic spiking mechanism we analyze the estimation of one such model for which this nonlinear step is implemented by a noisy leaky integrate and fire mechanism specifically we formulate the problem in term of maximum likelihood estimation and show that the computational problem of optimizing this cost function is tractable our main contribution is an algorithm and a proof that this algorithm is guaranteed to find the global optimum we demonstrate the effectiveness of our estimator with numerical simulation 
in many application domain there is a large amount of unlabeled data but only a very limited amount of labeled training data one general approach that ha been explored for utilizing this unlabeled data is to construct a graph on all the data point based on distance relationship among example and then to use the known label to perform some type of graph partitioning one natural partitioning to use is the minimum cut that agrees with the labeled data blum chawla which can be thought of a giving the most probable label assignment if one view label a generated according to a markov random field on the graph zhu et al propose a cut based on a relaxation of this field and joachim give an algorithm based on finding an approximate min ratio cut in this paper we extend the mincut approach by adding randomness to the graph structure the resulting algorithm address several short coming of the basic mincut approach and can be given theoretical justification from both a markov random field perspective and from sample complexity consideration in case where the graph doe not have small cut for a given classification problem randomization may not help however our experiment on several datasets show that when the structure of the graph support small cut this can result in highly accurate classifier with good accuracy coverage tradeoff in addition we are able to achieve good performance with a very simple graph construction procedure 
we present a probabilistic approach to learning a gaussian process classier in the presence of unlabeled data our approach involves a null category noise model ncnm inspired by ordered categorical noise model the noise model reects an assumption that the data density is lower between the class conditional density we illustrate our approach on a toy problem and present comparative result for the semi supervised classication of handwritten digit 
abstract given a directed graph in which some of the node are labeled we investigate the question of how to exploit the link structure of the graph to infer the label of the remaining unlabeled node to that extent we propose a regularization framework for function dened over node of a directed graph that force the classication function to change slowly on densely linked sub graph a powerful yet computationally simple classication algorithm is derived within the proposed framework the experimental evaluation on real world web classication problem demonstrates encouraging result that validate our approach 
this paper analyzes the performance of semisupervised learning of mixture model we show that unlabeled data can lead to an increase in classification error even in situation where additional labeled data would decrease classification error we present a mathematical analysis of this degradation phenomenon and show that it is due to the fact that bias may be adversely affected by unlabeled data we discus the impact of these theoretical result to practical situation 
we describe an extensible markup language xml based methodology for web data extraction that extends beyond simple screen scraping an ideal data extraction process can digest target web database that are visible only a hypertext markup language html page and create a local replica of those database a a result what is needed is more than a web crawler and set of web site wrapper a comprehensive data extraction process must deal with such obstacle a session identifier html form client side javascript incompatible datasets and vocabulary and missing and conflicting data proper data extraction also requires solid data validation and error recovery to handle data extraction failure our andes software framework help solve these problem and provides a platform for building a production quality web data extraction process key aspect of andes are that it us xml technology for data extraction including extensible html and extensible stylesheet language transformation and provides access to the deep web 
moment before the launch of every space vehicle engineering discipline specialist must make a critical go no go decision the cost of a false positive allowing a launch in spite of a fault or a false negative stopping a potentially successful launch can be measured in the ten of million of dollar not including the cost in morale and other more intangible detriment the aerospace corporation is responsible for providing engineering assessment critical to the go no go decision for every department of defense space vehicle these assessment are made by constantly monitoring streaming telemetry data in the hour before launch we will introduce viztree a novel time series visualization tool to aid the aerospace analyst who must make these engineering assessment viztree wa developed at the university of california riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry the use of a single tool for both aspect of the task allows a natural and intuitive transfer of mined knowledge to the monitoring task our visualization approach work by transforming the time series into a symbolic representation and encoding the data in a modified suffix tree in which the frequency and other property of pattern are mapped onto color and other visual property we demonstrate the utility of our system by comparing it with state of the art batch algorithm on several real and synthetic datasets 
we report and compare the performance of different learning algorithm based on data from cortical recording the task is to predict the orientation of visual stimulus from the activity of a population of simultaneously recorded neuron we compare several way of improving the coding of the input i e the spike data a well a of the output i e the orientation and report the result obtained using different kernel algorithm 
the bradley terry model for paired comparison ha been popular in many area we propose a generalized version in which paired individual comparison are extended to paired team comparison we introduce a simple algorithm with convergence proof to solve the model and obtain individual skill a useful application to multi class prob ability estimate using error correcting code is demonstrated 
the problem of pose estimation arises in many area of computer vision including object recognition object tracking site inspection and updating and autonomous navigation using scene model we present a new algorithm called softposit for determining the pose of a d object from a single d image in the case that correspondence between model point and image point are unknown the algorithm combine gold s iterative softassign algorithm for computing correspondence and dementhon s iterative posit algorithm for computing object pose under a full perspective camera model our algorithm unlike most previous algorithm for this problem doe not have to hypothesize small set of match and then verify the remaining image point instead all possible match are treated identically throughout the search for an optimal pose the performance of the algorithm is extensively evaluated in monte carlo simulation on synthetic data under a variety of level of clutter occlusion and image noise these test show that the algorithm performs well in a variety of difficult scenario and empirical evidence suggests that the algorithm ha a run time complexity that is better than previous method by a factor equal to the number of image point the algorithm is being applied to the practical problem of autonomous vehicle navigation in a city through registration of a d architectural model of building to image obtained from an on board camera 
we derive multiplicative update for solving the nonnegative quadratic programming problem in support vector machine svms the update have a simple closed form and we prove that they converge monotonically to the solution of the maximum margin hyperplane the update optimize the traditionally proposed objective function for svms they do not involve any heuristic such a choosing a learning rate or deciding which variable to update at each iteration they can be used to adjust all the quadratic programming variable in parallel with a guarantee of improvement at each iteration we analyze the asymptotic convergence of the update and show that the coefficient of non support vector decay geometrically to zero at a rate that depends on their margin in practice the update converge very rapidly to good classifier 
we present set an architecture for efficient search in peer to peer network building upon idea drawn from machine learning and social network theory the key idea is to arrange participating site in a topic segmented overlay topology in which most connection are short distance connecting pair of site with similar content topically focused set of site are then joined together into a single network by long distance link query are matched and routed to only the topically closest region we discus a variety of design issue and tradeoff that an implementor of set would face we show that set is efficient in network traffic and query processing load 
the computation of the first complete approximation of game theoretic optimal strategy for full scale poker is addressed several abstraction technique are combined to represent the game of player texas hold em having size o using closely related model each having size o o despite the reduction in size by a factor of billion the resulting model retain the key property and structure of the real game linear programming solution to the abstracted game are used to create substantially improved poker playing program able to defeat strong human player and be competitive against world class opponent 
this paper present a new two phase pattern pp discovery technique for information extraction pp consists of orthographic pattern discovery opd and semantic pattern discovery spd where the opd determines the structural feature from an identified region of a document and the spd discovers a dominant semantic pattern for the region via inference apposition and analogy then the discovered pattern is applied back into the region to extract required data item through pattern matching we evaluated pp using data item and obtained effective result 
when the goal is to achieve the best correct classification rate cross entropy and mean squared error are typical cost function used to optimize classifier performance however for many real world classification problem the roc curve is a more meaningful performance measure we demonstrate that minimizing cross entropy or mean squared error doe not necessarily maximize the area under the roc curve auc we then consider alternative objective function for training a classifier to maximize the auc directly we propose an objective function that is an approximation to the wilcoxon mann whitney statistic which is equivalent to the auc the proposed objective function is dierentiable so gradient based method can be used to train the classifier we apply the new objective function to real world customer behavior prediction problem for a wireless service provider and a cable service provider and achieve reliable improvement in the roc curve 
we explore the application of machine learningtechniques to the problem of contentbasedimage retrieval cbir unlike mostexisting cbir system in which only globalinformation is used or in which a user mustexplicitly indicate what part of the image isof interest we apply the multiple instance mi learning model to use a small numberof training image to learn what image fromthe database are of interest to the user 
we present a system that is capable of segmenting detecting and tracking multiple people in a cluttered scene using multiple synchronized camera located far from each other the system improves upon existing system in many way including we do not assume that a foreground connected component belongs to only one object rather we segment the view taking into account color model for the object and the background this help u to not only separate foreground region belonging to different object but to also obtain better background region than traditional background subtraction method a it us foreground color model in the algorithm it is fully automatic and doe not require any manual input or initialization of any kind instead of taking decision about object detection and tracking from a single view or camera pair we collect evidence from each pair and combine the evidence to obtain a decision in the end this help u to obtain much better detection and tracking a opposed to traditional system several innovation help u tackle the problem the first is the introduction of a region based stereo algorithm that is capable of finding d point inside an object if we know the region belonging to the object in two view no exact point matching is required this is especially useful in wide baseline camera system where exact point matching is very difficult due to self occlusion and a substantial change in viewpoint the second contribution is the development of a scheme for setting prior for use in segmentation of a view using bayesian classification the scheme which assumes knowledge of approximate shape and location of object dynamically assigns prior for different object at each pixel so that occlusion information is encoded in the prior the third contribution is a scheme for combining evidence gathered from different camera pair using occlusion analysis so a to obtain a globally optimum detection and tracking of object the system ha been tested using different density of people in the scene which help u to determine the number of camera required for a particular density of people 
important information when mining temporal sequence knowledge discovery technique can be applied that discover interesting pattern of interaction existing approach use frequency and sometimes length a measurement for interestingness because these are temporal sequence additional characteristic such a periodicity may also be interesting we propose that information theoretic principle can be used to evaluate interesting characteristic of time ordered input sequence in this paper we present a novel data mining technique based on the minimum description length principle that discovers interesting feature in a time ordered sequence we discus feature of our real time mining approach show application of the knowledge mined by the approach and present a technique to bootstrap a decision maker from the mined pattern 
bayesian network model are widely used for discriminative prediction task such a classification usually their parameter are determined using unsupervised method such a maximization of the joint likelihood the reason is often that it is unclear how to find the parameter maximizing the conditional supervised likelihood we show how the discriminative learning problem can be solved efficiently for a large class of bayesian network model including the naive bayes nb and tree augmented naive bayes tan model we do this by showing that under a certain general condition on the network structure the discriminative learning problem is exactly equivalent to logistic regression with unconstrained convex parameter space hitherto this wa known only for naive bayes model since logistic regression model have a concave log likelihood surface the global maximum can be easily found by local optimization method 
we investigate the influence of the mirror shape on the imaging quality of catadioptric sensor for axially symmetrical mirror we calculate the location of the virtual image point considering incident quasi parallel light ray using second order approximation we give analytical expression for the two limiting surface of this virtual image zone this is different to numerical or ray tracing approach for the estimation of the blur region e g we show how these equation can be used to estimate the image blur caused by the shape of the mirror a example we present two different omnidirectional stereo sensor with single camera and equi angular mirror that are used on mobile robot to obtain a larger stereo baseline one of these sensor consists of two separated mirror of the same angular magnification and differs from a similar configuration proposed by ollis et al we calculate the caustic surface and show that this stereo configuration can be approximated by two single view point yielding an effective vertical stereo baseline of approx cm an example of panoramic disparity computation using a physiologically motivated stereo algorithm is given 
this paper examines projectively invariant local property ofsmooth curve and surface oriented projective differentialgeometry is proposed a a theoretical framework for establishingsuch invariant and describing the local shape of surface andtheir outline this framework is applied to two problem aprojective proof of koenderink s famous characterization ofconvexities concavity and inflection of apparent contour andthe determination of the relative orientation of rim tangent atfrontier point 
this paper present a systematic study of the property of a large number of web site hosted by a major isp to our knowledge ours is the first comprehensive study of a large server farm that contains thousand of commercial web site we also perform a simulation analysis to estimate potential performance benefit of content delivery network cdns for these web site we make several interesting observation about the current usage of web technology and web site performance characteristic first compared with previous client workload study the web server farm workload contains a much higher degree of uncacheable response and response that require mandatory cache validation a significant reason for this is that cookie use is prevalent among our population especially among more popular site however we found an indication of wide spread indiscriminate usage of cooky which unnecessarily impedes the use of many content delivery optimization we also found that most web site do not utilize the cache control feature ofthe http protocol resulting in suboptimal performance moreover the implicit expiration time in client cache for response is constrained by the maximum value allowed in the squid proxy finally our simulation result indicate that most web site benefit from the use of a cdn the amount of the benefit depends on site popularity and somewhat surprisingly a cdn may increase the peak to average request ratio at the origin server because the cdn can decrease the average request rate more than the peak request rate 
this paper proposes a data mining approach to modeling relationship among category in image collection in our approach with image feature grouping a visual dictionary is created for color texture and shape feature attribute respectively labeling each training image with the keywords in the visual dictionary a classification tree is built based on the statistical property of the feature space we define a structure called semantics graph to discover the hidden semantic relationship among the semantic category embodied in the image collection with the semantics graph each semantic category is modeled a a unique fuzzy set to explicitly address the semantic uncertainty and semantic overlap among the category in the feature space the model is utilized in the semantics intensive image retrieval application an algorithm using the classification accuracy measure is developed to combine the built classification tree with the fuzzy set modeling method to deliver semantically relevant image retrieval for a given query image the experimental evaluation have demonstrated that the proposed approach model the semantic relationship effectively and the image retrieval prototype system utilizing the derived model is promising both in effectiveness and efficiency 
p nbsp p div what make a neural microcircuit computationally powerful or more precisely which measurable quantity could explain why one microcircuit img width height border align bottom src http www igi tugraz at abstract maassetal img png alt c is better suited for a particular family of computational task than another microcircuit img width height border align bottom src http www igi tugraz at abstract maassetal img png alt c textquoteright we propose in this article quantitative measure for evaluating the computational power and generalization capability of a neural microcircuit and apply them to generic neural microcircuit model drawn from different distribution we validate the proposed measure by comparing their prediction with direct evaluation of the computational performance of these microcircuit model this procedure is applied first to microcircuit model that differ with regard to the spatial range of synaptic connection and with regard to the scale of synaptic efficacy in the circuit and then to microcircuit model that differ with regard to the level of background input current and the level of noise on the membrane potential of neuron in this case the proposed method allows u to quantify difference in the computational power and generalization capability of circuit in different dynamic regime upand down state that have been demonstrated through intracellular recording in vivo div p nbsp p 
this paper address the problem of clustering image of object seen from different viewpoint that is given an unlabelled set of image of n object we seek an unsupervised algorithm that can group the image into n disjoint subset such that each subset only contains image of a single object we formulate this clustering problem under a very broad geometric framework the theme is the interplay between the geometry of appearance manifold and the symmetry of the d affine group specifically we identify three important notion for image clustering the l distance metric of the image space the local linear structure of the appearance manifold and the action of the d affine group in the image space based on these notion we propose a new image clustering algorithm in a broad outline the algorithm us the metric to determine a neighborhood structure in the image space for each input image using local linear structure comparison affinity between image are computed only among the neighbor these local comparison are agglomerated into an affinity matrix and a spectral clustering algorithm is used to yield the final clustering result the technical part of the algorithm is to make all of these compatible with the action of the d affine group using human face image and image from the coil database we demonstrate experimentally that our algorithm is effective in clustering image according to ojbect identity where there is a large range of pose variation 
we propose a novel named entity matching model which considers both semantic and phonetic clue the matching is formulated a an optimization problem one major component is a phonetic matching model which exploit similarity at the phoneme level we investigate three learning algorithm for obtaining the similarity information of basic phoneme unit based on training example by applying this proposed named entity matching model we also develop a mining framework for discovering new unseen named entity translation from online daily web news this framework harvest comparable news in different language using an existing bilingual dictionary it is able to discover new name translation not found in the dictionary 
recent observation through experiment that we have performed incurrent third generation wireless network have revealed that the achieved throughput over wireless link varies widely depending on the application in particular the throughput achieved by file transfer application ftp and web browsing application http are quite different the throughput achieved over a http session is much lower than that achieved over an ftp session the reason for the lower http throughput is that the http protocol is affected by the large round trip time rtt across wireless link http transfer require multiple tcp connection and dns lookup before a http page can be displayed each tcp connection requires several rtts to fully open the tcp send window and each dns lookup requires several rtts before resolving the domain name to ip mapping these tcp dns rtts significantly degrade the performance of http over wireless link to overcome these problem we have developed session level optimization technique to enhance http download mechanism these technique a minimize the number of dns lookup over the wireless link and b minimize the number of tcp connection opened by the browser these optimization bridge the mismatch caused by wireless link between application level protocol such a http and transport level protocol such astcp our solution do not require any client side software and can be deployed transparently on a service provider network toprovide decrease in end to end user perceived latency and increase in data throughput across wireless link for http session 
an investment of effort over the last two year ha begun to produce a wealth of data concerning computational psycholinguistic model of syntax acquisition the data is generated by running simulation on a recently completed database of word order pattern from over abstract language this article present the design of the database which contains sentence pattern grammar and derivation that can be used to test acquisition model from widely divergent paradigm the domain is generated from grammar that are linguistically motivated by current syntactic theory and the sentence pattern have been validated a psychologically developmentally plausible by checking their frequency of occurrence in corpus of child directed speech a small case study simulation is also presented 
mobile knowledge seeker often need access to information on the web during a meeting or on the road while away from their desktop a common practice today is to use pervasive device such a personal digital assistant or mobile phone however these device have inherent constraint e g slow communication form factor which often make information discovery task impractical in this paper we present a new focused search approach specifically oriented for the mode of work and the constraint dictated by pervasive device it combine focused search within specific topic with encapsulation of topic specific information in a persistent repository one key characteristic of these persistent repository is that their footprint is small enough to fit on local device and yet they are rich enough to support many information discovery task in disconnected mode more specifically we suggest a representation for topic specific information based on knowledge agent base that comprise all the information necessary to access information about a topic under the form of key concept and key web page and assist in the full search process from query formulation assistance to result scanning on the device itself the key contribution of our work is the coupling of focused search with encapsulated knowledge representation making information discovery from pervasive device practical a well a efficient we describe our model in detail and demonstrate it aspect through sample scenario 
this paper present the design and user evaluation of smartback a feature that complement the standard back button by enabling user to jump directly to key page in their navigation session making common navigation activity more efficient defining key page wa informed by the finding of a user study that involved detailed monitoring of web usage and analysis of web browsing in term of navigation trail the page accessible through smartback are determined automatically based on the structure of the user s navigation trail or page association with specific user s activity such a search or browsing bookmarked site we discus implementation decision and present result of a usability study in which we deployed the smartback prototype and monitored usage for a month in both corporate and home setting the result show that the feature brings qualitative improvement to the browsing experience of individual who use it 
we present a framework for statistical machine translation of natural language based on direct maximum entropy model which contains the widely used source channel approach a a special case all knowledge source are treated a feature function which depend on the source language sentence the target language sentence and possible hidden variable this approach allows a baseline machine translation system to be extended easily by adding new feature function we show that a baseline statistical machine translation system is significantly improved using this approach 
deformable d d medical image registration is an essential technique in computer integrated surgery ci to fuse d pre operative data with d intra operative data several factor may affect the accuracy of d d registration including the number of d view the angle between view the view angle relative to anatomical object the co registration error between view the image noise and the image distortion in this paper we investigate and ass the relationship between these factor and the accuracy of d d registration we proposed a deformable d d registration method based on a statistical model we conducted experiment using a hemi pelvis model and simulated x ray image some discussion are provided on how to improve the accuracy of d d registration based on our assessment 
in a federated digital library system it is too expensive to query every accessible library resource selection is the task to decide to which library a query should be routed most existing resource selection algorithm compute a library ranking in a heuristic way in contrast the decision theoretic framework dtf follows a different approach on a better theoretic foundation it computes a selection which minimises the overall cost e g retrieval quality time money of the distributed retrieval for estimating retrieval quality the recall precision function is proposed in this paper we introduce two new method the first one computes the empirical distribution of the probability of relevance from a small library sample and assumes it to be representative for the whole library the second method assumes that the indexing weight follow a normal distribution leading to a normal distribution for the document score furthermore we present the first evaluation of dtf by comparing this theoretical approach with the heuristical state of the art system cori here we find that dtf outperforms cori in most case 
in this paper we present a programming language approach for the assembly of arbitrary two dimensional shape by decentralized identically programmed agent our system compiles a predetermined global shape into a program that instructs these agent to grow the shape via replication and location based control mechanism in the global to local compilation phase an input shape is decomposed into a network of covering disc the disc network parameterizes the agent program a biologically inspired framework allowing agent to amorphously produce the shape using replication and local interaction our system is robust to random agent failure and regenerates in the event of region death 
the aim of this paper is to find the best representation for the appearance of surface with lambertian reflectance under varying illumination previous work using principal component analysis pca found the best sub space to represent all image of an object under a varying point light source we extend this to image from any illumination distribution specifically we calculate the base for all configuration of a point plus ambient light source and two point light source a well a from a database of captured real world illumination we also reformulate the optimization criterion used in pca the resulting basis we believe ha higher representability and is better for analyzing image of shaded object the different base are compared on a database of image to test the representability 
example based method are effective for parameter estimation problem when the underlying system is simple or the dimensionality of the input is low for complex and high dimensional problem such a pose estimation the number of required example and the computational complexity rapidly become prohibitively high we introduce a new algorithm that learns a set of hashing function that efficiently index example relevant to a particular estimation task our algorithm extends locality sensitive hashing a recently developed method to find approximate neighbor in time sublinear in the number of example this method depends critically on the choice of hash function that are optimally relevant to a particular estimation problem experiment demonstrate that the resulting algorithm which we call parameter sensitive hashing can rapidly and accurately estimate the articulated pose of human figure from a large database of example image 
this paper extends the face detection framework proposed by viola and jones to handle profile view and rotated face a in the work of rowley et al and schneiderman et al we build different detector for different view of the face a decision tree is then trained to determine the viewpoint class such a right profile or rotated degree for a given window of the image being examined this is similar to the approach of rowley et al the appropriate detector for that viewpoint can then be run instead of running all detector on all window this technique yield good result and maintains the speed advantage of the viola jones detector 
dynamic analysis of video sequence often relies on the segmentation of the sequence into region of consistent motion approaching this problem requires a definition of which motion are regarded a consistent common approach to motion segmentation usually group together point or image region that have the same motion between successive frame where the same motion can be d d or non rigid in this paper we define a new type of motion consistency which is based on temporal consistency of behavior across multiple frame in the video sequence our definition of consistent temporal behavior is expressed in term of multi frame linear subspace constraint this definition applies to d d and some non rigid motion without requiring prior model selection we further present a multi frame multi body segmentation algorithm which applies the new motion consistency constraint directly to image brightness measurement without requiring prior correspondence estimation nor feature tracking 
