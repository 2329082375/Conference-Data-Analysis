we extend work on difference identification and reduction a a technique for automated reason ing we generalise unification so that term are made equal not only by finding substitution for variable but also by hiding term structure this annotation of structural difference serf to direct rippling a kind of rewriting designed to remove difference in a controlled way on the technical side we give a rule based algo rithm for difference unification and analyze it correctness completeness and complexity on the practical side we present a novel search strategy for efficiently applying these rule fi nally we show how this algorithm can be used in new way to direct rippling and how it can play an important role in theorem proving and other kind of automated reasoning 
we discus the persistence of the indirect ef fects of an action the question when such ef fects are subject to the commonsense law of in ertia and how to describe their evolution in the case when inertia doe not apply our model of nonpersistent effect involves the assumption that the value of the fluent in question is deter mined by the value of other fluents although the dependency may be partially or completely unknown this view lead u to a new highlevel action language ard for action ram ifications and dependency that is capable of describing both persistent and nonpersistent ef fects unlike the action language introduced in the past ard is non markovia n in the sense that the evolution of the fluents described in this language may depend on their history and not only on their current value 
model based diagnosis algorithm face a combi natorial explosion to combat this explosion this paper present a fundamentally new architecture implode which construct an abstract representa tion of the environment the conflict and the diag nosis space using a sensitivity analysis of assump tions experimental result show that the most dra matic improvement is obtained for circuit which are the most difficult to diagnose using previous algorithm moreover typical source of combina torial explosion such a reconvergent fanout are a source of combinatorial implosion for implode 
starling with a first order modal conditional logic which allows unlimited nesting of default and emheddings into any context analyzable in possibleworlds theory i introduce two simple notion of default reasoning the syntactic notion of pnorniied allowed consequence and the semantic notion of prioritized allowed entailment i prove that the one is sound and complete relative to the other 
mechanical linkage are used to transmit and transform motion in this paper we investigate what it mean for a program to understand a linkage our system extract itsunderstanding by analyzing the result of a numerical simulation of the mechanism finding interestingqualitativefeatures looking forsymbolic relationship between these feature and conjecturing a causal relationship between them our system is capable of understanding a varietyof mechanism producing explanation very much like those in standard text 
geographical database system deal with certainbasic topological relation such a quot a overlapsb quot and quot b contains c quot between simplyconnected region in the plane it is of greatinterest to make sound inference from elementarystatements of this form this problem hasbeen identified extensively in the recent literature but very limited progress ha been madetowards addressing the considerable technicaldifficulties involved in this paper we studythe computational problem involved in 
a knowledge representation problem can be sometimesviewed a an element of a family of problem with parameter corresponding to possibleassumptions about the domain under consideration when additional assumption are made the class of domain that are being described becomessmaller so that the class of conclusion thatare true in all the domain becomes larger asa result a satisfactory solution to a parametricknowledge representation problem on the basis ofsome nonmonotonic 
we provide a method based on the theory of markov decision problem for efficient planning in stochastic domain goal are encoded a reward function expressing the desirability of each world state the planner must find a policy mapping from state to action that maximizes future reward standard goal of achievement a well a goal of maintenance and prioritized combination of goal can be specified in this way an optimal policy can be found using existing method but these method are at best polynomial in the number of state in the domain where the number of state is exponential in the number of proposition or state variable by using information about the starting state the reward function and the transition probability of the domain we can restrict the planner s attention to a set of world state that are likely to be encountered in satisfying the goal furthermore the planner can generate more or le complete plan depending on the time it ha available we describe experiment involving a mobile robotics application and consider the problem of schedulilng different phase of the planning algorithm given time constraint 
ftp cmu c p in many real world task the ability to focus attention on the important feature of the input is crucial for good performance in this paper a mechanism for achieving task specific focus of attention is presented a saliency map which is based upon a computed expectation of the content of the input at the next time step indicates which region of the input retina are important for performing the task the saliency map can be used to accentuate the feature which are important and de emphasize those which are not the performance of this method is demonstrated on a real world robotics task autonomous road following the applicability of this method is also demonstrated in a non visual domain architectural and algorithmic detail are provided a well a empirical result 
this paper present an analysis of static and dynamic organizational structure for naturally distributed homogeneous cooperative problem solving environment exemplified by distributed sensor network we first show how the performance of any static organization can be statistically described and then show under what condition dynamic organization do better and worse than static one finally we show how the variance in the agent performance lead to uncertainty about whether a dynamic organization will perform better than a static one given only agent a priori expectation in these case we show when meta level communication about the actual state of problem solving will be useful to agent in constructing a dynamic organizational structure that outperforms a static one viewed in it entirety this paper also present a methodology for answering question about the design of distributed problem solving system by analysis and simulation of the characteristic of a complex environment rather than by relying on single instance example 
this paper study property of iterated revision first rationality result show that in the agm original framework the only revision operation that satisfies two resonable property is the trivial revision then an altenative to the agm framework for studying belief revision and probability postulate is proposed iterated revision are the object of this formalism and the rationality postulate deal with property of iterated revision a set of rationality postulate is presented closely related to the agm postulate a representation result show that those postulate simply serious limitation to the way revision cell dope with the principle of minimal change those postulate are not suitable for belief update but then consideration raise doubt about the adequacy of previous treatment of belief implate 
this paper along with the following paper by john mccarthy introduces some of the topic to be discussed at the ijcai event a philosophical encounter an interactive presentation of some of the key philosophical problem in ai and ai problem in philosophy philosophy need ai in order to make progress with many difficult question about the nature of mind and ai need philosophy in order to help clar ify goal method and concept and to help with several specific technical problem whilst philosophical attack on ai continue to be wel comed by a significant subset of the general public ai defender need to learn how to avoid philosophically naive rebuttal 
accuracy play a central role in developing model of continuous physical system both in the context of developing a new model to fit observation or approximating an existing model to make analysis faster the need for simple yet sufficiently accurate model pervades engineering analysis design and diagnosis task this paper focus on two issue related to this topic first it examines the process by which idealized model are derived second it examines the problem of determining when an idealized model will be sufficiently accurate for a given task in a way that is simple and doesn t overwhelm the benefit of having a simple model it describes ideal a system which generates idealized version of a given model and specifies each idealized model s credibility domain this allows valid future use of the model without resorting to more expensive measure such a search or empirical confirmation the technique is illustrated on an implemented example 
intermediate decision tree are the subtrees of the full unpruned decision tree generated in a breadth first order an extensive empirical in vestigation evaluates the classification error of intermediate decision tree and compare their performance to full and pruned tree em pirical result were generated using c with database from the uci machine learning database repository result show that when attempting to minimize the error of the pruned tree produced by c the best intermediate tree performs significantly better in of the database these and other result question the effectiveness of decision tree pruning strategy and suggest further consideration of the full tree and it intermediate also the result reveal specific property satisfied by database in which the intermediate full tree performs best such relationship improve guideline for selecting appropriate inductive strategy based on domain property 
endowing a computer with an ability to reason with diagram could be of great benefit in term of both human computer interaction and computational efficiency through explicit representation to date research in diagrammatic reasoning ha dealt with intra diagrammatic reasoning reasoning with a single diagram almost to the exclusion of inter diagrammatic reasoning reasoning with related group of diagram we postulate a number of general inter diagrammatic operator and show how such operator can be useful in various diagrammatic domain we develop a heuristic in the domain of game notation derive fingering information in the domain of musical notation and infer new information from related cartograms 
influence diagram id are a graphic formal ism able to provide a compact representation of decision problem id are based on the ax ioms of probability and decision theory and they define a normative framework to model decision making unfortunately id require a large amount of information that is not always available to the decision maker this paper in troduces a new class of id called ignorant in fluence diagram iids able to reason on the basis of incomplete information and to improve the accuracy of their decision a a monotonically increasing function of the available infor mation iids represent a net gain with respect to the traditional id since they are able to ex plicitly represent lack of information without loosing any capability of traditional id when the required information is available further more i id provide a new method to ass the reliability of the decision by replacing the tra ditional sensitivity analysis with a single ana lytical measure 
this paper present the expected solution quality esq method for statistically characterizing scheduling problem and the performance of scheduler the esq method is demonstrated by applying it to a practical telescope scheduling problem the method address the important and difficult issue of how to meaningfully evaluate the performance of a scheduler on a constrained optimization problem for which an optimal solution is not known at the heart of esq is a monte carlo algor ithm that estimate a problem s probability density function with respect to solution quality this quality density function provides a useful characterization of a scheduling problem and it also provides a background against which scheduler performance can be meaningfully evaluated esq provides a unitless measure that combine both schedule quality and the amount of time to generate a schedule 
although communication is generally considered to dominate overprocessing cost in distributed system the problem of communicationcost in multiagent planning ha not been sufficiently addressed onemethod for reducing both communication cost and planning time isthe use of social law social law however can be too restrictive limiting soundness flexible social law can enable multiagent systemsto reap the benefit of reduced communication cost and planning time except in the worst 
formalizing the ontological commitment of a logical language mean offering a way to specify the intended meaning of it vocabulary by constraining the set of it model giving explicit information about the intended nature of the modelling primitive and their a priori relationship we present here a formal definition of ontological commitment which aim to capture the very basic ontological assumption about the intended domain related to issue such a identity and internal structure to tackle such issue a modal framework endowed with mereo topological primitive ha been adopted the paper is mostly based on a revisitation of philosophical and linguistic literature in the perspective of knowledge representation 
interface agent are semi intelligent systemswhich assist user with daily computer basedtasks recently various researcher have proposeda learning approach towards building suchagents and some working prototype have beendemonstrated such agent learn by watchingover the shoulder of the user and detecting patternsand regularity in the user s behavior despitethe success booked a major problem withthe learning approach is that the agent ha tolearn from scratch and thus 
in a previous paper we defined the deep structure of a constraint satisfaction problem to be that set system produced by collecting the nogood ground instance of each constraint and keeping only those that are not supersets of any other we then showed how to use such deep structure to predict where in a space of problem instance an abrupt transition in computational cost is to be expected this paper explains how to augment this model with enough extra detail to make more accurate estimate of the location of these phase transition we also show that the phase transition phenomenon exists for a much wider class of search algorithm than had hitherto been thought and explain theoretically why this is the case 
most artificial intelligence program lack generalitybecause they reason with a single domain theory thatis tailored for a specific task and embodies a host ofimplicit assumption context have been proposedas an effective solution to this problem by providing amechanism for explicitly stating the assumption underlyinga domain theory in addition context canbe used to focus reasoning allow the representation ofmutually incoherent domain theory lift axiom fromone context into 
we develop a model based approach to reasoning in which the knowledge base is representedas a set of model satisfying assignment rather than a logical formula and the set of queriesis restricted we show that for every propositional knowledge base kb there exists a set ofcharacteristic model with the property that a query is true in kb if and only if it is satisfied bythe model in this set we characterize a set of function for which the model based representationis compact and 
for a logical database to faithfully represent our beliefsabout the world one should not only insist onits logical consistency but also on it causal consistency intuitively a database is causally inconsistentif it support belief change that contradict with ourperceptions of causal influence for example comingto conclude that it must have rained only becausethe sprinkler wa observed to be on in this paper we suggest the notion of a causal structure to represent 
we present new strategy for quot probably approximatelycorrect quot pac learning that usefewer training example than previous approach the idea is to observe training examplesone at a time and decide quot on line quot when toreturn a hypothesis rather than collect a largefixed size training sample this yield sequential learning procedure that pac learn by observinga small random number of example we provide theoretical bound on the expectedtraining sample size of our procedure but 
many problem of practical interest can be solved using tree search method because carefully tuned successor ordering heuristic guide the search toward region of the space that are likely to contain solution for some problem the heuristic often lead directly to a solution but not always limited discrepancy search address the problem of what to do when the heuristic fail our intuition is that a failing heuristic might well have succeeded if it were not for a small number of 
interface agent are computer program that employ artificial intelligence technique in order to provide assistance to a user dealing with a particular computer application the paper discus an interface agent which ha been modelled closely after the metaphor of a personal assistant the agent learns how to assist the user by i observing the user s action and imitating them ii receiving user feedback when it take wrong action and iii being trained by the user on the basis of hypothetical example the paper discus how this learning agent wa implemented using memory based learning and reinforcement learning technique it present actual result from two prototype agent built using these technique one for a meeting scheduling application and one for electronic mail it argues that the machine learning approach to building interface agent is a feasible one which ha several advantage over other approach it provides a customized and adaptive solution which is le costly and ensures better user acceptability the paper also argues what the advantage are of the particular learning technique used 
this video demonstrates the program mu the mathematics understander which learns university level pure mathematics the video is both concerned with mu s performance in learning and performing mathematics and it underlying cognitive architecture the contextual memory system cm the change in knowledge representation during proof checking and problem solving are demonstrated graphically 
in a dynamic environment relatedto earlier work on goal directed diagnosis rymon a major issue being addressed in this research ishow contextual change influence the ongoing explanationprocess a an example of the issue to address consider thefollowing scenario implemented in our system theplanner is trying to achieve the goal of catching a planeand generates two possible plan driving to the airportor taking a taxi choosing the option of driving the plan step are 
we consider the problem of assigning probabilistic rating to hypothesis in a natural language interpretation system to facilitate integrating syntactic semantic and conceptual constraint we allow a fully compositional frame representation which permit co indexed syntactic constituent and or semantic entity filling multiple role in addition the knowledge base contains probabilistic information encoded by marginal probability on frame these probability are used to specify typicality of realworld scenario on one hand and conventionality of linguistic usage pattern on the other because the theoretical maximum entropy solution is infeasible in the general case we propose an approximate method this method s strength are it ability to rate compositional structure and it flexibility with respect to the input chosen by the system it is embedded in arbitrary set of hypothesis from the front end processor can be accepted a well a arbitrary subset of constraint heuristically chosen from the long term knowledge base 
this paper describes an uncertainty model ofstereo vision and it application to a visionmotionplanning for a mobile robot in general recognition of an environment requiresmuch computation and the recognition resultincludes uncertainty in planning therefore atrade off must be considered between the costof visual recognition and the effect of informationobtained by recognition such a trade offmust be formulated on the basis of a model ofvision which describes the required time for 
our experience in the ida natural language generation project ha shown u that ida s klone like classifier originally built solely to hold a domain knowledge base could also be used to perform many of the computation required by a natural language generation system in fact it seems possible to use the classifier to encode and execute arbitrary program we discus ida s classification system and how it differs from other such system perhaps most notably in the presence of template construct that enable recursion to be encoded give example of program fragment encoded in the classification system and compare the classification approach to other ai programming paradigm e g logic programming 
we report progress on a new approach to combatting illiteracy getting computer to listen to child read aloud we describe a fully automated prototype coach for oral reading it display a story on the screen listens a a child read it and decides whether and how to intervene we report on pilot experiment with low reading second grader to test whether these intervention are technically feasible to automate and pedagogically effective to perform by adapting a continuous speech recognizer we detected of the misread word with a false alarm rate under by incorporating the intervention in a simulated coach we enabled the child to read and comprehend material at a reading level year higher than what they could read on their own we show how the prototype us the recognizer to trigger these intervention automatically 
learning system that express theory in firstorderlogic must ensure that the theory areexecutable and in particular that they do notlead to infinite recursion this paper presentsa heuristic method for preventing infinite recursionin the multi clause definition of arecursive relation the method ha been implementedin the latest version of foil but couldalso be used with any learning method thatgrows clause from ground fact by repeatedspecialization result on several 
in del val and shoham we showed that the postulate for belief update recently proposed by katsuno and mendelzon can be analytically derived using the formal theory of action proposed by lin and shoham the contribution of this paper is twofold whereas in del val and shoham we only showed that our encoding of the update problem satisfied the km postulate here we use an independently motivated generalization of the theory of action used in that paper to provide a one to one correspondence between our construction and km update semantics we show how the km semantics can be generalized by relaxing our construction in a number of way each justified in certain intuitive circumstance and each corresponding to one specific postulate it follows that there are reasonable update operator outside the km family 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
this paper is concerned with the problem of determining the indirect effect or ramification of action we argue that the standard framework in which background knowledge is given in the form of state constraint is inadequate and that background knowledge should instead be given in the form of causal law we represent causal law first a inference rule and later a sentence in a modal conditional logic gflatfor the framework with causal law we propose a simple fixpoint condition defining the possible next state after performing an action this fixpoint condition guarantee minimal change between state but also enforces the requirement that change be caused ramification and qualification constraint can be expressed a causal law 
new approach to solving constraint satisfaction problem using iterative improvement technique have been found to be successful on certain very large problem such a the million queen however on highly constrained problem it is possible for these method to get caught in local minimum in this paper we present genet a connectionist architecture for solving binary and general constraint satisfaction problem by iterative improvement genet incorporates a learning strategy to escape from 
agent may sub contract some of their task to other agent s even when they don t share a common goal an agent try to contract some of it task that it can t perform by itself or when the task may be performed more efficiently or better by other agent a selfish agent may convince another selfish agent to help it with it task even if the agent are not assumed to be benevolent by promise of reward we propose technique that provide efficient way to reach subcontracting in varied situation the agent have full information about the environment and each other v subcontracting when the agent don t know the exact state of the world we consider situation of repeated encounter case of asymmetric information situation where the agent lack information about each other and case where an agent subcontract a task to a group of agent we also consider situation where there is a competition either among contracted agent or contracting agent in all situation we would like the contracted agent to carry out the task efficiently without the need of close supervision by the contracting agent the contract that are reached are simple pareto optimal and stable 
the dempster shafer theory give a solid ba si for reasoning application characterized by uncertainty a key feature of the theory is that proposition are represented a subset of a set which represents a hypothesis space this power set along with the set operation is a boolean algebra can we generalize the theory to cover arbitrary boolean algebra we show that the answer is yes the theory then cover for example infinite set the practical advantage of generalizatio n are that increased flexibility of representatio n is al lowed and that the performance of evidence ac cumulation can be enhanced in a previous paper we generalized the dempster shafer orthogonal sum operation to support practical evidence pooling in the present paper we provide the theoretical underpinning of that procedure by systemat ically considering familiar evidential function in turn for each we present a weaker form and we look at the relationship between these variation of the function the relationship are not so strong a for the conventional func tions however when we specialize to the fa miliar case of subset we do indeed get the wellknown relationship 
though dealing with different number of agent share negotiation among multiple agent remains an important topic of research in distributed artificial intelligence dai most previous work this subject however ha focused on bilateral negotiation deal that are reached between two agent there ha also been research on n agent agreement which ha considered consensus mechanism such a voting that allow the full group to coordinate itself these group decision making technique however assume that the entire group will or ha to coordinate it action sub group cannot make sub agreement that exclude other member of the group in some domain however it may be possible for beneficial agreement to be reached among subgroup of agent who might be individually motivated to work together to the exclusion of others outside the group this paper considers this more general case of n agent coalition formation we present a simple coalition formation mechanism that us cryptographic technique for subadditive task oriented domain the mechanism is efficient symmetric and individual rational when the domain is also concave the mechanism also satisfies coalition rationality 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
this paper deal with the qualitative visualrecognition of continuous human action sequence based on an analysis of the structureof physical action a framework for temporalsegmentation and qualitative classificationof physical action is proposed in orderto achieve correctness a well a efficiencyin real time action recognition a hierarchicalspatio temporal attention control method is developed a the integration of these idea acognitive architecture for action recognition is 
a planner in the real world must be able to handle uncertainty it must be able to reason about the effect of uncertainty on it plan select plan that avoid uncertain outcome when possible and make contingency plan against different possible outcome when uncertainty cannot be avoided we have constructed such a planner cassandra which ha these property using cassandra we have produced the ant general solution to the key and box challenge problem proposed by michie over twenty year ago 
genetic programming gp is an automatic programming technique that ha recently been applied to a wide range of problem including block world planning this paper describes a series of illustrative experiment in which gp technique are applied to traditional block world planning problem we discus genetic planning in the context of traditional ai planning system and comment on the cost and benefit to be expected from further work 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
novelty detection technique are concept learning method that proceed by recognizing positive instance of a concept rather than differentiating between it positive and negative instance novelty detection approach consequently require very few if any negative training instance this paper present a particular novelty detection approach to classification that us a redundancy compression and non redundancy differentiation technique based on the gluck myers model of the hippocampus a part of the brain critically involved in learning and memory in particular this approach consists of training an autoencoder to reconstruct positive input instance at the output layer and then using this autoencoder to recognize novel instance classification is possible after training because positive instance are expected to be reconstructed accurately while negative instance are not the purpose of this paper is to compare hippo the system that implement this technique to c and feedforward neural network classification on several application 
sensor based navigation is fundamental to any mobile robot conventional statistical approach to the navigation problem maintain an exact global description of environment geometry however in practise the behaviour of real physical sensor and the observation they make of the environment make such central geometric representation extremely fragile to overcome such problem this paper proposes the use of qualitative model of physical sensor observation these aim to describe the world in term of local sensor centric representation of the observed environment each representation exploit those landmark most natural to the physical sensor involved and no explicit geometric representation of the world is assumed this lead naturally to a navigation process defined in term of relationship between different sensor observables an intrinsically more robust mechanism than found in conventional navigation algorithm the representation and navigation methodology proposed is illustrated using sonar data from a real vehicle 
the problem of making optimal decision in uncertain condition is central to artificial intelligence if the state of the world is known at all time the world can be modeled a a markov decision process mdp mdps have been studied extensively and many method are known for determining optimal course of action or policy the more realistic case where state information is only partially observable partially observable markov decision process pomdps have received much le attention the best exact algorithm for these problem can be very inefficient in both space and time we introduce smooth partially observable value approximation spova a new approximation method that can quickly yield good approximation which can improve over time this method can be combined with reinforcement learning meth od a combination that wa very effective in our test case 
current computer aided engineering paradigm for supporting synthesis activity in engineering design require the designer to use analysis simulator iteratively in an optimization loop while optimization is necessary to achieve a good final design it ha a number of disadvantage during the early stage of design in the inverse engineering methodology machine learning technique are used to learn a multidirectional model that provides vastly improved synthesis and analysis support to the designer this methodology is demonstrated on the early design of a diesel engine combustion chamber for a truck 
we extend work on difference identification and reduction a a technique for automated reason ing we generalise unification so that term are made equal not only by finding substitution for variable but also by hiding term structure this annotation of structural difference serf to direct rippling a kind of rewriting designed to remove difference in a controlled way on the technical side we give a rule based algo rithm for difference unification and analyze it correctness completeness and complexity on the practical side we present a novel search strategy for efficiently applying these rule fi nally we show how this algorithm can be used in new way to direct rippling and how it can play an important role in theorem proving and other kind of automated reasoning 
we discus the persistence of the indirect ef fects of an action the question when such ef fects are subject to the commonsense law of in ertia and how to describe their evolution in the case when inertia doe not apply our model of nonpersistent effect involves the assumption that the value of the fluent in question is deter mined by the value of other fluents although the dependency may be partially or completely unknown this view lead u to a new highlevel action language ard for action ram ifications and dependency that is capable of describing both persistent and nonpersistent ef fects unlike the action language introduced in the past ard is non markovia n in the sense that the evolution of the fluents described in this language may depend on their history and not only on their current value 
model based diagnosis algorithm face a combi natorial explosion to combat this explosion this paper present a fundamentally new architecture implode which construct an abstract representa tion of the environment the conflict and the diag nosis space using a sensitivity analysis of assump tions experimental result show that the most dra matic improvement is obtained for circuit which are the most difficult to diagnose using previous algorithm moreover typical source of combina torial explosion such a reconvergent fanout are a source of combinatorial implosion for implode 
starling with a first order modal conditional logic which allows unlimited nesting of default and emheddings into any context analyzable in possibleworlds theory i introduce two simple notion of default reasoning the syntactic notion of pnorniied allowed consequence and the semantic notion of prioritized allowed entailment i prove that the one is sound and complete relative to the other 
mechanical linkage are used to transmit and transform motion in this paper we investigate what it mean for a program to understand a linkage our system extract itsunderstanding by analyzing the result of a numerical simulation of the mechanism finding interestingqualitativefeatures looking forsymbolic relationship between these feature and conjecturing a causal relationship between them our system is capable of understanding a varietyof mechanism producing explanation very much like those in standard text 
geographical database system deal with certainbasic topological relation such a quot a overlapsb quot and quot b contains c quot between simplyconnected region in the plane it is of greatinterest to make sound inference from elementarystatements of this form this problem hasbeen identified extensively in the recent literature but very limited progress ha been madetowards addressing the considerable technicaldifficulties involved in this paper we studythe computational problem involved in 
a knowledge representation problem can be sometimesviewed a an element of a family of problem with parameter corresponding to possibleassumptions about the domain under consideration when additional assumption are made the class of domain that are being described becomessmaller so that the class of conclusion thatare true in all the domain becomes larger asa result a satisfactory solution to a parametricknowledge representation problem on the basis ofsome nonmonotonic 
we provide a method based on the theory of markov decision problem for efficient planning in stochastic domain goal are encoded a reward function expressing the desirability of each world state the planner must find a policy mapping from state to action that maximizes future reward standard goal of achievement a well a goal of maintenance and prioritized combination of goal can be specified in this way an optimal policy can be found using existing method but these method are at best polynomial in the number of state in the domain where the number of state is exponential in the number of proposition or state variable by using information about the starting state the reward function and the transition probability of the domain we can restrict the planner s attention to a set of world state that are likely to be encountered in satisfying the goal furthermore the planner can generate more or le complete plan depending on the time it ha available we describe experiment involving a mobile robotics application and consider the problem of schedulilng different phase of the planning algorithm given time constraint 
ftp cmu c p in many real world task the ability to focus attention on the important feature of the input is crucial for good performance in this paper a mechanism for achieving task specific focus of attention is presented a saliency map which is based upon a computed expectation of the content of the input at the next time step indicates which region of the input retina are important for performing the task the saliency map can be used to accentuate the feature which are important and de emphasize those which are not the performance of this method is demonstrated on a real world robotics task autonomous road following the applicability of this method is also demonstrated in a non visual domain architectural and algorithmic detail are provided a well a empirical result 
this paper present an analysis of static and dynamic organizational structure for naturally distributed homogeneous cooperative problem solving environment exemplified by distributed sensor network we first show how the performance of any static organization can be statistically described and then show under what condition dynamic organization do better and worse than static one finally we show how the variance in the agent performance lead to uncertainty about whether a dynamic organization will perform better than a static one given only agent a priori expectation in these case we show when meta level communication about the actual state of problem solving will be useful to agent in constructing a dynamic organizational structure that outperforms a static one viewed in it entirety this paper also present a methodology for answering question about the design of distributed problem solving system by analysis and simulation of the characteristic of a complex environment rather than by relying on single instance example 
this paper study property of iterated revision first rationality result show that in the agm original framework the only revision operation that satisfies two resonable property is the trivial revision then an altenative to the agm framework for studying belief revision and probability postulate is proposed iterated revision are the object of this formalism and the rationality postulate deal with property of iterated revision a set of rationality postulate is presented closely related to the agm postulate a representation result show that those postulate simply serious limitation to the way revision cell dope with the principle of minimal change those postulate are not suitable for belief update but then consideration raise doubt about the adequacy of previous treatment of belief implate 
this paper along with the following paper by john mccarthy introduces some of the topic to be discussed at the ijcai event a philosophical encounter an interactive presentation of some of the key philosophical problem in ai and ai problem in philosophy philosophy need ai in order to make progress with many difficult question about the nature of mind and ai need philosophy in order to help clar ify goal method and concept and to help with several specific technical problem whilst philosophical attack on ai continue to be wel comed by a significant subset of the general public ai defender need to learn how to avoid philosophically naive rebuttal 
accuracy play a central role in developing model of continuous physical system both in the context of developing a new model to fit observation or approximating an existing model to make analysis faster the need for simple yet sufficiently accurate model pervades engineering analysis design and diagnosis task this paper focus on two issue related to this topic first it examines the process by which idealized model are derived second it examines the problem of determining when an idealized model will be sufficiently accurate for a given task in a way that is simple and doesn t overwhelm the benefit of having a simple model it describes ideal a system which generates idealized version of a given model and specifies each idealized model s credibility domain this allows valid future use of the model without resorting to more expensive measure such a search or empirical confirmation the technique is illustrated on an implemented example 
intermediate decision tree are the subtrees of the full unpruned decision tree generated in a breadth first order an extensive empirical in vestigation evaluates the classification error of intermediate decision tree and compare their performance to full and pruned tree em pirical result were generated using c with database from the uci machine learning database repository result show that when attempting to minimize the error of the pruned tree produced by c the best intermediate tree performs significantly better in of the database these and other result question the effectiveness of decision tree pruning strategy and suggest further consideration of the full tree and it intermediate also the result reveal specific property satisfied by database in which the intermediate full tree performs best such relationship improve guideline for selecting appropriate inductive strategy based on domain property 
endowing a computer with an ability to reason with diagram could be of great benefit in term of both human computer interaction and computational efficiency through explicit representation to date research in diagrammatic reasoning ha dealt with intra diagrammatic reasoning reasoning with a single diagram almost to the exclusion of inter diagrammatic reasoning reasoning with related group of diagram we postulate a number of general inter diagrammatic operator and show how such operator can be useful in various diagrammatic domain we develop a heuristic in the domain of game notation derive fingering information in the domain of musical notation and infer new information from related cartograms 
influence diagram id are a graphic formal ism able to provide a compact representation of decision problem id are based on the ax ioms of probability and decision theory and they define a normative framework to model decision making unfortunately id require a large amount of information that is not always available to the decision maker this paper in troduces a new class of id called ignorant in fluence diagram iids able to reason on the basis of incomplete information and to improve the accuracy of their decision a a monotonically increasing function of the available infor mation iids represent a net gain with respect to the traditional id since they are able to ex plicitly represent lack of information without loosing any capability of traditional id when the required information is available further more i id provide a new method to ass the reliability of the decision by replacing the tra ditional sensitivity analysis with a single ana lytical measure 
this paper present the expected solution quality esq method for statistically characterizing scheduling problem and the performance of scheduler the esq method is demonstrated by applying it to a practical telescope scheduling problem the method address the important and difficult issue of how to meaningfully evaluate the performance of a scheduler on a constrained optimization problem for which an optimal solution is not known at the heart of esq is a monte carlo algor ithm that estimate a problem s probability density function with respect to solution quality this quality density function provides a useful characterization of a scheduling problem and it also provides a background against which scheduler performance can be meaningfully evaluated esq provides a unitless measure that combine both schedule quality and the amount of time to generate a schedule 
although communication is generally considered to dominate overprocessing cost in distributed system the problem of communicationcost in multiagent planning ha not been sufficiently addressed onemethod for reducing both communication cost and planning time isthe use of social law social law however can be too restrictive limiting soundness flexible social law can enable multiagent systemsto reap the benefit of reduced communication cost and planning time except in the worst 
formalizing the ontological commitment of a logical language mean offering a way to specify the intended meaning of it vocabulary by constraining the set of it model giving explicit information about the intended nature of the modelling primitive and their a priori relationship we present here a formal definition of ontological commitment which aim to capture the very basic ontological assumption about the intended domain related to issue such a identity and internal structure to tackle such issue a modal framework endowed with mereo topological primitive ha been adopted the paper is mostly based on a revisitation of philosophical and linguistic literature in the perspective of knowledge representation 
interface agent are semi intelligent systemswhich assist user with daily computer basedtasks recently various researcher have proposeda learning approach towards building suchagents and some working prototype have beendemonstrated such agent learn by watchingover the shoulder of the user and detecting patternsand regularity in the user s behavior despitethe success booked a major problem withthe learning approach is that the agent ha tolearn from scratch and thus 
in a previous paper we defined the deep structure of a constraint satisfaction problem to be that set system produced by collecting the nogood ground instance of each constraint and keeping only those that are not supersets of any other we then showed how to use such deep structure to predict where in a space of problem instance an abrupt transition in computational cost is to be expected this paper explains how to augment this model with enough extra detail to make more accurate estimate of the location of these phase transition we also show that the phase transition phenomenon exists for a much wider class of search algorithm than had hitherto been thought and explain theoretically why this is the case 
most artificial intelligence program lack generalitybecause they reason with a single domain theory thatis tailored for a specific task and embodies a host ofimplicit assumption context have been proposedas an effective solution to this problem by providing amechanism for explicitly stating the assumption underlyinga domain theory in addition context canbe used to focus reasoning allow the representation ofmutually incoherent domain theory lift axiom fromone context into 
we develop a model based approach to reasoning in which the knowledge base is representedas a set of model satisfying assignment rather than a logical formula and the set of queriesis restricted we show that for every propositional knowledge base kb there exists a set ofcharacteristic model with the property that a query is true in kb if and only if it is satisfied bythe model in this set we characterize a set of function for which the model based representationis compact and 
for a logical database to faithfully represent our beliefsabout the world one should not only insist onits logical consistency but also on it causal consistency intuitively a database is causally inconsistentif it support belief change that contradict with ourperceptions of causal influence for example comingto conclude that it must have rained only becausethe sprinkler wa observed to be on in this paper we suggest the notion of a causal structure to represent 
we present new strategy for quot probably approximatelycorrect quot pac learning that usefewer training example than previous approach the idea is to observe training examplesone at a time and decide quot on line quot when toreturn a hypothesis rather than collect a largefixed size training sample this yield sequential learning procedure that pac learn by observinga small random number of example we provide theoretical bound on the expectedtraining sample size of our procedure but 
many problem of practical interest can be solved using tree search method because carefully tuned successor ordering heuristic guide the search toward region of the space that are likely to contain solution for some problem the heuristic often lead directly to a solution but not always limited discrepancy search address the problem of what to do when the heuristic fail our intuition is that a failing heuristic might well have succeeded if it were not for a small number of 
interface agent are computer program that employ artificial intelligence technique in order to provide assistance to a user dealing with a particular computer application the paper discus an interface agent which ha been modelled closely after the metaphor of a personal assistant the agent learns how to assist the user by i observing the user s action and imitating them ii receiving user feedback when it take wrong action and iii being trained by the user on the basis of hypothetical example the paper discus how this learning agent wa implemented using memory based learning and reinforcement learning technique it present actual result from two prototype agent built using these technique one for a meeting scheduling application and one for electronic mail it argues that the machine learning approach to building interface agent is a feasible one which ha several advantage over other approach it provides a customized and adaptive solution which is le costly and ensures better user acceptability the paper also argues what the advantage are of the particular learning technique used 
this video demonstrates the program mu the mathematics understander which learns university level pure mathematics the video is both concerned with mu s performance in learning and performing mathematics and it underlying cognitive architecture the contextual memory system cm the change in knowledge representation during proof checking and problem solving are demonstrated graphically 
in a dynamic environment relatedto earlier work on goal directed diagnosis rymon a major issue being addressed in this research ishow contextual change influence the ongoing explanationprocess a an example of the issue to address consider thefollowing scenario implemented in our system theplanner is trying to achieve the goal of catching a planeand generates two possible plan driving to the airportor taking a taxi choosing the option of driving the plan step are 
we consider the problem of assigning probabilistic rating to hypothesis in a natural language interpretation system to facilitate integrating syntactic semantic and conceptual constraint we allow a fully compositional frame representation which permit co indexed syntactic constituent and or semantic entity filling multiple role in addition the knowledge base contains probabilistic information encoded by marginal probability on frame these probability are used to specify typicality of realworld scenario on one hand and conventionality of linguistic usage pattern on the other because the theoretical maximum entropy solution is infeasible in the general case we propose an approximate method this method s strength are it ability to rate compositional structure and it flexibility with respect to the input chosen by the system it is embedded in arbitrary set of hypothesis from the front end processor can be accepted a well a arbitrary subset of constraint heuristically chosen from the long term knowledge base 
this paper describes an uncertainty model ofstereo vision and it application to a visionmotionplanning for a mobile robot in general recognition of an environment requiresmuch computation and the recognition resultincludes uncertainty in planning therefore atrade off must be considered between the costof visual recognition and the effect of informationobtained by recognition such a trade offmust be formulated on the basis of a model ofvision which describes the required time for 
our experience in the ida natural language generation project ha shown u that ida s klone like classifier originally built solely to hold a domain knowledge base could also be used to perform many of the computation required by a natural language generation system in fact it seems possible to use the classifier to encode and execute arbitrary program we discus ida s classification system and how it differs from other such system perhaps most notably in the presence of template construct that enable recursion to be encoded give example of program fragment encoded in the classification system and compare the classification approach to other ai programming paradigm e g logic programming 
we report progress on a new approach to combatting illiteracy getting computer to listen to child read aloud we describe a fully automated prototype coach for oral reading it display a story on the screen listens a a child read it and decides whether and how to intervene we report on pilot experiment with low reading second grader to test whether these intervention are technically feasible to automate and pedagogically effective to perform by adapting a continuous speech recognizer we detected of the misread word with a false alarm rate under by incorporating the intervention in a simulated coach we enabled the child to read and comprehend material at a reading level year higher than what they could read on their own we show how the prototype us the recognizer to trigger these intervention automatically 
learning system that express theory in firstorderlogic must ensure that the theory areexecutable and in particular that they do notlead to infinite recursion this paper presentsa heuristic method for preventing infinite recursionin the multi clause definition of arecursive relation the method ha been implementedin the latest version of foil but couldalso be used with any learning method thatgrows clause from ground fact by repeatedspecialization result on several 
in del val and shoham we showed that the postulate for belief update recently proposed by katsuno and mendelzon can be analytically derived using the formal theory of action proposed by lin and shoham the contribution of this paper is twofold whereas in del val and shoham we only showed that our encoding of the update problem satisfied the km postulate here we use an independently motivated generalization of the theory of action used in that paper to provide a one to one correspondence between our construction and km update semantics we show how the km semantics can be generalized by relaxing our construction in a number of way each justified in certain intuitive circumstance and each corresponding to one specific postulate it follows that there are reasonable update operator outside the km family 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
this paper is concerned with the problem of determining the indirect effect or ramification of action we argue that the standard framework in which background knowledge is given in the form of state constraint is inadequate and that background knowledge should instead be given in the form of causal law we represent causal law first a inference rule and later a sentence in a modal conditional logic gflatfor the framework with causal law we propose a simple fixpoint condition defining the possible next state after performing an action this fixpoint condition guarantee minimal change between state but also enforces the requirement that change be caused ramification and qualification constraint can be expressed a causal law 
new approach to solving constraint satisfaction problem using iterative improvement technique have been found to be successful on certain very large problem such a the million queen however on highly constrained problem it is possible for these method to get caught in local minimum in this paper we present genet a connectionist architecture for solving binary and general constraint satisfaction problem by iterative improvement genet incorporates a learning strategy to escape from 
agent may sub contract some of their task to other agent s even when they don t share a common goal an agent try to contract some of it task that it can t perform by itself or when the task may be performed more efficiently or better by other agent a selfish agent may convince another selfish agent to help it with it task even if the agent are not assumed to be benevolent by promise of reward we propose technique that provide efficient way to reach subcontracting in varied situation the agent have full information about the environment and each other v subcontracting when the agent don t know the exact state of the world we consider situation of repeated encounter case of asymmetric information situation where the agent lack information about each other and case where an agent subcontract a task to a group of agent we also consider situation where there is a competition either among contracted agent or contracting agent in all situation we would like the contracted agent to carry out the task efficiently without the need of close supervision by the contracting agent the contract that are reached are simple pareto optimal and stable 
the dempster shafer theory give a solid ba si for reasoning application characterized by uncertainty a key feature of the theory is that proposition are represented a subset of a set which represents a hypothesis space this power set along with the set operation is a boolean algebra can we generalize the theory to cover arbitrary boolean algebra we show that the answer is yes the theory then cover for example infinite set the practical advantage of generalizatio n are that increased flexibility of representatio n is al lowed and that the performance of evidence ac cumulation can be enhanced in a previous paper we generalized the dempster shafer orthogonal sum operation to support practical evidence pooling in the present paper we provide the theoretical underpinning of that procedure by systemat ically considering familiar evidential function in turn for each we present a weaker form and we look at the relationship between these variation of the function the relationship are not so strong a for the conventional func tions however when we specialize to the fa miliar case of subset we do indeed get the wellknown relationship 
though dealing with different number of agent share negotiation among multiple agent remains an important topic of research in distributed artificial intelligence dai most previous work this subject however ha focused on bilateral negotiation deal that are reached between two agent there ha also been research on n agent agreement which ha considered consensus mechanism such a voting that allow the full group to coordinate itself these group decision making technique however assume that the entire group will or ha to coordinate it action sub group cannot make sub agreement that exclude other member of the group in some domain however it may be possible for beneficial agreement to be reached among subgroup of agent who might be individually motivated to work together to the exclusion of others outside the group this paper considers this more general case of n agent coalition formation we present a simple coalition formation mechanism that us cryptographic technique for subadditive task oriented domain the mechanism is efficient symmetric and individual rational when the domain is also concave the mechanism also satisfies coalition rationality 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
this paper deal with the qualitative visualrecognition of continuous human action sequence based on an analysis of the structureof physical action a framework for temporalsegmentation and qualitative classificationof physical action is proposed in orderto achieve correctness a well a efficiencyin real time action recognition a hierarchicalspatio temporal attention control method is developed a the integration of these idea acognitive architecture for action recognition is 
a planner in the real world must be able to handle uncertainty it must be able to reason about the effect of uncertainty on it plan select plan that avoid uncertain outcome when possible and make contingency plan against different possible outcome when uncertainty cannot be avoided we have constructed such a planner cassandra which ha these property using cassandra we have produced the ant general solution to the key and box challenge problem proposed by michie over twenty year ago 
genetic programming gp is an automatic programming technique that ha recently been applied to a wide range of problem including block world planning this paper describes a series of illustrative experiment in which gp technique are applied to traditional block world planning problem we discus genetic planning in the context of traditional ai planning system and comment on the cost and benefit to be expected from further work 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
novelty detection technique are concept learning method that proceed by recognizing positive instance of a concept rather than differentiating between it positive and negative instance novelty detection approach consequently require very few if any negative training instance this paper present a particular novelty detection approach to classification that us a redundancy compression and non redundancy differentiation technique based on the gluck myers model of the hippocampus a part of the brain critically involved in learning and memory in particular this approach consists of training an autoencoder to reconstruct positive input instance at the output layer and then using this autoencoder to recognize novel instance classification is possible after training because positive instance are expected to be reconstructed accurately while negative instance are not the purpose of this paper is to compare hippo the system that implement this technique to c and feedforward neural network classification on several application 
sensor based navigation is fundamental to any mobile robot conventional statistical approach to the navigation problem maintain an exact global description of environment geometry however in practise the behaviour of real physical sensor and the observation they make of the environment make such central geometric representation extremely fragile to overcome such problem this paper proposes the use of qualitative model of physical sensor observation these aim to describe the world in term of local sensor centric representation of the observed environment each representation exploit those landmark most natural to the physical sensor involved and no explicit geometric representation of the world is assumed this lead naturally to a navigation process defined in term of relationship between different sensor observables an intrinsically more robust mechanism than found in conventional navigation algorithm the representation and navigation methodology proposed is illustrated using sonar data from a real vehicle 
the problem of making optimal decision in uncertain condition is central to artificial intelligence if the state of the world is known at all time the world can be modeled a a markov decision process mdp mdps have been studied extensively and many method are known for determining optimal course of action or policy the more realistic case where state information is only partially observable partially observable markov decision process pomdps have received much le attention the best exact algorithm for these problem can be very inefficient in both space and time we introduce smooth partially observable value approximation spova a new approximation method that can quickly yield good approximation which can improve over time this method can be combined with reinforcement learning meth od a combination that wa very effective in our test case 
current computer aided engineering paradigm for supporting synthesis activity in engineering design require the designer to use analysis simulator iteratively in an optimization loop while optimization is necessary to achieve a good final design it ha a number of disadvantage during the early stage of design in the inverse engineering methodology machine learning technique are used to learn a multidirectional model that provides vastly improved synthesis and analysis support to the designer this methodology is demonstrated on the early design of a diesel engine combustion chamber for a truck 
we have implemented a comprehensive constraint based programming language a an extension to common lisp this constraint package provides a unified framework for solving both numeric and non numeric system of constraint using a combination of local propagation technique including binding propagation boolean constraint propagation generalized forward checking propagation of bound and unification the backtracking facility of the nondeterministic dialect of common lisp used to implement this constraint package act a a general fallback constraint solving method mitigating the incompleteness of local propagation 
we introduce a general framework for con straint solving where classical csps fuzzy csps weighted csps partial constraint sat isfaction and others can be easily cast the framework is based on a semiring structure where the set of the semiring specifies the val ues to be associated to each tuple of value of the variable domain and the two semiring op erations and x model constraint projec tion and combination respectively local con sistency algorithm a usually used for clas sical csps can be exploited in this general framework a well provided that some condi tions on the semiring operation are satisfied we then show how this framework can be used to model both old and new constraint solving scheme thus allowing one both to formally justify many informally taken choice in exist ing scheme and to prove that the local con sistency technique can be used also in newly defined scheme 
a solution to the ramification problem caused by underlying domain constraint in strip like approach is presented we introduce the notion of causal relationship which are used in a post processing step after having applied an action description moreover we show how the information needed for these post computation can be automatically extracted from the domain constraint plus general knowledge of which fluents can possibly affect each other we illustrate the necessity of causal relationship by an example that show the limitedness of a common method to avoid unintended ramification namely the distinction between so called frame and non frame fluents finally we integrate our solution into a recently developed strip iike yet purely deductive approach to reasoning about action based on equational logic programming 
flexibility and e ciency these are the two con icting requirement for a multilingualtext planning component generating a natural language text requires a greatamount of exibility in the rule set employed and subsequently in the control mechanism for any local input semantic structure there are in general manyways toimplement it eachchoice produce ripple that move out and a ect other part of thetext a choice that is locally optimal may in the big picture constrain 
the paper evaluates the effectiveness of learning for speeding up the solution of constraint satisfaction problem it extends previous work dechter by introducing a new and powerful variant of learning and by presenting an extensive empirical study on much larger and more difficult problem instance our result show that learning can speed up backjumping when using either a fixed or dynamic variable ordering however the improvement with a dynamic variable ordering is not a great 
this thesis present em sodabot a general purpose software agent userenvironment and construction system it primary component is the em basic software agent a computational framework for building agent which is essentially an em agent operating system we also present a new language for programming the basic software agent whose primitive are designed around human level description of agent activity via this programming language em user can easily implement a wide range of typical software agent application e g personal on line assistant and meeting scheduling agent the sodabot system ha been implemented and tested and it description comprises the bulk of this thesis 
instantiation ordering over formula the relation of one formula bemg an instance of an other have long been central to the study of automated deduction and logic programming and are of rapidly growi ng importance in the study of database system and machine learn ing a variety of instantiation ordenngs are now ip use many of which incorporate some kind of background information in the form of a constraint theory even a casual exami nation of these instantiation ordering reveals that they are somehow related but in exactly what way this paper present a general in stantiation ordering of which all these order ings are special case a are other instantia tion ordenngs the paper show that this gen eral ordering ha the semantic property we desire in an instantiation ordering implying that the special case have these property a well the extension to this general ordering is useful in application to inductive logic programming automated deduction and logic programming knowledge base vivification and database sys tems 
reasoning with model based representation is an intuitive paradigm which ha been shown to be theoretically sound and to posse some computational advantage over reasoning with formula based representation of knowledge in this paper we present more evidence to the utility of such representation in real life situation one normally completes a lot of missing context information when answering query we model this situation by augmenting the available knowledge about the world with context specific information we show that reasoning with model based representation can be done efficiently in the presence of varying context information we then consider the task of default reasoning we show that default reasoning is a generalization of reasoning within context in which the reasoner ha many context rule which may be conflicting we characterize the case in which model based reasoning support efficient default reasoning and develop algorithm that handle efficiently fragment of reiter s default logic in particular this includes case in which performing the default reasoning task with the traditional formula based representation is intractable further we argue that these result support an incremental view of reasoning in a natural way 
this paper is to show that propositionalcontextual reasoning is decidable propositional logic of context extends classicalpropositional logic with a new modality ist c oe usedto express that the sentence oe is true in the context 
this paper describes a diagnosis algorithm called structure based abduction sab which wa developed in the framework of constraint network the algorithm exploit the structure of the constraint network and is most efficient for near tree problem domain by analyzing the structure of the problem domain the performance of such algorithm can be bounded in advance we present empirical result comparing sab with two modelbased algorithm mbd and mbd for the task of finding one or all minimal cardinality diagnosis mbd us the same computing strategy a algorithm gde mbd adopts a breadth first search strategy similar to the algorithm diagnose the main conclusion is that for nearly acyclic circuit such a the n bit adder the performance of sab being linear provides definite advantage a the size of the circuit increase 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
ever since strip wa first introduced fikes and nilsson it logical semantics ha been problematic there have been many proposal in the literature e g lifschitz erol nau and subrahmanian bacchus and yang these all have in common a reliance on metatheoretic operation on logical theory to capture the add and delete list of strip operator but it ha never been clear exactly what these operation correspond to declaratively especially when they are applied to logically incomplete theory in this paper we provide a semantics for strip like system in term of a purely declarative situation calculus axiomatization for action and their effect on our view strip is a mechanism for computing the progression lin and reiter pednault of an initial situation calculus database under the effect of an action we illustrate this idea by describing two different strip mechanism and proving their correctness with respect to their situation calculus specification 
model of physical system can differ according to computational cost accuracy and precision among other thing depending on the problem solving task at hand different model will be appropriate several investigator have recently developed method of automatically selecting among multiple model of physical system our research is novel in that we are developing model selection technique specifically suited to computer aided design our approach is based on the idea that artifact performance model for computer aided design should be chosen in light of the design decision they are required to support we have developed a technique called gradient magnitude model selection gmms which embodies this principle gmms operates in the context of a hillclimbing search process it selects the simplest model that meet the need of the hillclimbing algorithm in which it operates we are using the domain of sailing yacht design a a testbed for this research we have implemented gmms and used it in hillclimbing search to decide between a computationally expensive potential flow program and an algebraic approximation to analyze the performance of sailing yacht experimental test show that gmms make the design process faster than it would be if the most expensive model were used for all design evaluation gmms achieves this performance improvement with little or no sacrifice in the quality of the resulting design 
supervised classification problem have receivedconsiderable attention from the machinelearning community we propose a novel geneticalgorithm based prototype learning system please for this class of problem givena set of prototype for each of the possibleclasses the class of an input instance is determinedby the prototype nearest to this instance we assume ordinal attribute and prototypesare represented a set of feature value pair agenetic algorithm is used to evolve the 
this article discus the use of analogy to index and organize large database of information we describe the design and implementation of an analogical database supporting ten to hundred of thousand of case the content of the database are parsed news article represented a network of grammatical relation with reference into wordnet for word meaning information the virtue of this approach is it domain independent handling of content analysis efficient algorithm for indexing and matching in this database are described and bneflv discussed and example of their performance are discussed 
predicting the behavior of physical system is essential to both common sense and engineering task it is made especially challenging by the lack of complete precise knowledge of the phenomenon in the domain and the system being modelled we present an implemented approach to automatically building and simulating qualitative model of physical system imprecise knowledge of phenomenais expressed by qualitative representation of monotonic function and variable value incomplete knowledge about the system is either inferred or alternative complete description that will affect behavior are explored the architecture and algorithm used support both effective implementation and formal analysis the expressiveness of the modelling language and strength of the resulting prediction are demonstrated by substantial application to complex system 
abstraction technique are important for solving constraint satisfaction problem with global constraint and low solution density in the presence of global constraint backtracking search is unable to prune partial solution it therefore operates like pure generate and test abstraction improves on generate and test by enabling entire subset of the solution space to be pruned early in a backtracking search process this paper describes how abstraction space can be characterized in term of approximate symmetry of the original concrete search space it defines two special type of approximate symmetry called range symmetry and domain symmetry which apply to function finding problem it also present algorithm for automatically synthesizing hierarchic problem solver based on range or domain symmetry the algorithm operate by analyzing declarative description of class of constraint satisfaction problem both algorithm have been fully implemented this paper concludes by presenting data from experiment testing the two synthesis algorithm and the resulting problem solver on np hard scheduling and partitioning problem 
we address the issue of agent reasoning about other agent nonmonotonic reasoning ability in the framework of a multi agent autoepistemic logic ael in single agent ael nonmonotonic inference are drawn based on all the agent know in a multi agent context such a jill reasoning about jack s nonmonotonic inference this assumption must be abandoned since it cannot be assumed that jill know everything jack know given a specific subject matter like tweety the bird it is more realistic and sufficient if jill only assumes to know all jack know about tweety in order to arrive at jack s nonmonotonic inference about tweety this paper provides a formalization of all an agent know about a certain subject matter based on possible world semantics in a multi agent ael besides discussing various property of the new notion we use it to characterize formula that are about a subject matter in a very strong sense while our main focus is on subject matter that consist of atomic proposition we also address the case where agent are the subject matter 
useful equivalence preserving operation based on antilinks are described these operation eliminate a potentially large number of subsumed path in a negation normal form formula those anti link that directly indicate the presence of subsumed path are characterized these operation are useful for prime implicant implicate algorithm because most of the computational effort in computing the prime implicants and prime implicates of a propositional formula is spent on subsumption check the problem of removing all subsumed path in an nnf formula is shown to be np hard even though such formula may be small relative to the size of their path set the general problem of determining whether a pair of subsumed path is associated with an arbitrary anti link is shown to be np complete further reduction of subsumption check are shown to be available when strictly pure full block are present the effectiveness of operation based on anti link and strictly pure full block is examined with respect to some benchmark example from the literature 
admissible heuristic are worth discovering because they have desirable property in various search algorithm unfortunately effective one one that are accurate and efficiently computable are difficult for human to discover one source of admissible heuristic is from abstraction of a problem the length of a shortest path solution to an abstracted problem is an admissible heuristic for the original problem because the abstraction ha certain detail removed however often too many detail have to be abstracted to yield an efficiently computable heuristic resulting in inaccurate heuristic this paper describes a method to reconstitute the abstracted detail back into the solution to the abstracted problem thereby boosting accuracy while maintaining admissibility our empirical result of applying this paradigm to project scheduling suggest that reconstitution can make a good admissible heuristic even better 
although empirical machine learning ha seen many algorithm one of it most important goal ha been neglected important real world problem often have just a primitive represen tation to which the target concept bear only a remote obscure relationship this considera tion lead to a class of measure that may be ap plied to data to estimate difficulty for standard algorithm a the concept becomes harder current decision tree and decision list method give increasingly poor accuracy though backpropagation doe better a new system for feature construction scale up best the fun damental limitation of standard algorithm is caused by two problem greedy search and representational inadequacy critical analysis and empirical result show that lookahead alle viates the greedy hill climbing problem at high cost but even this is insufficient combining lookahead with feature construction alleviates the complex global replication problem with hard concept for principled algorithm devel opment and good progress researcher need to study hard concept and system behavior using them 
previous approach to bidirectional search require exponential space and they are either le efficient than unidirectional search for finding optimal solution or they cannot even find such solution for difficult problem based on a memory bounded unidirectional algorithm for tree sma we developed a graph search extension and we used it to construct a very efficient memory bounded bidirectional algorithm this bidirectional algorithm can be run for difficult problem with bounded memory in addition it is much more efficient than the corresponding unidirectional search algorithm also for finding optimal solution to difficult problem in summary bidirectional search appears to be the best approach to solving difficult problem and this indicates the extreme usefulness of a paradigm that wa neglected for long 
instance based learning method explicitly remember all the data that they receive they usually have no training phase and only at prediction time do they perform computation then they take a query search the database for similar datapoints and build an on line local model such a a local average or local regression with which to predict an output value in this paper we review the advantage of instance based method for autonomous system but we also note the ensuing cost hopelessly slow computation a the database grows large we present and evaluate a new way of structuring a database and a new algorithm for accessing it that maintains the advantage ot instance based learning earlier attempt to combat the cost of instancebased learning have sacrificed the explicit retention of all data or been applicable only to instancebased prediction based on a small number of near neighbor or have had to reintroduce an explicit training phase in the form of an interpolative data structure our approach build a multiresolution data structure to summarize the database of experience at all resolution of interest simultaneously this permit u to query the database with the same flexibility a a conventional linear search but at greatly reduced computational cost 
this paper address the problem of computing the minimal model of a given cnf propositional theory we present two group of algorithm algorithm in the first group are efficient when the theory is almost horn that is when there are few non horn clause and or when the set of all literal that appear positive in any non horn clause is small algorithm in the other group are efficient when the theory can be represented a an acyclic network of low arity relation our algorithm suggest several characterization of tractable subset for the problem of finding minimal model 
these note discus formalizing context a first class object the basic relation is ist c p it asserts that the proposition p is true in the context c the most important formula relate the proposition true in different context introducing context a formal object will permit axiomatizations in limited context to be expanded to transcend the original limitation this seems necessary to provide ai program using logic with certain capability that human fact representation and human reasoning posse fully implementing transcendence seems to require further extension to mathematical logic i e beyond the nonmonotonic inference method first invented in ai and now studied a a new domain of logic various notation are considered but these note are tentative in not proposing a single language with all the desired capability 
current computing system are just beginning to enable the computational manipulation of temporal medium like video and audio because of the opacity of these medium they must be represented in order to be manipulable according to their content knowledge representation technique have been implicitly designed for representing the physical world and it textual representation temporal medium pose unique problem and opportunity for knowledge representation which challenge many of it assumption about the structure and function of what is represented the semantics and syntax of temporal medium require representational design which employ fundamentally different conception of space time identity and action in particular the effect of the syntax of video sequence on the semantics of video shot demand a representational design which can clearly articulate the difference between the context dependent and contextindependent semantics of video data this paper outline the theoretical foundation for designing representation of video discus medium stream an implemented system for video representation and retrieval and critique related effort in this area 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
most research on machine learning ha focused on scenario in which a learner face a single isolated learning task the lifelong learning framework assumes instead that the learner encounter a multitude of related learning task over it lifetime providing the opportunity for the transfer of knowledge this paper study lifelong learning in the context of binary classification it present the invariance approach in which knowledge is transferred via a learned model of the invariance of the domain result on learning to recognize object from color image demonstrate superior generalization capability if invariance are learned and used to bias subsequent learning 
temporal difference method solve the temporalcredit assignment problem for reinforcementlearning an important subproblem ofgeneral reinforcement learning is learning toachieve dynamic goal although existing temporaldifference method such a q learning can be applied to this problem they do nottake advantage of it special structure this paperpresents the dg learning algorithm whichlearns efficiently to achieve dynamically changinggoals and exhibit good knowledge transfer 
ambiguity is a notorious problem for natural language processing according to result obtained by schmitz and quantz i see disambiguation a a process in which contextual default are used to derive the most preferred interpretation of an expression i show how contextual information comprising grammatical a well a conceptual knowledge can be modeled in a homogeneous manner using terminological logic tl i slightly modify the default extension to tl presented by quantz and royer to allow a relevance ordering between multisets of default the preferred interpretation is the one containing the fewest exception with respect to such an ordering interpretation is thus achieved by exception minimization i combine this idea with deductive and abductive approach to interpretation and show how they can be formalized in term of tl entailment furthermore i obtain a variable depth of analysis by controling the granularity of interpretation via a set of relevant feature 
this paper present an algorithm called justin case schedulkg for building robust schedule that tend not to break the algorithm implement the common sense idea of being prepared for likely error just in case they should occur the just in case algorithm analyzes a given nominal schedule determines the most likely break and reinvokes a scheduler to generate a contingent schedule to cover that break after a number of iteration the just in case algorithm produce a multiply contingent schedule that is more robust than the original nominal schedule the algorithm ha been developed for a real telescope scheduling domain in order to proactively manage schedule break that are due to an inherent uncertainty in observation duration the paper present empirical result showing that the algorithm performs extremely well on a representative problem from this domain 
this paper present a procedure to generate judgment determiner e g many few although such determiner carry very little objective information they are extensively used in everyday language the paper present a precise characterization of a class of such determiner using three semantic test a conceptual representation for set is then derived from this characterization which can serve a an input to a generator capable of producing judgment determiner in a second part a set of syntactic feature controlling the realization of complex determiner sequence is presented the mapping from the conceptual input to this set of syntactic feature is then presented the presented procedure relies on a description of the speaker s argumentative intent to control this mapping and to select appropriate judgment determiner 
l to robot action the developmentof this asrl in simulation and with robot demonstratedthat robot could learn to communicate andcould adapt their language to changing circumstance simulated robot have also created a context dependentasrl in a context dependent language robotwords can have different meaning depending on thethis research is supported by the nsf under professorlynn andrea stein s nsf young investigator awardno iri digital equipment corporation the 
in the light of the success of top down inductionsystems tdis the problem to be addressedconcern the structure of description and theexploration of strategy this paper introduces aformal model to describe tdis based on the use oflattice theory and more precisely on the use of agalois connection one of the advantage of thisformalization is that it justifies the extensive use ofattribute value representation in the case of classicaltdis and generalizes this representation to 
a decision method for reiter s default logic is developed it can determine whether a default theory ha an extension whether a formula is in some extension of a default theory and whether a formula is in every extension of a default the ory the method handle full propositional de fault logic it can be implemented to work in polynomial space and by using only a theorem prover for the underlying propositional logic a a subroutine the method divide default rea soning into two major subtasks the search task of examining every alternative for extension which is solved by backtracking search and the classical reasoning task which can be imple mented by a theorem prover for the underly ing classical logic special emphasis is given to the search problem the decision method em ploy a new compact representation of exten sion which reduces the search space efficient technique for pruning the search space further are developed 
we propose a novel approach to auditory stream segregation which extract individual sound auditory stream from a mixture of sound in auditory scene analysis the hb harmonic based stream segregation system is designed and developed by employing a multi agent system hb us only harmonic a a clue to segregation and extract auditory stream incrementally when the tracer generator agent detects a new sound it spawn a tracer agent which extract an auditory stream by tracing it harmonic structure the tracer sends a feedforward signal so that the generator and other tracer should not work on the same stream that is being traced the quality of segregation may be poor due to redundant and ghost tracer hb cope with this problem by introducing monitor agent which detect and eliminate redundant and ghost tracer hb can segregate two stream from a mixture of man s and woman s speech it is easy to resynthesize speech or sound from the corresponding stream additionally hb can be easily extended by adding agent of a new capability hb can be considered a the first step to computational auditory scene analysis 
a the field of computational vision matures more effort are devoted to vision system that are active and need to interact with their environment in real time a prerequisite for integrating vision and action is the development of a set of representation of the visual system s space time where space includes the system itself thus we are faced with the problem of studying the nature of appropriate representation and also with the computational task of acquiring them in a robust manner and in real time both of these problem are addressed in this paper from a computational point of view in particular we study representation needed by active visual system in order to understand their self motion and the structure of their environment the representation are of le metric information content than the one traditionally used including depth surface normal curvature and d metric value for the parameter of rigid motion etc but they are rich enough to allow the system to perform a large number of action these representation indexed in image coordinate are the direction of translation and the direction of rotation for the case of motion and a monotonic function of the depth value in the case of shape description their advantage come from the fact that they can be computed from minimal and well defined input flow or disparity value along image gradient a opposed to the traditional one which require image correspondence or the utilization of assumption about the environment 
both the dynamic of belief change and the process of reasoning by default can be based on the conditional bezief set of an agent represented a a set of if then rule in this paper we address the open problem of formalizing the dynamic of revising this conditional belief set by new if then rule be they interpreted a new default rule or new revision policy we start by providing a purely semantic characterization based on the semantics of conditional rule which induces logical constraint on any such revision process we then introduce logical syntax independent and syntax dependent technique and provide a precise characterization of the set of conditionals that hold after the revision in addition to formalizing the dynamic of revising a default knowledge base this work also provides some of the necessary formal tool for establishing the truth of nested conditionals and attacking the problem of learning new default 
this paper describes an interdisciplinary experiment in controlling semi autonomous animated human form with natural language input these computer generated character resemble traditional stage actor in that they are more autonomous than traditional hand guided animated character and le autonomous than fully improvisational agent we introduce the desktop tkeatev metaphor reserving for user the creative role of a theatrical writer or director 
formal ai system traditionally represent knowledge using logical formula we will show however that for certain kind of information a model based representation is more compact and enables faster reasoning than the corresponding formula based representation the central idea behind our work is to represent a large set of model by a subset of characteristic model more specifically we examine model based representation of horn theory and show that there are large horn theory that can be exactly represented by an exponentially smaller set of characteristic model in addition we will show that deduction based on a set of characteristic model take only linear time thus matching the performance using horn theory more surprisingly abduction can be performed in polynomial time using a set of characteristic model whereas abduction using horn theory is np complete 
in this paper we develop a conceptual framework in which act of manipulation are undertaken for the sake of perceiving material within this frame work we disambiguate different material by ac tively contacting and probing them and by sen ing the resulting force displacement and sound we report experimental result from four separate implementation of this framework using a vari ety of sensory modality including force vision and audition for each implementation we iden tify sensor derived measure that are diagnostic of material property and use those measure to cate gorize object by their material class based on the experimental result we conclude that the issue of shape in variance is of critical importance for future work 
abstract discrete relaxation is frequently used to com pute the fixed point of a discrete system where is monotonic with respect to some partial order given an appropriate initial value for x discrete relaxation repeat the assignment until a fixed point for is found monotonicity of with respect to is a sufficient but in general not necessary condition for iterative hill climbing tech niques such a discrete relaxation to find the fixed point of inthis paper we introduce monotonic asynchronous iteration a a novel way of imple menting parallel discrete relaxation in prob lem domain for which monotonicity is a necessary condition this is an optimistic technique that maintains monotonicity without limiting concurrency resulting in good parallel perfor mance we illustrate this technique with the parallel implementation of a constraint satisfac tion system that computes globally consistent solution and present performance number for experiment on a shared memory implementa tion the performance number show that it is indeed possible to obtain a reasonable speedup when parallelizing global constraint satisfac tion we believe that monotonic asynchronous 
machine learning approach to knowledge compilation seek to improve the perfonnance of problem solver by storing solution to previously solved problem in an efficient generalized fonn the problem solver retrieves these learned solution in appropriate later situation to obtain result more efficiently however by relying on it learned knowledge to provide a solution the problem solver may miss an alternative solution of higher quality one that could have been generated using the original non learned problem solving knowledge this phenomenon is referred to a the itulsking effect of learning in this paper we examine a sequence of possible solution for the masking effect each solution refines and build on the previous one the fmal solution is based on cascaded filter when learned knowledge is retrieved these filter alert the system about the inappropriateness of this knowledge so that the system can then derive a better alternative solution we analyze condition under which this solution will perfonn better than the others and present experimental data supportivt of the analysis this investigation is based on a simulated robot domain called groundworld 
since it inception artificial intelligence ha relied upon a theoretical foundation centred around perfect rationality a the desired property of intelligent system we argue a others have done that this foundation is inadequate because it imposes fundamentally unsatisfiable requirement a a result there ha arisen a wide gap between theory and practice in ai hindering progress in the field we propose instead a property called bounded optimality roughly speaking an agent is bounded optimal if it program is a solution to the constrained optimization problem presented by it architecture and the task environment we show how to construct agent with this property for a simple class of machine architecture in a broad class of real time environment we illustrate these result using a simple model of an automated mail sorting facility we also define a weaker property asymptotic bounded optimality abo that generalizes the notion of optimality in classical complexity theory we then construct universal abo program i e program that are abo no matter what real time constraint are applied universal abo program can be used a building block for more complex system we conclude with a discussion of the prospect for bounded optimality a a theoretical basis for ai and relate it to similar trend in philosophy economics and game theory 
several practical inductive logic programming system efficiently learn determinate clause of constant depth recently it ha been shown that while nonrecursive constant depth determinate clause are pat learnable most of the obvious syntactic generalization of this language are not pat learnable in this paper we introduce a new restriction on logic program called locality and present two formal result first the language of nonrecursive clause of constant locality is paclearnable second the language of nonrecursive clause of constant locality is strictly more expressive than the language of nonrecursive determinate clause of constant depth hence constantlocality clause are a pat learnable generalization of constant depth determinate clause a they suggest that efficient general purpose learning algorithm for non determinate or arbitrary depth clause may be difficult to find in this paper we introduce a restriction on logic program called locality and present two new formal result first we show that clause of constant locality are pat learnable second we show that the language of clause of constant locality is strictly more expressive than the language of determinate clause of constant depth hence the language of constant locality clause is a pat learnable generalization of the language of constant depth determinate clause 
in this paper we investigate the simple logical property of context we describe both the syntax and semantics of a general propositional language of context and give a hilbert style proof system for this language a propositional logic of context extends classical propositional logic in two way firstly a new modality ist k is introduced it is used to express that the sentence hold in the context k secondly each context ha it own vocabulary i e a set of propositional atom which are defined or meaningful in that context the main result of this paper are the soundness and completeness of this hilbert style proof system we also provide soundness and completeness result i e correspondence theory for various extension of the general system 
the standard approach in al to knowledge representation is to represent an agent s knowledge symbolically a a collection of formula which we can view a a knowledge base an agent is then said to know a fact if it is provable from the formula in his knowledge base halpem and vardi advocated a model theoretic approach to knowledge representation in this approach the key step is representing the agent s knowledge using an appropriate semantic model here we model knowledge base operationally a multi agent system our result show that this approach offer significant advantage 
a major issue in case based system is retrievingthe appropriate case from memory to solve a given problem this implies that a case should be indexed appropriately when stored in memory a case based system being dynamic in that it store case for reuse need to learn indi ce for the new knowledge a the system designer cannot envision that knowledge irrespective of the type of indexing structural or functional a hierarchical organization of the case memory raise two distinct but rela ted issue in index learning learning the indexing vocabularyand learning the right level of generalization in this paper we show how structure behavior function sbf model help in learning structural index to design case in the domain of physical device the sbf model of a design provides the functional and causal explanation of how the structure of the design delivers it function we describe how the sbf model of a design provides both the vocabulary for structural indexing of design case and the inductive bias for index generalizat ion we further discus how model based learning can be integrated with similarity based learning that use s prior design case for learning the level of index generalization 
we propose a translation approach from modal logic to first order predicate logic which combine advantage from both the standard relational translation and the rather compact functional translation method and avoids many of their respective disadvantage exponential growth versus equality handling in particular in the application to serial modal logic it allows considerable simplification such that often even a simple unit clause suffices in order to express the accessibility relation property although we restrict the approach here to first order modal logic theorem proving it ha been shown to be of wider interest a e g sorted logic or terminological logic 
the traditional goal of computer vision to reconstruct or recover property of the scene ha recently been challenged by advocate of a new purposive approach in which the vision problem is defined in term of the goal of an active agent in the starkest light the debate can be characterized a one about the role of explicit representation the extreme traditionalist strive for a detailed representation of the d world while the other extreme adopts a strict behaviorist stance which eschews representation in favor of direct sensing this panel will explore the role of action representation and purpose in computer vision and in doing so will hopefully discover area of agreement 
the outline and concept of the new year japanese project called real world computing program will be introduced it is the successor to the fifth generation computer project and aim at theoretical and technological foundation for human like flexible information processing and intelligence toward the highly information based society of the st century 
the connection between vision and natural language system in ai research relies on what is often called reference semantics in the situation of a radio reporter for soccer game an utterance must be perceptually anchored and coherent in order to be understandable to a listener not able to see the scene accordingly the speaker must be able to anticipate the listener understanding by mean of mental image in this paper we demonstrate the comparison of mental image and visual perception on the level of spatial relation and show how to employ the result for cooperatively filling optional deep case and for controlling the use of underspecific definite description 
sequel is a new generation functional programming language which allows the specification of type in a notation based on the sequent calculus the sequent calculus notation suffices for the construction of type for type checking and for the specification of arbitrary logic compilation technique derived from both functional and logic programing are used to derive high performance atp from these specification 
using the method demonstrated in this paper a robot with an unknown sensorimotor system can learn set of feature and behavior adequate to explore a continuous environment and abstract it to a finitestate automaton the structure of this automaton can then be learned from experience and constitutes a cognitive map of the environment a generate andtest method is used to define a hierarchy of feature defined on the raw sense vector culminating in a set of continuously differentiable local 
there are many formal approach to specifying how the mental state of an agent entail that it perform particular action these approach put the agent at the center of analysis for some question and purpose it is more realistic and convenient for the center of analysis to be the task environment domain or society of which agent will be a part this paper present such a task environment oriented modeling framework that can work hand in hand with more agent centered approach our approach feature careful attention to the quantitative computational interrelationship between task to what information is available and when to update an agent s mental state and to the general structure of the task environment rather than single instance example this framework avoids the methodological problem of relying solely on single instance example and provides concrete meaningful characterization with which to state general theory task environment model built in our framework can be used for both analysis and simulation to answer question about how agent should be organized or the effect of various coordination algorithm on agent behavior this paper is organized around an example model of cooperative problem solving in a distributed sensor network 
an active area of research in machine learning is learning logic program from example this paper investigates formally the problem of learning a single horn clause we focus on generalization of the language of constant depth determinate clause which is used by several practical learning system we show first that determinate clause of logarithmic depth are not learnable next we show that learning indeterminate clause with at most k indeterminate variable is equivalent to learning dnf finally we show that recursive constant depth determinate clause are not learnable our primary technical tool is the method of predictionpreserving reducibilities introduced by pitt and warmuth a a consequence our result are independent of the representation used by the learning system 
a knowledge representation server is described which provides a fast memory efficient and principled system component modeling the server through intensional algebraic semantics lead naturally to an open architecture class library into which new data type may be plugged in a required without change to the basic deductive engine it is shown that the operation of an existing knowledge representation system classic may be implemented through one data type supporting set with upper and lower set and cardinality bound the architecture developed is cleanly layered by complexity of inference so that fast propagation of constraint is separated from potentially slow model checking search client program may obtain estimate of the complexity of a request and may control the resource allocated to it complete solution 
difficult concept arise in many complex formative or poorly understood real world domain high interaction among the data attribute cause problem for many learning algorithm including greedy decision tree builder extension of basic method and even backpropagation and mar a new algorithm lfc us directed lookahead search to address feature interaction improving hypothesis accuracy at reasonable cost lfc also address a second problem the general verbosity or global replication problem the algorithm cache search information a new feature for decision tree construction the combination of these two design factor lead to improved prediction accuracy concept compactness and noise tolerance empirical result with synthetic boolean concept bankruptcy prediction and bond rating show typical accuracy improvement of with lfc over several alternative algorithm in case of moderate feature interaction lfc also explicates latent relationship in the training data to provide useful intermediate concept from the perspective of domain expert 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
in this paper we present a logical framework for defining consistent axiomatizations of planning domain a language to define basic action and structured plan is embedded in a logic this allows general property of a whole plan ning scenario to be proved a well a plan to be formed deductively in particular frame a sertions and domain constraint a invariant of the basic action can be formulated and proved even for complex plan most frame assertion are obtained by purely syntactic analysis in such case the formal proof can be generated in a uniform way the formalism we introduce is especially useful when treating recursive plan a tactical theorem prover the karlsruhe inter active verifier kiv is used to implement this logical framework 
we present a logic which allows u to reason about acting and more specifically about sensing i e action that acquire information from the real world and planning i e action that generate and execute plan of action this logic take into account the fact that a it happens in real system action may fail and provides the ability of reasoning about failure handling in acting sensing and planning we see this work a a first step towards a formal account of system which are able to plan to act plan to sense and plan to plan and therefore to integrate action perception and reasoning 
the primary goal of inductive learning is to generalize well that is induce a function that accurately produce the correct output for future input hansen and salamon showed that under certain assumption combining the prediction of several separately trained neural network will improve generalization one of their key assumption is that the individual network should be independent in the error they produce in the standard way of performing backpropagation this assumption may be violated because the standard procedure is to initialize network weight in the region of weight space near the origin this mean that backpropagation s gradient descent search may only reach a small subset of the possible local minimum in this paper we present an approach to initializing neural network that us competitive learning to intelligently create network that are originally located far from the origin of weight space thereby potentially increasing the set of reachable local minimum we report experiment on two real world datasets where combination of network initialized with our method generalize better than combination of network initialized the traditional way 
this paper describes the integration of analogical reasoni ng into general problem solving a a method of learning at the strategy level to solve problem more effectively learnin g occurs by the generation and replay of annotated derivational trace of problem solving episode the problem solver is extended with the ability to examine it decision cycle and accumulate knowledge from the chain of success and failure encountered during it search experien ce instead of investing substantial effort deriving general r ules of behavior to apply to individual decision the analogica l reasoner compiles complete problem solving case that are used to guide future similar situation learned knowledge is flexibly applied to new problem solving situation even if only a partial match exists among problem we relate this work with other alternative strategy learning method and also with plan reuse we demonstrate the effectiveness of the analogical replay strategy by providing empiric al result on the performance of a fully implemented system prodigy analogy accumulating and reusing a large case library in a complex problem solving domain 
simulated annealing sa procedure can potentially yield near optimal solution to many difficult combinatorial optimization problem though often at the expense of intensive computational effort the single most significant source of inefficiency in sa search is it inherent stochasticity typically requiring that the procedure be rerun a large number of time before a near optimal solution is found this paper describes a mechanism that attempt to learn the structure of the search space over multiple sa run on a given problem specifically probability distribution are dynamically updated over multiple run to estimate at different checkpoint how promising a sa run appears to be based on this mechanism two type of criterion are developed that aim at increasing search efficiency a cuto criterion used to determine when to abandon unpromising run and restart criterion used to determine whether to start a fresh sa run or restart search in the middle of an earher run experimental result obtained on a class of complex job shop scheduling problem show that sa can produce high quality solution for this class of problem if run a large number of time and that our learning mechanism can significantly reduce the computation time required to find high quality solution to these problem the result further indicate that the closer one want to be to the optimum the larger the speedup 
in this paper we present a content planning system which take into consideration a user s boredom and cognitive overload our system applies a constraint based optimization mechanism which maximizes a probabilistic function of a user s belief and us a representation of boredom and overload a constraint that affect the possible value of this function further we discus two orthogonal policy for relaxing the parameter of the communication process when these constraint are violated conveying le information or breaking up the material into smaller chunk 
in recent year various formalization of nonmonotonic reasoning and di rent semantics for normal and disjunctive logic program have been proposed including autoepisttic logic circumscription cwa gcwa ecwa epi mic specfications stable well founded stationary and static semantics of normal and disjunctive logic program in this paper we introduce a simple non monotonic knowledge representation framework which isomorphically contains all of the above mentioned nonmonotonic formalism and semantics a special case and yet is significantly more expressive than each one of these formalism considered individually the new formalism called the autoepidemic logic of minimal belief aelb is obtained by augmenting moore s autoepistemic logic ael with an additional minimal belief operator b which allows u to explicitly talk about minimally entailed formula the existence of such a uniform framework not only result in a new powerful non monotonic formalism 
this paper describes a complete face tracking systemthat interprets human head movement in real time the system combine motion analysis with reliable andefficient object recognition strategy it classifies headmovements a quot yes quot nodding head quot no quot shakinghead or quot nothing quot still head the system s skill allowscontactless man machine interaction thus givingaccess to a number of new application introductionas industrialization proceeds the importance of interaction 
we derive from first principle the basic equation for a few of the basic hidden markov model word tagger a well a equation for other model which may be novel the description in previous paper being too spare to be sure we give performance result for all of the model the result from our best model on an unused test sample from the brown corpus with distinct tag is on the upper edge of reported result we also hope these result clear up some confusion in the literature about the best equation to use however the major purpose of this paper is to show how the equation for a variety of model may be derived and thus encourage future author to give the equation for their model and the derivation thereof 
path consistency algorithm which are polynomial for discrete problem are exponential when applied to problem involving quantitative temporal information the source of complexity stem from specifying relationship between pair of time point a disjunction of interval we propose a polynomial algorithm called ult that approximates path consistency in temporal constralllt satisfaction problem tcsps we compare ult empirically to path consistency and directional path consistency algorithm when used a a preprocessing to backtracking ult is shown to be time more effective then either dpc or pc 
we present a data driven protocol and a supporting architecture for communication among cooperating intelligent agent in real time diagnostic system the system architecture and the exchange of information among agent are based on simplicity of agent hierarchical organization of agent and modular non overlapping division of the problem domain these feature combine to enable efficient diagnosis of complex system failure in real time environment with high data volume and moderate failure rate preliminary result of the real world application of this work to the monitoring and diagnosis of complex system are discussed in the context of nasa s interplanetary mission operation 
we consider using machine learning technique to help understand a large software system in particular we describe how learning technique can be used to reconstruct abstract datalog specification of a certain type of database software from example of it operation in a case study involving a large more than one million line of c real world software system we demonstrate that off the shelf inductive logic programming method can be successfully used for specification recovery specifically grende can extract specification for about one third of the module in a test suite with high rate of precision and recall we then describe two extension to grende which improve performance on this task one which allows it to output a set of candidate hypothesis and another which allows it to output specification containing determination in combination these extension enable specification to be extracted for nearly two third of the benchmark module with perfect recall and precision of better than 
an important aspect of partial order planning is the resolution of threat between action and causal link in a plan we present a technique for automatically deciding which threat should be resolved during planning and which should be delayed until planning is otherwise complete in particular we show that many potential threat can be provably delayed until the end that is if the planner can find a plan for the goal while ignoring these threat there is a guarantee that the partial ordering in the resulting plan can be extended to eliminate the threat our technique involves construction of an operator graph that capture the interaction between operator relevant to a given goal decomposition of this graph into group of related threat and postponement of threat with certain property 
this research explores the interaction of textualand photographic information in document understanding the problem of performing generalpurposevision without apriori knowledge is verydifficult at best the use of collateral informationin scene understanding ha been explored in computervision system that use general scene contextin the task of object identification the workdescribed here extends this notion by defining visualsemantics namely technique for systematically 
we suggest a new approach for the study of the non monotonicity of human commonsense reasoning the two main premise that underlie this work are that commonsense reasoning is an inductive phenomenon and that missing information in the interaction of the agent with the environment may be a informative for future interaction a observed information this intuition is normalized and the problem of reasoning from incomplete information is presented a a problem of learning attribute function over a generalized domain we consider example that illustrate various aspect of the non monotonic reasoning phenomenon which have been used over the year a bench mark for various formalism and translate them into learning to reason problem we demonstrate that these have concise representation over the generalized domain and prove that these representation can be learned efficiently the framework developed suggests an operational approach to studying reasoning that is nevertheless rigorous and amenable to analysis we show that this approach efficiently support reasoning with incomplete information and at the same lime match our expectation of plausible pattern of reasoning in case where other theory do not this work continues previous work in the learning to reason framework and support the thesis that in order to develop a computational account for commonsense reasoning one should study the phenomenon of learning and reasoning together 
sri international ha a long tradition in the field of qualitative analysis and control of complex system starting with the development of the early mobile robot shakey more recently we have developed a fuzzy controller for our new platform flakey flakey s controller can pursue strategic goal while operating under condition of uncertainty incompleteness and imprecision this controller includes capability for robust uncertainty tolerating goal directed activity real time reactivity to unexpected contingency e g unknown obstacle blending of multiple goal e g reaching a position while avoiding static and moving obstacle 
collaboration to accomplish common goal necessitate negotiation to share and reach agreement on the belief that agent hold a part of the collaboration negotiation in communication can be simulated by a series of exchange in which agent propose reject counterpropose or seek supporting information for belief they wish to be held mutually in an artificial language of negotiation message display the state of the agent belief dialogue consisting of such message clarify the mean by which agent come to agree or fail to agree on mutual belief and individual intention 
this paper present a simple fast coordination algorithm for the dynamic reorganization of agent in a distributed sensor network dynamic reorganization is a technique for adapting to the current local problemsolving situation that can both increase expected system performance and decrease the variance in performance we compare our dynamic organization algorithm to a static algorithm with lower overhead oneshot refers to the fact that the algorithm only us one meta level communication action the other theme of this paper is our methodology for analyzing complex control and coordination issue without resorting to a handful of single instance example using a general model that we have developed of distributed sensor network environment decker and lesser a we present probabilistic performance bound for our algorithm given any number of agent in any environment that fit our assumption this model also allows u to predict exactly in what situation and environment the performance benefit of dynamic reorganization outweigh the overhead 
incremental concept learning algorithm using backtracking have to store previous data these data can be ordered by the is more specific than relation using this order only the most informative data have to be stored and the le informative data can be discarded moreover under certain condition some data can be replaced by automatically generated more informative data we investigate some condition for data to be discarded independently of the chosen concept learning algorithm or concept representation language then an algorithm for discarding data is presented in the framework of iterative versionspaces which is a depth first algorithm computing versionspaces a introduced by mitchell we update the datastructures used in the iterative versionspaces algorithm while preserving it most important property 
sometimes inferencesmade at somespecifictimeare valid at othertimes too inmodel based diagnosisand monitoring a wellasqualitativesimulationinferencesareoftenre done althoughthey have been performedpreviously wepropose a newmethod for sharingpredictions doneat differenttimes thus mutuallycutting down predictioncosts incurringat different time furthermore we generalizethe technique from sharing prediction across time to sharing predictionsacross time and logicalcontexts assumptionbased truth maintenanceis a form of sharing prediction acrosslogicalcontexts becauseof theclose connectionsto the atm we were able to use it a a mean for implementation we reportempiricalresults on monitoring differentconfigurationsof ballast water tank a used on offshoreplatformsand ship 
the problem of checking for consistency of constraint satisfaction problem csps is a fundamental problem in the field of constraint based reasonning moreover it is a hard problem since satisfiability of csps belongs to the class of npcomplete problem so in freuder freuder gave theoretical result concerning consistency of binary csps two variable per constraint in this paper we proposed an extension to these result to general csp n ary constraint on one hand we define a partial consistency well adjusted to general csps called hyper k consistency on the other hand we proposed a measure of the connectivity of hypergraphs called width of hypergraphs using width of hypergraphs and hyper k consistency we derive a theorem defining a sufficient condition for consistency of general csps 
although many decision combination method have been proposed most of them did not focus on dependency relationship among classifier id combining multiple decision that make classification performance of combining multiple decision be degraded and biased in case of adding highly dependent inferior classifier to overcome such weakness and obtain robust classification performance the present study used dependency relationship for better combining multiple decision in order to identify dependency relationship by observing output of multiple classifier two method are used on the basis of first order dependency relationship one is to use the concept of mutual information and the other one is to use the concept of statistically measured association the first order dependency identified are used to combine multiple decision using bayesian formalism a number of multiple classifier system are applied to totally uncontrained on line handwritten numeral and the english alphabet recognition the experimental result show that the classification performance of a multiple classifier system is superior to that of individual classifier also they show that considering the dependency relationship outperforms others in accuracy when the highly dependent inferior classifier are added 
we propose a kb testing procedure that us a kads conceptual model cm the set of v d inference path is derived from the inference structure and a high level trace representing the czsrrernt inference path is built using the link established between the cm and the kb the comparison of this trace to the vip can lead to modify either the code or the cm 
an important component of an intelligent tutoring system it for teaching geometry is it capacity to transform a figure into a many different figure a possible yet all of which respect the same underlying logical specification given a logical specification for a figure i a figure can be constructed automatically from the object and property in the spepification and ii once constructed one can transform a figure through displacement of any of it object and still obtain a figure that respect the specification for a student user this feature provides an invaluable tool for graphical exploration and discovery of property induced by the logical specification our problem domain is automatic construction of figure and we address this issue in restricted case using constraint logic programming we present solution to case in which figure can be constructed automatically and in which there is also a natural notion of completeness for our system for this automatic figure construction system we describe an implementation written in prolog iii which make use of both constraint and coroutines provided in the language result of experimentation are also included a well a way in which the system can be extended to handle non restricted case 
qualitative spatial reasoning ha many application insuch diverse area a natural language understanding cognitive mapping and reasoning about the physicalworld we address problem whose solution requireintegrated spatial and dynamic reasoning in this paper we present our spatial representation based on the extremal point of object and show thatthis representation is useful for modeling the spatialextent relative position and orientation of object and in reasoning 
gsat is a randomized local search procedure for solving propositional satisfiability problem selman et al gsat can solve hard randomly generated problem that are an order of magnitude larger than those that can be handled by more traditional approach such a the davis putnam procedure gsat also efficiently solves encoding of graph coloring problem n queen and boolean induction however gsat doe not perform a well on handcrafted encoding of block world planning problem and formula with a high degree of asymmetry we present three strategy that dramatically improve gsat s performance on such formula these strategy in effect manage to uncover hidden structure in the formula under consideration thereby significantly extending the applicability of the gsat algorithm 
decision tree have provided a classical mechanism for progressively narrowing down a search from a large group of possibility to a single alternative the structuring of a decision tree is based on a heuristic that maximizes the value of the information gained at each level in the hierarchy decision tree are effective when an agent need to reach the goal of complete diagnosis a quickly a possible and cannot accept a partial solution we present an alternative to the decision tree heuristic which is useful when partial solution do have value and when limited resource may require an agent to accept a partial solution our heuristic maximizes the improvement in the value of the partial solution gained at each level in the hierarchy we term the resulting structure an action based hierarchy we present the result of a set of experiment designed to compare these two heuristic for hierarchy structuring finally we describe some preliminary work we have done in applying these idea to a medical domain surgical intensive care unit sicu patient monitoring 
we present and experimentally evaluate the hypothesis that cooperative parallel search is well suited for hard graph coloring problem near a previously identified transition between underand overconstrained instance we find that simple cooperative method can often solve such problem faster than the same number of independent agent 
many reported discovery system build discrete model of hidden structure property or process in the diverse field of biology chemistry and physic we show that the search space underlying many well known system are remarkably similar when re interpreted a search in matrix space a small number of matrix type are used to represent the input data and output model most of the constraint can be represented a matrix constraint most notably conservation law and their analogue can be represented a matrix equation typically one or more matrix dimension grow a these system consider more complex model after simpler model fail and we introduce a notation to express this the novel framework of matrix space search serf to unify previous system and suggests how at least two of them can be integrated our analysis constitutes an advance toward a generalized account of model building in science 
situated interactive tutorial instruction give flexibility in teaching task by allowingcommunication of a variety of type of knowledge in a variety of situation toexploit this flexibility however an instructable agent must be able to learn differenttypes of knowledge from different instructional interaction this paper present an approachto learning from flexible tutorial instruction called situated explanation thattakes advantage of constraint in different instructional 
we discus the cost and benefit of usingnatural language nl generation technologyto produce documentation and on line helpmessages and propose some intermediate generation technique that provide many althoughnot all of the benefit of more principled deep generation technique but at a significantlylower cost introductionmost research on natural language nl generation hasstressed what might be called deep technique wheretext is generated in a principled way 
most constructive induction researcher focus only on new boolean attribute this paper report a new constructive induction algorithm called x of n that construct new nominal attribute in the form of x of n representation an x of n is a bet containing one or more attribute value pair for a given instance it value corresponds to the number of it attribute value pair that are true the promising preliminary experimental result on both artificial and real world domain show that constructing new nominal attribute in the form of x of n representation can significantly improve the performance of selective induction in term of both higher prediction accuracy and lower theory complexity 
recently several local hill climbing procedure for propositional satisfiability have been proposed which are able to solve large and difficult problem beyond the reach of conventional algorithm like davis putnam by the introduction of some new variant of these procedure we provide strong experimental evidence to support our conjecture that neither greediness nor randomness is important in these procedure one of the variant introduced seems to offer significant improvement over earlier procedure in addition we investigate experimentally how performance depends on their parameter our result suggest that runtime scale le than simply exponentially in the problem size 
qualitative probabilistic network qpns are an abstraction of bayesian belief network replacmg numerical relation by qualitative influence and synergy wellman b to reason in a qpn is to find the effect of new evidence on each node in term of the sign of the change in belief increase or decrease we introduce a polynomial time algorithm for reasoning in qpns based on local sign propagation it extends our previous scheme from singly connected to general multiply connected network unlike existing graph reduction algorithm it preserve the network structure and determines the effect of evidence on all node in the network this aid meta level reasoning about the model and automatic generation of intuitive explanation of probabilistic reasoning 
text classification the grouping of text into several cluster ha been used a a mean of improving both the efficiency and the effective des of text retrieval categorization in this paper we propose a hierarchical clustering algorithm that construct a bet of cluster having the maximum bayesian posterior probability the probability that the given text are classified into cluster we call the algorithm hierarchical bayesian clustering hbc the advantage of hbc are experimentally verified from several viewpoint hbc can reconstruct the original cluster more accurately than do other non probabilistic algorithm when a probabilistic text categorization is extended to a cluster based one the use of hbc offer better performance than doe the use of non probabilistic algorithm 
we define the probabilistic planning problem in termsof a probability distribution over initial world state a boolean combination of goal proposition a probabilitythreshold and action whose effect depend onthe execution time state of the world and on randomchance adopting a probabilistic model complicatesthe definition of plan success instead of demandinga plan that provably achieves the goal we seek planswhose probability of success exceeds the threshold this paper describes 
we describe a multilingual implementation of such a grammar and it advantage over both principlebased parsing and ad hoc grammar design we show how x bar theory and language independent semantic constraint facilitate grammar development our implementation includes innovative handling of syntactic gap logical structure alternation and conjunction each of these innovation enhances performance in both large scale and multilingual natural language processing application phrase structure grammar are hardly new the novelty in this paper come from the use of practical guideline and real number based on our experience with three language and ten of thousand of text the issue of grammar design is worth revisiting because of the increasing bifurcation between semantic phrase grammar on thv one hand and principle based parsing in toy domain on the other semantic grammar are brittle and must be rewritten for each new domain and language principle based parsing is not yet mature enough for our application we offer an extensible multilingual application of the traditional approach that extends theoretical linguistic insight to industrial strength data 
although backpropagation neural network generally predict better than decision tree do for pattern classification problem they are often regarded a black box i e their prediction are not a interpretable a those of decision tree this paper argues that this is because there ha been no proper technique that enables u to do so with an algorithm that can extract rule by drawing parallel with those of decision tree we show that the prediction of a network can be explained via rule extracted from it thereby the network can be understood experiment demonstrate that rule extracted from neural network are comparable with those of decision tree in term of predictive accuracy number of rule and average number of condition for a rule they preserve high predictive accuracy of original network 
the video demonstrates research accomplishment in interactive design and assembly with d computer graphic environment agent technique and dynamic knowledge representation technique are used to process qualitative verbal instruction to quantitative scene change a key idea is to exploit situated perceptive information by inspecting the computer graphic scene model 
the use of primary effect in planning is an effective approach to reducing search the underlying idea of this approach is to select certain important effect among the effect of each operator and to use an operator only for achieving it important effect in the past there ha been little analysis of planning with primary effect and few experimental result we provide empirical and analytical result on the use of primary effect first we experimentally demonstrate that the use of primary effect may lead to an exponential reduction of the planning time second we analytically ex plain the experimental result and identify the factor that influence the efficiency of planning with primary effect third we describe an application of our analysis to predicting the performance of a planner for a given selection of primary effect 
we present the result of an empirical study of severalconstraint satisfaction search algorithm and heuristic using a random problem generator that allows usto create instance with given characteristic we showhow the relative performance of various search methodsvaries with the number of variable the tightnessof the constraint and the sparseness of the constraintgraph a version of backjumping using a dynamicvariable ordering heuristic is shown to be extremely 
this paper investigates alternative estimator of the accuracy of concept learned from example in particular the cross validation and bootstrap estimator are studied using synthetic training data and the foil learning algorithm our experimental result contradict previous paper in statistic which advocate the bootstrap method a superior to cross validation nevertheless our result also suggest that conclusion based on cross validation in previous machine learning paper are unreliable specifically our observation are that i the true error of the concept learned by foil from independently drawn set of example of the same concept varies widely ii the estimate of true error provided by cross validation ha high variability but is approximately unbiased and iii the bootstrap estimator ha lower variability than cross validation but is systematically biased 
traditionally large area of research in machine learning have concentrated on pattern recognition and it application to many diversified problem both within the realm of ai a well a outside of it over several decade of intensified research an array of learning methodology have been proposed accompanied by attempt to evaluate these method with respect to one another on small set of real world problem unfortunately little emphasis wa placed on the problem of learning bias common to all learning algorithm and a major culprit in preventing the construction of a zsniuerscsl pattern recognizer state of the art learning algorithm exploit some inherent bias when performing pattern recognition on yet unseen pattern automatically adapting this learning bias dependent on the type of pattern classification problem seen over time is largely lacking in this paper weakness of the traditional one shot learning environment are pointed out and the move towards a learning method displaying the ability to learn about lecarning is undertaken trans dimensional learning is introduced a a mean to automatically adjust learning bias and empirical evidence is provided showing that in some instance beurning the whole can be simpler than learning a part of it 
the paper attempt to establish a basis upon which it could plausibly be said that knowledge level model typically used in the development of at system such a expert system could have psychological import various modelling methodology are set out and it is shown that these methodology cannot supply psychological explanation of expertise on the basis of ordinary realist assumption about the mind since the knowledge level primitive cannot supply the right sort of link between task and ai method in contrast an anti realist interpretative view of mind is set out and it is shown how ai modelling methodology could in that context be of psychological value finally a short example give some concrete expression to these idea 
anytime algorithm whose quality of result improves gradually a computation time increase provide useful performance component for timecritical planning and control of robotic system in earlier work we introduced a compilation scheme for optimal composition of anytime algorithm in this paper we present an implementation of a navigation system in which an off line compilation process and a run time monitoring component guarantee the optimal allocation of time to the anytime module the crucial meta level knowledge is kept in the anytime library in the form of conditional performance profile we also extend the notion of gradual improvement to sensing and plan execution the result is an efficient flexible control for robotic system that exploit the tradeoff between time and quality in planning sensing and plan execution 
belief revision and belief update have been proposed a two type of belief change serving differ ent purpose belief revision is intended to capture change of an agent s belief state reflecting new information about a static world belief update is intended to capture change of belief in response to a changing world we argue that both belief revision and belief update are too restrictive routine belief change involves element of both we present a model for generalized update that allows update in response to external change to inform the agent about it prior belief this model of update combine aspect of revision and update providing a more realistic characterization of belief change we show that under certain assumption the original update postulate are satisfied we also demonstrate that plain revision and plain update are special case of our model in a way that formally verifies the intuition that revision is suitable for static belief change 
in this paper we give a general analysis of dyadic deontir logic that were introduced in the early eventies to formalize deontic reasoning about subideal behavior recently it wa observed that they are closely related to nonmonotonic logic theory of diagnosis and dc cision theory in particular we argue that two type of defeasibihty must be distinguished in a defeasible deontic logic overridden defeasibility that formalizes cancelling of in obligation by other conditional obligation and factual defeasibility that formalizes overshadowing of an obligation by a violating fact we also show that this distinction is essential for an adequate analysis of notorious paradox of deontic logic such a the chisholm and for rester paradox 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality we can construct distributed representation with oriented energy measure used in model of biological vision surface model of orientation velocity and disparity can easily be fit to distributed representation of texture motion and stereo by combining tool of orientation analysis and regularization we describe base representation construction and model fitting process in these domain 
understanding the design of an engineered device requires both knowledge of the general physical principle that determine the behavior of the device and knowledge of what the device is intended to do i e it functional specification however the majority of work in modelbased reasoning about device behavior ha focused on modeling a device in term of general physical principle or intended functionality but not both in order to use both functional and behavioral knowledge in understanding a device design it is crucial that the functional knowledge is represented in such a way that it ha a clear interpretation in term of actual behavior we propose a new formalism for representing device function with well defined semantics in term of actual behavior we call the language cfrl causal functional representation language cfrl allows the specification of condition that a behavior must satisfy such a occurrence of a temporal sequence of expected event and causal relation among the event and the behavior of device component we have used cfrl a the basis for a functional verification program which determines whether a behavior achieves an intended function 
the study of situated system that are capable of reactive and goal directed behaviour ha received increased attention in recent year one approach to the design of such system is based upon agent oriented architecture this approach ha led to the development of expressive but computationally intractable logic for describing or specifying the behaviour of agent oriented system in this paper we present three propositional variant of such logic with different expressive power and analyze the computational complexity of verifying if a given property is satisfied by a given abstract agent oriented system we show the complexity to be linear time for one of these logic and polynomial time for another thus providing encouraging result with respect to the practical use of such logic for verifying agent oriented system 
we have written a computer program called trendx for automated trend detection during process monitoring the program us a representation called trend template that define disorder a typical pattern of relevant variable these pattern consist of a partially ordered set of temporal interval with uncertain endpoint attached to each temporal interval are value constraint on real valued function of measurable parameter a trendx receives measured data of the monitored process the program creates hypothesis of how the process ha varied over time we introduce the importance of a distinct trend representation in knowledge based system then we demonstrate how trend template may represent trend that occur at fixed time or at unknown time and their utility for domain that are quantitalively both poorly and well understood finally we present experimental result of trendx diagnosing pediatric growth disorder from height weight bone age and pubertal data of twenty patient seen at boston child s hospital 
we have implemented an incremental lexical acquisition mechanism that learns the meaning of previously unknown word from the context in which they appear a a part of the process of parsing and semantically interpreting sentence implement at ion of this algorithm brought to light a fundamental difference between learning verb and learning noun specifically because verb typically play the predicate role in english sentence whereas noun typically function a argument we found that different mechanism were required to learn verb and noun because of this difference in usage our learning algorithm formulates the most specific hypothesis possible consistent with the data for verb meaning but the most general hypothesis possible for noun subsequent example may falsify a current hypothesis causing verb meaning to be generalized and noun meaning to be made more specific this paper describes the two approach used to learn verb and noun in the system and report on the system s performance in substantial empirical testing resnik yarowsky camille us it domain knowledge when inferring the meaning of unknown word the actual process of meaning inference however is not dependent on any particular domain hierarchy it is a weak method that search the hierarchy for an appropriate node for the meaning of a word 
human episodic memory provides a seemingly unlimited storage for everyday experience and a retrieval system that allows u to access the experience with partial activation of their component this paper present a computational model of episodic memory inspired by damasio s idea of convergence zone the model consists of a layer of perceptual feature map and a binding layer a perceptual feature pattern is coarse coded in the binding layer and stored on the weight between layer a partial activation of the stored feature activates the binding pattern which in turn reactivates the entire stored pattern a worst case analysis show that with realistic size layer the memory capacity of the model is several time larger than the number of unit in the model and could account for the large capacity of human episodic memory 
in this paper i extend the standard first order resolution method with special reasoning mechanism for sort sort are unary predicate literal built from unary predicate are called sort literal negative sort literal can be compiled into restriction of the relevant variable to sort or can be deleted if they fulfill special condition positive sort literal define the sort theory sorted unification exploit the sort restriction of variable with respect to the sort theory a occurrence of sort literal are not restricted it may be necessary to add additional literal to resolvent and factor and to dynamically change the sort theory used by sorted unification during the deduction process the calculus i propose thus extends the standard resolution method with sorted unification residue literal and a dynamic processing of the sort information i show that this calculus generalizes and improves existing approach to sorted reasoning finally i give some application to automated theorem proving and abduction 
this paper describes a new admissible tree search algorithmcalled iterative threshold search it itscan be viewed a a much simplified version of ma and a generalized version of mrec we alsopresent the following result every node generated by it is also generatedby ida even if it is given no more memory thanida in addition there are tree on which it generates o n node in comparison to o n log n nodesgenerated by ida where n is the number of node 
numerical simulation of partial differential equation pdes play a crucial role in predicting the behavior of physical system and in modern engineering design however in order to produce reliable result with a pde simulator a human expert must typically expend considerable time and effort in setting up the simulation most of this effort is spent in generating the grid the discretization of the spatial domain which the pde simulator requires a input to properly design a grid the gridder must not only consider the characteristic of the spatial domain but also the physic of the situation and the peculiarity of the numerical simulator this paper describes an intelligent gridder that is capable of analyzing the topology of the spatial domain and predicting approximate physical behavior based on the geometry of the spatial domain to automatically generate grid for computational fluid dynamic simulator typically gridding program are given a pcsrtitioning of the spatial domain to assist the gridder our gridder is capable of performing this partitioning this enables the gridder to automatically grid spatial domain of arbitrary configuration 
one of the main theme in the area of terminologicalreasoning ha been to identify description logic dl that are both very expressive and decidable arecent paper by schild showed that this issue can beprofitably addressed by relying on a correspondencebetween dl and propositional dynamic logic pdl however schild left open three important problem related to the translation into pdl of functional restrictionson role both direct and inverse numberrestrictions and assertion 
problem solving with incomplete information is usually very costly since multiple alternative must be taken into account in the planning process in this paper we present some pruning rule that lead to substantial cost saving the rule are all based on the simple idea that if goal achievement is the sole criterion for performance a planner need not consider one branch in it search space when there is another branch characterized by equal or greater information the idea is worked out for the case of sequential planning conditional planning and interleaved planning and execution the rule are of special value in this last case a they provide a way for the problem solver to terminate it search without planning all the way to the goal and yet be assured that no important alternative are overlooked 
design to time is an approach to real time scheduling in situation where multiple method exist for many task that the system need to solve often ithese method will have relationship with one other such a the execution of one method enabling the execution of another or the use of a rough approximation by one method affecting the performance of a method that us it result most previous work in the scheduling of real time ai task ha ignored these relationship this paper present an optimal design to time scheduler for particular kind of relationship that occur in an actual ai application and examines the performance of that scheduler in a simulation environment that model the task of that application 
automating the construction of semantic grammar is a difficult and interesting problem for machine learning this paper show how the semantic grammar acquisition problem can be viewed a the learning of search control heuristic in a logic program appropriate control rule are learned using a new first order induction algorithm that automatically invents useful syntactic and semantic category empirical result show that the learned parser generalize well to novel sentence and out perform previous approach based on connectionist technique 
in order to measure and analyze the performance of rule based expert system it is necessary to explicate the internal structure of their rule base although a number of attempt have been made in the literature to formalize the structure of a rule base using the notion of a rule base execution path none of these are entirely adequate this paper report a new formal definition for the notion of a rule base execution path which adequately support both validation and performance analysis of rule based expert system this definition for the execution path in a rule base ha been embodied in a rule base analysis tool called path hunter path hunter is used to analyse a rule base consisting of clip rule in this analysis the problem of combinatorial explosion which arises during path enumeration is controlled due to the manner in which path are defined the analysis raise several issue which should be taken into account in the engineering of rule based system 
we describe irs a program that combine partial order planning with gde style model based diagnosis to achieve an integrated approach to repair our system make three contribution to the field of diagnosis first we provide a unified treatment of both information gathering and state altering action via the uwl representation language second we describe a way to use part replacement operation in addition to probe to gather diagnostic information finally we define a cost function for decision making that account for both the eventual need to repair broken part and the dependence of cost on the device state 
this paper proposes a method of nonmonotonic theory change we first introduce a new form of abduction that can account for observation in nonmonotonic situation then we provide a framework of autoepistemic update which describes nonmonotonic theory change through the extended abductive framework the proposed update semantics is fairly general and provides a unified framework for various update semantics such a first order update view update of database and contradiction removal of nonmonotonic theory 
the assumption based truth maintenance system atm de kleer is the most well known implementation of any dynamic reasoning system some connection have been established between the atm and various nonmonotonic logic e g autoepistemic logic reinfrank et al we describe the relationship between the atm and the agm logic of belief gardenfors and show that it is possible to simulate the behaviour of the atm using the agm logic by encoding the justificational information a an epistemic entrenchment ordering the atm context switching is performed by agm expansion and contraction operation we present an algorithm for calculating this entrenchment ordering and prove it correctness relative to a functional specification of the atm this result demonstrates that the agm logic which is based on the coherence theory of justification is able to achieve both coherence and foundational style behaviour via the choice of epistemic entrenchment 
modeling of a connectionist rule based system or neuro aj hybrid system discussed through the paper will be a fruitful step towards the practical modeling of human cognition this paper investigates a plausible and useful integration method of symbolic ai technique and connectionist model and proposes a practical implementation mainly how variable can be included in the structured information provided a fact and rule in the system 
this paper present a new measure of semantic similarity in an is a taxonomy based on the notion of information content experimental evaluation suggests that the measure performs encouragingly well a correlation of r with a benchmark set of human similarity judgment with an upper bound of r for human subject performing the same task and significantly better than the traditional edge counting approach r 
we analyze condition that allow for sound and efficient non monotonic inference for that we consider theory comprised of rule and observation and a semantic framework developed elsewhere that allows u to view such theory a dynamic system system with a transition function f that map state to set of possible successor state and a plausibility function that determines the relative likelihood of those transition in this framework the transition function f is determined by the rule and the plausibility function is provided independently in this work we aim to identify plausibility function that have good semantical and computational property we do so by identifying a vet of tore prediction to be accounted for that can be computed in polynomial time can be justified in simple term and are not tied to either horn theory or closed world assumption the resulting function allow u to handle an interesting class of theory in a justifiable and efficient manner 
identifying that part of a knowledge base kb are irrelevant to a specific query is a powerful method of controlling search during problem solving however finding method of such irrelevance reasoning and analyzing their utility are open problem we present a framework based on a proof theoretic analysis of irrelevance that enables u to address these problem within the framework we focus on a class of strong irrelevance claim and show that they have several desirable property for example in the context of horn rule theory we show that strong irrelevance claim can be derived efficiently either by examining the kb or a logical consequence of other strong irrelevance claim an important aspect is that our algorithm reason about irrelevance using only a small part of the kb consequently the reasoning is efficient and the derived irrelevance claim are independent of change to other part of the kb 
what is the nature of expertise this paper posit an answer to that question in the domain of geometry problem solving we present a computer program called polya which make use of explicit planning knowledge to solve geometry proof problem integrating the process of parsing the diagram and writing the proof 
abstract several study about complexity of nmr showed thatinferring in non monotonic knowledge base is significantlyharder than reasoning in monotonic one thiscontrasts with the general idea that nmr can be usedto make knowledge representation and reasoning simpler not harder in this paper we show that tosome extent nmr ha fulfilled it goal in particularwe prove that circumscription allows for more compactand natural representation of knowledge resultsabout intractability 
in this paper we present one aspect of our research on machine translation mt defining the relation between the interlingua il and a knowledge representation kr within an mt system our interest lie in the translation of natural language nl sentence where the message contains a spatial relation in particular where the sentence conveys information about the location or path of physical entity in the real physical world we explore several argument for clarifying the source of constraint on the particular il structure needed to translate these sentence this paper develops one approach to defining these constraint and building an mt system where the il structure designed to satisfy these constraint may be tested in this way we have begun to address one of the basic issue in mt research providing independent justification for the il itself 
inducing concept description in first order logic is inherently a complex task there are two main reason on one hand the task is usually formulated a a search problem inside a very large space of logical description which need strong heuristic to be kept to manageable size on the other hand most developed algorithm are unable to handle numerical feature typically occurring in realworld data in this paper we describe the learning system smart that embeds sophisticated knowledge based heuristic to control the search process and is able to deal with numerical feature smart can use different learning strategy such a inductive deductive and abductive one and exploit both backgruond knowledge and statistical evaluation criterion furthermore it can use simple genetic algorithm to refine predicate semantics and this aspect will be described in detail finally an evaluation of smart performance is made on a complex task 
mcallester and rosenblitts systematic nonlinear planner snlp remove threat a they are discovered in other planner such a sipe wilkins and noah sacerdoti threat resolution is partially or completely delayed in this paper we demonstrate that planner efficiency may be vastly improved by the use of alternative to these threat removal strategy we discus five threat removal strategy and prove that two of these strategy dominate the other three resulting in a provably smaller search space furthermore the systematicity of the planning algorithm is preserved for each of the threat removal strategy finally we confirm our result experimentally using a large number of planning example including example from the literature 
scott semantically constrained otter is a resolution based automatic theorem prover for first order logic it is based on the high performance prover otter by w mccune and also incorporates a model generator this find finite model which scott is able to use in a variety of way to direct it proof search clause generated by the prover are in turn used a axiom of theory to be modelled thus prover and model generator inform each other dynamically this paper describes the algorithm and some sample result 
a number of algorithm have recently been proposed that use iterative improvement a form of hill climbing to solve constraint satisfaction problem these technique have had dramatic success on certain problem however one factor limiting their wider application is the possibility of getting stuck at non solution local minimum in this paper we describe an iterative improvement algorithm called breakout that can escape from local minimum we present empirical evidence that this method is very effective in case where previous approach have difficulty although breakout is not theoretically complete in practice it appears to almost always find solution for solvable problem we prove that an idealized but le efficient version of the algorithm is complete 
when specificity consideration are incorporated in default reasoning system it is hard to ensure that exceptional subclass inherit all legitimate feature of their parent class to reconcile these two requirement specificity and inheritance this paper proposes the addition of a new rule called coherence rule to the desideratum for default inference the coherence rule capture the intuition that formula which are more compatible with the default in the database are more believable we offer a formal definition of this extended desideratum and analyze the behavior of it associated closure relation which we call coference closure we provide a concrete embodiment of a system satisfying the extended desideratum by taking the coherence closure of system z a procedure for computing the unique most compact be lief ranking in the coherence closure of system z is also described 
there is no need to show the importance of arc consistency in constraint network mohr and henderson have proposed ac an algorithm having an optimal worst case time complexity but it ha two drawback it space complexity and it average time complexity in problem with many solution where the size of the constraint is large these drawback become so important that user often replace ac by ac a nonoptimal algorithm in this paper we propose a new algorithm ac which 
model generation can be regarded a a special case of the constraint satisfaction problem csp it ha many application in ai computer science and mathematics in this paper we describe sem a system for enumerating finite model of first order many sorted theory to the best of our knowledge sem outperforms any other finite model generation system on many test problem the high performance of sem relies on the following two technique a an efficient implementation of constraint propagation which requires little dynamic allocation of storage b a powerful heuristic which eliminates many isomorphic partial model during the search we will present the basic algorithm of sem along with these two technique our experimental result show that general purpose finite model generator are indeed useful in many application 
choosing appropriate model is crucial in analyzing complex physical phenomenon especially when supercomputing resource and complex partial differential equation are involved this paper present an approach to formulating mathematical model guided by the structure of a domain theory and the gross behavior of a physical problem the approach is motivated by the observation that many physical domain though complex and computationally expensive to analyze have strong domain theory based on a few fundamental conservation law and well defined physical process furthermore modeling decision have to be guided by the behavior specific to a physical problem that the system is trying to model by exploiting a domain theory and using problem specific behavior the approach offer an uniform and efficient way of formulating model of various complexity ranging from algebraic ordinary to partial differential equation the approach ha been implemented in a computer program msg and tested in the heat transfer domain 
instead of trying to compare methodology for reasoning about action on the basis of specific example we focus here on a general class of problem expressible in a declarative language a we propose three translation p r and b from a representing respectively the first order method of reasoning about action proposed by pednault and reiter and the circumscriptive approach of baker we then prove the soundness and completeness of these translation relative to the semantics of a this let u compare these three method in a mathematically precise fashion moreover we apply the method of baker in a general setting and prove a theorem which show that if the domain of interest can be expressed in a circumscription yield result which are intuitively expected 
this paper describes a discovery system for trigonometric fuctions dst which ha ability to acquire new knowledge in the form of theorcins ancl foriiialas ii a plant geometry cloiiiaiil the systcn is composed of two suhystenls a plaiic geonhry analysis systeiri ad a niatlicniatical fornda t ratnsforlllat ioll system the former change the length and angle of a figure and extract geouietric relation and the lat tcr transforiiis the relation to acquire nsefnl formnlas with little lmsic knowledge such a the clefinitioii of the congruence of triangle aid the dcfiiiition of fiuidameiit al trigonometric fiuictions our system ha recliscovered many trigononietric formula ant geometric theorem including the pythagorean tlieoreui 
semiquantitative model combine both qualitative and quantitative knowledge within a single semiquantitative qualitative differential equation sqde representation with current simulation method the quantitative knowledge is not exploited a fully a possible this paper describes dynamic envelope a method to exploit quantitative knowledge more fully by deriving and numerically simulating an extremal system whose solution is guaranteed to bound all solution of the sqde it is shown that such system can be determined automatically given the sqde and an initial condition a model precision increase the dynamic envelope bound become more precise than those derived by other semiquantitative inference method we demonstrate the utility of our method by showing how it improves the dynamic monitoring and diagnosis of a vacuum pump down system 
we propose a comprehensive framework for modeling and specifying multimodal interaction to this end we employ an extended notion of dialogue act which can be realized by linguistic and non linguistic mean first a set of constraint is presented that describes the temporal structure and all pattern of exchange during a cooperative informationseeking dialogue second we introduce a strategic level of description which allows the specification of the topical structure according to an information seeking strategy the model wa used to design and implement the merit system and led to a reduction in the complexity of the user interface while preserving most of the useful but sometimes confusing dialogue option of advanced direct manipulation interface 
in expert consultation dialogue it is inevitable that an agent will at time have insufficient information to determine whether to accept or reject a proposal by the other agent this result in the need tor the agent to initiate an information sharing subdialogue to form a set of shared belief within which the agent can effectively re evaluate the proposal this paper present a computational strategy for initiating such information sharing subdialogues to resolve the system uncertainty regarding the acceptance of a user proposal our model determines when information sharing should be pursued selects a focus of information sharing among multiple uncertain belief chooses the most effective information sharing strategy and utilizes the newly obtained information to re evaluate the user proposal furthermore our model is capable of handling embedded informauon sharing subdialogues 
this paper present an overview of some openresearch problem in the representation of emotionon computer the issue discussed arise inthe context of a broad albeit shallow emotionreasoning platform based originally on the ideasof ortony clore and collins ortony clore amp collins in addressing these problem wehope to correct and expand our content theoryof emotion and pseudo personality which underliesall aspect of the research answer feasibilityquestions 
this paper present the first implementation of explanation based learning technique for a partial order planner we describe the basic learning framework of including regression explanation propagation and rule generation we then concentrate on s ability to learn from failure and present a novel approach that us stronger domain and planner specific consistency check to detect explain and learn from the failure of plan at depth limit we will end with an empirical evaluation of the efficacy of this approach in improving planning performance 
we present new algorithm for local planning over markov decision process the base level algorithm posse several interesting feature for control of computation based on selecting computation according to their expected benefit to decision quality the algorithm are shown to expand the agent s knowledge where the world warrant it with appropriate responsiveness to time pressure and randomness we then develop an introspective algorithm using an internal representation of what computational work ha already been done this strategy extends the agent s knowledge base where warranted by the agent s world model and the agent s knowledge of the work already put into various part of this model it also enables the agent to act so a to take advantage of the computational saving inherent in staying in known part of the state space the control flexibility provided by this strategy by incorporating natural problem solving method directs computational effort towards where it s needed better than previous approach providing grcatcr hope for scalability to large domain assign the goal state a reward of and all other state a reward of problem using such a reward function include the path planning problem on a grid with obstacle and imperfect motor control and the ubiquitous puzzle but with random error associated with action the model can also handle problem having several stop state of differcnt value in this domain a plan take the form of a policy assigning to each state an action choice the agent try to choose a policy maximizing it cumulative reward for domain involving unbounded time it is common to discount future gain by an amount exponential in time to 
an approach to nonmonotonic inference based on a closure operation on a conditional knowledge base is presented the central idea is that given a theory of default conditionals an extension to the theory le defined that satisfies certain intuitive restriction two notion for forming an extension are given corresponding to the incorporation of irrelevant property in conditionals and of transitivity among conditionals in this approach these notion coincide several equivalent definition for an extension are developed general nonconstructive definition and a general pseudo iterative definition reasoning with irrelevant property is correctly handled a is specificity reasoning within exceptional circumstance and inheritance reasoning tina approach is intented to ultimately serve a the proof theoretic analogue to an extant semantic development based on preference ordering among possible world 
understanding flow in the three dimensional phase space is challenging both to human expert and current computer science technology to break through the barrier we are building a program called psx that can autonomously explore the flow in a three dimensional phase space by integrating ai and numerical technique in this paper i point out that quasi symbolic representation called flow mapping is effective a a mean of capturing qualitative aspect of three dimensional flow and present a method of generating flow mapping for a system of ordinary differential equation with three unknown function the method is based on a finding that geometric cue for generating a set of flow pattern can be classified into five category i demonstrate how knowledge about interaction of geometric cue is utilized for intelligently controlling numerical computation 
vision system that have successfully supported nontrivial task have invariably taken advantage of constraint derived from the task and environment to increase reliability and lower the complexity of perception we propose that it is possible to build a general purpose vision system that is one that can support a wide variety of task and take advantage of such constraint the central idea within our proposed architecture is the reactive skill skill are concurrent control routine assembled at run time using instruction from a symbolic execution system visual module are used a resource in the construction of these skill skill control the agent a continuous feedback loop but are constructed using discrete symbolic instruction the key to general purpose vision is the ability to parametrize the primitive element of the vision system and to compose visual and control routine in a variety of way we demonstrate the architecture in the context of an implemented example task of a robot collecting trash off a floor and depositing it in a garbage can 
this paper describes how meta level theory are used for analytic learning in multi tac multi tac operationalizes generic heuristic for constraint satisfaction problem in order to create program that are tailored to specific problem for each of it generic heuristic multi tac ha a meta theory specifically designed for operationalising that heuristic we present example of the specialisation process and discus how the theory influence the tractability of the learning process we also describe an empirical study showing that the specialised program produced by multitac compare favorably to hand coded program 
even with significant advance in model based diagnosis methodology it is recognizedthat effectivemodelingis the key to developingefficientdiagnosisalgorithms for complexcontinuous valued system in this paper we developa formal modeling methodology basedon the bondgraphmodelinglanguage andthen presentschemes for focusingthe systemmodelto the diagnosistaskby convertingequationsto conflictsets this representationgreatly facilitates the candidate generation and the measurement selectionprocesses 
we describe a method for generating causal explanation in natural language of the simulated behavior of physical device the method is implemented in dme a system that help formulate mathematical simulation model from a library of model fragment using a compositional modeling approach because explanation are generated from model that are dynamically constructed from modular piece several of the limitation of conventional explanation technique are overcome since the explanation system ha access to the derivation of mathematical equation from the original model specification the system can explain low level quantitative behavior predicted by conventional simulation technique in term of salient behavioral abstraction such a physical process idealized component and operating mode instead of relying on ad hoc causal model crafted specifically for the explanation task the program infers causal relationship among parameter in a constraint based equation model rather than using canned top down template the text generator composes textual annotation associated with individual model fragment into coherent sentence we show how these technique can be combined to produce a variety of explanation about simulated system 
we provide syntactic characterization for a number of propositional model based belief revision and update operator proposed in the literature a well a algorithm based on these characterization 
in this paper we suggest an approach to multiagentplanning that contains heuristic element ourmethod make use of subgoals and derived sub plan to construct a global plan agent solve their individualsub plan which are then merged into a globalplan the suggested approach may reduce overallplanning time and derives a plan that approximatesthe optimal global plan that would have been derivedby a central planner given those original subgoals we consider two different scenario 
i present an average case analysis of propositional strip planning the analysis assumes that each possible precondition likewise postcondition is equally likely too appear within an operator under this assumption i derive bound for when it is highly likely that a planning instanee can be efficiently solved either by finding a plan or proving that no plan exists roughly if planning instance have no condition ground atom g goal and o n operator then a simple efficient algorithm can prove that no plan exists for at least of the instance if instance have n ln g ln g operator then a simple efficient algorithm can find a plan for at least of the instance a similar result hold for plan modification i e solving a planning instance that is close too another planning instance with a known plan thus it would appear that propositional strip planning a pspace complete problem is hard only for narrow parameter range which complement previous average case analysis for np complete problem future work is needed to narrow the gap between the bound and to consider more realistic distributional assumption and more sophisticated algorithm 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
this paper describes an extension to the constraintsatisfaction problem csp approach called musecsp mu ltiply segmented constraint satisfactionproblem this extension is especially useful for thoseproblems which segment into multiple set of partiallyshared variable such problem arise naturally in signalprocessing application including computer vision speech processing and handwriting recognition forthese application it is often difficult to segment thedata in only one way 
this paper introduces g genetic state space search the integration of two general search paradigm genetic search and state space search provides a general framework which can be applied to a large variety of search problem here we show how g solves constrained optimization problem cop basically it search for promising search state from which good solution can be easily found domain knowledge in the form of constraint is used to limit the space to be searched interestingly our approach allows the handling of constraint within genetic search at a general domain independent level first we introduce a genetic representation of search state next we provide empirical result which compare the relative merit of the introduction of constraint during the generation of the initial population during the fitness calculation and during the application of genetic operator finally we describe some extension to our method which came about when applying it to factory floor scheduling problem 
in previous work bennett dejong and bennetl we proposed a machine learning approach called permissive planning to extend classical planning into the realm of real world plan execution our prior result have been favorable but empirical bennett and dejong here we examine the analytic foundation of our empirical success we advance a formal account of realworld planning adequacy we prove that permissive planning doe what it claim to do it probabilistically achieves adequate real world performance or guarantee that no adequate real world planning behavior is possible within the flexibility allowed we prove that the approach scale tractably we prove that restriction are necessary without them permissive planning is impossible we also show how these restriction can be quite naturally met through schema based planning and explanation based learning 
this paper describes a technique whereby anautonomous agent such a a mobile robot canexplore an unknown environment and make atopological map of it it is assumed that the environmentcan be represented a a graph thatis a a fixed set of discrete location or regionswith an ordered set of path between them in previous work it ha been shown that suchworlds can be fully explored and described usinga single movable marker even if there areno spatial metric and almost no sensory 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
resource allocation is a difficult constraint satisfaction problem that ha many practical application fully automatic system are often rejected by the ultimate user because in many real world environment constraint cannot be formalized completely on the other hand human are overwhelmed by the complexity of their task we present a new way of solving the resource allocation where a computer build dynamic abstraction that simplify problem solving to the point that the user can intervene in the solution of the problem these abstraction are based on the concept of interchangeability introduced by freuder in this paper we describe a heuristic for decomposing a resource allocation problem into abstraction that reflect interchangeable set of task or resource we ass the quality of the discovered neighborhood interchangeable set by comparing them to the one obtained by the exact algorithm described by freuder both for data taken from a real world application and for randomly generated problem 
reinforcement learning rl ha become a central paradigm for solving learning control problem in robotics and artificial intelligence r l researcher have focussed almost exclusively on problem where the controller ha to maximize the discounted sum of payoff however a emphasized by schwartz x in many problem e g those for which the optimal behavior is a limit cycle it is more natural and computationally adva ntageous to formulatae task so that the controller s objective is to ma ximize the avera ge payoff received per time step in this paper i derive new average payofl rl algorithm a stochastic approximation method for solving the system of equation associated with the policy evctl tiot and optimal control question in avera ge payoff rl task these algorithm are analogous to the popular td and q learning a lgorithms a lready developed for the discounted payoff case one of the a lgorit hm clerived here is a significant variation of schwartz s r lea rning algorithni prelimina ry empirica result arc presented to validate these new algorithm 
table lookup with interpolation is used for many learning and adaptation task redundant mapping capture the important concept of motor skill which is important in real behaving system few if any robot skill implementation have dealt with redundant mapping in which the space to be searched to create the table ha much higher dimensionality than the table itself a practical method for inverting redundant mapping is important in physical system with limited time for trial we present the guided table fill in algorithm which us data already stored in the table to guide search through the space of potential table entry the algorithm is illustrated and tested on a robot skill learning task both in simulation and on a robot with a flexible link our experiment show that the ability to search high dimensional action space efficiently allows skill learner to find new behavior that are qualitatively different from what they were presented or what the system designer may have expected thus the use of this technique can allow researcher to seek higher dimensional action space for their system rather than constraining their search space at the risk of excluding the best action 
this paper present an improved backjumping algorithm for the constraint satisfaction problem namely conflict directed backjumping cbj cbj is then modified such that it can detect infeasible value and remove them from the domain of variable once and for all a similar modification is then made to gaschnig s backjumping routine bj and to haralick and elliott s forward checking routine fc empirical analysis show that these modification tend to result in an improvement in average performance the existence of a peculiar phenomenon is then shown the removal of infeasible value may result in a degradation in the performance of intelligent backjumping algorithm and conversely the addition of infeasible value may lead to an improvement in performance 
this paper provides a backdrop to my invited talk at the conference the talk itself will focus on selected success of artificial intelligence from the industrial perspective the selected success story will demonstrate that artificial intelligence ha a strong and meaningful influence on our life by impacting development of product and service we use daily they will show how the technical result in the field have been used to make a difference in designing complex artifact in coordinating our action in playing game and in other activity the emphasis is on transformation process rather than on specifies of achievement in knowledge representation case based reasoning or sophisticated search technique i could not have given this type of a talk ten or even five year ago because this wa the initial period of attempt to insert ai into the business process of company today ten year later we have a large number of ai system that are an integral part of critical business process i shall describe some shining example in my talk however the expectation generated ten year ago were much higher than what we were able to achieve in ten year because of this we have pessimist who talk of failure and ai winter since i want to concentrate on the accomplishment in my talk i wish to get the negative aspect out of the way here in this paper taking the metaphor of a partially filled glass of water i will describe the empty portion of it in this paper and only remind you of it during the talk conversely this paper just outline the content of the filled portion while the talk will give you it full taste 
hybrid kl one style logic are knowledge representation formalism of considerable applicative interest a they are specifically oriented to the vast class of application domain that are describable by mean of taxonomic organization of complex object in this paper we consider the problem of endowing such logic with capability for default inheritance reasoning a kind of default reasoning that is specifically oriented to reasoning on taxonomy the formalism that result from our work ha a reasonable and simple behaviour when dealing with the interplay of defeasible and strict inheritance of property of complex object 
we propose a formal approach to the problem of prediction based on the following step first a mental level model is constructed based on the agent s previous action next the model is updated to account for any new observation by the agent and finally we predict the optimal action w r t the agent s mental state a it next action this paper formalizes this prediction process in order to carry out this process we need to understand how a mental state can be ascribed to an agent and how this mental state should be updated in brafman and tennenholtz b we examined the first stage here we investigate a particular update operator and show that it ascription requires making only weak modeling assumption 
this paper present a prototype of an interface system with an active human like agent in usual human communication non verbal expression play important role they convey emotional information and control timing of interaction a well this project attempt to introduce multi modality into computer human interaction our human like agent with it realistic facial expression identifies the user by sight and interacts actively and individually to each user in spoken language that is the agent see human and visually recognizes who is the person keep eye contact in it facial display with human start spoken language interaction by talking to human first 
agent tracking involves monitoring the observable action of other agent a well a inferring their unobserved action plan goal and behavior in a dynamic real time environment an intelligent agent face the challenge of tracking other agent flexible mix of goal driven and reactive behavior and doing so in real time despite ambiguity this paper present resc real time situated commitment an approach that enables an intelligent agent to meet this challenge resc s situatedness derives from it constant uninterrupted attention to the current world situation it always track other agent on going action in the context of this situation despite ambiguity resc quickly commits to a single interpretation of the on going action without an extensive examination of the alternative and us that in service of interpretation of future action however should it commitment lead to inconsistency in tracking it us single state backtracking to undo some of the commitment and repair the inconsistency together resc s situatedness immediate commitment and single state backtracking conspire in providing resc it real time character resc is implemented in the context of intelligent pilot agent participating in a real world synthetic air combat environment experimental result illustrating resc s effectiveness are presented 
this paper address dynamic trajectory planning which is defined a motionplanning for a robot a moving in a dynamic workspace w i e with moving obstacle besides a issubject both to kinematic constraint and dynamic constraint we consider the case of a car likerobot a with bounded velocity and acceleration moving in a dynamic workspace w ir our approachis an extension to the path velocity decomposition kant and zucker we introduce the conceptof adjacent path and 
spatial relation play an important role in the researcharea of connecting visual and verbal space in the last decade several approach to semanticsand computation of spatial relation in dspace have been developed presented here isa new approach to the computation and evaluationof basic spatial relation meaning in dspace we propose the use of various kind of approximationswhen defining the basic semantics the vagueness of the applicability of a spatialrelation is accounted 
one of the hardest problem in reasoning about a physical system is finding an approximate model that is mathematically tractable and yet capture the essence of the problem approximate model in science are often constructed by informal reasoning based on consideration of limiting case knowledge of relative importance of term in the model and understanding of gross feature of the solution we show how an implemented program can combine such knowledge with a heuristic simplification procedure and an inequality reasoner to simplify difficult fluid equation 
one of the open problem listed in rivest andschapire is whether and how that thecopies of lin their algorithm can be combinedinto one for better performance thispaper describes an algorithm called dthatdoes that combination the idea is to representthe state of the learned model using observablesymbols a well a hidden symbol that are constructedduring learning these hidden symbolsare created to reflect the distinct behaviorsof the model state the distinct 
fuzzy logic method have been used successfully in many real world application but the foundation of fuzzy logic remain under attack taken together these two fact constitute a paradox a second paradox is that almost all of the successful fuzzy logic application are embedded controller while most of the theoretical paper on fuzzy method deal with knowledge representation and reasoning i hope to resolve these paradox by identifying which aspect of fuzzy logic render it useful in practice and which aspect are inessential my conclusion are based on a mathematical result on a survey of literature on the use of fuzzy logic in heuristic control and in expert system and on practical experience in developing expert system 
this paper proposes a method to find the most suitable architecture for a given response time requirement for example retrieval er which search for the best match from a bulk collection of lingusitic example in the example based approach eba which attains substantially higher accuracy than traditional approach er is extensively used to carry out natural language processing task e g parsing and translation er however is so computationally demanding that it often take up most of the total sentence processing time this paper compare several acceleration of er on different architecture i e serial mimd and simd experimental result reveal the relationship between architecture and response time which will allows u to find the most suitable architecture for a given response time requirement 
between sensing the world after every action a in a reactive plan and not sensing at all a in an openloop plan lie a continuum of strategy for sensing during plan execution if sensing incurs a cost in time or resource the most cost effective strategy is likely to fall somewhere between these two extreme yet most work on plan execution assumes one or the other in this paper an efficient anytime planner is described that control the rate of sensing during plan execution the sensing interval is determined by the state during plan execution a well a by the cost of sensing so that an agent can sense more often when necessary the planner is based on a generalization of stochastic dynamic programming 
for non monotonic reasoning explicit ordering over formula offer an important solution to problem such a multiple extension however a criticism of such a solution is that it is not clear in general from where the ordering should be obtained here we show how ordering can be derived from statistical information about the domain which the formula cover for this we provide an overview of prioritized logic a general class of logic that incorporate explicit ordering over formula this class of logic ha been shown elsewhere to capture a wide variety of proof theoretic approach to non monotonic reasoning and in particular to highlight the role of preference both implicit and explicit in such proof theory we take one particular prioritized logic called sf logic and describe an experimental approach for comparing this logic with an important example of a logic that doe not use explicit ordering of preference namely horn clause logic with negation a failure finally we present the result of this companson showmg how sf logic is more skeptical and more accurate than negation a failure 
we discus the type of functional knowledge about an environment an agent can use in order to act effectively we demonstrate the use of structural regularity for acting efficiently and the use of physical regularity for designing effective sensor these idea are described in the context of an everyday task grocery store shopping we discus how shopper a program us regularity of grocery store in order to act appropriately and sense efficiently in groceryworld a simulated grocery store 
in this paper we present a formalization of behavior based planning for nonholonomic robotic system this work provides a framework that integrates feature of reactive planning model with modern control theory based robotic approach in the area of path planning for nonholonomic robot in particular we introduce a motion description language mdle that provides a formal basis for robot programming using behavior and at the same time permit incorporation of kinematic model of robot given in the form of differential equation the structure of the language mdle is such a to allow description of trigger generated by sensor in the language feedback and feedforward control law are selected and executed by the triggering event we demonstrate the use of mdle in the area of motion planning for nonholonomic robot such model impose limitation on stabilizability via smooth feed back i e piecing together open loop and closed loop trajectory becomes essential in these circumstance and mdle enables one to describe such piecing together in a systematic manner a reactive planner using the formalism of the paper is described we demonstrate obstacle avoidance with limited range sensor a a test of this planner 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
an approach to analytic learning is described that search for accurate entailment of a horn clause domain theory a hill climbing search guided by an information based evaluation function is performed by applying a set of operator that derive frontier from domain theory the analytic learning system is one component of a multi strategy relational learning system we compare the accuracy of concept learned with this analytic strategy to concept learned with an analytic strategy that operationalizes the domain theory 
many learning algorithm form concept description composed of clause each of which cover some proportion of the positive training data and a small to zero proportion of the negative training data this paper present a method using likelihood ratio attached to clause to classify test example one concept description is learned for each class each concept description competes to classify the test example using the likelihood ratio assigned to clause of that concept description by testing on several artificial and real world domain we demonstrate that attaching weight and allowing concept description to compete to classify example reduces an algorithm s susceptibility to noise 
this paper describes a bootstrapping approach to the engineering of appropriate training data representation for inductive learning the central idea is to begin with an initial set of human created feature and then generate additional feature that have syntactic form that are similar to the human engineered feature more specifically we describe a two stage process for the engineering of good representation for learning first generating by hand usually in consultation with domain expert an initial set of feature that seem to help learning and second bootstrapping off of these feature by developing and applying operator that generate new feature that look syntactically like the expertbased feature our experiment in the domain of dna sequence identification show that an initial successful humanengineered representation for data can be expanded in this fashion to yield dramatically improved result for learning 
inductive logic programming is a rapidly growing area of research that center on the development of inductive learning algorithm for first order definite clause theory an obvious framework for inductive logic programming research is the study of the pac learnability of various restricted class of these theory of particular interest are theory that include recursive definite clause because little work ha been done within this framework the need for initial result and technique is great this paper present result about the pac learnability of several class of simple definite clause theory that are allowed to include a recursive clause in so doing the paper us technique that may be useful in studying the learnability of more complex class 
modgen model generation is a complete theoremprover for first order logic with finite herbrand domain modgen take first order formula a input and generates model of the input formula modgenconsists of two major module a module for transformingthe input formula into propositional clause and a module to find model of the propositionalclauses the first module can be used by other researchersso that the sat problem can be easily represented stored and communicated 
the human mind is capable of absorbing and processing large volume of information most of this processing however occurs at a precognitive level the result of which serve to alert the cognitive mind to area of potential interest the multidimensional user oriented synthetic environment fluse is an open ended software shell that provides a new approach to interacting with computer based information by using a real time device independent software design and incorporating both cognitive and experiential model of human perception fluse greatly enhances a person s ability to examine interact with and understand relationship in complex information space a jluse shell may be wrapped around data model simulation or even complete program using a design based on human functionality it provides tool for the presentation exploration navigation manipulation and examination of information user experience a highly interactive environment capable of dynamically mapping information into visual auditory or kinesthetic representation 
finding the lowest cost path through a graph is central to many problem including route planning for a mobile robot if arc cost change during the traverse then the remainder of the path may need to be replanned this is the case for a sensor equipped mobile robot with imperfect information about it environment a the robot acquires additional information via it sensor it can revise it plan to reduce the total cost of the traverse if the prior information is grossly incomplete the robot may discover useful information in every piece of sensor data during replanning the robot must either wait for the new path to be computed or move in the wrong direction therefore rapid replanning is essential the d algorithm dynamic a plan optimal traverse in real time by incrementally repairing path to the robot s state a new information is discovered this paper describes an extension to d that focus the repair to significantly reduce the total time required for the initial path calculation and subsequent replanning operation this extension completes the development of the d algorithm a a full generalization of a for dynamic environment where arc cost can change during the traverse of the solution path 
recent work ha pointed out that diagnosis strategy are a necessary tool for the diagnosis of complex system nevertheless though current diagnosis system are able to use explicit system model their representation of diagnosis strategy is only implicit in this paper we introduce a formal meta language to express strategic knowledge in an explicit way this language is sufficient to formalize all strategy introduced in previous work and extends previous diagnosis strategy by the integration of empirical knowledge and by explicit statement about dependency between action we provide a declarative semantics for this language and an architecture for implementation 
this paper rest on several contribution first we introduce the notion of a consequence which is a boolean expression that characterizes consistency based diagnosis second we introduce a basic algorithm for computing consequence when the system description is structured using a causal network we show that if the causal network ha no undirected cycle then a consequence ha a linear size and can be computed in linear time finally we show that diagnosis characterized by a consequence and meeting some preference criterion can be extracted from the consequence in time linear in it size a dual set of result is provided for abductive diagnosis 
a crucial problem in inductive logic programming is learning recursive logic program from example alone current system such a golem and foil often achieve success only for carefully selected set of example we describe a program called force that us the new technique of forced simulation to learn twoclause closed linear recursive ij determinate program although this class of program is fairly restricted it doe include most of the standard benchmark problem experimentally force requires fewer example than foil and is more accurate when learning from randomly chosen datasets formally force is also shown to be a pac learning algorithm in a variant of valiant s model in which we assume the ability to make two type of query one which give an upper bound on the depth of the proof for an example and one which determines if an example can be proved in unit depth 
best first search bfs expands the fewest node among all admissible algorithm using the same cost function but typically requires exponential space depth first search need space only linear in the maximum search depth but expands more node than bfs using a random tree we analytically show that the expected number of node expanded by depth first branch and bound dfbnb is no more than o d n where d is the goal depth and n is the expected number of node expanded by bfs we also show that dfbnb is asymptotically optimal when bfs run in exponential time we then consider how to select a linear space search algorithm from among dfbnb iterative deepening id and recursive best first search rbfs our experimental result indicate that dfbnb is preferable on problem that can be represented by bounded depth tree and require exponential computation and rbfs should be applied to problem that cannot be represented by bounded depth tree or problem that can be solved in polynomial time 
the range of possible domain model on which an explanation can be based is often large yet human explainers are able to choose model that address a questioner s informative need without undue obscurity however few existing explanation system use knowledge base providing multiple model of their topic of explanation let alone account for the selection of a model for a given explanation this paper demonstrates the utility of a preference based mechanism for model selection selection heuristic are made explicit a preference and can be added modified or removed without modifying the interpreter or other planning knowledge the mechanism is more general than previous mechanism for model selection or perspective and can apply previously identified model selection criterion 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
this paper present a major revision of the either propositional theory refinement system two issue are discussed first we show how run time efficiency can be greatly improved by changing from a exhaustive scheme for computing repair to an iterative greedy method second we show how to extend either to refine m of n rule the resulting algorithm neither new either is more than an order of magnitude faster and produce significantly more accurate result with theory that fit the m of n format to demonstrate the advantage of neither we present preliminary experimental result comparing it to either and various other system on refining the dna promoter domain theory 
the complexity of reasoning is a fundamental issue in ai in many case the fact that an intelligent system need to perform reasoning on line contributes to the difficulty of this reasoning in this paper we investigate a couple of context in which an initial phase of off line preprocessing and design can improve the on line complexity considerably the first context is one in which an intelligent system computes whether a query is entailed by the system s knowledge base we present the notion of an efficient bast for a query language and show that off line preprocessing can be very effective for query language that have an efficient basis the usefulness of this notion is illustrated by showing that a fairly expressive language ha an efficient basis the second context is closely related to the artificial social system approach introduced in mt we present the design of a social law for a multi agent environment a primarily an instance of off line processing and study this problem in a particular model we briefly review the artificial social system approach to design of multi agent system introduced in mt computing or coming up with a social law is viewed a a primarily off line activity that ha major impact on the effectiveness of the on line activity of the agent the tradeoff between the amount of effort invested in computing the social law and the cost of the on line activity can thus be viewed a an off line v on line tradeoff 
with increased processor speed and improved robotic and ai technology researcher are beginning to design program that can behave intelligently and interact in the real world a large increase in processing power ha come from parallel machine but taking advantage of this power is challenging in this paper we address the issue in designing planner for real time ai and robotic application and provide guiding principle these principle were designed to minimize the difference between the new real time model and the standard off line model applying these principle yield a better structured application easier design and implementation and improved performance the focus of the paper is on a design methodology for implementing effective planner in real world application using ephor our runtime environment and applying the described planner principle we demonstrate improved performance in a real world shepherding application 
knowledge based neural network are networkswhose topology is determined by mapping thedependencies of a domain specific rulebase intoa neural network however existing networktraining method lack the ability to add newrules to the reformulated rulebases thus ondomain theory that are lacking rule generalizationis poor and training can corrupt theoriginal rule even those that were initially correct we present topgen an extension to thekbann algorithm that heuristically 
recent work in planning ha focussed on the reuse of previous plan in order to re use a plan in a novel situation the plan ha to be transformed into an applicable plan we describe an approach to plan transformation which utilises reasoning experience a well a planning experience some of the additional information is generated by a series of self generated question and answer a well a appropriate experiment furthermore we show how transformation strategy can be learned 
we analyze the difficulty in applying bayesian belief network to language interpretation domain which typically involve many unification hypothesis that posit variable binding a an alternative we observe that the structure of the underlying hypothesis space permit an approximate encoding of the joint distribution based on marginal rather than conditional probability this suggests an implicit binding approach that circumvents the problem with explicit unification hypothesis while still allowing hypothesis with alternative unification to interact probabilistically the proposed method accepts arbitrary subset of hypothesis and marginal probability constraint is robust and is readily incorporated into standard unification based and frame based model 
computer based forecasting of weather wa first experimented in at princeton university since then there have been newer and more accurate method to predict the incoming climate one common practice of weather prediction is by using the general circulation model which are based on the law of physic j m moran m d morgan these model are highly complex and computational intensive limiting their use for only short range prediction and that too needing supercomputer the accuracy of forecasting deteriorates rapidly for period longer than hour and it often becomes minimal beyond day due to imperfection in the model the analog technique of weather forecasting is another approach which search for period in the past when the current condition were similar and use the past spatial pattern a analog j t houghton g j jenkins j j ephraums long term trend and recurring event guide the decision this is more relevant for long range prediction a well a in single station prediction the araudog method is relatively simple compared to the complex process of development validation use and maintenance of numerical model 
one of the challenge in process control is providing reliable control of poorly understood system before such a system can be controlled we must first be able to predict it future behavior so that we know what control action is necessary this paper present two approach to this prediction task both using qualitative model augmented by record of historical system behavior our hypothesis is that qualitative information about a system is more easily available than quantitative equation moreover the information need not be complete or totally correct we restructure the historical information into a case base suitable for the prediction task and use the qualitative model to identify the attribute to use a case index the case base then provides the quantitative information needed for the prediction task our technique are extensively evaluated on data taken from a real world system 
the problem of driving an autonomous vehicle in normal traffic engages many area of ai research and ha substantial economic significance we describe work in progress on a new approach to this problem that us a decision theoretic architecture using dynamic probabilistic network the architecture provides a sound solution to the problem of sensor noise sensor failure and uncertainty about the behavior of other vehicle and about the effect of one s own action we report on advance in the theory of inference and decision making in dynamic partially observable domain our approach ha been implemented in a simulation system and the autonomous vehicle successfully negotiates a variety of difficult situation 
admissible heuristic are an important class of heuristic worth discovering they guarantee shortest path solution in search algorithm such a a and they guarantee le expensively produced but boundedly longer solution in search algorithm such a dynamic weighting unfortunately effective accurate and cheap to compute admissible heuristic can take year for people to discover several researcher have suggested that certain transformation of a problem can be used to generate admissible heuristic this article defines a more general class of transformation called abstraction that are guaranteed to generate only admissible heuristic it also describes and evaluates an implemented program absolver ii that us a mean end analysis search control strategy to discover abstracted problem that result in effective admissible heuristic absolver ii discovered several well known and a few novel admissible heuristic including the first known effective one for rubik s cube thus concretely demonstrating that effective admissible heuristic can be tractably discovered by a machine 
many real life constraint satisfaction problem csps involve some constraint similar to the alldifferent constraint these constraint are called constraint of difference they are defined on a subset of variable by a set of tuples for which the value occuring in the same tuple are all different in this paper a new filtering algorithm for these constraint is presented it achieves the generalized arc consistency condition for these non binary constraint it is based on matching theory and it complexity is low in fact for a constraint defined on a subset of p variable having domain of cardinality at most d it space complexity is o pd and it time complexity is o p d this filtering algorithm ha been successfully used in the system resyn to solve the subgraph isomorphism problem 
we argue that a quot theory bottleneck quot encountered in the s and early s in attempt to build comprehensivenlu system led to a fragmentation of nlu research which still persists to some extent this fragmentationrepresents an appropriate response to the varietyand subtlety of remaining problem but at this point italso represents a loss of nerve nlu is an organic phenomenon and enough ha been learned about the vexingproblems of the s to try to integrate these insightsand 
dynamic object such a liquid wave and flame can easily change their position shape and number snapshot image produced by finite element simulator show these change hut lack an explicit representation of the object and their cause for the example of seismic wave we develop a method for interpreting snapshot which is based on hayes concept of a history 
we present episodic logic el a highly expressive knowledge representation well adapted to general commonsense reasoning a well a the interpretive and inferential need of natural language processing one of the distinctive feature of el is it extremely permissive ontology which admits situation episode event state of affair etc proposition possible fact and kind and collection and which allows representation of generic sentence el is natural language like in appearance and support intuitively understandable inference at the same time it is both formally analyzable and mechanizable a an efficient inference engine 
a prototyped data mining system dblearn wa developed in simon fraser univ which integrates machine learning methodology with database technology and efficiently and effectively extract characteristic and discriminant rule from relational database further development of dblearn lead to a new generation data mining system dbminer with the following feature mining new kind of rule from large database including multiple level association rule classification rule cluster description rule etc automatic generation and refinement of concept hierarchy high level sql like and graphical data mining interface and client server architecture and performance improvement for large application the major feature of the system are demonstrated with experiment in a research grant information database 
the ability to answer prediction question is crucialto reasoning about physical system a predictionquestion pose a hypothetical scenarioand asks for the resulting behavior of variable ofinterest prediction question can be answeredby simulating a model of the scenario an appropriatesystem boundary which separate aspectsof the scenario that must be modeled fromthose that can be ignored is critical to achievinga simple yet adequate model this paper presentsan efficient 
a description classifier organizes concept and relation into a taxonomy based on the result of subsumption computation applied to pair of relation definition until now description classifier have only been designed to operate over definition phrased in highly restricted subset of the predicate calculus this paper describes a classifier able to reason with definition phrased in the full first order predicate calculus extended with set cardinality equality scalar inequality and predicate variable the performance of the new classifier is comparable to that of existing description classifier our classifier introduces two new technique dual representation and auto socratic elaboration that may be expected to improve the performance of existing description 
we introduce a new approach to ga genetic algorithm based problem solving earlier gas did not contain local search i e hill climbing mechanism which led to optimization difficulty especially in higher dimension to overcome such difficulty we introduce a bug based search strategy and implement a system called bug the idea behind this new approach are derived from biologically realistic bug behavior these idea were confirmed empirically by applying them to some optimization and computer vision problem 
discovering conceptually interesting and repetitive substructure in a structural data improves the ability to interpret and compress the data the substructure are evaluated by their ability to describe and compress the original data set using the domain s background knowledge and the minimum description length mdl of the data once discovered the substructure concept is used to simplify the data by replacing instance of the substructure with a pointer to the newly discovered concept the discovered substructure concept allow abstraction over detailed structure in the original data iteration of the substructure discovery and replacement process construct a hierarchical description of the structural data in term of the discovered substructure this hierarchy provides varying level of interpretation that can be accessed based on the goal of the data analysis 
the use of primary effect of operator in planningis an effective approach to reduce searchcosts however the characterization of quot good quot primary effect ha remained at an informallevel in this paper we present a formal criterionfor selecting useful primary effect whichguarantees planning efficiency completeness and optimality we also describe an inductivelearning algorithm based on this criterion thatautomatically selects primary effect of operator both the sample complexity 
we present a new technique for tracking d objectsfrom d image sequence through the integrationof qualitative and quantitative technique the deformable model are initializedbased on a previously developed part based qualitativeshape segmentation system using aphysics based quantitative approach object aresubsequently tracked without feature correspondencebased on generalized force computed fromthe stereo image the automatic prediction ofpossible edge occlusion and 
we recently proposed a data structure called associative commutative discrimination net that support efficient algorithm for many to one term matching in the presence of associative commutative function in this paper we discus the integration of such discrimination net into an actual equational theorem prover and report on corresponding experiment the general associative commutative matching problem is known to be np complete but can be solved in polynomial time if the given term are linear i e do not contain multiple occurrence of the same variable we therefore have implemented a two stage matching procedure first we check whether a match exists for the linearized version of the given term where different occurrence of the same variable are replaced by different new variable if a match for the linearized term doe exist we then determine whether there is also a match for the original non linear term i e whether the proposed substitution for different occurrence of the same variable are consistent our experimental result indicate that this approach work very well in theorem proving where most matching attempt actually fail and are filtered out during the first stage so that the second more expensive stage of the algorithm is only needed in comparatively few case 
we study how an autonomous robot can attain a cognitive process that account for it symbolic manipulation of acquired knowledge without generating fatal gap from the reality the paper focus on two essential problem one is the symbol grounding problem and the other is how the internal symbolic process can be situated with respect to the behavioral context we investigate these problem by applying a dynamical system s approach to the robot navigation problem our formulation based on a forward modeling scheme using recurrent neural learning show that the robot is capable of learning grammatical structure hidden in the geometry of the workspace from the local sensory input through it navigational experience furthermore the robot is capable of mentally simulating it own action plan using the acquired forward model our assertion is that the internal representation obtained is grounded since it is self organized solely through interaction with the physical world we also show that structural stability arises in the interaction between the neural dynamic and the environmental dynamic which account for the situatedness of the internal symbolic process 
we describe a supervised learning algorithm eodg that us mutual information to build an oblivious decision tree the tree is then converted to an oblivious read once decision graph oodg by merging node at the same level of the tree for domain that art appropriate for both decision tree and oodgs performance is approximately the same a that of c but the number of node in the oodg is much smaller the merging phase that convert the oblivious decision tree to an oodg provides a new way of dealing with the replication problem and a new pruning mechanism that work top down starting from the root the pruning mechanism is well suited for finding symmetry and aid in recovering from split on irrelevant feature that may happen during the tree construction 
the min conflict heuristic minton et al ha been introduced into backtracking algorithm and iterative improvement algorithm a a powerful heuristic for solving constraint satisfaction problem backtracking algorithm become inefficient when a bad partial solution is constructed since an exhaustive search is required for revising the bad decision on the other hand iterative improvement algorithm do not construct a consistent partial solution and can revise a bad decision without exhaustive search however most of the powerful heuristic obtained through the long history of constraint satisfaction study e g forward checking haralick elliot presuppose the existence of a consistent partial solution therefore these heuristic can not be applied to iterative improvement algorithm furthermore these algorithm are not theoretically complete in this paper a new algorithm called we commitment search which utilizes the min conflict heuristic is developed this algorithm remove the drawback of backtracking algorithm and iterative improvement algorithm i e the algorithm can revise bad decision without exhaustive search the completeness of the algorithm is guaranteed and various heuristic can be introduced since a consistent partial solution is constructed the experimental result on various example problem show that this algorithm is to time more efficient than other algorithm 
conceptual information retrieval system use structured document index domain knowledge and a set of heuristic retrieval strategy to match user query with a set of index describing the document s content such retrieval strategy increase the set of relevant document retrieved increase recall but at the expense of returning additional irrelevant document decrease precision usually in conceptual information retrieval system this tradeoff is managed by hand and with difficulty this paper discus way of managing this tradeoff by the application of standard induction algorithm to refine the retrieval strategy in an engineering design domain we gathered example of query retrieval pair during the system s operation using feedback from a user on the retrieved information we then fed these example to the induction algorithm and generated decision tree that refine the existing set of retrieval strategy we found that induction improved the precision on a set of query generated by another user without a significant loss in recall and in an interactive mode the decision tree pointed out flaw in the retrieval and indexing knowledge and suggested way to refine the retrieval strategy 
we have developed a domain independent systematic methodology for plan merging at the various level of plan abstraction this method manifest itself in the hierarchical plan graph where each level contains a complete partially merged plan the principle advantage of this approach is that once external interaction between node on a given level have been established the continued merging of the plan fragment in one node can take place independently of plan fragment in other node on that level this provides a decomposition or divide and conquer approach to plan merging another advantage to this decomposition approach is that replanning effort is minimized in the presence of the selection of alternative action at some level of the hierarchical plan graph only those plan fragment which are in the same branch a the alternative selection need be considered for replanning also an algorithm is proposed which take a bilateral approach to breaking cyclic dependency between node in the hierarchical plan graph we demonstrate the utility of this hierarchical approach to plan merging through example in the process planning domain 
this paper discus the important issue of knowledge base comprehensibility and describes a technique for comprehensibility improvement comprehensibility is often measured by simplicity of concept description even in the simplest form however there will be a number of different dnf disjunctive normal form description possible to represent the same concept and each of these will have a different degree of comprehensibility in other word simplification doe not necessarily guarantee improved comprehensibility in this paper the author introduce three new comprehensibility criterion similarity continuity and conformity for use with tabular knowledge base in addition they propose an algorithm to convert a decision table with poor comprehensibility to one with high comprehensibility while preserving logical equivalency in experiment the algorithm generated either the same or similar table to those generated by human 
we present a new paradigm for minimax search algorithm mt a memory enhanced version of pearl s test procedure by changing the way mt is called a number of practical best first search algorithm can be simply constructed reformulating ss a an instance of mt eliminates all it perceived implementation drawback most assessment of minimax search performance are based on simulation that do not address two key ingredient of high performance game playing program iterative deepening and memory usage instead we use experimental data gathered from tournament checker othello and chess program the use of iterative deepening and memory make our result differ significantly from the literature one new instance of our framework mtd i out performs our best alpha beta searcher on leaf node total node and execution time to our knowledge these are the first reported result that compare both depth first and best first algorithm given the same amount of memory 
this paper conjecture a computational account of how child might learn the meaning of word in their native language first a simplified version of the lexical acquisition task faced by child is modeled by a precisely specified formal problem then an implemented algorithm for solving this formal problem is presented key advance of this algorithm over previously proposed algorithm are it ability to learn homonymous word sens in the presence of noisy input and it ability to scale up to problem of the size faced by real child 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
we apply dijkstra s semantics for programming language to formalization of reasoning about action and change the basic idea is to view action a formula transformer i e function from formula into formula the major advantage of our proposal is that it is very simple and more effective than most of other approach yet it deal with a broad class of action including those with random and indirect effect also both temporal prediction and postdiction reasoning task can be solved without restricting initial nor final state to completely specified 
best first search algorithm require exponential memory while depth first algorithm require only linear memory on graph with cycle however depth first search do not detect duplicate node and hence may generate asymptotically more node than best first search we present a technique for reducing the asymptotic complexity of depth first search by eliminating the generation of duplicate node the automatic discovery and application of a finite state machine fsm that enforces pruning rule in a depth first search ha significantly extended the power of search in several domain we have implemented and tested the technique on a grid the fifteen puzzle the twenty four puzzle and two version of rubik s cube in each case the effective branching factor of the depth first search is reduced reducing the asymptotic time complexity 
we present a method of representing some class of default theory a normal logic program the main point is that the standard semantics i e sldnf resolution computes answer substitution that correspond exactly to the extension of the represented default theory we explain the step of constructing a logic program logprog p d from a given default theory p d and present the proof idea of the soundness and completeness result for the approach 
in this paper i will describe polly a low cost visionbased robot that give primitive tour the system is very simple robust and efficient and run on a hardware platform which could be duplicated for le than k u the system wa built to explore how knowledge about the structure the environment can be used in a principled way to simplify both visual and motor processing i will argue that very simple and efficient visual mechanism can often be used to solve real problem in real unmodified environment in a principled manner i will give an overview of the robot discus the property of it environment show how they can be used to simplify the design of the system and discus what lesson can drawn for the design of other system 
the people finder is a knowledge based tool to assist user in determining the whereabouts of other staff located in an office or network environment the tool make use of several mode of input and output a well a employing a number of interface and communication medium with which to present information and interconnect geographically distributed system user the accompanying video contains example us of the tool which help illustrate some of it functionality 
at the institute for the learning science we have been developing large scale hypermedia system called ask system that are designed to simulate aspect of conversation with expert they provide access to manually indexed multimedia database of story unit we are particularly concerned with finding a practical solution to the problem of finding index for thes unit when the database grows too large for manual technique our solution is to provide automated assistance that proposes relative link between unit eliminating the need for manual unit to unit comparison in this paper we describe eight class of link and show a representation and inference procedure to assist in locating instance of each 
because complex real world domain defy perfect formalization real world planner must be able to cope with incorrect domain knowledge this paper offer a theoretical framework for permissive planning a machine learning method for improving the real world behavior of planner permissive planning aim to acquire technique that tolerate the inevitable mismatch between the planner s internal belief and the external world unlike the reactive approach to this mismatch permissive planning embrace projection the method is both problem independent and domain independent unlike classical planning permissive planning doe not exclude real world performance from the formal definition of planning 
whilst much emphasis in ai ha been placed on the use of goal in problem solving le emphasis ha been placed on the role of perception and experience in this paper we show that in the domain that may be considered the most abstract namely mathematics that perception and experience play an important role the mathematician ha a vast amount of mathematical knowledge and yet is able to utilise the appropriate knowledge without difficulty we argue that it is essential to model how well the knowledge is grasped so that mathematical knowledge can grow from partial knowledge to important result that are easily accessed not all knowledge is equal in it importance and we argue that perception and experience play a key role in ordering our knowledge feature play a role in both representing the information from the environment and indexing the knowledge of our memory but a key requirement is that the feature should be dynamic and not be built in this research is implemented in the program mu the mathematics understander which utilises the cm contextual memory system mu ha sucessfully read university level text in pure mathematics checking the proof and solving the simple problem 
approach to text processing that rely on parsing the text with a context free grammar tend to be slow and error prone because of the massive ambiguity of long sentence in contrast fastus employ a nondeterministic nite state language model that produce a phrasal decomposition of a sentence into noun group verb group and particle another nite state machine recognizes domain specic phrase based on combination of the head of the constituent found in the rst pas fastus ha been evaluated on several blind test that demonstrate that state of the art performance on information extraction task is obtainable with surprisingly little computational eort 
with few exception the study of nonmonotonic reasoning ha been confined to the single agent case however it ha been recognized that intelligent agent often need to reason about other agent and their ability to reason nonmonotonically in this paper we present a formalization of multi agent autoepistemic reasoning which naturally extends earlier work by levesque in particular we propose an n agent modal belief logic which allows u to express that a formula or finite set of them is all an agent know which may include belief about what other agent believe the paper present a formal semantics of the logic in the possible world framework we provide an axiomatization which is complete for a large fragment of the logic and sufficient to characterize interesting form of multi agent autoepistemic reasoning we also extend the stable set and stable expansion idea of single agent autoepistemic logic to the multi agent case 
tree is an optimized rete like pattern matching algorithm it ha been designed for a production system whose restricted data formalism lead to a highly combinatorial join step like in soar tree aim at reducing the join search space without using hashing technique it join strategy us constraint propagation to define the solution space of a join then a constraint relaxation to determine the index to be used in the join computation constraint relaxation is heuristic driven and based on the relational paradigm unlike rete the indexing scheme tree requires is not based on the membership of condition element but on the sharing of reference to symbol on the basis of experimental evidence tree s strategy showed better result than the standard rete one the number of comparison during join step ha been reduced by a factor ranging from to nearly two order of magnitude 
a counterpart to von neumann and morgenstern expected utility theory is proposed in the framework of possibility theory the existence of a utility function representing a preference ordering among possibility distribution on the consequence of decision maker s action that satisfies a series of axiom pertaining to decision maker s behavior is established the obtained utility is a generalization of wald s criterion which is recovered in case of total ignorance when ignorance is only partial the utility take into account the fact that some situation are more plausible than others mathematically the qualitative utility is nothing but the necessity measure of a fuzzy event in the sense of possibility theory a so called sugeno integral the possibilistic representation of uncertainty which only requires a linearly ordered scale is qualitative in nature only max min and order reversing operation are used on the scale the axiom express a risk averse behavior of the decision maker and correspond to a pessimistic view of what may happen the proposed qualitative utility function is currently used in flexible constraint satisfaction problem under incomplete information it can also be used in association with possibilistic logic which is tailored to reasoning under incomplete state of knowledge 
an approach to nonmonotonic inference based on preference ordering between possible world or state of affair is presented we begin with an extant weak theory of default conditionals using this theory ordering on world are derived the idea is that if a conditional such a bird fly is true then all other thing being equal world in which bird fly are preferred over those where they don t in this case a red bird would fly by virtue of red bird world being among the least exceptional world in which bird fly in this approach irrelevant property are correctly handled a is specificity reasoning within exceptional circumstance and inheritance reasoning a sound proof theoretic characterisation is also given lastly the approach is shown to subsume that of conditional entailment 
a variant is proposed of the preference based semantics for nonmonotonic logic that wa originally considered by shoham in this variant it is not assumed that preference between standard model are aggregated into one preference order this allows the capturing of all main nonmonotonic formalism including default logic on reiter the preferential model introduced in this paper are motivated from an epistemic point of view and are therefore called epistemic preference model the consequence operation induced by epistemic preference model are characterized further the view is defended that the rationality of cumulative monotonicity doe not imply that nonmonotonic logic have to be cumulative but only that a rational agent should not believe a set of default rule that induces a noncumulative consequence operation 
an important problem in geometric reasoning is to find the configuration of a collection of geometric body so a to satisfy a set of given constraint recently it ha been suggested that this problem can be solved efficiently by symbolically reasoning about geometry using a degree of freedom analysis the approach employ a set of specialized routine called plan fragment that specify how to change the configuration of a set of body to satisfy a new constraint while preserving existing constraint in this paper we show how these plan fragment can be automatically synthesized using first principle about geometric body action and topology 
in dague a formal system rom k involving four relation ha been defmed to reason with relative order of magnitude in this paper problem of introducing guantitative informahon and of ensuring validity of the result in ir are tackled correspondent overlapping relation are defmed in r and all rule of rom k are transposed to r unlike other proposed system the obtained system rom r ensures a sound calculus in r while keeping the ability to provide commonsense explanation of the result if needed these result can be refmed by using additional and complementary technique k bound consistency which generalizes interval propagation symbolic computation which considerably improves the result by delaying numeric evaluation symbolic algebra calculus of the root of partial derivative which allows the exact extremum to be obtained transformation of rational function when possible so that each variable occurs only once which allows interval propagation to give the exact result rom r possibly supplemented by these various technique constitutes a rich powerful and flexible tool for performing mixed qualitative and numeric reasoning essential for engineering task 
many of today s electro mechanical device exhibit both continuous and discrete behavior modeling these hybrid system present special challenge for automated modeling and simulation we show how nonstandard analysis overcomes these challenge provides a firm mathematical foundation and satisfies our intuition about the behavior of hybrid system 
we introduce a new subclass of allen s interval algebra we call quot ord horn subclass quot which is a strict superset of the quot pointisable subclass quot we prove that reasoning in the ord horn subclass is a polynomial time problem and show that the path consistency method is sufficient for deciding satisfiability further using an extensive machine generated case analysis we show that the ord horn subclass is a maximal tractable subclass of the full algebra assuming p np in fact it is the unique 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
for more than two decade there ha been consensus that bidirectional heuristic search is afflicted by the problem of search wavefront missing each other however our result indicate that a different problem appears to be of primary importance the front typically meet rather early even without using waveshaping technique especially when aiming for optimal solution however much effort ha to be spent for subsequently improving the solution quality and finally for proving that there is indeed no better solution possible therefore only slightly relaxing the requirement on the solution quality already lead to strong improvement in efficiency we describe several new e admissible bidirectional search algorithm which do not use wave shaping technique the most efficient of these use a novel termination criterion designed to address the suspected primary problem of bidirectional heuristic search we prove e admissibility and a dominance result based on this termination criterion in summary we show that and how bidirectional best first search can be more efficient than the corresponding unidirectional counterpart without using computationally very demanding waveshaping technique 
this paper proposes fbrl a language for representing function and behavior with the primitive we identified and discus it application to explanation generation fbrl explicitly represents model of each component in a system in term of two element one is a necessary and sufficient information for simulation of the component which we call behavior the other is the interpretation of the behavior under a desirable state which the component is expected to achieve which we call function by identifying primitive necessary for the interpretation of the behavior in various domain we can capture what function is and represent it by selection and combination of them we also investigate the relation between function and behavior based on the primitive of fbrl a fbrl can represent concept at various level of abstraction it contributes to explanation generation by providing information for mapping behavior of a component to a term which represents it function 
we extend two notion of only knowing that of halpern and moses and that of levesque to many agent the main lesson of this paper is that these approach do have reasonable extension to the multi agent case our result also shed light on the single agent case for example it wa always viewed a significant that the hm notion of only knowing wa based on s while levesque s wa based on k in fact our result show that the hm notion is better understood in the context of k indeed in the singleagent case the hm notion remains unchanged if we use k or kd instead of s however in the multiagent case there are significant difference between k and s moreover all the result proved by halpern and moses for the single agent case extend naturally to the multi agent case for k but not for s 
a central issue in non linear planning is the ordering of operator so a to avoid undesirable interaction between their effect the modal truth criterion chapman state the condition under which these interaction will occur non linear planner use the criterion directly or indirectly to promote or demote operator or to co designate variable so a to avoid interaction this abstract describes a method called goal clobbering avoidance gca to avoid some interaction in a partially ordered plan by promoting or demoting a sequence of operator rather than individual operator effectively it simultaneously applies the modal truth criterion to all operator in the sequence using pm compiled information about the domain gca will be illustrated in the familiar blocksworld domain with the operator 
multi method planning is an approach to using a set of different planning method to simultaneously achieve planner completeness planning time efficiency and plan length reduction although it ha been shown that coordinating a set of method in a coarse grained problem by problem manner ha the potential for approaching this ideal such an approach can waste a significant amount of time in trying method that ultimately prove inadequate this paper investigates an approach to reducing this wasted effort by refining the granularity at which method are switched the experimental result show that the fine grained approach can improve the planning time significantly compared with coarse grained and single method approach 
genetic algorithm gas and heuristic search are shown to bestructurally similar the strength of the correspondence and it practicalconsequences are demonstrated by considering the relationshipbetween fitness function in gas and the heuristic function of ai by examining the extent to which fitness function approximate anai ideal a measure of ga search difficulty is defined and applied topreviously studied problem the success of the measure in predictingga performance 
this paper is a study on lraam based labeling recursive auto associative memory classification of symbolic recursive structure encoding term the result reported here have been obtained by combining an lraam network with an analog perceptron the approach used wa to interleave the development of representation unsupervised learning of the lraam with the learning of the classification task in this way the representation are optimized with respect to the classification task the intended application of the approach described in this paper are hybrid symbolic connectionist system where the connectionist part ha to solve logic oriented inductive learning task similar to the term classification problem used in our experiment these problem range from the detection of a specific subterm to the satisfaction of a specific unification pattern and they can get a very satisfactory solution by our approach 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
viewpoint are coherent collection of fact thatdescribe a concept from a particular perspective they are essential for a wide variety of task suchas explanation generation and qualitative modeling we have identified many type of viewpointsand developed a program the view retriever for extracting them from knowledge base eithersingly or in combination the view retrieverprovides a general solution to the central problemin extracting viewpoint determining whichfacts are 
in this paper we address the problem of scalability in temporal reasoning in particular new algorithm for efficiently managing large set of relation in the point algebra are provided our representation of time is based on timegraphs graph partitioned into a set of chain on which the search is supported by a rnetagraph data structure the approach is an extension of the time representation proposed by schubert taugher and miller in the context of story comprehension the algorithm presented in this work concern the construction of a timegraph from a given set of relation and are implemented in a temporal reasoning system called tg ii ex perimental result show that our approach is very efficient especially when the given relation admit representation a a collection of chain connected by relatively few cross chain link 
markov decision process mdps have recently been applied to the problem of modeling decision theoretic planning while traditional method for solving mdps are often practical for small state space their effectiveness for large ai planning problem is questionable we present an algorithm called structured policy iteration spi that construct optimal policy without explicit enumeration of the state space the algorithm retains the fundamental computational step of the commonly used modified policy iteration algorithm but exploit the variable and prepositional independency reflected in a temporal bayesian network representation of mdps the principle behind spi can be applied to any structured representation of stochastic action policy and value function and the algorithm itself can be used in conjunction with recent approximation method 
this paper explores different technique for explanation within the framework of the situation calculus using the so called stolen car problem a it main example two approach to explanation are compared the deductive approach usually found in the literature and a le common abductive approach both approach are studied in the context of two different style of representation 
most artificial natural language processing nlp system make use of some simple algorithm for parsing these algorithm overlook the inextricable link between parsing natural language and understanding it human parse language in a linear fashion our goal is to develop an nlp system that par in a linear and psychologically valid fashion when this goal is achieved our nlp system will be efficient and it will generate the correct interpretation in ambiguous situation in this paper we describe two nlp system whose parsing is driven by several heuristic the first is a bottom up system which is based on the work of ford bresnan kaplan the second system is a more expansive attempt incorporating the initial heuristic and several more this system run on a much larger domain and incorporates several new syntactic form it ha it weakness but it show good progress toward the goal of linearity 
roughly speaking adequatness is the property of a theorem proving method to solve simpler problem faster than more difficult one automated inferencing method are often not adequate a they require thousand of step to solve problem which human solve effortlessly spontaneously and with remarkable efficiency l shastri and v ajjanagadde who call this gap the artificial intelligence paradox suggest that their connectionist inference system is a first step toward bridging this gap in this paper we show that their inference method is equivalent to reasoning by reduction in the well known connection method in particular we extend a reduction technique called evaluation of isolated connection such that this technique together with other reduction technique solves all problem which can be solved by shastri and ajjanagadde s system under the same parallel time and space requirement consequently we obtain a semantics for shastri and ajjanagadde s logic but most importantly if shastri and ajjanagadde s logic really capture the kind of reasoning which human can perform efficiently then this paper show that a massively parallel implementation of the connection method is adequate 
in order to deal with unexpected or illegal behavior in multi agent system underlying causal model connecting the target system s behavior and each agent s behavior are indispensable in this paper we present a method for generating causal network which consist of arithmetic and differential relation for explicitly defined parameter and implicitly existing parameter embedded in the target system the task consists of three component a macro behavior rule generator which prepares implicit parameter and generates the rule about system s behavior at macro level a causal network constructor an explanation generator in the course of this process spatial extent are represented and reasoned with qualitative region we took a an example for this method the foraging behavior of ant colony which are typical mobile multi agent system with a local communication method by mean of the chemical pheromone 
information retrieval system that use conceptual indexing to describe the information content perform better than syntactic indexing method based on word from a text however since conceptual index represent the semantics of a piece of information it is difficult to extract them automatically from a document and it is tedious to build them manually we implemented an information retrieval system that acquires conceptual index of text graphic and videotaped document our approach is to use an underlying model of the domain covered by the document to constrain the usds query this facilitates question based acquisition of conceptual index converting user query into index which accurately model the content of the document and can be reused we discus dedal a system that facilitates the indexing and retrieval of design document in the mechanical engineering domain a user formulates a query to the system and if there is no corresponding index dedal us the underlying domain model and a set of retrieval heuristic to approximate the retrieval and ask for confirmation from the user if the user find the retrieved information relevant dedal acquires a new index based on the query we demonstrate the relevance and coverage of the acquired index through experimentation 
in this project we study the effect of a user s high level expository goal upon the detail of how case based reasoning cbr is performed and vice versa the effect of feedback from cbr on them our thesis is that case retrieval should reflect the user s ultimate goal in appealing to case and that these goal can be affected by the case actually available in a case base to examine this thesis we have designed and built frank flexible report and analysis system which is a hybrid blackboard system that integrates case based rule based and planning component to generate a medical diagnostic report that reflects a user s viewpoint and specification frank s control module relies on a set of generic hierarchy that provide taxonomy of standard report type and problemsolving strategy in a mixed paradigm environment our second focus in frank is on it response to a failure to retrieve an adequate set of supporting case we describe frank s planning mechanism that dynamically re specify the memory probe or the parameter for case retrieval when an inadequate set of case is retrieved and give an extended example of how the system responds to retrieval failure 
this paper is concerned with an inferential approach to information extraction reporting in particular on the result of an empirical study that wa performed to validate the approach the study brings together two line of research the rho framework for tractable terminological knowledge representation and the alembic message understanding system there are correspondingly two principal aspect of interest to this work from the knowledge representation perspective the present study serf to validate experimentally a normal form hypothesis that guarantee tractability of inference in the rho framework from the message processing perspective this study substantiates the utility of limited inference to information extraction 
we deal with the moving target search problem where the location of the goal may change during the search process the trailblazer search tb chimura and tokoro achieves a systematic and effective search by maintaining a map the map store path information about the region where the algorithm ha already searched through however because of the growth of the map there is a problem that the time to make decision of search step increase rapidly we propose an algorithm the trailblazer search with an abstract map tbsa that reduces the cost of map maintenance and hence improves the reactiveness of the problem solver we partition the information about the problem space into local map and build an abstract map that control maintenance of the local map in this way the problem solver can systematically manage information about the problem space and it can utilize the map with le cost we evaluate the efficiency of our method and show how significant cost reduction in map maintenance can be achieved by using a two layered map 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
covering and divide and conquer are two well established search technique for top down induction of propositional theory however for top down induction of logic program only covering ha been formalized and used extensively in this work the divide and conquer technique is formalized a well and compared to the covering technique in a logic programming framework covering work by repeatedly specializing an overly general hypothesis on each iteration focusing on finding a clause with a high coverage of positive example divide and conquer work by specializing an overly general hypothesis once focusing on discriminating positive from negative example experimental result are presented demonstrating that there are case when more accurate hypothesis can be found by divide and conquer than by covering moreover since covering considers the same alternative repeatedly it tends to be le efficient than divide and conquer which never considers the same alternative twice on the other hand covering search a larger hypothesis space which may result in that more compact hypothesis are found by this technique than by divide and conquer furthermore divide and conquer is in contrast to covering not applicable to learning recursive definition 
in this article a new characterization of the set of diagnosis in the domain of consistency based diagnosis is developed the need for such a characterization arise from the inability of the current characterization e g minimal diagnosis and kernel diagnosis to deal with abstraction like component hierarchy we propose a characterization based on theory which take advantage of hierarchy and therefore allows a concise description of various set of diagnosis e g minimal diagnosis and kernel diagnosis we investigate a concise description of the set of all minimal diagnosis in detail and focus on hierarchical theory finally we show that theory based diagnosis and hierarchical theory provide efficient generator for computing minimal diagnosis 
automatic recognition of promoter sequence is an important open problem in molecular biology unfortunately the usual machine learning version of this problem is critically flawed in particular the dataset available from the irvine repository wa drawn from a compilation of promoter sequence that were preprocessed to conform to the biologist related notion of the corrserzsus sequence a first order approximation with a number of shortcoming that are well known in molecular biology although concept description learned from the irvine data may represent the consensus sequence they do not represent promoter more generally imperfection in preprocessed data and statistical variation in the location of biologically meaningful feature within the raw data invalidate standard attribute based approach i suggest a dataset a concept description language and a model of uncertainty in the promoter data that are all biologically justified then address the learning problem with incremental probabilistic evidence combination this knowledge based approach yield a more accurate and more credible solution than other more conventional machine learning system 
a new inductive learning system lab learning for abduction is presented which acquires abductive rule from a set of training example the goal is to find a small knowledge base which when used abductively diagnosis the training example correctly and generalizes well to unseen example this contrast with past system that inductively learn rule that are used deductively each training example is associated with potentially multiple category disorder instead of one a with typical learning system lab us a simple hill climbing algorithm to efficiently build a rule base for a set covering abductive system lab ha been experimentally evaluated and compared to other learning system and an expert knowledge base in the domain of diagnosing brain damage due to stroke 
this paper is about some of the social aspect of knowledge and action relevant to thinking in ai and in particular the basic experience of multiple perspective and integrating different kind of local knowledge it discus way of rethinking a number of familiar concept including fact interaction knowledge and organization raising question about how well we can currently capture their social dimension conceptually representationally and computationally it suggests several approach to developing more complete computational model of these phenomenon 
in delayed reinforcement learning an agent is concerned with theproblem of discovering an optimal policy a function mapping statesto action the most popular delayed reinforcement learning technique q learning ha been proven to produce an optimal policyunder certain condition however often the agent doe not followthe optimal policy faithfully the agent must also explore theworld the optimal policy produced by q learning is no longeroptimal if it prescription are only 
trospective reasoning we can reuse expectation that apply to theplanner a a whole for it case based part during the planning process the introspective reasonercompares the planner s reasoning to it assertionsabout ideal behavior when a failure is detected forinstance if the system judge that the retrieved case isnot the quot best quot case in memory the introspective reasonerconsiders related assertion to pinpoint the sourceof the failure and to suggest a solution in this case our 
terminological knowledge representation sys tems tkrss are tool for designing and u ing knowledge base that make use of termino logical language or concept language the tkrs we consider in this paper is of practi cal interest since it go beyond the capabil ities of presently available tkrss first our tkrs is equipped with a highly expressive con cept language called alcnr including gen eral complement of concept number restriction and role conjunction second it allows one to express inclusion statement between general concept in particular to express ter minological cycle we provide a sound com plete and terminating calculus for reasoning in alcnr knowledge base based on the general technique of constraint system 
the utility problem occurs when the cost associated with searching for relevant knowledge outweighs the benefit of applying this knowledge one common machine learning strategy for coping with this problem ensures that stored knowledge is genuinely useful deleting any structure that do not contribute to performance in a positive sense and essentially limiting the size of the knowledge base we will examine this deletion strategy in the context of case based reasoning cbr system in cbr the impact of the utility problem is very much dependant on the size and growth of the case base larger case base mean more expensive retrieval stage an expensive overhead in cbr system traditional deletion strategy will keep performance in check and thereby control the classical utility problem but they may cause problem for cbr system competence this effect is demonstrated experimentally and in reply two new deletion strategy are proposed that can take both competence and performance into consideration during deletion 
my system us an interactive genetic algorithm to learn a user s criterion for the task of generating musical rhythm interactive genetic algorithm smith are well suited to solving this problem because they allow for a user to simply execute fitness function that is to choose which rhythm or feature of rhythm he like without necessarily understanding the detail or parameter of these function a the system learns develops an increasingly accurate model of the function which represents the user s criterion the quality of the rhythm it produce improves to suit the user s taste this approach is largely motivated by richard dawkins who succinctly summarizes the attraction of iga for artistic endeavor in stating effective searching procedure become when the search space is sufficiently large indistinguishable from true creativity dawkins in the context of this project rhythm are one measure long sequence of note and rest occurring on natural pulse subdivision of a beat i only deal with a specific subset of the enormous class of rhythm in order to provide a well defined domain for the application of the learning algorithm the benefit of this reduction of the domain is that a rhythm phenotype can now be viewed a a simple vector thus the set of rhythm satisfying the user s criterion could be represented by a boolean formula i actually use a slightly more complex representation for the rhythm genotype motivated by the benefit of using a diploid genetic structure consisting of several short array template the order of the layering of these template in creating the phenotype effectively determines the dominance hierarchy between the gene the simplest mode of interaction is for the user to playback each of the rhythm in a randomly generated population and then subjectively assign them fitness value based upon their satisfaction of his criterion the system then us standard ga selection with fitness scaling reproduction with crossover monitor and mutation operator in order to deal with the difficulty resultant from the subjectivity and variability of the user s criterion there are also several objective function with which the system can automatically evolve generation of rhythm syncopation density downbeat beat repetition cross rhythm and cluster function are currently included each of these function represents an axis in a feature space which is useful for distinguishing rhythm while these are only a few of the many possible objective function that could be implemented they provide a richset of possibility with which to begin exploring the user can specify the ideal target value for each of these fitness function and also their relative importance weighting of coefficient in determining the overall fitness of a rhythm the system then automatically evolves the indicated number of successive generation using the objective fitness value to determine selection the system also make use of a meta level genetic algorithm designed to evolve population of parameter target value and weight to the objective fitness function defined above this is motivated by the research done in the application of genetic algorithm to the k nearest neighbor technique of classification punch et al each meta level individual represents a warping of k nn space such that the fitness of each individual is determined by how well it warping of the feature space help to discriminate useful feature and thus correctly perform classification evolving population of metaindividuals allows a user to quickly reduce the search space by subjective evaluation of the rhythm generated by the meta individual without having to directly specify value for the objective function this combination of method prof to be a powerful hybrid approach to the subjectivity problem one which allows for greater coverage of the search space than would have been possible ordinarily using a small population which is necessitated by most iga s and is particularly important when dealing with sequential acoustic data and more efficient convergence on a satisficing solution the system is able to converge on near optimal solution acceptable to test user after about fifty user evaluation of rhythm while the ga itself is mechanically quite simple it is important to note that the implementation of appropriate fitness function is difficult and largely determines the musicality of the output the major future improvement will involve adding the capacity for the system to learn to design it own fitness function to represent feature characteristic of rhythm selected by user in past session 
the puzzle is the largest puzzle of it type that can be completely solved it is simple and yet obeys a combinatorially large problem space of state the n n extension of the puzzle is np hard in the first part of this paper we present complete statistical data based on an exhaustive evaluation of all possible tile configuration our result include data on the expected solution length the easiest and worst configuration and the density and distribution of solution node in the search tree in our second set of experiment we used the puzzle a a workbench model to evaluate the benefit of node ordering scheme in iterative deepening a ida one highlight of our result is that almost all ida implementation perform worse than would be possible with a simple random ordering of the operator 
we describe a new algorithm for table driven parsing with context free grammar designed to support efficient syntactic analysis of natural language the algorithm provides a general framework in which a variety of parser control strategy can be freely specified bottom up strategy top down strategy and strategy that strike a balance between the two the framework permit better sharing of parse forest substructure than other table driven approach and facilitates the early termination of semantically ill formed partial par the algorithm should thus find ready application to large scale natural language processing 
the turing test wa proposed by alan turing in he called it the imitation game in hu loebner prize competition offering a f h loebner started the prize to the author of the first computer program to pas an unrestricted turing test annual competition are held each year with smaller prize for the best program on a restricted turing test this paper describes the development of one such turing system including the technical design of the program and it performance on the first three loebner prize competition we also discus the program s four year development effort which ha depended heavily on constant interaction with people on the internet via tinymuds multiuser network communication server finally we discus the design of the loebner competition itself and address it usefulness in furthering the development of artificial intelligence 
the rete and treat algorithm are considered the most efficient implementation technique for forward chaining rule system these algorithm support a language of limited expressive power assertion are not allowed to contain variable making universal quantification impossible to express except a a rule in this paper we show how to support full unification in these algorithm we also show that supporting full unification is costly full unification is not used frequently a combination of compile time and run time check can determine when full unification is not needed we present data to show that the cost of supporting full unification can be reduced in proportion to the degree that it isn t employed and that for many practical system this cost is negligible 
winslett proposed a method for reasoning about action called the possible model approach pma the pma successfully removed the major difficulty manifested by ginsberg and smith s possible world approach pwa in this paper we show that winslett s pma fails to solve the frame and ramification problem for some action a doe the pwa from this observation we classify action a definite and indefinite and find that in general the pma is not appropriate for both definite and indefinite action we propose a new approach to formalize action based on persistence we compare our approach with the pma in detail and show that our new formalization can avoid the problem in the pma and pwa in most case and give more intuitive result for reasoning about action regardless of whether the action is definite or indefinite 
this paper proposes a new algorithm which when provided the relative cost of computation v probing minimizes the total cost of diagnosis during the diagnosis process the decision of whether to probe or to compute is dependent on the expected cost and benefit of each alternative it is unlikely that we will be able to find general analytic and simple to compute model for the cost and benefit therefore we base our algorithm on simple empirically derived model of cost and benefit with these model our algorithm operates by continuously choosing the optimum action to make next this algorithm will not blow up on the rare pathological case and will always on average find diagnosis at equal to or better cost than a conventional gde sherlock when the cost of probing is high then our algorithm behaves exactly the same a gde sherlock when the cost of computation is high the algorithm performs the diagnosis at far lower cost than gde sherlock 
we present a simple circumscriptive method for formalizing action with indirect effect ramification and show that in several example all second order quantifier can be eliminated from these formalization using existing technique for computing circumscription one of the two symbolic computation method employed here is a generalization of predicate completion and the other is based on the scan algorithm the simplicity of our new approach to representing action is due to the use of the formalism of nested abnormality theory 
most practical work on ai planning system during the last fifteen year ha been based on hierarchical task network htn d ecomposition but until now there ha been very little analytical work on the property of htn planner this paper describes how the complexity of htn planning varies with various condition on the task network network are required to be totally ordered and whether variable are allowed from this table we can draw the following conclusion 
this paper present a precise market model for awell defined class of distributed configuration designproblems given a design problem the model definesa computational economy to allocate basic resourcesto agent participating in the design the result ofrunning these quot design economy quot constitutes themarket solution to the original problem after definingthe configuration design framework i describe themapping to computational economy and our resultsto date for some simple 
this paper present a fast probabilistic method for coordination based on markov process provided the agent goal and preference are sufficiently compatible by using markov chain a the agent inference mechanism we are able to analyze convergence property of agent interaction and to determine bound on the expected time of convergence should the agent goal or preference not be compatible they can detect this situation since coordination ha not been achieved within a probabilistic time bound and the agent can then resort to a higher level protocol the application used for motivating the discussion is the scheduling of task though the methodology may be applied to other domain using this domain we develop a model for coordinating the agent and demonstrate it use in two example 
abstract two standard scheme for learning in classifier system have been proposed in the literature the bucket brigade algorithm bba and the profit sharing plan psp the bba is a local learning scheme which requires le memory and lower peak computation than the psp whereas the psp is a global learning scheme which typically achieves a clearly better performance than the bba this requirement versus achievement difference known a the locality globality dilemma is addressed in this paper a new algorithm called hierarchical chunking algorithm hca is presented which aim at synthesizing the local and the global learning scheme this algorithm offer a solution to the locality globality dilemma for the important class of reactive classifier system the content is a follows section describes the locality globality dilemma and motivates the necessity of it solution section briefly introduces basic aspect of reactive classifier system that are relevant to this paper section present the hca section give an experimental comparison of the hca the bba and the psp section concludes the paper with a discussion and an outlook on future work motivation 
in this paper we propose a propositional temporal language based on fuzzy temporal constraint which turn out to be expressive enough for domain like many coming from medicinewhere knowledge is of propositional nature and an explicit handling of time imprecision and uncertainty are required the language is provided with a natural possibilistic semantics to account for the uncertainty issued by the fuzziness of temporal constraint we also present an inference system based on specific rule dealing with the temporal constraint and a general fuzzy modus ponens rule whereby behaviour is shown to be sound the analysis of the different choice a fuzzy operator lead u to identify the well known lukasiewicz implication a very appropriate to define the notion of possibilistic entailment an essential element of our inference system 
a main obstacle for building large diagnosis system is the problem of deciding when to use which inference pattern or representation if diagnostic action such a changing representation or applying specific inference pattern are understood in term of change of working hypothesis the control problem becomes a belief revision problem when to adopt or drop belief our approach proceeds in two step first we adopt the principle of informational economy a kind of a law of inertia for diagnostic process it proposes candidate for revised belief state in a second step we employ specific diagnostic knowledge to actually choose the next belief state 
generating language that reflects the temporal organization of represented knowledge requires a language generation model chat integrates contemporary theory of tense and aspect temporal representation and method to plan text this paper present a model that produce complex sentence that reflect temporal relation present in underlying temporal concept the mam result of this work is the successful application of constrained linguistic theory of tense and aspect to a generator which produce meaningful event combination and selects appropriate connecting word that relate them 
a model based diagnosis procedure trace connection between component only where these are provided explicitly in the system description consequently structure fault fall between the mesh this problem ha been known since research started in this field davis but no general solution ha been presented so far we present a procedure to diagnose structure fault based on a scheme to detect hidden interaction guided by the observation that structure fault lead to discrepancy in apparently unrelated area and which in contrast to preist welham modifies the system description dynamically like davis approach the one presented in the paper is based on the principle that an interaction can occur only where component are adjacent in some way davis unlike davis approach we introduce an explicit representation scheme for hidden interaction a hidden interaction model link a required contextual behaviour independent constellation to the impact of the interaction on the overall system behaviour in order to control hidden interaction hypothesis we exploit the structure of diagnosis based on behavioural mode assignment 
in this paper a framework is developed for measuring the complexity of deduction in an abstract and computationally perspicuous manner a a notion of central importance appears the so called polynomial transparency of a calculus if a logic calculus posse this property then the complexity of any deduction can be correctly measured in term of it inference step the resolution calculus lack this property it is proven that the number of inference step of a resolution proof doe not give a representative measure of the actual complexity of the proof even if only shortest proof are considered we use a class of formula which have proof with a polynomial number of inference step but for which the size of any proof is exponential the polynomial intransparency of resolution is due to the renaming of derived clause which is a fundamental deduction mechanism this result motivates the development of new data structure for the representation of logical formula 
probabilistic network which provide compact description of complex stochastic relationship among several random variable are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence we show that network with fixed structure containing hidden variable can be learned automatically from data using a gradient descent mechanism similar to that used in neural network we also extend the method to network with intensionally represented distribution including network with continuous variable and dynamic probabilistic network because probabilistic network provide explicit representation of causal structure human expert can easily contribute pnor knowledge to the training process thereby significantly improving the learning rate adaptive probabilistic network apns may soon compete directly with neural network a model in computational neuroscience a well a in industrial and financial application 
our aim is to clarify which nonmonotonic consequence relation j is given by a set of supernormal default i e default of the form true there are in fact a number of proposal forj e g the skeptical and the credulous semantics in this paper we look at the space of all possible default semantics and try to characterize the known one by their property especially the valid deduction rule for instance it seems reasonable to require that any useful semantics should coincide with the original cwa if this is consistent we might also want to allow proof by case analysis then we get the skeptical semantics assuming some other very natural deduction rule our result are in fact completeness proof for natural deduction system based on dieren t default semantics 
device representation and reasoning with affective relation occupies a middle ground between classical model based diagnosis and heuristic expert system a device is modeled by specifying a set of diagnostically motivated affective relation among it component reasoning is then performed by a set of inference rule that reason with the model to propagate symptom through the component representation and reasoning with affective relation extends several benefit of classical model based diagnosis the model a a unifying framework for knowledge methodical coverage of the domain and diagnostic reasoning based on equipment design and causality to a class of problem where classical model based diagnosis cannot be applied because the required model cannot be reasonably obtained or represented our work evolved from our redesign of a heuristic expert system for monitoring long distance telephone switching system and is applicable to highly complex self checking system 
in this paper we propose the language of state specification to uniformly specify effect of action executability condition of action and dynamic and static constraint this language allow u to be able to express effect of action and constraint with same first order representation but different intuitive behavior to be specified differently we then discus how we can use state specification to extend the action description language a and lo 
this paper describes an experimental investigationinto the map building and exploration capability ofa mobile robot two type of map are used a setof line and point feature and a grid based free spacemap potential feature are extracted from sonar rangereadings and classed a confirmed if detected repeatedly the free space map is derived from the set ofconfirmed feature a distance transform algorithm isthen used to plan path on this map the confirmedfeatures are used by a 
we investigated a set of query message taken from an usenet newsgroup and analyzed relation between the nature of problem solving activity and their natural language description based on the corpus investigation this paper proposes an efficient computational mechanism for recovering problem solving activity from query message written in japanese the main claim of the paper is that the underlying problem solving activity described in a natural language message can be efficiently recovered if provided with general knowledge on human problem solving and the associated linguistic pattern in the description 
one of the original motivation for research in qualitative physic wa the development of intelligent tutoring system and learning environment for physical domain and complex system this paper demonstrates how a synergistic combination of qualitative physic and other ai technique can be used to create an intelligent learning environment for student learning to analyze and design thermodynamic cycle pedagogically this problem is important because thermodynamic cycle express the key property of system which interconvert work and heat such a power plant propulsion system refrigerator and heat pump and the study of thermodynamic cycle occupies a major portion of an engineering student s training in thermodynamics this paper describes cyclepad a fully implemented learning environment which capture a substantial fraction of a thermodynamics textbook s knowledge and is designed to scaffold student who are learning the principle of such cycle we analyze the combination of idea that made cyclepad possible comment on some lesson learned about the utility of various technique and describe our plan for classroom experimentation 
in data oriented parsing dop an annotated language corpus is used a a virtual stochastic grammar an input string is parsed by combining subtrees from the corpus a a consequence one parse tree can usually be generated by several derivation that involve different subtrees this lead to a statistic where the probability of a parse is equal to the sum of the probability of all it derivation in scha an informal introduction to dop is given while bod provides a formalization of the theory in this paper we show that the maximum probability parse can be estimated in polynomial time by applying monte carlo technique the model wa tested on a set of hand parsed string from the air travel information system atis corpus preliminary experiment yield test set parsing accuracy 
this paper present bidirectional chart generation bcg algorithm a an uniform control mechanism for sentence generation and text planning it is an extension of semantic head driven generation algorithm shieber et al in that recomputation of partial structure and backtracking are avoided by using a chart table these property enable to handle a large scale grammar including text planning and t o implement the algorithm in parallel programming language other merit of the algorithm are to deal with multiple context and to keep every partial structure in the chart it becomes easier for the generator to find a recovery strategy when user cannot understand the generated text 
the purpose of this paper is to study the fundamental mechanism human use in argumentation and it role in different major approach to commonsense reasoning in ai and logic programming we present three novel result we develop a theory for argumentation in which the acceptability of argument is precisely defined we show that logic programming and nonmonotonic reasoning in ai are different form of argumentation we show that argumentation can be viewed a a special form of logic programming with negation a failure this result introduces a general method for generating metainterpreters for argumentation system 
this paper describes an efficient control mechanism for incorporating picture specific context in the task of image interpretation although other knowledge based vision system use general domain context in reducing the computational burden of image interpretation to our knowledge this is the first effort in exploring picture specific collateral information we assume that constraint on the picture are generated from a natural language understanding module which process descriptive text accompanying the picture we have developed a unified framework for exploiting these constraint both in the object location and identification labeling stage in particular we describe a technique for incorporating constrained search in context based vision finally we demonstrate the effectiveness of this approach in piction a system that us caption to label human face in newspaper photograph 
this paper describes research that characterizes the development of routine behavior based on a model of the historic relation of the agent to his environment the view developed is that the agent form a habitat outside of which his performance degrades routine behavior emerges from the history of the relation between the agent and his habitat in the service of recurring goal routine are customized to the agent s environment but so constructed a to support future related activity and the adaptation to new circumstance that extend the agent s range of activity in this paper we focus on examining quantitatively how this customization reduces the agent s workload 
a novel combination of idea from cognitive linguistics and spatial occupancy model in robotics ha led to the wip word into picture system wip automatically generates depiction of natural language description of indoor scene a qualitative layer in the conceptual representation of object underlies a mechanism by which alternative depiction arise for qualitatively distinct interpretation a often occurs a a result of deictic intrinsic reference frame ambiguity at the same time a quantitative layer in conjunction with a potential field model of the semantics of projective preposition is used in the process of capturing the inherently fuzzy character of the meaning of natural language spatial predication 
what skill is more important to teach than reading unfortunately million of american cannot read although a large body of educational software exists to help teach reading it inability to hear the student limit what it can do this paper report a significant step toward using automatic speech recognition to help child learn to read an implemented system that display a text follows a a student read it aloud and automatically identifies which word he or she missed we describe how the system work and evaluate it performance on a corpus of second grader oral reading that we have recorded and transcribed 
when looked at from a multilingual perspective grapheme to phoneme conversion is a challenging task fraught with most of the classical nlp quot vexed question quot bottle neck problem of data acquisition pervasiveness of exception difficulty to state range and order of rule application proper treatment of context sensitive phenomenon and long distance dependency and so on the hand crafting of transcription rule by a human expert is onerous and time consuming and yet for some european 
we study a simple general framework for search called bootstrap search which is defined a global search using only a local search procedure along with some memory for learning intermediate subgoals we present a simple algorithm for bootstrap search and provide some initial theory on it performance in our theoretical analysis we develop a random digraph problem model and use it to make some performance prediction and comparison we also use it to provide some technique for approximating the optimal resource bound on the local search to achieve the best global search we validate our theoretical result with empirical demonstration on the puzzle we show how to reduce the cost of a global search by order of magnitude using bootstrap search we also demonstrate a natural but not widely recognized connection between search cost and the lognormal distribution 
inductive logic programming ilp is often situated a a research area emerging at the intersection of machine learning and logic programming lp this paper make the link more clear between ilp and lp in particular between ilp and abductive logic programming alp i e lp extended with abductive reasoning we formulate a generic framework for handling incomplete knowledge this framework can be instantiated both to alp and ilp approach by doing so more light is shed on the relationship between abduction and induction a an example we consider the abductive procedure sldnfa and modify it into an inductive procedure which we call sldnfai 
this paper describes a computer accompaniment system capable of providing musical accompaniment for an ensemble of performer the system track the performance of each musician in the ensemble to determine current score location and tempo of the ensemble missing part in the composition i e the accompaniment are synthesized and synchronized to the ensemble the paper present an overview of the component problem of automated musical accompaniment and discus solution and their implementation the system ha been tested with solo performer a well a ensemble having a many a three performer ducing an accompaniment in synchrony with the detected performance a solution for each subproblem and a method for it implementation is also provided 
several early game playing computer program used forward pruning i e the practice of deliberately ignoring node that are believed unlikely to affect a game tree s minimax value but this technique did not seem to result in good decision making the poor performance of forward pruning present a major puzzle for ai research on game playing because some version of forward pruning seems to be what people do and the best chess playing program still do not play a well a the best human a a step toward deeper understanding of forward pruning we have set up model of forward pruning on two different kind of game tree and used these model to investigate how forward pruning affect the probability of choosing the correct move in our study forward pruning did better than minimaxing when there wa a high correlation among the minimax value of sibling node in a game tree this result suggests that forward pruning may possibly be a useful decision making technique in certain kind of game in particular we believe that bridge may be such a game 
although creativity ha largely been studied in problem solving context creativity consists of both a generative component and a comprehension component in particular creativity is an essential part of reading and understanding of natural language story we have formalized the understanding process and have developed an algorithm capable of producing creative understanding behavior we have also created a novel knowledge organization scheme to assist the process our model of creativity is implemented a a portion of the isaac integrated story analysis and creativity reading system a system which model the creative reading of science fiction story 
this thesis present a real time tracking and saccade system that is designed to be thebasis for a more complete vision system for a humanoid robot it consists of severalsemi independent functional unit including a tracking system a saccade system anda calibration system these three unit are designed to be simple and run at videorate on a single digital signal processor chip each system recognizes algorithmicfailures in the other system and corrects for these failure thesis 
constructing an appropriate model is crucialin reasoning successfully about the behaviorof a physical situation to answer a query incompositional modeling a system is providedwith a library of composible piece of knowledgeabout the physical world called modelfragments it task is to select appropriatemodel fragment to describe the situation eitherfor static analysis of a single state orfor the more complicated case simulation ofdynamic behavior over a sequence of state in 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
we consider the question of whether or not a successful attempt to simulate human rational thought on a computer can contribute to our understanding of the mind including perhaps consciousness the now fashionable concept of emergence may turn out to be more appropri ate but still doe not seem to provide a final answer 
this work develops a computational model for representing and reasoning about dialogue in term of the mutuality of belief of the conversants we simulated cooperative dialogue at the speech act level and compared the simulation with actual dialogue between pilot and air traffic controller engaged in real task in the simulation addressee and overhearers formed belief and took action appropriate to their individual role and context the result is a computational model capable of representing the evolving context of complete real world multiparty task oriented conversation in the air traffic control domain 
in this paper we present a semantic theory of abstraction based on viewing abstraction a model level mapping this theory capture important aspect of abstraction not captured in the syntactic theory of abstraction presented by giunchiglia and walsh instead of viewing abstraction a syntactic mapping we view abstraction a a two step process first the intended domain model is abstracted and then a set of abstract formula is constructed to capture the abstracted domain model viewing and justifying abstraction a model level mapping is both natural and insightful this basic theory yield abstraction that are weaker than the base theory we show that abstraction that are stronger than the base theory are model level mapping under certain simplifying assumption we provide a precise characterization of the abstract theory that exactly implement an intended abstraction and show that this theory while being axiomatizable is not always finitely axiomatizable we present an algorithm that automatically construct the strongest abstract theory that implement the intended abstraction 
any belief function can be decomposed into a confidence and a diffidence component each component is uniquely decomposable into simple support function that represent the impact of the simplest form of evidence the one that only partially support a given subset of the frame of discernment the nature of the inverse of dempster s rule of combination is detailed the confidence component translates the impact of good reason to believe it is the component classically considered when constructing a belief the diffidence component translates the impact of good reason not to believe 
this paperpresentsa causalsimulationmethod forincompletelyknown dynamic system inprocessengineering the causalmodel of a processisrepresentedas both a causal network ofinteractingelementarydynamic system calledqualitative automaton influencing one another and a setofqualitative constraint linkingpossiblyseveralofsuchautomata associated with each influenceisa weightwhich expressesitssensitivity a procedureispresentedwhich allowsus togeneratefuzzyweightsfrom therelativeorderofmagnitude relationsbetweenthem formula preservingnon linearity and fulfilling some relevantrequirementsareprovidedtocompute net influenceson extensivevariablesas wellason intensiveones finallyan algorithmis givenforsimulatingthe causalmodel and an example applicationto a fed batchfermentation processispresented 
we present a directed improvisation paradigm in which computer character improvise a joint course of behavior that follows user direction but also engages and entertains user with the novelty life like quality and performance property of their improvisation we present requirement for improvisational character that differ from the usual requirement for conventional computer agent and present an architecture that is designed to meet the new requirement two implemented character exploit some of these architectural feature to meet simple version of the requirement finally we illustrate the utility of improvisational character for a variety of application related to the art and entertainment including a suite of interaction mode in our testbed environment a virtual theater for child 
many physical phenomenon are sufficiently complexthat the corresponding equation afford littleinsight or no analytical method provides anexact solution decompositional modeling dm capture a modeler s tacit skill at solving nonlinearalgebraic system dm divide statespaceinto a patchwork of simpler subregimes calledcaricatures each of which preserve only thedominant characteristic of that regime it thensolves the simpler nonlinear system and identifiesits domain of 
this paper present the ipus integrated processing and understanding of signal architecture to address the traditional perceptual paradigm s shortcoming in complex environment it ha two premise the search for correct interpretation of signal processing algorithm spa output requires concurrent search for spa and control parameter appropriate for the environment and interaction between these search process must be structured by a formal theory of how inappropriate spa usage can distort spa output we describe ipus s key component discrepancy detection diagnosis reprocessing and differential diagnosis and their instantiation in an acoustic interpretation system this application along with another in the radar domain support our claim that the ipus paradigm is feasible and generic 
in this paper we study the problem of achieving efficient interaction in a distributed scheduling system whose scheduling agent may borrow resource from one another specifically we expand on sycara s use of resource texture measure in a distributed scheduling system with a central resource monitor for each resource type and apply it to the decentralized case we show how analysis of the abstracted resource requirement of remote agent can guide an agent s choice of local scheduling activity not only in determining local constraint tightness but also in identifying activity that reduce global uncertainty we also exploit meta level information to allow the scheduling agent to make reasoned decision about when to attempt to solve impasse locally through backtracking and constraint relaxation and when to request resource from remote agent finally we describe the current state of negotiation in our system and discus plan for integrating a more sophisticated cost model into the negotiation protocol this work is presented in the context of the distributed airport resource management system a multi agent system for solving airport ground service scheduling problem 
we analyse the basic of eleven measure for estimating the quality of the multivalued attribute the value of information gain j measure gini index and relevance tend to lin early increase with the number of value of an attribute the value of gam ratio dis tance measure relief and the weight of evidence decrease for informative attribute and increase for irrelevant attribute the bias of the statistic test based on the chi square distribution is similar but these function are not able to discriminate among the attribute of different quality we also introduce a new func tion based on the mdl principle whose value slightly decrease with the increasing number of attribute value 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
it is well known that knowledge based system would be more robust and smarter if they can deal with the inconsistent incomplete or imprecise knowledge which ha been referred to a common sense knowledge in this paper we discus fuzzy implication in the sense of common sense reasoning firstly we analyse the rationality of some existing fuzzy implication based on the discussion of implicational paradox secondly we present a new fuzzy preferential implication that is nonmonotonic paraconsistent and without the general implicational paradox finally we propose sound and complete decision tableau of such implication which can be used a the inference engine of adaptive expert system or framework for the fuzzy prolog 
according to the utilitarian paradigm an autonomous intelligent agent s interaction with the environment should be guided by the principle of expected utility maximization we apply this paradigm to reasoning about an agent s physical action and exploratory behavior in urgent time constrained situation we model an agent s knowledge with a temporalized version of kripke structure a a set of branching time line described by fluents with accessibility relation holding among the state comprising the time line we describe how to compute utility based on this model which reflects the urgency that the environment imposes on time since the physical and exploratory action that an agent could undertake transform the model of branching time line in specific way the expected utility of these action can be computed dictating rational tradeoff among them depending on the agent s state of knowledge and the urgency of the situation 
the purpose of conditional causal logic ccl is to constitute a formal theory of the process by which the representation of the world emerges in a cognitive system ccl is presented a a two level language this article concern the first called the e language this e language is a formal theory for the determination process by which a cognitive system construct it objective knowledge the internal dynamic of this construction do not belong to the world of the a e language but to the e language which constitutes the second level of ccl this e language is still being developed and it will only be referred to briefly in this article the t language in itself offer some original feature such a the notion of identity and distinction by determination and also a type of negation functional negation which ha no equivalent in other model of conventional logic or nonstandard logic in conclusion some word will be said about design of a connexionist system founded on this theory of e language 
cat is an instruction environment for practicing basic skill of legal research to use case in argument about a problem situation and to test a theory about a legal domain using the cat tool law student analyze a legal problem frame query of cato s database of legal case and judge how relevant the retrieved case are to their developing argument or theory cat aid hming by making explicit an abstract model of the process of argument it allows student to focus on the high level argumentation issue by assisting the student in various way by providing an abstract representation of the text of case it help student to reason about the text and help guide their critical analysis of the text cat make available opportunity for practice that are hard to set up with traditional instructional method cat differs from other instructional environment in the following respect few instructional environment focus on argumentation skill although there are other instructional environment in which student work with an abstract representation of the task domain abstracting from text is unusual cato demonstrates a contribution that case based reasoning technique can make to instructional environment 
case based reasoning cbr ha a great deal tooffer in supporting creative design particularlyprocesses that rely heavily on previous design experience such a framing the problem and evaluatingdesign alternative however most existingcbr system are not living up to their potential they tend to adapt and reuse old solution inroutine way producing robust but uninspiredresults little research effort ha been directedtowards the kind of situation assessment evaluation and 
irrelevant and redundant feature may reduce both predictive accuracyand comprehensibility of induced concept most common machinelearning approach for selecting a good subset of relevant feature rely oncross validation a an alternative we present the application of a particularminimum description length mdl measure to the task of featuresubset selection using the mdl principle allows taking into account all ofthe available data at once the new measure is 
we provide a general method which can be used in an algorithmic manner to reduce certain class of nd order circumscription axiom to logically equivalent st order formula the algorithm take a input an arbitrary nd order formula and either return a output an equivalent st order formula or terminates with failure in addition to demonstrating the algorithm by applying it to various circumscriptive theory we analyze it strength and provide formal subsumption result based on compan son with existing approach 
any attempt to introduce automation into the monitoring of complex physical system must start from a robust anomaly detection capability this task is far from straightforward for a single definition of what constitutes an anomaly is difficult to come by in addition to make the monitoring process efficient and to avoid the potential for information overload on human operator attention focusing must also be addressed when an anomaly occurs more often than not several sensor are affected and the partially redundant information they provide can be confusing particularly in a crisis situation where a response is needed quickly the focus of this paper is a new technique for attention focusing the technique involves reasoning about the distance between two frequency distribution and is used to detect both anomalous system parameter and broken causal dependency these two form of information together isolate the locus of anomalous behavior in the system being monitored 
a popular saying claim that innovation is inspiration and perspiration in this paper we present technique for automating most of the perspiration involved in creative design we assume that creative design consists of three step discovery of a new technique understanding it and generalizing it to useful application our program us first principle to automate the understanding and generalization phase which involve most of the perspiration and extends it knowledge accordingly so that it can construct practical design based on the new technique the technique could be used to create new device automatically but in practice user interaction is necessary to control the search we present a system which implement the method in the domain of elementary mechanism also called kinematic pair it main novelty are a formalism for modeling qualitative mechanical function and a technique similar to explanation based learning which generalizes a qualitative analysis of a novel device to extend domain knowledge the result are generalizable to other domain with similar characteristic 
recurrent neural network solving the task of short term traffic forecasting arepresented in this report they turned out to be very well suited to this task theyeven outperformed the best result obtained with conventional statistical method the outcome of a comparative study show that multiple combination of feedbackcan greatly enhance the network performance best result were obtained with thenewly developed multi recurrent network combining output hidden and input layermemories 
reasoning with multiple level of abstraction is a powerful method of controlling problem solving in complex domain we consider the problem of simplifying a knowledge base by creating an abstraction that is tailored for a given set of query our approach is based on associating formally an abstraction with some irrelevant detail that is removed from the knowledge base we show how creating an abstraction and determining it utility amount to automatically deciding which aspect of a representation are irrelevant to a query a a result we derive a general algorithm schema for automatically generating abstraction for a query a an instance of the schema we describe a novel algorithm for automatically abstracting a kb by projecting out relation argument 
in this paper we consider constrained and rational default logic we provide two characterization of constrained extension one of them is used to derive complexity result for decision problem involving constrained extension in particular we show that the problem of membership of a formula in at least one in all constrained extension s of a default theory is ef complete ilf complete we establish the relationship between constrained and rational default logic we prove that rational extension determine constrained extension and that for seminormal default theory there is a one to one correspondence between these object we also show that the definition of a constrained extension can be extended to cover the case of default theory which may contain justification free default 
this paper introduces new approach to the conceptual design of electro mechanical system from qualitative specification of behaviour and function the power of these method stem from the integration of technique in qualitative physic symbolic mathematics computational geometry and constraint programming this is illustrated with an effective kinematic synthesis method that integrates reasoning with configuration space and constraint programming technique 
the study of belief change ha been an active area in philosophy and ai in recent year two special case of belief change belief revision and belief update have been studied in detail belief revision and update are clearly not the only possible notion of belief change in this paper we investigate property of a range of possible belief change operation we start with an abstract notion of a belief change system and provide a logical language that describes belief change in such system we then consider several reasonable property one can impose on such system and characterize them axiomatically we show that both belief revision and update fit into our classification a a consequence we get both a semantic and an axiomatic proof theoretic characterization of belief revision and update a well a some belief change operation that generalize them in one natural framework 
one of the central knowledge source of an information extraction ie system is a dictionary of linguistic pattern that can be used to identify reference to relevant information in a text automatic creation of conceptual dictionary is important for portability and scalability of an ie system this paper describes crystal a system which automatically induces a dictionary of concept node definition sufficient to identify relevant information from a training corpus each of these concept node definition is generalized a far a possible without producing error so that a minimum number of dictionary entry cover the positive training instance because it test the accuracy of each proposed definition crystal can often surpass human intuition in creating reliable extraction rule 
the combination of induction axiom is investiga ted it is shown how a pair of competing induction axiom which e g are suggested by a heuristic of an induction theorem prover on a specific verification problem are combined yielding a new induction axiom the relation implicitly defined by the new axiom is the set theoretic union of the well founded relation implicitly defined by the induction axiom initially given the proposed approach is non heu ristic but safe in the sense that an induction proof with the new axiom can be obtained whenever an induction proof with one of the given axiom would have been successful based on a result of bachmair and dershowitz for proving term rewriting system noctherian a commutation test is developed a a de ductive requirement to verify the soundness of the combined axiom it is shown how so called commu tation formula can be derived by machine from the given axiom such that a verification of these for mulas e g by an induction theorem prover guaran tee the well foundcdness of the relation defined by the combined axiom example are presented to demonstrate the usefulness and strength of the pro posed technique 
the situation calculus is a popular technique for reasoning about action and change however it restriction to a firstorder syntax and pure deductive reasoning make it unsuitable in many context in particular we often face uncertainty due either to lack of knowledge or to some probabilistic aspect of the world while attempt have been made to address aspect of this problem most notably using nonmonotonic reasoning formalism the general problem of uncertainty in reasoning about action ha not been fully dealt with in a logical framework in this paper we present a theory of action that extends the situation calculus to deal with uncertainty our framework is based on applying the random world approach of bghk to a situation calculus ontology enriched to allow the expression of probabilistic action effect our approach is able to solve many of the problem imposed by incomplete and probabilistic knowledge within a unified framework in particular we obtain a default markov property for chain of action a derivation of conditional independence from irrelevance and a simple solution to the frame problem 
a proper characterization of a rational agent s action involves much more than simply recounting the change in the world affected by the agent it should also include an explanatory account connecting the upshot of an agent s action with the reason behind those action where those upshot might represent actual change either intentional or unintentional or merely counterfactual possibility the conventional view of action make it difficult to distinguish inter alia case of 
the original formulation of sharedplans grosz and sidner wa developed to provide a model of collaborative planning in which it wa not necessary for one agent to have intention toward an act of a different agent this formulation provided for two agent to coordinate their activity without introducing any notion of jointly held intention or weintentions however it only treated activity that directly decomposed into single agent action in this paper we provide a revised and expanded version of sharedplans that accommodates action involving group of agent a well a complex action that decompose into multi agent action the new definition also allow for contracting out certain action and provide a model with the feature required in bratman s account of shared cooperative activity bratman a reformulation of the model of individual plan that mesh with the definition of sharedplans is also provided 
a dempster shafer belief structure provides a mechanism for representing uncertain knowledge about a variable a compatibility relation provides a structure for obtaining information about one variable based upon our knowledge about a second variable an inference scheme in the theory of evidence involves the use of a belief structure on one variable called the antecedent and a compatibility relationship to infer a belief structure on the second variable called the consequent the concept of monotonicity in this situation is related to change in the specificity of the consequent belief structure a the antecedent belief structure becomes more specific we show that the usual compatibility relation type are always monotonic we introduce type ii compatibility relation and show that a special class of these which we call irregular are needed to represent nonmonotonic relation between variable we discus a special class of nonmonotonic relation called default relation 
using revision to produce extended natural language text through a series of draft provides three significant advantage over a traditional natural language generation system first it reduces complexity through task decomposition second it promotes text polishing technique that benefit from the ability to examine generated text in the context of the underlying knowledge from which it wa generated third it provides a mechanism for the interaction of conceptual and stylistic decision kalos is a natural language generation system that produce advanced draft quality text for a microprocessor user guide from a knowledge base describing the microprocessor it us revision iteratively to polish it initial generation the system performs both conceptual and stylistic revision example output of the system showing both type of revision is presented and discussed 
many problem can be expressed in term of a numeric constraint satisfaction problem over finite or continuous domain numeric csp the purpose of this paper is to show that the consistency technique that have been developed for csps can be adapted to numeric csps since the numeric domain are ordered the underlying idea is to handle domain only by their bound the semantics that have been elaborated plus the complexity analysis and good experimental result confirm that these technique can be used in real application 
abstract a solution to the ramiflcation problem caused by underlying domain constraint in stripslike approach is presented we introduce the notion of causal relationship which are used in a post processing step after having applied an action description moreover we show how the information needed for these post computation can be automatically extracted from the domain constraint plus general knowledge of which uents can possibly affect each other we illustrate the necessity of causal relationship by an example that show the limitedness of a common method to avoid unintended ramiflcations namely the distinction between so called frame and non frame uents finally we integrate our solution into a recently developed strip like yet purely deductive approach to reasoning about action based on equational logic programming 
in a recent paper we have proposed terminological default logic a a formalism which combine both mean for structured representation of class and object and for default inheritance of property the major drawback which terminological default logic inherits from general default logic is that it doe not take precedence of more specific default over more general one into account the present paper address the problem of modifying terminological default logic such that more specific default are preferred it turn out that the existing approach for expressing priority between default do not seem to be appropriate for this purpose therefore we shall consider an alternative approach for dealing with prioritization in the framework of heifer s default logic the formalism is presented in the general setting of default logic where priority are given by an arbitrary partial ordering on the default we shall exhibit some interesting property of the new formalism compare it with existing approach and describe an algorithm for computing extension 
to coordinate with other agent in it environment an agent need model of what the other agent are trying to do when communication is impossible or expensive this information must be acquired indirectly via plan recognition typical approach to plan recognition start with a specification of the possible plan the other agent may be following and develop special technique for discriminating among the possibility perhaps more desirable would be a uniform procedure for mapping plan to general structure supporting inference based on uncertain and incomplete observation in this paper we describe a set of method for converting plan represented in a flexible procedural language to observation model represented a probabilistic belief network and we outline issue in applying the resulting probabilistic model of agent when coordinating activity in physical domain 
this paper present an approach to a new leanring problem the problem of learning from an approximate theory and a set of noisy example this problem requires a new learning approach since it cannot be satisfactorily solved by either indictive or analytic learning algorithm or their existing combination our approach can be viewed a an extension of the minimum description length mdl principle and is unique in that it is based on the encoding of the refinement required to transform the given theory into a better theory rather than on the encoding of the resultant theory a in traditional mdl experimental result show that based on our approach the theory learned from an approximate theory and a set of noisy example is more accnrate than either the approximate theory itself or a theory learned from the example alone this suggests that our approach can combine useful iuformation from both the theory and the training set even though both of them are only partially correct 
the idea of ordering play a basic role in commonsense reasoning for addressing three interrelated task inconsistency handling belief revision and plausible inference we study the behavior of non monotonic inference induced by various method for priority based handling of inconsistent set of classical formula one of them is based on a lexicographic ordering of maximal consistent subset and refines brewka s preferred sub theory this new approach lead to a nonmonotonic inference which satisfies the rationality property while solving the problem of blocking of property inheritance it differs from and improves previous equivalent approach such a gardenfors and makinson s expectation based inference pearl s system z and possibilistic logic 
although they are applicable to a wide array of problem and have demonstrated good performance on anumber of difficult real world task neural network are not usually applied to problem in which comprehensibilityof the acquired concept is important the concept representation formed by neural networksare hard to understand because they typically involve distributed nonlinear relationship encoded by alarge number of real valued parameter to address this limitation we have been 
we propose a method to build thesaurus on the basis of grammatical relation the proposed method construct thesaun by using a hierarchical clustering algorithm an important point in this paper is the claim that thesaurus in order to be efficient need to take surface case information into account we refer to the thesaurus a relation based thesaurus rbt in the experiment four rbts of japanese noun were constructed from verb noun cooccurrences and each rbt wa evaluated fry objective criterion the experiment ha shown that the rbts have better property for selectional restriction of case frame than conventional one 
this paper is concerned with modeling planning problem involving uncertainty a discrete time finite state stochastic automaton solving planning problem is reduced to computing policy for markov decision process classical method for solving markov decision process cannot cope with the size of the state space for typical problem encountered in practice a an alternative we investigate method that decompose global planning problem into a number of local problem solve the local problem separately and then combine the local solution to generate a global solution we present algorithm that decompose planning problem into smaller problem given an arbitrary partition of the state space the local problem are interpreted a markov decision process and solution to the local problem are interpreted a policy restricted to the subset of the state space defined by the partition one algorithm relies on constructing and solving an abstract version of the original decision problem a second algorithm iteratively approximates parameter of the local problem to converge to an optimal solution we show how property of a specified partition affect the time and storage required for these algorithm 
in this paper we present an average case analysis of the nearest neighbor algorithm a simple induction method that ha been studied by many researcher our analysis assumes a conjunctive target concept noise free boolean attribute and a uniform distribution over the instance space we calculate the probability that the algorithm will encounter a test instance that is distance d from the prototype of the concept along with the probability that the nearest stored training case is distance e from this test instance from this we compute the probability of correct classification a a function of the number of observed training case the number of relevant attribute and the number of irrelevant attribute we also explore the behavioral implication of the analysis by presenting predicted learning curve for artificial domain and give experimental result on these domain a a check on our reasoning 
what is needed for an analysis of the existing approach to qualitative spatial reasoning and for a deeper understanding of the domain of space is a unifying theory that explains all of the concept used for the representation of the different aspect of space by some primitive but well understood relation in order to provide such primitive relation it will be shown that the concept used in the existing approach can be explained by simple order relation between point on some low dimensional structure one of the property of an order relation is transitivity it will be shown that this property alone is sufficient to explain all the inference described in the various approach to qualitative spatial reasoning 
various theory of event or action have been proposed to account for commonsence conclusion the typical way in which this proceeds is for a scenario to be invented and a logic tried against it then someone else may perturb the scenario a bit and the old logic fails while a new one may succeed the basic difficulty with this methodology is it ad hoc character to overcome this recent work l ha tried to delineate class of monotonic theory that the non monotonic one are to be measured against we believe a better methodology is to go the whole hog and admit model class a the arbiter to this end we outline some postulate about a possible world semantics that is suitable for evaluating non monotonic theory of event 
when planning system deal with realistic domain they must cope with a large variety of constraint imposed by the environment such a temporal or resource constraint the robustness of the generated plan is a direct consequence of a correct handling of these constraint we argue that increasing the expressiveness of a representation can be achieved without fundamentally affecting the global efficiency of the search this paper present a temporal planner lxtet which integrates sharable resource management into the process of plan generation in lxtet planning operator are described a temporal structure of condition effect and sharable resource us during the search pending subgoals protection threat and resource conflict are detected by three flaw analysis module the detection of sharable resource conflict is performed thanks to an efficient clique search algorithm on a possible intersection graph the control of the search is based on a least commitment opportunistic strategy our approach ha been implemented tested and shown to be satisfactory in various application domain 
a variety of reactive plan execution system have been developed in recent year each attempting to solve the problem of taking reasonable course of action fast enough in a dynamically changing world comparing these competing approach and collecting the best feature of each ha been problematic because of the diverse representation and sometimes implicit control structure that they have employed to rectify this problem we have extended the circuit semantics notion of teleo reactive program into richer yet compact semantics called structured circuit semantics sc that can be used to explicitly represent the control behavior of various reactive execution system by transforming existing system into sc we can identify underlying control assumption and begin to identify more rigorously the strength and limitation of these system moreover sc provides a basis for constructing new reactive execution system with more understandable semantics that can be tailored to particular domain need 
we describe a method for learning formula in firstorder logic using a brute force smallest first search the method is exceedingly simple it generates all irreducible well formed formula up to a fixed size and test them against a set of example although the method ha some obvious limitation due to it computational complexity it performs surprisingly well on some task this paper describes experiment with two application of the method in the multi tac system a program synthesizer for constraint satisfaction problem in the first application axiom are learned and in the second application search control rule are learned we describe these experiment and consider why searching the space of small formula make sense in our application 
this paper describes projective visualization which us previous observation of a process or activity to project the result of an agent s action into the future action which seem likely to sueceed are selected and applied action which seem likely to fail are rejected and other action can be generated and evaluated this paper present a description of the architecture for projective visualization preliminary result on learning to act from observation of a reactive system and a comparison of two type of case projection how situation are projected into the future 
the aim of this work is to combine advantageously the two existing approach for theorem proving in non classical logic proving in the considered non classical logic called here the direct approach and proving in classical logic by way of translation called here the translation approach some result in propositional s show evidence of the relevance of this approach we assume a translation from s into first order logic and then we define a partial inverse formula translation from first order classical logic into s semantic relation are proved to hold between the backward translated formula we answer positively for s to one conjecture stated in a previous work by the author an interpolation theorem stating a property stronger than refutational completeness is also proved a plausible conjecture stronger than the interpolation theorem is proposed these result are interpreted in the framework of a slight variant of an existing resolution calculus for s we illustrate our method on a simple example future work includes application of the approach to other modal logic 
many knowledge based system need to represent vague concept although the practical approach of representing vague concept a precise interval over number is well accepted in ai there is no systematic method to delimit the boundary of interval only ad hoc method we present a framework to reason precisely with vague concept based on the observation that the vague concept and their interval boundary are constrained by the underlying domain knowledge the framework is comprised of a constraint language to represent logical constraint on vague concept a well a numerical constraint on the intervalboundaries a query language to request information about the interval boundary and a computational mechanism to answer the query a key step in answering query is preprocessing the constraint by extracting the numerical constraint from the logical constraint and combining them with the given numerical constraint 
the search space in partial order planning grows quickly with the number of subgoals and initial condition a well a le countable factor such a operator ordering and subgoal in teractions for partial order planner to solve more than simple problem the expansion of the search space will need to be controlled this paper present four new approach to controlling search space expansion by exploiting commonality in emerging plan these approach are described in term of their algorithm their effect on the completeness and correctness of the underlying planner and their expected performance the four new and two existing approach are compared on several metric of search space and planning overhead 
in our research we explore the role of negotiation for conflict resolution in distributed search among heterogeneous and reusable agent we present negotiated search an algorithm that explicitly recognizes and exploit conflict to direct search activity across a set of agent in negotiated search loosely coupled agent interleave the task of local search for a solution to some subproblem integration of local subproblem solution into a shared solution information exchange to define and refine the shared search space of the agent and assessment and reassessment of emerging solution negotiated search is applicable to diverse application area and problem solving environment it requires only basic search operator and allows maximum flexibility in the distribution of those operator these quality make the algorithm particularly appropriate for the integration of heterogeneous agent into application system the algorithm is implemented in a multi agent framework team that provides the infrastructure required for communication and cooperation 
we propose a new version of rippling called relational rippling rippling is a heuristic for guiding proof search especially in the step case of inductive proof relational rippling is designed for representation in which value passing is by shared existential variable a opposed to function nesting thus relational rippling can be used to guide reasoning about logic program or circuit represented a relation we give an informal motivation and introduction to relational rippling more detail including formal definition and termination proof can be found in the longer version of this paper bundy and lombart 
we investigate the improvement of therom provers by reusing previously computed proof a proof of a conjecture is generalized by replacing function symbol with function variable this yield a schematic proof of a schematic conjecture which is instantiated subsequently for obtaining proof of new similar conjecture our reuse method requires solving so called free function variable i e variable which cannot be instantiated by matching the schematic conjecture with a new conjecture we develop an algorithm for solving free function variable by combining the technique of symbolic evaluation and second order matching heuristic for controlling the algorithm are presented and several example demonstrate their usefulness we also show how our reuse proposal support the discovery of useful lemma 
our overall goal is to produce a automatic aspossible facial expression with wrinkle fromspoken input we focus on two aspect of thisproblem integration of the expressive wrinklesand generation of synchronized speechanimation our facial model integrates facialmuscles deformation and bulge we have produceda high level programming language toautomatically drive d animation of facial expressionsfrom speech our system embodiesrule governed translation from speech and utterance 
the truth condition for conditional sentence have been well studied but few compelling attempt have been made to define mean of evaluating iterated or nested conditionals in particular most approach impose very few constraint on the set of conditionals an agent can hold after revision of it belief set in this paper we describe the method of natural revision that ensures the preservation of conditional belief after revision by an objective belief our model based on a simple modal logic for belief and conditionals extends the agm theory of belief revision to account for sentence of objective revision of a belief set this model of revision ensures that an agent make a few change a possible to the conditional component of it belief set adopting the ramsey test natural revision provides truth condition for arbitrary right nested conditionals we show that the problem of determining acceptance of any such nested conditional can be reduced to acceptance test for unnested conditionals indicating that iterated revision can be simulated by virtual update we also briefly describe certain reduction to sometimes tractable propositional inference and other informational property 
we present pika an implemented self explanatory simulator that is more than time faster than simgen mk forbus and falkenhainer the previous state of the art like simgen pika automatically prepares and run a numeric simulation of a physical device specified a a particular instantiation of a general domain theory and it is capable of explaining it reasoning and the simulated behavior unlike simgen pika s modeling language allows arbitrary algebraic and differential equation with no prespecified causal direction pika infers the appropriate causality and solves the equation a necessary to prepare for numeric integration 
the immune system offer to be a rich source of metaphor to guide the exploration of the notion of an adaptive system we might define a class of system which are inspired by but diverge from description of the immune system and refer to them a immune bused system the research reported here is motivated by a desire to explore the possibility of such system specifically we attempt to construct an associative memory using immune system modelling a a starting point 
the ability to argue to support a conclusion or to encourage some course of action is fundamental to communication guided by examination of naturally occurring argument this paper classifies the communicative structure and function of several different kind of argument and indicates how these can be formalized a plan based model of communication the paper describes the use of these communication plan in the context of a prototype which cooperatively interacts with a user to allocate scarce resource this plan based approach to argument help improve the cohesion and coherence of the resulting communication 
the paper present genuinely interdisciplinary research in the intersection of ai machine learning and art music we describe an implemented system that learnsexpressive interpretation of music piece from performance by human musician this problem shown to be very difficult in the introduction is solved by combininginsights from music theory with a new machine learning algorithm theoreticallyfounded knowledge about music perception is used to transform the original learning 
non deductive reasoning system are often representation dependent representing the same situation in two different way may cause such a system to return two different answer some have viewed this a a significant problem for example the principle of maximum entropy ha been subjected to much criticism due to it representation dependence there ha however been almost no work investigating representation dependence in this paper we formalize this notion and show that it is not a problem specific to maximum entropy in fact we show that any representation independent probabilistic inference procedure that ignores irrelevant information is essentially entailment in a precise sense moreover we show that representation independence is incompatible with even a weak default assumption of independence we then show that invariance under a restricted class of representation change can form a reasonable compromise between representation independence and other desideratum and provide a construction of a family of inference procedure that provides such restricted representation independence using relative entropy 
nonmonotonic formalism and belief revision operator have been introduced a useful tool to describe and reason about evolving scenario both approach have been proven effective in a number of different situation however little is known about their relationship previous work by winslett ha shown some correlation between a specific operator and circumscription in this paper we greatly extend winslett s work by establishing new relation between circumscription and a large number of belief revision operator this highlight similarity and difference between these formalism furthermore these connection provide u with the possibility of importing result in one field into the other one 
agent plan in order to improve their performance however planning take time and consumes resource that may in fact degrade an agent performance ideally an agent should only plan when the expected improvement outweighs the expected cost and no resource should be expended on making this decision to do this an agent would have to be omniscient the problem of how to approximate this ideal without consuming too many resource in the process is the meta level control problem for a resource bounded rational agent there are two central question that have to be addressed for meta level control where to focus planning effort and when to start executing the current best plan these question are interrelated to start execution the beginning o f the plan must be elaborated to a level where it is operational even then execution should only begin when the expected improvement due to further planning is outweighed by the cost of delaying execution once the agent ha committed to executing some action the planner can then disregard any plan inconsistent with this action and can concentrate on elaborating and optimizing the rest of the plan in my thesis research i am exploring the use of sensitivity analysis based meta level control for focusing computational effort the object level problem of deciding which action to perform is modeled a a standard decision problem and an approximate sensitivity analysis is performed to facilitate the sensitivity analysis action both abst ract and operational are augmented with method for estimating their resource and time requirement method are also needed to estimate the likelihood of event and action outcome all estimate include both the expected value and the expected range or variance information about the preci sion of estimate is critical when deciding whether to commit to a particular plan or whether to refine estimate through further computation or sensing when presented with a new task the planner generates abstract plan for accomplishing the new and existing task a sensitivity analysis identifies which of these plan are potentially optimal and non dominated dominated and never optimal plan are discarded the sensitivity analys is also identifies which estimate the choice between plan is most sensitive to estimate that affect all plan more or le s equally need not be refined for instance the occurrence of an earthquake may adversely affect all plan equally determining the probability of an earthquake more exactly would not help in selecting between plan other factor may have differing affect for instance the likelihood of rain would help to choose between a plan to walk and a plan to drive somewhere the sensitivity of a plan to particular estimate can also suggest way of making the plan more robust for instance carrying an umbrella help to reduce sensitivity to the likelihood of rain for the plan to walk when there are a number of plan that are potentially optimal and non dominated and when the potential opportunity cost of selecting the wrong plan is significant the meta level controller directs the effort of the planner to refine critical estimate estimate of resource use and action time can be improved by elaborating abstract operator into more operational operator or by simulated execution other objec t level estimate can be refined by adding more sensing to the plan or by additional computation using technique such a temporal projection hank estimating computation time for complex planner is problematic further research is needed to determine how to best estimate and characterize expected plan improvement a a function of computation time information from the sensitivity analysis and estimate of the cost of improving the current plan are used to make the tradeoff between the cost of delaying execution and the expected improvement in the plan for doing additional planning often system that make this tradeoff ignore the fact that execution and planning can be overlapped in many situation the dta algorithm is one example russell and wefald in related work i show how taking into account overlapping of planning and execution can improve performance goodwin 
learning from reinforcement is a promising approach for creating intelligent agent however reinforcement learning usually requires a large number of training episode we present an approach that address this shortcoming by allowing a connectionist q learner to accept advice given at any time and in a natural manner by an external observer in our approach the advice giver watch the learner and occasionally make suggestion expressed a instruction in a simple programming language based on technique from knowledge based neural network these program are inserted directly into the agent s utility function subsequent reinforcement learning further integrates and refines the advice we present empirical evidence that show our approach lead to statistically significant gain in expected reward importantly the advice improves the expected reward regardless of the stage of training at which it is given 
cf loadingtexthtml cf contextpath cf ajaxscriptsrc cfide script ajax cf jsonprefix cf clientid b d b cf c f aa a a a model for hormonal modulation of learning function settab var mytabs coldfusion layout gettablayout citationdetails mytabs on tabchange function tabpanel activetab document cookie picked activetab id function letemknow coldfusion window show letemknow function testthis alert test function loadalert alert i am in the load alert function loadalert alert i am in the load alert google load visualization package orgchart google setonloadcallback drawchart function drawchart var data new google visualization datatable data addcolumn string name data addcolumn string manager data addcolumn string tooltip data addrows v f cc for this article 
a rational agent in a multi agent world must decide on it action based on the decision it expects others to make but it might believe that they in turn might be basing decision on what they believe the initial agent will decide such reciprocal rationality lead to a nesting of model that can potentially become intractable to solve such problem game theory ha developed technique for discovering rational equilibrium solution and ai ha developed computational recursive method these different approach can involve different solution concept for example the recursive modeling method rmm find different solution than game theoretic method when solving problem that require mixed strategy equilibrium solution in this paper we show that a crucial difference between the approach is that rmm employ a solution concept that is overeager this eagerness can be reduced by introducing into rmm second order knowledge about what it know in the form of a flexible function for mapping relative expected utility of an option into the probability that the agent will pursue that option this modified solution concept can allow rmm to derive the same mixed equilibrium solution a game theory and thus help u delineate the type of knowledge that lead to alternative solution concept 
we have written a prototype computer program called trendx for automated trend detection during process monitoring the program us a representation called trend template that define disorder a typical pattern of relevant variable these pattern consist of a partially ordered set of temporal interval with uncertain endpoint bound to each temporal interval arc value constraint on real valued function of measurable parameter trendx ha been used to diagnose trend in growth pattern from examining height weight and other parameter of pediatric patient a trendx analyzes successive data point the program update it hypothesis about which stage of the growth process each data point belongs to we present an example of trendx reaching temporally plausible diagnosis for an actual patient with delayed growth currently being seen at boston child s hospital 
in conventional knowledge acquisition a domain expert interacts with a knowledge engineer who interview the expert and code knowledge about the domain object and procedure in a rule based language or other textual representation language this indirect methodology can be tedious and errorprone since the domain expert s verbal description can be inaccurate or incomplete and the knowledge engineer may not correctly interpret the expert s intent we describe a user interface that allows a domain expert who is not a programmer to construct representation of object and procedure directly from a video of a human performing an example procedure the domain expert need not be fluent in the underlying representation language since all interaction is through direct manipulation starting from digitized video the user selects significant frame that illustrate beforeand afterstates of important operation then the user graphically annotates the content of each selected frame selecting portion of the image to represent each part labeling the part and indicating part whole relationship finally programming by demonstration technique describe the action that represent the transition between frame the result is object description for each object in the domain generalized procedural description and visual and natural language documentation of the procedure we illustrate the system in the domain of documentation of operational and maintenance procedure for electrical device 
we introduce a novel stochastic inversion transduction grammar formalism for bilingual language modeling of sentence pair and the concept of bilingual parsing with potential application to a variety of parallel corpus analysis problem the formalism combine three tactic against the constraint that render finite state transducer le useful it skip directly to a context free rather than finite state base it permit a minimal extra degree of ordering flexibility and it probabilistic formulation admits an efficient maximum likelihood bilingual parsing algorithm a convenient normal form is shown to exist and we discus a number of example ot how stochastic inversion transduction grammar bring bilingual constraint to bear upon problematic corpus analysis task 
with the increasing role of high performance computing in attacking complex physical problem there is an urgent need for the development of advanced computational technology to provide scientist with high level assistance in the analysis interpretation and modeling of a massive amount of quantitative data a critical area where this need is quite evident is the problem of turbulence the overall research goal is to develop a computational environment to help scientist efficiently make observation and conceptual model of turbulence data set this paper present the progress of this project my approach is based on two key idea local interaction and evolution of coherent object like vortex enable high level qualitative interpretation of turbulence data and abstracting from the particular feature of fluid dynamical reasoning propose five core operation aggregation classification re description spatial inference and configuration change a part of a general theory of imagistic reasoning a new vortex finding algorithm is also presented 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
the scenario based engineering process sep is a novel approach to developing complex system haddock z harbison sep build new application system through a selection process that group primitive component into application specific component the selection of primitive component and the construction of interface among component in an application system is currently a tedious manual undertaking the automation of this process will require a configuration system that can support the complex interaction of the component the dynamic requirement of user and the capability of providing multiple viewpoint and managing extensive domain the university of michigan procedural reasoning system um pr lee et al is a reactive reasoning and planning system based on pr georgeff lansky um pr is currently being used in the autonomous vehicle domain it ability to continually consider the real time dynamic environment and access plan accordingly fit well into military application where plan have already been generated in the form of standing operating procedure and reaction to the quickly changing environment are paramount our sep domain doe not require the hard real time speed of a reactionary system however much of the um pr architecture map readily to the configuration problem in the sep domain the scenario based engineering procedural reasoning system seprs will use the architecture of um pr to implement a configuration system for sep primitive component will take the place of plan and will be selected according to the application requirement and the application architecture in progress the interpreter will use the application requirement a goal to satisfy by accessing the primitive component component previously selected for an application architecture will be in the in process area they are accessed by the interpreter to determine which goal are not yet satisfied the interpreter will activate relevant primitive component that are maintained by the intention structure the intention structure will release the chosen primitive component to the component integrator the 
proposing measurement in diagnosis system for static system is a well understood task usually entropy based approach are used sometimes extended by cost and other consid erations how to do the same task in dynamic system is le clear and so far measurement proposal algorithm have been ignored in the recent approach advanced for dynamic sys tems in this paper we will describe a set of technique and algorithm suitable for mea surement proposal in a temporal diagnosis for malism discussed in our previous work this formalism is based on qualitative allen con straints the current paper introduces a mea surement proposal algorithm and improves it in several way finally an entropy based com putation method is described for this temporal setting 
this paper introduces oc a new algorithm forgenerating multivariate decision tree multivariatetrees classify example by testing linear combinationsof the feature at each non leaf node ofthe tree each test is equivalent to a hyperplane atan oblique orientation to the ax because of thecomputational intractability of finding an optimalorientation for these hyperplanes heuristic methodsmust be used to produce good tree thispaper explores a new method that combine 
gr is a hybrid knowledge based system consisting of a multilayer perceptron mlp and a rule based system for hybrid knowledge representation and reasoning knowledge embedded in the trained mlp is extracted in the form of general production rule a natural format of abstract knowledge representation the rule extraction method integrates black box and open box technique obtaining feature salient and statistical property of the training pattern set the extracted general rule are quantified and selected in a rule validation process multiple inference facility such a categorical reasoning probabilistic reasoning and exceptional reasoning are performed in gr 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
the applicability of lexicographic comparison in nonmonotonic reasoning with specificity is investigated a priority mechanism based on lexicographic comparison is defined for rciter s default logic the following principle earlier used by geffner and pearl in conditional entailment is used a the basis for specificity based priority for normal default theory for each rule there is a context where it may not be defeated by any other rule a method for computing priority according to the principle is given connection to earlier work is discussed 
self explanatory simulator have many potential application including supporting engineering activity intelligent tutoring system and computer based training system to fully realize this potential requires improving the technology to efficiently generate highly optimized simulator this paper describes an algorithm for compiling selfexplanatory simulator that operates in polynomial time it is capable of constructing self explanatory simulator with thousand of parameter which is an order of magnitude more complex than any previous technique the algorithm is fully implemented and we show evidence that suggests it performance is quadratic in the size of the system being simulated we also analyze the tradeoff between compiler and interpreter for self explanatory simulation in term of application imposed constraint and discus plan for application 
this paper present an action scheme for dialogue management for natural language interface the scheme guide a dialogue manager which directs the interface s dialogue with the user communicates with the background system and assist the interpretation and generation module the dialogue manager wa designed on the basis of an investigation of empirical material collected in wizard of oz experiment the empirical investigation revealed that in dialogue with database system user specify an object or a set of object and ask for domain concept information e g the value of a property of that object or set of object the interface responds by performing the appropriate action e g providing the requested information or initating a clarification subdialogue the action to bt carried out by the interface can be determined based on how object and property are specified from information in the user utterance the dialogue context and the response from the background system and it domain model 
this paper present an action scheme for dialoguemanagement for natural language interface the scheme guide a dialogue managerwhich directs the interface s dialogue with theuser communicates with the background system and assist the interpretation and generationmodules the dialogue manager wasdesigned on the basis of an investigation ofempirical material collected in wizard of ozexperiments the empirical investigation revealedthat in dialogue with database systemsusers specify 
evaluation oriented information provision is a function performed by many system that serve a personal assistant advisor or sale assistant five general task are distinguished which need to be addressed by such system for each task technique employed in a sample of system are discussed and it is shown how the lesson learned from these system can be taken into account with a set of unified technique that make use of well understood concept and principle from multi attribute utility theory and bayesian network these technique are illustrated a realized in the dialog system pracma 
this paper describes a logic for reasoning in a multi source environment and a theorem prover for this logic we assume the existence of several source of information data knowledge base each of them providing information the main problem dealt with here is the problem of the consistency of the information even if each separate source is consistent the global set of information may be inconsistent in our approach we assume that the different source are totally ordered according to their reliability this order is then used in order to avoid inconsistency the logic we define for reasoning in this case is based on a classical logic augmented with pseudo modality it semantic is first detailed then a sound and complete axiomatic is given finally a theorem prover is specified at the meta level we prove that it is correct with regard to the logic we then implement it in a prolog like language 
this paper proposes logic program a a specification for robot control these provide a formal specification of what an agent should do depending on what it sens and it previous sensory input and action we show how to axiomatise reactive agent event a an interface between continuous and discrete time and persistence a well a axiomatising integration and differentiation over time in term of the limit of sum and difference this specification need not be evaluated a a prolog program we use can the fact that it will be evaluated in time to get a more efficient agent we give a detailed example of a nonholonomic maze travelling robot where we use the same language to model both the agent and the environment one of the main motivation for this work is that there is a clean interface between the logic program here and the model of uncertainty embedded in probabilistic horn abduction this is one step towards building a decision theoretic planning system where the output of the planner is a plan suitable for actually controlling a robot 
we address the problem of automatically learning object model for recognition and pose estimation in contrast to the traditional approach we formulate the recognition problem a one of matching visual appearance than shape the appearance of an object in a two dimensional image depends on it shape reflectance property pose in the scene and the illummation condition while shape and reflectance are intrinsic property of an object and are constant pose and illumination vary from scene to scene we present a new compact representation of object appearance that is parametrized by pose and illumination for each object of interest a large set of image is obtained by automatically varying pose and illumination this large image set is compressed to obtain a low dimensional subspace called the eigenspace in which the object is represented a a hypersurface given an unknown input image the recognition system project the image onto the eigenspace the object is recognized based on the hypersurface it lie on the exact position of the projection on the hypersurface determines the object s pose in the image we have conducted experiment usmg several object with complex appearance characteristic these result suggest the proposed appearance representation to be a valuable tool for a variety of machine vision application 
generating production quality plan is an essential element in transforming planner from research tool into real world application however most research on planning so far ha concentrated on method for constructing sound and complete planner that find a satisficing solution and on how to find such solution in an efficient way similarly most of the work to date on automated control knowledge acquisition ha been aimed at improving the eficiency of planning this work ha been termed speed up learning our work focus on how control knowledge may guide a planner towards better plan and how such control knowledge can be learned better may be defined in 
we introduce a new approach to planning in strip like domain based on constructing and analyzing a compact structure we call a planning graph we describe a new planner graphplan that us this paradigm graphplan always return a shortest possible partial order plan or state that no valid plan exists we provide empirical evidence in favor of this approach showing that graphplan outperforms the total order planner prodigy and the partial order planner ucpop on a variety of interesting natural and artificial planning problem we also give empirical evidence that the plan produced by graphplan are quite sensible since search made by this approach are fundamentally different from the search of other common planning method they provide a new perspective on the planning problem 
many ai problem can be modeled a constraint satisfaction problem csp but many of them are actually dynamic the set of constraint to consider evolves because of the environment the user or other agent in the framework of a distributed system in this context computing a new solution from scratch after each problem change is possible but ha two important drawback inefficiency and instability of the successive solution in this paper we propose a method for reusing any previous solution and producing a new one by local change on the previous one first we give the key idea and the corresponding algorithm then we establish it property termination correctness and completeness we show how it can be used to produce a solution either from an empty assignment or from any previous assignment and how it can be improved using filtering or learning method such a forward checking or nogoodrecording experimental result related to efficiency and stability are given with comparison with well known algorithm such a backtrack heuristic repair or dynamic backtracking 
we describe a bottom up approach to the design of software agent we built and tested an agent system that address the real world problem of handling the activity involved in scheduling a visitor to our laboratory the system employ both task specific and user centered agent and communicates with user using both email and a graphical interface this experiment ha helped u to identify crucial requirement in the successful deployment of software agent including issue of reliability security and ease of use the architecture we developed to meet these requirement is fle xible and extensible and is guiding our current research on principle of agent design 
in this paper we present a technique for automatically generating constraint on parameter derivative that reduce ambiguity in the behaviour prediction starting with a behaviour prediction using an initial library containing general domain knowledge the technique employ feedback about valid and spurious state of behaviour and knowledge about the causal dependency between the parameter in the model in order to determine the constraint that remove the undesired state of behaviour that result from spurious ambiguity in addition the technique point out the assembly of physical object to which the generated constraint apply 
bounded response time is an important requirement when rule based expert system are used in real time application in the case the rule based system cannot terminate in bounded time we should detect the culprit condition causing the non termination to assist programmer in debugging this paper describes a novel tool which analyzes ops program to achieve this goal the first step is to verify that an ops program can terminate in bounded time a graphical representation of an ops program is defined and evaluated once the termination of the ops program is not expected the culprit condition are detected these condition are then used to correct the problem by adding extra rule to the original program 
researcher in distributed tificial intelligence have suggested that it would be worthwhile to isolate aspect of cooperative behavior general rule that cause agent to act in way conducive to cooperation one kind of cooperative behavior is when agent independently alter the environment to make it easier for everyone to function effectively cooperative behavior of this kind might be to put away a hammer that one find lying on the floor knowing that another agent will be able to find it more easily later on we examine the effect a specific cooperation rule ha on agent in the multi agent tileworld domain agent are encouraged to increase tile degree of freedom even when the tile is not involved in an agent s own primary plan the amount of extra work an agent is willing to do is captured in the agent s cooperation level result from simulation are presented we present a way of characterizing domain a multi agent deterministic finite automaton and characterizing cooperative rule a transformation of these automaton we also discus general characteristic of cooperative state changing rule it is shown that a relatively simple easily calculated rule can sometimes improve global system performance in the tileworld coordination emerges from agent who use this rule of cooperation without any explicit coordination or negotiation 
in the design of system of multiple agent we must deal with the potential for conflict that is inherent in the interaction among agent to ensure efficient operation these interaction must be coordinated we extend in two related way an existing framework that allows behavioral convention to emerge in agent society we first consider localizing agent thus limiting their interaction we then consider giving some agent authority over others by implementing asymmetric interaction our primary interest is to explore how locality and authority affect the emergence of convention through computer simulation of agent society of various configuration we begin to develop an intuition about what feature of a society promote or inhibit the spontaneous generation of coordinating convention 
we investigate two realization of parallel abductive reasoning system using the model generation theorem prover mgtp the first one called the mgtp mgtp method is a co operative problem solving architecture in which model generation and consistency checking communicate with each other there parallelism is exploited by checking consistency in parallel however since this system consists of two different component the possibility for parallelization are limited in contrast the other method called the skip method doe not separate the inference engine from consistency checking but realizes both function in only one mgtp that is used a a generate and test mechanism in this method multiple model can be kept in distributed memory thus a great amount of parallelism can be obtained we also attempt the upside down meta interpretation approach for abduction in which top down reasoning is simulated by a bottom up reasoner 
classical planner have traditionally made the closed world assumption fact absent from the planner s world model are false incompleteinformation planner make the open world assumption the truth value of a fact absent from the planner s model is unknown and must be sensed the open world assumption lead to two difficulty h ow can the planner determine the scope of a universally quantified goal when is a sensory action redundant yielding information already known to the planner this paper describes the fully implemented xii planner which solves both problem by representing and reasoning about zocal closed world information lcw we report on experiment utilizing our unix softbot software robot which demonstrate that lcw can substantially improve the softbot s performance by eliminating redundant information gathering 
we introduce curbing a new nonmonotonic technique of commonsense reasoning that is based on model minimality but unlike circumscription treat disjunction inclusively a finitely axiomatized first order theory t is transformed to a formula curb t whose set of model is defined a the smallest collection of model which contains all minimal model of t and which is closed under formation of minimal upper bound with respect to inclusion we first give an intuitive definition of curb in third order logic and then show how curb can be equivalently expressed in second order logic we study the complexity of inferencing from a curbed propositional theory and present a pspace algorithm for this problem finally we address different possibility to approximate the curb of a theory 
this paper address a model of analogy driven theorem proving that is more general and cognitively more adequate than previous approach the model work at the level of proof plan more precisely we consider analogy a a control strategy in proof planning that employ a source proof plan to guide the construction of a proof plan for the target problem our approach includes a reformulation of the source proof plan this is in accordance with the well known fact that constructing an analogy in math often amount to first finding the appropriate representation which brings out the similarity of two problem i e finding the right concept and the right level of abstraction several well known theorem were processed by our analogy driven proof plan construction that could not be proven analogically by previous approach 
we describe the least cost flaw repair lcfr strategyfor performing flaw selection during partial ordercausal link pocl planning lcfr can be seenas a generalization of peot and smith s quot delay unforcedthreats quot dunf strategy peot amp smith where dunf treat threat differently from open condition lcfr ha a uniform mechanism for handling allflaws we provide experimental result that demonstratethat the power of dunf doe not come from delayingthreat repair per se but 
we investigate the use of oblivious read once decision graphsas structure for representing concept over discrete domain and presenta bottom up hill climbing algorithm for inferring these structure fromlabelled instance the algorithm is robust with respect to irrelevant attribute and experimental result show that it performs well on problemsconsidered difficult for symbolic induction method such a the monk sproblems and parity introductiontop down induction of 
chapman s paper planning for conjunctive goal ha been widely acknowledged for it contribution toward understanding the nature of nonlinear partial order plann ing and it ha been one of the base of later work by others but it is not free of problem this paper address some problem involving modal truth and the modal truth criterion mtc our result are a follows even though modal duality is a fundamental axiom of classical modal logic it doe not hold for modal truth in chapman s plan i e necessarily p is not equivalent to not possibly p although the mtc for necessary truth is correct the mtc for possible truth is incorrect it provides necessary but insufficient condition for ensuring possible truth furthermore even though necessary truth can be determined in polynomial time possible truth is np hard if we rewrite the mtc to talk about modal conditional truth i e modal truth conditional on executability rat her than modal truth then both the mtc for necessary conditional truth and the mtc for possible conditional truth are correct and both can be computed in polynomial time 
we analyze the amount of information needed tocarry out model based recognition task in thecontext of a probabilistic data collection model and independently of the recognition methodemployed we consider the very rich class ofsemi algebraic d object and derive an upperbound on the number of data feature that provably suffice for localizing the object with somepre specified precision our bound is based onanalysing the combinatorial complexity of the hypothesesclass that one 
condition satisfaction in planning ha received a great deal of experimental and formal attention a truth criterion lie at the heart of many planner and is critical to their capability and performance however there ha been little study of way in which the search space of a planner incorporating such a truth criterion can be guided the aim of this document is to give a description of the use of condition type information to inform the search of an ai planner and to guide the production of answer by a planner s truth criterion algorithm the author aim to promote discussion on the merit or otherwise of using such domain dependent condition type restriction a a mean to communicate valuable information from the domain writer to a general purpose domain independent planner 
cale at which navigationoccurs ffl navigation is much simpler than arbitrarygraph search because street are topologically sensible it is impossible to drive along a street and suddenly endup on the other side of town cul de sac are relativelyrare so hill climbing tends to work one way streetsnever completely isolate region ffl navigation occursin a topographically translucent environment some informationis available by virtue of the d nature ofthe environment although not 
the paper is concerned with the design of a module system for logic programming so a to satisfy many of the requirement of software engineering the design is based on the language godel which is a logic programming language which already ha a simple type and module system the module system described here extends the godel module system so a to include parameterised module in particular this extended system allows general purpose predicate that depend on fact and rule for specific application to be defined in module that are independent of their application 
interpreting spectral image requires comparing known pattern with input data image to identify which pattern are contained in the input data in practice however it is hard to identify any pattern when the inaccuracy of input data is not slight in this paper we present a method for interpreting spectral image by using qualitative reasoning first we put forward a new concept called support coefficient function scf which can be used to extract represent and calculate qualitative correlation among data then we introduce an approach to determining dynamic shift interval of inaccurate data on the basis of qualitative correlation finally we discus how to use qualitative correlation a evidence of enhancing or depressing hypothesis for in accurate data the method ha been applied to a practical system for interpreting infrared spectral image we have fully tested the system against several hundred real spectral image the rate of identification ri and the rate of correctness rc are near and respectively and the latter is the highest among known system 
reasoning about time often involves incomplete information about period and their relationship variety of incompleteness include uncertainty about the number of object involved the distribution of a set of temporal relation among these object and what can be called the participation of a set of object in a temporal relation a solution to the problem of representing and reasoning about incomplete temporal information of these kind is forthcoming if a restricted class of non convex interval called n tntervals is added to the temporal domain of discourse an n interval corresponds to the common sense notion of a recurring period of time with a possibly unspecified number of occurrence in this paper we formalize a representation for temporal reasoning problem using n interval the language of the framework is restricted in such a way that tractable technique from constraint satisfaction can be applied specifically it is demonstrated how the problem of determining path consistency in a network of binary n interval relation can be solved 
this paper describes a flexible framework and an efficient algorithm for constraint based spatio temporal configuration problem binary constraint between spatio temporal object are first converted to constraint region which are then decomposed into hierarchical data structure based on this constraint decomposition an improved backtracking algorithm called hbt can compute a solution quite efficiently in contrast to other approach the proposed method is characterized by the efficient handling of arbitrarily shaped object and the flexible integration of quantitative and qualitative constraint it allows a wide range of object and constraint to be utilized for specifying a spatio temporal configuration the method is intended primarily for configuration problem in user interface but can effectively be applied to similar problem in other area a well 
we study a bimodal nonmonotonic logic mbnf suggested in lifschitz a a generalization of a number of nonmonotonic formalism we show first that it is equivalent to a certain nonmodal system involving rule of a special kind next it is shown that the latter admits a modal representation that us only one modal opera tor the operator of belief moreover under this translation the model of mbnf correspond to expansion of the associated modal nonmonotonic logic finally we show that a far a such model are concerned mbnf is redunhle to nonmodal default consequence relation from bochman these result have general consequence concerning relationship between different formalization of nonmonotonic reasoning 
a distributed neural network model called spec for processing sentence with recursive relative clause is described the model is based on separating the task of segmenting the input word sequence into clause forming the case role representation and keeping track of the recursive embeddings into different module the system need to be trained only with the basic sentence construct and it generalizes not only to new instance of familiar relative clause structure but to novel structure a well spec exhibit plausible memory degradation a the depth of the center embeddings increase it memory is primed by earlier constituent and it performance is aided by semantic constraint between the constituent the ability to process structure is largely due to a central executive network that monitor and control the execution of the entire system this way in contrast to earlier subsymbolic system parsing is modeled a a controlled high level process rather than one based on automatic reflex response 
vital information for corporate activity is generally stored in large database while conventional data base management system offer limited query flexibility system capable of generating similarity based query such a those seen in case based reasoning research would certainly enhance the utility of data resource this paper describes a method for building case based system using a conventional relational data base rdb the core of the algorithm is a novel approach to similarity computing in which database query form similarity rather than similarity of individual case are computed the method us standard query language sql to achieve nearest neighbor matching thus allowing similarity based database retrieval it ha been implemented a a part of the caret case retrieval tool and evaluated through the use of a newly developed corporate wide case based system for a software quality control domain experiment have shown the proposed method to provide retrieval result equivalent to those of non rdb implementation at a sufficiently fast response time 
this paper examines several system which learn a large number of rule production including one which learns rule the largest number ever learned by an ai system and the largest number in any production system in existence it is important to match these rule dficiently in order to avoid the machine learning utility problem moreover examination of such large system reveals new phenomenon and call into question some common assumption based on previous observation of smalkr system we first show that the rete and treat match algorithm do not scale well with the number of rule in our system in part because the number of rule affected by a change to working memory increase with the total number of rule in these system we also show that the sharing of node in the beta part of the rete network becomes more and more important a the number of rule increase finally we describe and evaluate a new optimization for rete which improves it scalability and allows two of our system to learn over rule without significant performance degradation 
decision made in setting up and running search program bias the search that they perform search bias refers to the definition of a search space and the definition of the program that navigates the space this paper address the problem of using knowledge regarding the complexity of various syntactic search bias to form a policy for selecting bias in particular this paper show that a simple policy iterative weakening is optimal or nearly optimal in case where the bias can be ordered by computational complexity and certain relationship hold between the complexity of the various bias the result are obtained by viewing bias selection a a higher level search problem iterative weakening evaluates the state in order of increasing complexity an offshoot of this work is the formation of a near optimal policy for selecting both breadth and depth bound for depth first search with very large possibly unbounded breadth and depth 
ai need many idea that have hitherto been studied only by philosopher this is because a robot if it is to have human level intelligence and ability to learn from it experience need a general world view in which to organize fact it turn out that many philosophical problem take new form when thought about in term of how to design a robot some approach to philosophy are helpful and others are not 
we propose a natural model of abduction based on the revision of the epistemic state of an agent we require that explanation be sufficient to induce belief in an observation in a manner that adequately account for factual and hypothetical observation our model will generate explanation that nonmonotonically predict an observation thus generalizing most current account which require some deductive relationship between explanation and observation it also provides a natural preference ordering on explanation defined in term of normality or plausibility we reconstruct the theorist system in our framework and show how it can be extended to accommodate our predictive explanation and semantic preference on explanation 
compositionality at aaai elkan ha claimed to have a result trivializing fuzzy logic this trivialization is based on too strong a view of equivalence in fuzzy logic and relates to a fully compositional treatment of uncertainty such a treatment is shown to be impossible in this paper we emphasize the distinction between i degree of partial truth which are allowed to be truth functional and which pertain to gradual or fuzzy proposition and ii degree of uncertainty which cannot be compositional with respect to all the connective when attached to classical proposition this distinction is exemplified by the difference between fuzzy logic and possibilistic logic we also investigate an almost compositional uncertainty calculus but it is shown to lack expressiveness 
this paper make the following two contribution toformal theory of action showing that a causal minimizationframework can be used effectively to specifythe effect of indeterminate action and showing thatfor certain class of such action regression an effectivecomputational mechanism can be used to reasonabout them introductionmuch recent work on theory of action ha concentratedon primitive determinate action in this paper we pose ourselves the problem of 
when a case based planner is retrieving a previous case in preparation for solving a new similar problem it is often not aware of all of the implicit feature of the new problem situation which determine if a particular case may be successfully applied this mean that some case may fail to improve the planner s performance by detecting and explaining these case failure a they occur retrieval may be improved incrementally in this paper we provide a definition of case failure for the case based planner derivation replay in which solves new problem by replaying it previous plan derivation we provide explanationbased learning ebl technique for detecting and constructing the reason for the case failure we also describe how the case library is organized so a to incorporate this failure information a it is produced finally we present an empirical study which demonstrates the effectiveness of this approach in improving the performance of 
we present a very simple selective search algorithm for two player game it always expands next the frontier node that determines the minimax value of the root the algorithm requires no information other than a static evaluation function and it time overhead per node is similar to that of alpha beta minimax we also present an implementation of the algorithm that reduces it space complexity from exponential to linear in the search depth at the cost of increased time complexity in the game of othello using the evaluation function from biil lee mahajan bestfirst minimax outplays alpha beta at moderate depth a hybrid best first extension algorithm which combine alpha beta and best first minimax performs significantly better than either pure algorithm even at greater depth similar result were also obtained for a class of random game tree 
autonomous mobile robot need very reliable navigation capability in order to operate unattended for long period of time this paper report on first result of a research program that us par tially observable markov model to robustly track a robot location in office environment and to direct it goal oriented action the approach explicitly maintains a probability distribution over the possible location of the robot taking into account various source of uncertainly including approximate knowledge of the environment and actuator and sensor uncertainty a novel feature of our approach is it integration of topological map information with approximate metric information we demonstrate the robustness of this approach in controlling an actual indoor mobile robot navigating corridor 
in previous work we presented an algorithm for tense interpretation which employ a temporal focus to determine the intended temporal relation between the state and event mentioned in a narrative in this paper we propose a new two phased classification scheme for aspect each situation described in an utterance is first classified a static state or dynamic event and if dynamic a telic event with a culmination point or atelic event without a culmination point then independent of the class the view of the situation is identified either a a point or a an interval we then demonstrate how the determination of aspect can be integrated into our tense interpretation algorithm to produce a richer analysis of temporal relation our classification for aspect is more detailed than most of the existing scheme allowing u to extract the interval relation between situation and cover a wide range of english narrative 
an approach to reduce number of spurious symptom in aircraft engine fault monitoring is investigated two strategy were utilized a set of rule designed to filter spurious symptom wa created then a neural network wa designed to generate expectation value for each of the sensor monitored the neural net wa trained for a specific engine during normal operation after capturing pattern for normal engine behavior in the neural net an expectation value for the sensor is predicted the success of this approach relies on generating better expectation value which in turn produce smaller variation from actual operating behavior and hence generate fewer spurious symptom resulting hybrid system of neural network and rule based model demonstrates a drastic reduction of overall spurious symptom 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
this paper describes an integrated method for processing grammatically ill formed input we use partial par of the input for recovering from parsing failure in order to select partial par appropriate for error recovery cost and reward are assigned to them cost and reward represent the badness and goodness of a partial parse respectively the most appropriate partial parse is selected on the basis of cost and reward trade off the system contains three module module a handle local ill formedness such a constraint violation module b handle non local ill formedness such a word order violation and module c handle non local ill formedness such a contextual ellipsis these three module work in a uniform framework based on the notion of cost and reward 
stratified case based reasoning is a technique in which abstract solution produced during hierarchical problem solving are used to assist case based retrieval matching and adaptation we describe the motivation for the integration of case based reasoning with hierarchical problem solving exemplify it benefit detail a set of algorithm that implement our approach and present their comparative empirical evaluation on a path planning task our result show that stratified case based reasoning significantly decrease the computational expense required to retrieve match and adapt case leading to performance superior both to simple case based reasoning and to hierarchical problem solving ab initio 
contract algorithm offer a tradeoff between output quality and computation time provided that the amount of computation time is determined prior to their activation originally they were introduced a an intermediate step in the composition of interruptible anytime algorithm however for many real time task such a information gathering game playing and a large class of planning problem contract algorithm offer an ideal mechanism to optimize decision quality this paper extends previous result regarding the meta level control of contract algorithm by handling a more general type of performance description the output quality of each contract algorithm is described by a probabilistic rather than deterministic conditional performance profile such profile map input quality and computation time to a probability distribution of output quality the composition problem is solved by an efficient off line compilation technique that simplifies the run time monitoring task 
termination of logic program with negated body atom here called general logic program is an important topic this is also due to the fact that the computational mechanism used to process negated atom like clark s negation a failure and chan s constructive negation are based on termination condition this paper introduces a methodology for proving termination of general logic program when the prolog selection rule is considered this methodology is based on the notion of low up acceptable program we prove that low up acceptable program characterize a class of general logic program which terminate for a large class of query which contains the set of all ground query we consider a operational model sld resolution augmented with a procedure to deal with negative literal known a chan s constructive negation general logic program can be used to express concept and problem in non monotonic reasoning we show here that interesting problem in non monotonic reasoning can be formalized and implemented by mean of up low general logic program 
given a set of number the two way partitioning problem is to divide them into two subset so that the sum of the number in each subset are a nearly equal a possible the problem is np complete and is contained in many scheduling application based on a polynomial time heuristic due to karmarkar and karp we present a new algorithm called complete karmarkar karp ckk that optimally solves the general number partitioning problem ckk significantly outperforms the best previously known algorithm for this problem by restricting the number to twelve significant digit we can optimally solve two way partitioning problem of arbitrary size in practice ckk first return the karmarkar karp solution then continues to find better solution a time allows almost five order of magnitude improvement in solution quality is obtained within a minute of running time rather than building a single solution one element at a time ckk construct subsolutions and combine them in all possible way ckk is directly applicable to the knapsack problem since it can be reduced to number partitioning this general approach may also be applicable to other np hard problem a well 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
this paper deal with learning in reactive multi agent system the central problem addressed is how several agent can collectively learn to coordinate their action such that they solve a given environmental task together in approaching this problem two important constraint have to be taken into consideration the incompatibility constraint that is the fact that different action may be mutually exclusive and the local information constraint that is the fact that each agent typically know only a fraction of it environment the content of the paper is a follows first the topic of learning in multi agent system is motivated section then two algorithm called ace and age standing for action estimation and action group estimation respectively for the reinforcement learning of appropriate sequence of action set in multi agent system are described section next experimental result illustrating the learning ability of these algorithm are presented section finally the algorithm are discussed and an outlook on future research is provided section 
abstract diagrammatic reasoning is a type of reason ing in which the primary mean of inference is direct manipulation and inspection of a di agram diagrammatic reasoning is prevalent in human problem solving behavior especially for problem involving spatial relation among physical object our research examines the relationship between diagrammatic and sym bolic reasoning in a computational framework we have built a system called redraw that emulates the human capability for reasoning with picture in civil engineering the class of structural analysis problem chosen provides a realistic domain whose solution process re quire domain specific knowledge a well a pic torial reasoning skill we hypothesize that dia grammatic representation provide an environ ment where inference about the physical re sults of proposed structural configuration can take place in a more intuitive manner than that possible through purely symbolic representa tions 
we review accuracy estimation method and compare the two most common method crossvalidation and bootstrap recent experimental result on artificial data and theoretical re cult in restricted setting have shown that for selecting a good classifier from a set of classifier model selection ten fold cross validation may be better than the more expensive leaveone out cross validation we report on a largescale experiment over half a million run of c and a naive bayes algorithm to estimate the effect of different parameter on these algrithms on real world datasets for crossvalidation we vary the number of fold and whether the fold are stratified or not for bootstrap we vary the number of bootstrap sample our result indicate that for real word datasets similar to ours the best method to use for model selection is ten fold stratified cross validation even if computation power allows using more fold 
last summer aaai sponsored a mobile robot competition in conjunction with the aaai conference in san jose california ten robot from across the country competed in the competition with carmel from the university of michigan finishing first carmel is a cybermotion k a mobile platform with a ring of sonar sensor and a single black and white ccd camera for computing carmel ha three processor one for motor control one for sonar ring firing and one executing high level routine such a obstacle avoidance and object recognition all computation and power is contained entirely on board 
problem of liveness and fairness are considered in multi agent system by mean of abstract language different approach to define such property for the agent and for a multi agent system a a whole are discussed it turn out that the property of a multi agent system need not correspond to separately definable property of the agent e g a community of fair agent need not constitute a fair multi agent system in general analysis and verification need the consideration of the whole system and the agent have to be considered in the context of the system too the result are not unique there are different result for deadlock freedom liveness and fairness respectively 
example form an integral and very important part of many description especially in context such a tutoring and documentation generation the ability to tailor a description for a particular situation is particularly important when different situation can result in widely varying description this paper considers the generation of description with example for two different situation introductory text and advanced reference manual style text previous study have focused on any the example or the language component of the explanation in isolation however there is a strong interaction between the example and the accompanying description and it is therefore important to study how both these component are affected by change in the situation in this paper we characterize example in the context of their description along three orthogonal ax the information content the knowledge type of the example and the text type in which the explanation is being generated while variation along either of the three ax can result in different description this paper address variation along the text type axis we illustrate our discussion with a description of a list from our domain of lisp documentation and present a trace of the system a it generates these description 
current specialized planner for query processing are designed to work in local reliable and predictable environment however a number of problem arise in gathering information from large network of distributed information in this environment the same information may reside in multiple place action can be executed in parallel to exploit distributed resource new goal come into the system during execution action may fail due to problem with remote database or network and sensing may need to be interleaved with planning in order to formulate efficient query we have developed a planner called sage that address the issue that arise in this environment this system integrates previous work on planning execution replanning and sensing and extends this work to support simultaneous and interleaved planning and execution sage ha been applied to the problem of information gathering to provide a flexible and efficient system for integrating heterogeneous and distributed data 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
analytical result from ai planning research provide the motivation for this experimental study of ordering relationship in human planning we examine timing of human performing specific task from the ai planning literature and present evidence that normal human planner like state of the art ai planning system use partial order plan representation we also describe ongoing experiment that are designed to shed light on the plan representation used by child and by adult with planning deficit due to brain damage several point of interest for collaboration between ai scientist and neuropsychologists are noted a are impact that we feel this research may have on future work in ai planning 
abstract this paper analyzes the complexity of on line reinforcement learning algorithm namely asynchronous realtime version of q learning and value iteration applied to the problem of reaching a goal state in deterministic domain previous work had concluded that in many case tabula rasa reinforcement learning wa exponential for such problem or wa tractable only if the learning algorithm wa augmented we show that to the contrary the algorithm are tractable with only a simple change in the task representation or initialization we provide tight bound on the worst case complexity and show how the complexity is even smaller if the reinforcement learning algorithm have initial knowledge of the topology of the state space or the domain ha certain special property we also present a novel bidirectional q learning algorithm to nd optimal path from all state to a goal state and show that it is no more complex than the other algorithm 
it is well known that state abstraction can speed up planning exponentially under ideal condi tions we add to the knowledge showing that state abstraction may likewise slow down planning exponentially and even result in generat ing an exponentially longer solution than necessary this phenomenon can occur for abstraction hierarchy which are generated automatically by the alpine and highpoint algorithm we further show that there is little hope of any drastic improvement upon these algorithm it is computationally difficult to generate abstraction hierarchy which allow finding good approximation of optimal plan 
in multiagent planning an agent sometimes need to collaborate with others to construct complex plan or to accomplish large organizational task which it cannot do alone since each agent in a group may have incorrect belief about the world and incomplete knowledge and because agent s ability differ constructing a coordinated collaborative plan among agent is a difficult proposition in previous work osawa and tokoro we developed a scheme for constructing collaborative plan from the possibly incomplete individual plan of agent this scheme wa designed to provide availability based assignment of goal to agent and opportunistic collaboration to distributed planning in open multiagent environment based on the contract net in this paper we formalize incomplete individual plan and collaborative planning among rational agent using the multi world model and provide a utility based model for rational choice of action agent can effectively balance workload based on the utility theory a condition for incomplete collaborative plan is also presented 
this paper present a statistical analysis of the davis putnam procedure and propositional satisfiability problem sat sat ha been researched in ai because of it strong relationship to automated reasoning and recently it is used a a benchmark problem of constraint satisfaction algorithm the davis putnam procedure is a well known satisfiability checking algorithm based on tree search technique in this paper i analyze two average case complexity for the davis putnam procedure the complexity for satisfiability checking and the complexity for finding all solution i also discus the probability of satisfiability the complexity and the probability strongly depend on the distribution of formula to be tested and i use the fixed clause length model a the distribution model the result of the analysis coincides with the experimental result well 
technique that traditionally have been useful for retrieving same domainanalogies from small single use knowledge base such a spreading activationand indexing on selected feature are inadequate for retrieving cross domainanalogies from large multi use knowledge base in this paper we describeknowledge directed spreading activation kdsa a new method for retrievinganalogies in a large semantic network kdsa us task specific knowledgeto guide a spreading activation search 
real time algorithm need to address the time constraint e g deadline imposed by application like process control and robot navigation furthermore dependable real time algorithm need to be predictable about their ability to meet the time constraint of given task a real time algorithm is predictable if it can decide the feasibility of meeting time constraint of a given task or an arbitrary task from a task set well ahead of the deadline lastly a real time algorithm should exhibit progressively optimizing behavior i e the quality of the solution produced should improve a time constraint are relaxed we propose a new algorithm sarts that is based on a novel on line technique to choose the proper value of parameter which control the time allocated to planning based on the time constraint sarts also provides criterion to predict it ability to meet the time constraint of a given task the paper provides theoretical and experimental characterization of sarts a a dependable real time algorithm 
we propose a propositional language for temporal reasoning that is computationally effective yet expressive enough to describe information about fluents event and temporal constraint although the complete inference algorithm is exponential we characterize a tractable core with limited expressibility and inferential power our result render a variety of constraint propagation technique applicable for reasoning with constraint on auents 
this paper report on an implementation of kanerva s sparse distributed memory for the connection machine in order to accomplish a modular and adaptive software library we applied a plain object oriented programming style to the common lisp extension ltsp some variation of the original model the selected coordinate design the hyperplane design and a new general design a well a the folded sdm due to kanerva are realized it ha been necessary to elaborate a uniform presentation of the theoretical foundation the different design are based on we demonstrate the simulator s functionality with some simple application runtime comparison are given we encourage the use of our simulation tool when outlining research topic of special interest to sdm 
we present teleassistance a two tiered control structure for robotic manipulation that combine the advantage of autonomy and teleoperation at the top level a teleoperator provides global deictic reference via a natural sign language each sign indicates the next action to perform and a relative and hand centered coordinate frame in which to perform it for example the teleoperator may point to an object for reaching or preshape the hand for grasping at the lower level autonomous servo routine run within the reference frame provided teleassistance offer two benefit first the servo routine can position the robot in relative coordinate and interpret feedback within a constrained context this significantly simplifies the computational load of the autonomous routine and requires only a sparse model of the task second the operator s action are symbolic conveying intent without requiring the person to literally control the robot this help to alleviate many of the problem inherent to teleoperation including poor mapping between operator and robot physiology reliance on a broad communication bandwidth and the potential for robot damage when solely under remote control to demonstrate the concept a utah mit hand mounted on a puma arm open a door 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
although the idea of generating plan through nonlinear or partially ordered partially instantiated popi planningi ha been around for almost twenty year it is only recently that the search space characteristic of popi planner have received particular attention a big thrust in this work ha been on reducing the redundancy in the search space of popi planner this wa largely motivated by the belief that redundancy reduction will lead to improvement in planning efficiency one approach towards redundancy elimination that turned out to be particularly influential a evidencedby several closely related extension wa that of mcallester s mcallester showed that it is possible to design a popi planner that is systematic in the strong sense that it never visit two equivalent plan or plan having overlapping linearizations such systematic planner were claimed to be more efficient than planner that admit redundancy in their search space while search space redundancy is an important factor affecting the efficiency of a p o pi planner another perhaps equally important one is the level of commitment in the planner after all avoiding premature commitment wa one of the primary motivation for popi planning there is a tradeoff between the redundancy elimination and least commitment in that often the redundancy is eliminated at the expense of increased commitment in the planner for example mcallester s planner achieves systematicity by keeping track of the causal structure of the plan generated during search and ensuring that each branch of the search space commits to and protects mutually exclusive causal structure for the partial plan we will see that such protection amount to a strong form of premature commitment which increase the amount of backtracking a well a the solution depth and can have an adverse effect on the performance of the planner in this paper we shall argue that the performance of a popi planner depends more closely on the way it deal with the tradeoff between redundancy and commitment than with the systematicity of it search we will start with a rational reconstruction of the motivation behind systematicity in popi planning and show that systematicity is just one extreme solution for the tradeoff between redundancy and commitment we will show that there are a spectrum of solution to this tradeoff and identify the dimension along which they vary we will explore the relative utility of the different solution through a comparative study of seven planner that fall at different point on the spectrum our study show that 
this is a report of the application of the model generation theorem prover developed at icot to problem in the theory of finite quasigroups several of the problem were previously open in this paper we discus our theorem proving method related to those of the existing provers satchmo manthey bry and otter mccune and note how parallel processing on the icot parallel inference machine wa used to obtain high speed we then present and discus our machine aided investigation of seven problem concerning the existence of type of quasigroup 
this paper introduces a new concept a decisiontree or list over tree pattern which is anatural extension of a decision tree or decisionlist for dealing with tree structured object the learnability of this class is studied withinthe framework of the probably approximatelycorrect learning model and the identificationin the limit model it is found that the classk node dlrtp a subclass of decision list overregular tree pattern is not polynomial timepac learnable if np rp 
during the planning of multimedia presentation at least two distinct processesare required planning the underlying discourse structure that is ordering andinterrelating the information to be presented and allocating the medium that is delimiting the portion to be displayed by each individual medium the formerprocess ha been the topic of several study in the area of text planning butnumerous question remain for the latter including what is the nature of theallocation 
in many diagnosis and repair domain diagnostic reasoning cannot be abstracted from repair action nor from action necessary to obtain diagnostic information in general in exploratory corrective domain an agent ha to interleave exploratory activity with activity aimed at achieving it goal in traumaid a consultation system for multiple trauma management we implement a reasoning framework for such domain which integrates diagnostic reasoning with planning and action this paper present goal directed diagnosis gdd a formalization of traumaid s diagnostic reasoning taking the view that a diagnosis is only worthwhile to the extent that it can affect repair decision gdd us goal to focus on such goal are also useful a a mean of communicating with it accompanying planner 
the problem of representing and reasoning about two notion of time that are relevant in the context of knowledge base is addressed these are called historical time and belief time respectively historical time denotes the time for which information model reality belief time denotes the time lor which a belief is held by an agent or a knowledge base we formalize an appropriate theory of time using logic a a meta language we then present a metalogic program derived from this theory through fold unfold transformation the metalogic program enables the temporal reasoning required for knowledge base application to be carried out efficiently the metalogic program is directly implementable a a prolog program and hence the need for a more complex theorem prover is obviated the approach is applicable for such knowledge base application a legislation and legal reasoning and in the context of multi agent reasoning where an agent reason about the belief of another agent 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
case based reasoning refers to the class of memory based problem solving method which emphasize the adaptation of recalled solution explanation diagnosis plan over the generation of solution from first principle cbr ha become a popular methodology resulting in a proliferation of case organization and representation proposal the goal of this paper is to sort through some of these proposal using the formal model of procedure and case based reasoning introduced in zito wolf and alterman we compare three current proposal for the organization of procedural case base individual case microcases and multi case we give a worst case analysis that show the advantage of the multi case in term of case storage and retrieval cost the model predicts that multi case reduce case storage and retrieval cost a compared to the other two model we then provide some empirical evidence from an implemented system that suggests that the trend observed in the formal model are also observable in case base of practical size 
we have developed and implemented in the qportrait program a qualitative simulation based method to construct phase portrait for a significant class of system of two coupled first order autonomous differential equation even in the presence of incomplete qualitative knowledge differential equation model are important for reasoning about physical system the field of nonlinear dynamic ha introduced the powerful phase portrait representation for the global analysis of nonlinear differential equation qportrait us qualitative simulation to generate the set of all possible qualitative behavior of a system constraint on two dimensional phase portrait from nonlinear dynamic make it possible to identify and classify trajectory and their asymptotic limit and constrain possible combination by exhaustively forming all combination of feature and filtering out inconsistent combination qportrait is guaranteed to generate all possible qualitative phase portrait we have applied qportrait to obtain tractable result for a number of nontrivial dynamical system guaranteed coverage of all possible behavior of incompletely known system complement the more detailed but approximation based result of recently developed method for intelligentlyguided numeric simulation nishida et al sack yip zhao combining the strength of both approach would better facilitate automated understanding of dynamical system 
multilingual instruction generation ha been the object of many study recently motivated by the increased need to produce multilingual manual coupled with the cost of technical writing and translating these study concentrate on the automatic generation of instruction leaving technical writer out of the loop in many case however it is not possible to dispense with human intervention entirely for at least two reason first the system must be provided with a semantic knowledge base from which the instruction can be generated second it is the technical writer who have the expertise necessary for producing instruction appropriate for a specific product or company and it is not necessarily an easy task to make this expertise available to a system the result of a requirement analysis study confirm the view that the most useful tool is not a stand alone writing tool but rather one that support technical writer in their task in this paper we describe such a support tool which we developed based on the result of our user requirement analysis 
detecting interaction and resolving conflict is one of the key issue for generative planning system hierarchical task network htn planning sytems use critic for this purpose critic have provided extra efficiency and flexibility to htn planning system but their procedural and sometimes domain specific nature ha not been amenable to analytical study a a result little work is available on the correctness or efficiency of critic this paper describes a principled approach to handling conflict a implemented in umcp an htn planning system critic in umcp have desirable property such a systematicity and the preservation of soundness and completeness 
semantic hyper linking plaisted et al chu and plaisted chu and plaisted ha been proposed recently to use semantics with hyper linking lee and plaisted an instance based theorem proving technique ground instance are generated until an unsatisfiable ground set is obtained semantics is used to greatly reduce the search space one disadvantage of semantic hyper linking is that large ground literal if needed in the proof sometimes are hard to generate in this paper we propose rough resolution a refinement of resolution robinson to only resolve upon maximum literal that are potentially large in ground instance and obtain rough resolvent rough resolvent can be used by semantic hyper linking to avoid generating large ground literal since maximum literal have been deleted a an example we will show how rough resolution help to prove lim bledsoe which cannot be proved using semantic hyper linking only we will also show other result in which rough resolution help to find the proof faster though incomplete rough resolution can be used with other complete method that prefer small clause 
the agm paradigm is a formal approach to ideal and rational information change from a practical perspective it suffers from two shortcoming the first involves difficulty with respect to the finite representation of information and the second involves the lack of support for the iteration of change operator in this paper we show that these practical problem can be solved in theoretically satisfying way wholely with in the agm paradigm we introduce a partial entrenchment ranking which serf a a canonical representation for a theory base and a well ranked episterruc entrenchment and we provide a computational model for adjusting partial entrenchment ranking when they receive new information using a procedure based on the principle of minimal change the connection between the standard agm theory change operator and the theory base change operator developed herein suggest that the proposed computational model for iterated theory base change exhibit desirable behaviour 
this paper provides a search based algorithm for computing prior and posterior probability in discrete bayesian network this is an anytime algorithm that at any stage can estimate the probability and give an error bound whereas the most popular bayesian net algorithm exploit the structure of the network for efficiency we exploit probability distribution for efficiency the algorithm is most suited to the case where we have extreme close to zero or one probability a is the case in many diagnostic situation where we are diagnosing system that work most of the time and for commonsense reasoning task where normality assumption allegedly dominate we give a characterisation of those case where it work well and discus how well it can be expected to work on average 
this paper describes a case based approach to knowledge acquisition for natural language system that simultaneously learns part of speech word sense and concept activation knowledge for all open class word in a corpus the parser begin with a lexicon of function word and creates a case base of context sensitive word definition during a humansupervised training phase then given an unknown word and the context in which it occurs the parser retrieves definition from the case base to infer the word s syntactic and semantic feature by encoding context a part of a definition the meaning of a word can change dynamically in response to surrounding phrase without the need for explicit lexical disambiguation heuristic moreover the approach acquires all three class of knowledge using the same case representation and requires relatively little training and no hand coded knowledge acquisition heuristic we evaluate it in experiment that explore two of many practical application of the technique and conclude that the case based method provides a promising approach to automated dictionary construction and knowledge acquisition for sentence analysis in limited domain in addition we present a novel case retrieval algorithm that us decision tree to improve the performance of a k nearest neighbor similarity metric 
we introduce a logic based system which improvesthe performance of intelligent help systemsby supplying them with plan generationand plan recognition component bothcomponents work in close mutual cooperation there are two mode of cross talk betweenthem one where plan recognition is done on thebasis of abstract plan provided by the plannerand the other where optimal plan are generatedbased on recognition result the exampleswhich are presented are taken from an operatingsystem 
in multi agent environment where agent independently generate and execute plan to satisfy their goal the resulting plan may sometimes overlap in this paper we propose a collaboration mechanism using social law through which rational agent can smoothly delegate and receive the execution of the overlapping part of plan in order to reduce the cost of plan execution also we consider collaboration with agent that do not abide by social law that is self centered agent simulation result show that our mechanism also ha the property of balancing the cost of plan execution and show flexibility towards selfcentered agent 
during the conceptual analysis the coexistence of several kind of ambiguity tends to complicate the task of every kind of disambiguation module we tackle this problem here for two type of ambiguity anaphor and the pp attachment we show what kind of problem every conceptual analyser ha to face we present then our solution to resolve these problem we study the efficiency of the proposed solution and it adequacy regarding dependency between both ambiguity 
we describe a new approach to default reasoning based on a principle on indifference among possible world we interpret default rule a extreme statistical statement thus obtaining a knowledge base kb comprised of statistical and first order statement we then assign equal probability to all world consistent with kb in order to assign a degree of belief to a statement the degree of belief can be used to decide whether to defeasibly conclude various natural pattern of reasoning such a a preference for more specific default indifference to irrelevant information and the ability to combine independent piece of evidence turn out to follow naturally from this technique furthermore our approach is not restricted to default reasoning it support a spectrum of reasoning from quantitative to qualitative it is also related to other system for default reasoning in particular we show that the work of goldszmidt et al which applies maximum entropy idea to semantics can be embedded in our framework 
this paper introduces a problem solving task involving common sense reasoning that human are adept at but one which ha not received much attention within the area of cognitive modeling until recently this is the task of predicting the operation of simple mechanical device in term of behavior of their component from labeled schematic diagram showing the spatial configuration of component and a given initial condition we describe this task present a cognitive process model developed from task and protocol analysis and illustrate it using the example of a pressure gauge then the architecture of a corresponding computer model and a control algorithm embodying the cognitive strategy are proposed 
this paper considers the problem of simulating creativity in the domain of jazz improvisation and accompaniment unlike most current approach we try to model the musician behavior by taking into account their experience and how they use it with respect to the evolving context of live performance to represent this experience we introduce the notion of musical memory which exploit the principle of case based reasoning slade to produce live music using this musical memory we propose a problem solving method based on the notion of pact potential action that are activated according to the context and then combined in order to produce note we show that our model support two of the main feature of creativity i e non determinism and the absence of well defined goal johnson laird 
this paper explores the relationship among reactivity heuristic reasoning and search it describes a hybrid hierarchical reasoner that first ha the opportunity to react correctly if no ready reaction is computed the reasoner activates a set of reactive trigger for time limited search procedure if they too fail to produce a response the reasoner resort to collaboration among a set of heuristic rationale in a series of experiment this hybrid reasoner is shown to be effective and efficient the data also show how each of the three process correct reaction time limited search with reactive trigger and heuristic rationale play an important role in problem solving reactivity is demonstrably enhanced by brief knowledge based intelligent search to generate solution fragment 
this paper provides a systematic analysis of the relative utility of basing ebg based plan reuse technique in partial ordering v total ordering planning framework we separate the potential advantage into those related to storage compaction and those related to the ability to exploit stored plan we observe that the storage compaction provided by partially ordered partially instantiated plan can to a large ex ent be exploited regardless of the underlying planner we argue that it is in the ability to exploit stored plan during planning that partial ordering planner have some distinct advantage in particular to be able to flexibly reuse and extend the retrieved plan a planner need the ability to arbitrarily and efficiently splice in new step and sub plan into the retrieved plan this is where partial ordering planner with their least commitment strategy and flexible plan representation score significantly over state based planner a well a planner that search in the space of totally ordered plan we will clarify and supporll this hypothesis through an empirical study of three planner and two reuse strategy 
constraint satisfaction problem involve finding value for problem variable that satisfy constraint on what combination of value are permitted they have application in many area of artificial intelligence from planning to natural language understanding a new method is proposed for decomposing constraint satisfaction problem using inferred disjunctive constraint the decomposition reduces the size of the problem some solution may be lost in the process but not all the decomposition support an algorithm that exhibit superior performance analytical and experimental evidence suggests that the algorithm can take advantage of local weak spot in globally hard problem 
a constraint satisfaction problem may not admit a complete solution in this case a good partial solution may be acceptable this paper present new technique for organizing search with branch and bound algorithm so that maximal partial solution those having the maximum possible number of satisfied constraint can be obtained in reasonable time for moderately sized problem the key feature is a type of variable ordering heuristic that combine width at a node of the constraint graph number of constraint shared with variable already chosen with factor such a small domain size that lead to inconsistency in value of adjacent variable ordering based on these heuristic lead to a rapid rise in branch and bound s cost function together with local estimate of future cost which greatly enhances lower bound calculation both retrospective and prospective algorithm based on these heuristic are dramatically superior to earlier branch and bound algorithm developed for this domain 
this paper present a method for constructing deterministicprolog parser from corpus of parsedsentences our approach us recent machinelearning method for inducing prolog rule fromexamples inductive logic programming we discussseveral advantage of this method comparedto recent statistical method and present resultson learning complete parser from portion of theatis corpus introductionrecent approach to constructing robust parsersfrom corpus primarily use statistical 
intellectics ie artificial intelligence and cognitive science is an interdisciplinaryfield whose goal are to understand and explain intelligence on the one hand and todevelop computational model which show intelligent behavior on the other hand the system presented in this paper is based on idea taken from automated reasoningand connectionism the connectionist inference system chcl is applied tosolve the question whether a given sentence correctly describes the spatial relationsof 
domain oriented knowledge acquisition tool provide efficient support for the design of knowledge based system however the cost of developing such tool is high especially when their restricted scope is taken into account developer can use metalevel tool to generate domain oriented knowledge acquisition tool that are custom tailored for a small group of expert with considerably le effort than is required for manual tool development an epistemic obstacle to creating such metatools is the specification model for target knowledge acquisition tool the metatool dot is based on an abstract architecture approach to the specification and generation of knowledge acquisition tool dot is domain and method independent because it is based on an architectural model of the target knowledge acquisition tool 
a consistent text contains rich resource of information such a collocation pattern that can be used to resolve ambiguity within it sentence for example attachment ambiguity in a sentence can be resolved by selecting a candidate attachment that match attachment found in other sentence in the same discourse thus discourse can be regarded a a valuable knowledge resource for sentence analysis in this paper we examine some feature of discourse a a knowledge resource and propose a framework for natural language processing that provides a simple algorithm for using information extracted from discourse together with information stored in knowledge base the experimental result of using our framework to disambiguate sentence in technical document offer good prospect for improving the accuracy of a broad coverage natural language processing system that handle various text without constructing knowledge base for each text in advance some noteworthy feature of discourse information are also deduced from the result of our experiment 
this panel explores issue of systematic and stochastic control in the context of constraint satisfaction 
abstract thisarticlcshcsws howrational analysis canbcuscclto minimize learning cost for a general class of statistical learning problem wc discus the factor that inftuence learning cost and show that the problem of efficient learning can bccast a arescrurce optimization problem solution found inthiswaycan besignificantlymore efficient than the best solution that do not account for these factor we introduce a heuristic learning algorithm that approximately solves this optimization problem and document it performance in provemcntson synthetic and real world problem lntrodudion machine learning technique are valuable tool both in ac 
the situation recognition system to which this paper is devoted receives a input a stream of time stamped event it performs recognition of instance of occurring situation a they are developing and it generates a output deduced event and action to trigger it is mainly a temporal reasoning system it is predictive in the sense that it predicts forthcoming event relevant to it task it focus it attention on them and it maintains their temporal window of relevance it main functionality is to recognize efficiently complex temporal pattern on the fly while they are taking place this system ha been tested for the surveillance of an environment by a multisensory perception machine it is being applied to monitoring a complex dynamic system 
a formal theory of action and change in dynamic system is presented our formalism is based on the paradigm that state transition in a system naturally occur while time pass by one or more agent have the possibility to direct the development of the system by executing action our theory cover concurrency of action and event and includes a natural way to express delayed efiects and nondeterminism a uniform semantics for speciflcations of dynamic system is developed which enables u to express solution to problem like temporal projection planning and postdiction in term of logical entailment 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
we present zeno a least commitment planner that handle action occurring over extended interval of time deadline goal metric precondition metric effect and continuous change are supported simultaneous action are aliowed when their effect do not interfere unlike most planner that deal with complex language the zeno planning algorithm is sound and complete the running code is a complete implementation of the formal algorithm capable of solving simple problem i e those involving le than a dozen step 
programming language interpreter proving theorem of the form a abstract data type and program optimization can all be represented by a finite set of rule called a rewrite system in this paper we study two fundamental concept uniqueness of normal form and confluence for nonlinear system in the absence of termination this is a difficult topic with only a few result so far through a novel approach we show that every persistent system ha unique normal form this result is tight and a substantial generalization of previous work in the process we derive a necessary and sufficient condition for persistence for the first time and give new class of persistent system we also prove the confluence of the union function symbol can be shared of a nonlinear system with a left linear system under fairly general condition again persistence play a key role in this proof we are not aware of any confluence result that allows the same level of function symbol sharing 
a knowledge based system us it database a k a it theory to produce answer to the query it receives unfortunately these answer may be incorrect if the underlying theory is faulty standard theory revision system use a given set of labeled query each a query paired with it correct answer to transform the given theory by adding and or deleting either rule and or antecedent into a related theory that is a accurate a possible after formally defining the theory revision task and bounding it sample complexity this paper address the task s computational complexity it first prof that unless p np no polynomial time algorithm can identify the optimal theory even given the exact distribution of query except in the most trivial of situation it also show that except in such trivial situation no polynomial time algorithm can produce a theory whose inaccuracy is even close i e within a particular polynomial factor to optimal these result justify the standard practice of hill climbing to a locally optimal theory based on a given set of labeled sample 
task decomposition planner make use of schematathat define task in term of partially ordered setsof task and primitive action most existing taskdecompositionplanners synthesize plan via a topdownapproach called task reduction which usesschemata to replace task with network of task andactions until only action remain in this paper we present a bottom up plan parsingapproach to task decomposition instead of reducingtasks into action we use an incremental parsing 
there ha been some recent interest in intelligent backtracking procedure that can return to the source of a dif ulty without erasing the intermediate work in this paper we show that for some problem it can be counterproductive to do this and in fact that such inteiiigence can cause an exponentkd increase in the size of the ultimate search space we discus the reason for this phenomenon and we present one way to deal with it 
the problem of deciding which subset of value of a categorical valued attribute to branch on during decision tree generation is addressed algorithm such a id and c do not address the issue and simply branch on each value of the selected attribute the gid algorithm is presented and evaluated the gid algorithm is a generalized version of quinlan s id and c and is a non parametric version of the gid algorithm presented in an earlier paper it branch on a subset of individual value of an attribute while grouping the rest under a single default branch it is empirically demonstrated that gid outperforms id c and gid for any parameter setting of the latter the empirical test include both controlled synthetic randomized domain a well a real world data set the improvement in tree quality a measured by number of leaf and estimated error rate is significant 
in this paper we present a method for improving search efficiency in the area of constraint satisfaction problem in finite domain this method is based on the analysis of the micro structure of a csp we call micro structure of a csp the graph defined by the compatible relation between variablevalue pair vertex are these pair and edge are defined by pair of compatible vertex given the micro structure of a csp we can realize a preprocessing to simplify the problem with a decomposition of the domain of variable so we propose a new approach to problem decomposition in the field of csps well adjusted in case such a classical decomposition method are without interest i e when the constraint graph is complete the method is described in the paper and a complexity analysis is presented given theoretical justification of the approach furthermore two polynomial class of csps are induced by this approach the recognition of them being linear in the size of the instance of csp considered 
we discus the use of case based reasoning cbr to drive an information retrieval ir system our hybrid cbr ir approach take a input a standard frame based representation of a problem case and output text of relevant case retrieved from a document corpus dramatically larger than the case base available to the cbr system while the smaller case base is accessible by the usual case based indexing and is amenable to knowledge intensive method the larger ir corpus is not our approach provides two benefit it extends the reach of cbr for retrieval purpose to much larger corpus and it enables the injection of knowledge based technique into traditional ir our system work by first performing a standard hypo style cbr analysis and then using text associated with certain important case found in this analysis to seed a modified version of inquery s relevance feedback mechanism in order to generate a query we describe our approach and report on experiment performed in two different legal domain 
in planning task an agent may often find himself in a situation demanding that he choose an action that would prevent some unwanted event from occurring similarly in task involving the generation of description or explanation of sequence of event it is often useful to draw a many informative connection a possible between event in the sequence often this mean explaining why certain event are not possible in this paper i consider the semantics of event prevention and argue that a naive semantics which equates prevention with the elimination of all future possibility of the event in question is often difficult if not impossible to implement i argue for a more useful semantics which fall out of some reasonable assumption regarding restriction on the set of potential action available to an agent those action about which the agent ha formed intention those action consistent with the agent s attitude including it other intention and the set of action evoked by the type of situation in which the agent is embedded 
the circuit fix it shoppe is a voice interactive dialog system which ha been constructed in our laboratory the mission of the system is to help people repair electronic circuit the system contains a domain modeler a reasoning system a dialog controller a user modeling system an error correcting natural language parser and a natural language generator a commercial speech recognizer and speech synthesizer are used for voice input and output more detailed information about our dialog system can be found in and this videotape record two live dialog between the circuit fix it shoppe program and a user who ha no special knowledge of computer electronic repair or our system a brief description of the experimental setup and of the circuit fix it shoppe program precedes these dialog the circuit fix it shoppe program is capable of varying it level of initiative it can be highly directive in which case it control the conversation or it may be passive in which case the user control the dialog or it may take some level of initiative between these two extreme in the first videotape demonstration the system is running in directive mode in this second demonstration the system is set to operate in declarative mode in this mode the user is free to take the initiative and to control the conversation declarative mode is appropriate for user who are much more familiar with the circuit and require only minimal help from the computer duration minute second tape format vhs 
when learning classifier more extensive search for rule is shown to lead to lower predictive accuracy on many of the leal world domain investigated this counter intuitive re suit is particularly relevant to recent system the search method that use risk free pruning to achieve the same outcome a exhaustive search we propose an iterated search method that commences with greedy search extending it scope at each iteration until a stopping criterion is satisfied this layered search is often found to produce theory that are more accurate than those obtained with either greedy search or moderately extensive beam search 
testing the satisfiability of a boolean formula over linear constraint is not a simple matter existing ai system handle that kind of problem with a general proof method for their boolean part and a separate module for combining linear constraint on the contrary traditional operation research method need the problem to be transformed and solved with a mixed integer linear programming algorithm both approach appear to be improvable if no early separation is introduced between the logical and numerical part in this case combinatorial explosion can be dramatically reduced thanks to efficient looking ahead technique and learning method indeed propagating bound following the initial formula give precious information besides an especially tight linear relaxation can be driven from the formula and allows a simplex algorithm to make a good test for satisfiability finally these two looking ahead method can be easily coupled for more efficiency and completed by local enumeration moreover discovering a good failure explanation is relatively easy in the proposed framework by learning these explanation it is possible to prune important redundant part of the search tree 
abstract critical to the success of any real world agent is the ability to detect and recover from unsuccessful action failing to detect these error may cause the execution of the remainder of a plan to have unex pected and dangerous effect in this paper we present ed the error detective which systematically exe cutis lesioned operator in order to generate a table of error and associated cause for a software agent we describe feature of software environment that allow u to efficiently build this table without searching for simultaneous error or making the single fault assumption we then report on experiment compar 
this paper describes a distributed adaptive first order logic engine with exceptional performance characteristic the system combine serial search reduction technique such a bounded overhead subgoal caching and intelligent backtracking with a novel parallelization strategy particularly well suited to coarse grained parallel execution on a network of workstation we present empirical result that demonstrate our system s performance using workstation on over first order logic problem drawn from the thousand of problem for theorem provers collection utroduction we have developed an distributed adaptive first order logic engine a the core of a planning system intended to solve large logistics and transportation scheduling problem calistri yeh segre this underlying inference engine called dali distributed adaptive logical inference is based on an extended version of the warren abstract machine wam architecture ait kaci which also serf a the basis for many modem prolog implementation dali take a first order specification of some application domain the domain theory and us it to satisfy a series of query via a model elimination inference procedure our approach is inspired by ptip stickel in that it is based on prolog technology i e the wam but circumvents the inherent limitation thereof to provide an inference procedure that is complete relative to first order logic unlike pttp however dali employ a number of serial search reduction technique such a bounded overhead subgoal caching segre scharstein and intelligent backtracking kumar lin to improve search efficiency dali also exploit a novel parallelization scheme called nagging sturgill segre that support the effective use of a large number of loosely coupled processing element the message of this paper is that efficient implementation technology serial search reduction technique and parallel nagging can be successfully combined to produce a highperformance first order logic engine we support this claim with an extensive empirical performance evaluation 
when designing a device the final product of the design process is usually considered to be a physical specification of a device however the design of the causal mechanism underlying the physical specification i e how the device is intended to work to achieve it function is a product just a important a the physical specification if not more capturing this knowledge of causal mechanism is necessary in order to understand the physical specification of the device a well a to evaluate and refine the specification during the design process despite the importance of such knowledge existing cad tool do not support it explicit representation or manipulation we describe a design support system under development in which knowledge of both the causal mechanism and the physical structure of a device being designed is explicitly represented and manipulated the system allows the designer to provide functional specification at various level of abstraction in a language called cfrl causal functional representation language the cfrl specification acquired from the user enables the system to evaluate the physical specification a it is being developed in order to provide useful feedback to the designer furthermore functional specification provide an important basis for recording the engineer s design rationale 
this paper present a formalization of the bidding and awarding decision process that wa left undefined in the original contract net task allocation protocol this formalization is based on marginal cost calculation based on local agent criterion in this way agent having very different local criterion based on their selfinterest can interact to distribute task so that the network a a whole function more effectively in this model both competitive and cooperative agent can interact in addition the contract net protocol is extended to allow for clustering of task to deal with the possibility of a large number of announcement and bid message and to effectively handle situation in which new bidding and awarding is being done during the period when the result of previous bid are unknown the protocol is verified by the traconet transportation cooperation net system where dispatch center of different company cooperate automatically in vehicle routing the implementation is asynchronous and truly distributed and it provides the agent extensive autonomy the protocol is discussed in detail and test result with real data are presented 
this paper present a theoretical framework for mapping from structure to function in engineering domain we argue that a generative approach grounded in qualitative process theory produce useful functional explanation these explanation are articulate in that they enable the user to explore their theoretical justification and perform counterfactual reasoning these explanation stem from a teleological representation based on goal plan role and view we show that an ontology based on aggregated process facilitates the recognition of recurring thermodynamic structure we describe an implementation of this theory a system called carnot that explains steady flow thermodynamic cycle ranging in complexity from four to component 
oz is an experimental higher order concurrent constraint programming system under development at dfki it combine idea from logic and concurrent programming in a simple yet expressive language from logic programming oz inherits logic variable and logic data structure which provide for a programming style where partial information about the value of variable is imposed concurrently and incrementally a novel feature of oz is that it accommodates higher order programming without sacrificing that denotation and equality of variable are captured by first order logic another new feature of oz is constraint communication a new form of asynchronous communication exploiting logic variable constraint communication avoids the problem of stream communication the conventional communication mechanism employed in concurrent logic programming constraint communication can be seen a providing a minimal form of state fully compatible with logic data structure based on constraint communication and higher order programming oz readily support a variety of object oriented programming style including multiple inheritance 
an important result of visual understanding is an explanation of a scene s causal structure how action usually motion is originated constrained and prevented and how this determines what will happen in the immediate future to be useful for a purposeful agent these explanation must also capture the scene in term of the functional property of it object their purpose us and affordances for manipulation design knowledge describes how the world is organized to suit these function and causal knowledge describes how these arrangement work we have been exploring the hypothesis that vision is an explanatory process in which causal and functional reasoning play an intimate role in mediating the activity of low level visual process in particular we have explored two of the consequence of this view for the construction of purposeful vision system causal and design knowledge can be used to drive focus of attention and choose between ambiguous image interpretation both principle are at work in sprocket a system which visually explores simple machine integrating diverse visual clue into an explanation of a machine s design and function 
recently markov decision process and optimal control policy have been applied to the problem of decision theoretic planning however the classical method for generating optimal policy are highly intractable requiring explicit enumeration of large state space we explore a method for generating abstraction that allow approximately optimal policy to be constructed computational gain are achieved through reduction of the state space ab traction are generated by identifying proposition that are r elevant either through their direct impact on utility or their influ ence on action this information is gleaned from the representati on of utility and action we prove bound on the loss in value due to abstraction and describe some preliminary experimental re sults reduced space our approach ha several advantage over the envelope method foremost among these is the fact that no state are ignored in abstract policy generation each state may have some influence on the constructed policy by membership in an abstract state this allows u to prove bound on the value of abstract policy with respect to an optimal policy furthermore finer grained abstraction are guaranteed to increase the value of policy finally abst ractions can be generated quickly these factor allow abstract policy of varying degree of accuracy to be constructed in response to time pressure the information obtained in abstract policy generation can then be used in a real time fashion to refine the abstract policy a we describe in the concluding section this is also well suited to circumstan ce where the goal or reward structure communicated to an agent change frequently thus problem specific abstraction can be generated a needed in the next section we describe the mdps howard s policy iteration algorithm for optimal policy construction and briefly the anytime approach of in section we discus a possible knowledge representation scheme for action and utility the information implicit in such a sp ecification will be crucial in generating useful abstraction in section we present an algorithm for generating an abstrac t state space and an appropriate decision model we show how policy iteration is used to generate abstract policy in this state space that are directly applicable to the origi nal concrete space and prove bound on the possible loss due to abstraction we also discus preliminary experimental result that suggest that abstraction of this form is qui te valuable in certain type of domain 
any phenomenon can be seen under a more or le precise granularity depending on the kind of detail which are perceivable this can be applied to time and space a characteristic of abstract space such a the one used for representing time is their granularity independence i e the fact that they have the same structure under different granularity so time place and their relationship can be seen under different granularity and they still behave like time place and relationship under each granularity however they do not remain exactly the same time place and relationship here is presented a pair of operator for converting upward and downward qualitative time relationship from one granularity to another these operator are the only one to satisfy a set of six constraint which characterize granularity change they are also shown to be useful for spatial relationship 
this paper proposes a solution to the frame problem for knowledge producing action an example of a knowledge producing action is a sense operation performed by a robot to determine whether or not there is an object of a particular shape within it grasp the work is an extension of reiter s solution to the frame problem for ordinary action and moore s work on knowledge and action the property of our specification are that knowledge producing action do not affect fluents other than the knowledge fluent and action that are not knowledge producing only affect the knowledge fluent a appropriate in addition memory emerges a a side effect if something is known in a certain situation it remains known at successor situation unless something relevant ha changed also it will be shown that a form of regression examined by reiter for reducing reasoning about future situation to reasoning about the initial situation now also applies to knowledge producing action 
we argue that many ai planning problem should be viewed a process oriented where the aim is to produce a policy or behavior strategy with no termination condition in mind a opposed to goal onented the full power of markov decision model adopted recently for ai planning becomes apparent with process oriented problem the question of appropriate optimality criterion becomes more critical in this case we argue that average reward optimal is most suitable while construction of averageoptimal policy involves a number of subtlety and computational difficulty certain aspect of the problem can be solved using compact action representation such a bayes net in particular we provide an algorithm that identifies the structure of the markov process underlying a planning problem a crucial element of constructing average optimal policy without explicit enumeration of the problem state space 
a novel approach to integrating case based reasoning with model based diagnosis is presented the main idea is to use the model of the device and the result of diagnostic test to index and match case representing past diagnostic situation with the current one the initial diagnostic methodology is presented a well a the problem encountered while applying this methodology to two real world device the incorporation of a case based reasoning system is then motivated and described in detail experimental result show the effectiveness of both the indexing schema and the matching algorithm the paper also discus how and why these result can be generalized to a multiple fault situation to other type of device model and to other application in the field of artificial intelligence 
when autonomous agent attempt to coordinate action it is often necessary that they reach some kind of consensus reaching consensus ha traditionally been dealt with in the distributed artificial intelligence literature via negotiation another alternative is to have agent use a voting mechanism each agent express it preference and a group choice mechanism is used to select the result some choice mechanism are better than others and ideally we would like one that cannot be manipulated by untruthful agent coordination of action by a group of agent corresponds to a group planning process we here introduce a new multi agent planning technique that make use of a dynamic iterative search procedure through a process of group constraint aggregation agent incrementally construct a plan that brings the group to a state maximizing social welfare at each step agent vote about the next joint action in the group plan i e what the next transition state will be in the emerging plan using this technique agent need not fully reveal their preference and the set of alternative final state need not be generated in advance of a vote with a minor variation the entire procedure can be made resistant to untruthful agent 
this paper present the semantics of trio an object oriented language devoted to specify realtime system referring to different time granularity time granularity allows to describe the behavior and the property of a system and it environment with respect to different time scale trio semantics is expressed by translation into a logical framework supporting the notion of time granularity such a semantics provides the executability of object oriented specification 
generating explanation of device behavior is a long standmg goal of ai research in reasoning about physical system much of the relevant work ha concentrated on new method for modeling and simulation such a qualitative physic or on sophisticated natural language generation in which the device model are specially crafted for explanatory purpose we show how two technique from the modeling research compositional modeling and causal ordering can be effectively combined to generate natural language explanation of device behavior from engineering model the explanation offer three advance over the data display produced by conventional simulation software causal interpretation of the data summary at appropriate level of abstraction physical mechanism and component operating mode and query driven natural language summary furthermore combining the compositional modeling and causal ordering technique allows model that are more scalable and le brittle than model designed solely for explanation however these technique produce model with detail that can be distracting in explanation and would be removed in hand crafted model e g intermediate variable we present domain independent filtering and aggregation technique that overcome these problem 
we present a new context based approach to default logic called contextual default logic the approach extends the notion of a default rule and supply each extension with a context contextual default logic allows for embedding all existing variant of default logic along with more traditional approach like the closed world assumption a key advantage of contextual default logic is that it provides a syntactical instlmment for comparing existing default logic in a unified setting in particular it reveals that existing default logic mainly differ in the way they deal with an explicit or implicit underlyiug context 
an abstraction scheme is developed to simplify bayesian belief network structure for future inference session the concept of abstract network and abstract junction tree are proposed based on the inference time efficiency good abstraction are characterized furthermore an approach for automatic discovery of good abstraction from the past inference session is presented the learned abstract network is guaranteed to have a better average inference time efficiency if the characteristic of the future session remains moreorless the same a preliminary experiment is conducted to demonstrate the feasibility of this abstraction scheme 
a common technique for bounding the runtime required to solve a constraint satisfaction problem is to exploit the structure of the problem s constraint graph dechter we show that a simple structure based technique with a minimal space requirement pseudo tree search freuder quinn is capable of bounding runtime almost a effectively a the best exponential space consuming scheme specifically if we let n denote the number of variable in the problem w denote the exponent in the complexity function of the best structure based technique and h denote the exponent from pseudotree search we show h w lg n the result should allow reduction in the amount of real time accessible memory required for predicting runtime when solving csp equivalent problem 
automatic symbolic traffic scene analysis is essential to many area of ivhs intelligent vehicle highway system traffic scene information can be used to optimize traffic flow during busy period identify stalled vehicle and accident and aid the decision making of an autonomous vehicle controller improvement in technology for machine vision based surveillance and high level symbolic reasoning have enabled u to develop a system for detailed reliable traffic scene analysis the machine vision component of our system employ a contour tracker and an affine motion model based on kalman filter to extract vehicle trajectory over a sequence of traffic scene image the symbolic reasoning component us a dynamic belief network to make inference about traffic event such a vehicle lane change and stall in this paper we discus the key task of the vision and reasoning component a well a their integration into a working prototype 
the standard approach to decision tree induction is a top down greedy agorithm that make locally optimal irrevocable decision at each node of a tree in this paper we empircally study an alternative approach in which the algorithm use one level lookahead to decide what test to use at a node we systematically compare using a very large number of artificial data set the quality of dimension tree induced by the greedy approach to that of tree induced using lookahead the main observation from our experiment are the greedy approach consistently produced tree that were just a at accurate a tree produced with the much more expensive lookahead step and n we observed many instance of pathology i e lookahead produced tree that were both larger and le accurate than tree produced without it 
in this paper we describe a content planningmechanism which take into consideration auser s possible inference in order to generatethe most concise discourse that achieves agiven communicative goal the considerationof a user s inference result in the additionof information that address erroneous inference and the omission of easily inferred information given a communicative goal ourmechanism applies inference rule in backwardreasoning mode to plan rhetorical 
machine assisted language translation system for technical document guide human through a process of selecting and composing variant partial translation the constrained nature of technical sublanguages make language processing aid cost effective to build and use analogously we have developed kit a knowledge based translation system for converting informal english scenario of the desired behavior of complex reactive system into formal executable test script a trainable parser and reference resolver capture domain specific linguistic knowledge a logic analyzer establishes coherence in the translation process in a role comparable to a story understander it check the consistency of each step of a translated test script using a theorem prover a planner and logic encoded background knowledge about the system under test this help correct common but serious specification error including underspecificity omitted step and even some outright mi statement to evaluate how well such technology can scale we have exercised our technology progressively on a graduated corpus of behavior scenario spanning advanced calling feature for a private telephone switch pbx successfully translating into test script without any manual post hoc editing our experience with kit ha enabled u to identify many of the tradeoff in accommodating informality in specification versus demanding formality from a human agent 
this paper describes a set of experiment with a system that synthesizes constraint satisfaction program the system multi tac is a csp expert that can specialize a library of generic algorithm and method for a particular application multi tac not only proposes domain specific version of it generic heuristic but also search for the best combination of these heuristic and integrates them into a complete problem specific program we demonstrate multi tac s capability on a combinatorial problem minimum maximal matching and show that multi tac can synthesize program for this problem that are on par with hand coded program in synthesizing a program multi tac base it choice of heuristic on the instance distribution and we show that this capability ha a significant impact on the result 
this paper present an algorithm that combine traditional ebl technique and recent development in inductive logic programming to learn effective clause selection rule for prolog program when these control rule are incorporated into the original program significant speed up may be achieved the algorithm is shown to be an improvement over competing ebl approach in several domain additionally the algorithm is capable of automatically transforming some intractable algorithm into one that run in polynomial time 
we describe recent extension to our frameworkfor the automatic generation of music makingprograms we have previously used geneticprogramming technique to produce musicmakingprograms that satisfy user providedcritical criterion in this paper we describe newwork on the use of connectionist technique toautomatically induce musical structure from acorpus we show how the resulting neural networkscan be used a critic that drive ourgenetic programming system we argue thatthis 
we present algorithm for the discovery and useof topological map of an environment by an activeagent such a a person or a mobile robot we discus several issue dealing with the useof pre existing topological map of graph likeworlds by an autonomous robot and present algorithm worst case complexity and experimentalresults for representative real world example for two key problem the first of theseproblems is to verify that a given input map isa correct description of the 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
the ability of an inductive learning system tofind a good solution to a given problem is dependentupon the representation used for thefeatures of the problem system that performconstructive induction are able to change theirrepresentation by constructing new feature we describe an important real world problem finding gene in dna that we believe offersan interesting challenge to constructiveinductionresearchers we report experimentsthat demonstrate that two different 
inspired by the success of the distributed computing community in applying logic of knowledge and time to reasoning about distributed protocol we aim for a similarly powerful and high level abstraction when reasoning about control problem involving uncertainty here we concentrate on robot motion planning with uncertainty in both control and sensing this problem ha already been well studied within the robotics community our contribution include the following we define a new natural problem in this domain obtaining a sound and complete termination condition given initial and goal location we consider a specific class of simple motion plan in rn from the literature and provide necessary and sufficient condition for the existence of sound and complete termination condition for plan in that class we define a high level language a logic of time and knowledge to reason about motion plan in the presence of uncertainty and use them to provide general condition for the existence of sound and complete termination condition for a broader class of motion plan 
though arrangement knowledge is well suited for qualitative representation of spatial situation if we only use this kind of knowledge we cannot do interesting inference about relative position of point in the plane for example if we know the orientation of two triangle over four point we cannot say anything about the orientation of the other two triangle in this paper we show that the augmentation of arrangement knowledge by qualitative angle lead to interesting and useful inference 
this article describes a connectionist method for refining algorithm represented a generalized finite state automaton the method translates the rule like knowledge in an automaton into a corresponding artificial neural network and then refines the reformulated automaton by applying backpropagation to a set of example this technique for translating an automaton into a network extends the kbann algorithm a system that translates a set of prepositional rule into a corresponding neural network the extended system fskbann allows one to refine the large class of algorithm that can be represented a state based process a a test fskbann is used to improve the chou fasman algorithm a method for predicting how globular protein fold empirical evidence show that the multistrategy approach of fskbann lead to a statistically significantly more accurate solution than both the original chou fasman algorithm and a neural network trained using the standard approach extensive statistic report the type of error made by the chou fasman algorithm the standard neural network and the fskbann network 
this paper investigates several method for coping with inconsistency caused by multiple source information by introducing suitable consequence relation capable of inferring non trivial conclusion from an inconsistent stratified knowledge base some of these method presuppose a revision step namely a selection of one or several consistent subset of formula and then classical inference is used for inferring from these subset two alternative method that do not require any revision step are studied inference based on argument and a new approach called safely supported inference where inconsistency is kept local these two last method look suitable when the inconsistency is due to the presence of several source of information the paper offer a comparative study of the various inference mode under inconsistency 
so far tractable planning problem reported in the literature have beendefined by syntactical restriction to better exploit the inherent structurein problem however it is probably necessary to study also structuralrestrictions on the state transition graph such restriction aretypically computationally hard to test though since this graph is ofexponential size we take an intermediate approach using a statevariablemodel for planning and restricting the state transition graph 
this paper present a new approach to inductive learning that combine aspect of instancebased learning and rule induction in a single simple algorithm the rise system search for rule in a specific to general fashion starting with one rule per training example and avoids some of the difficulty of separate and eonquer approach by evaluating each proposed induction step globally i e through an efficient procedure that is equivalent to checking the accuracy of the rule set a a whole on every training example classification is performed using a best match strategy and reduces to nearest neighbor if all generalization of instance were rejected an extensive empirical study show that rise consistently achieves higher accuracy than state of the art representative of it parent paradigm pebls and cn and also outperforms a decision tree learner c in out of test domain in with confidence 
although ai planning technique can potentially be useful in several manufacturing domain this potential remains largely unrealized in order to adapt ai planning technique to manufacturing it is important to develop more realistic and robust way to address issue important to manufacturing engineer furthermore by investigating such issue ai researcher may he able to discover principle that are relevant for ai planning in general a an example in this paper we describe the technique for manufacturing operation planning used in imacs interactive manufacturability analysis and critiquing system and compare and contrast them with the technique used in classical ai planning system we describe how one of imacs s planning technique may be useful for ai planning in general and a an example we describe how it help to explain a puzzling complexity result in ai planning 
the scientific study of biological system offer an approach to the development of sensor based robot that is complementary to the more formal analytic method currently favoured by roboticists i initially propose several general lesson from the biological field next i consider a specific example selected from the work of lederman klatzky which focus on human haptic object processing an empirical base and recent theoretical development from our research program on this topic are described the human haptic system is an information processing system that combine input from sensor in skin muscle tendon and joint with motor capability to extract different object property a general model of human haptic object identification which emphasis how object exploration is controlled is presented the model describes major architectural element including representation of haptically accessible object property and exploratory procedure eps which are dedicated movement pattern specialized to extract particular property these architectural unit are related in processing specific way the resulting architecture is treated a a system of constraint which guide the exploration of an object during the course of identification empirical support for the model is also examined to conclude i show how this scientifically based approach might be applied to developing strategy for active manual robotic exploration of unstructured environment 
gelfond and lifschitz introduce a declarative language a for describing effect of action and define a translation of theory in this language into extended logic program elp s the purpose of this paper is to extend the language and the translation to allow reasoning about the effect of concurrent action logic programming formalization of situation calculus with concurrent action presented in the paper can be of independent interest and may serve a a test bed for the investigation of various transformation and logic programming inference mechanism 
a decision list is an ordered list of conjunctiverules rivest inductive algorithm such a aqand cn learn decision list incrementally one ruleat a time such algorithm face the rule overlap problem the classification accuracy of the decision listdepends on the overlap between the learned rule thus even though the rule are learned in isolation they can only be evaluated in concert existing algorithmssolve this problem by adopting a greedy iterativestructure once a 
over the last few year constraint based grammar formalism have become the predominant paradigm in natural language processing and computational linguistics from the viewpoint of computer science typed feature structure can be seen a data structure that allow the representation of linguistic knowledge in a uniform fashion type expansion is an operation that make constraint of a typed feature structure explicit and determines it satisfiability we describe an efficient expansion algorithm that take care of recursive type definition and permit the exploration of different expansion strategy through the use of control knowledge this knowledge is specified on a separate layer independent of grammatical information the algorithm a presented in the paper ha been full implemented in common lisp and is an integrated part of the typed feature formalism tdc that is employed in several large nl project 
several widely accepted modal nonmonotonic logic for reasoning about knowledge and belief of rational agent with introspection power are based on strong modal logic such a kd s s f and s in this paper we argue that weak modal logic without even the axiom k and therefore below the range of normal modal logic also give rise to useful non monotonic system we study two such logic the logic n containing propositional calculus and necessitation but no axiom schema for manipulating the modality and the logic nt the extension of n by the schema t for the nonmonotonic logic n and nt we develop minimal model semantics we use it to show that the nonmonotonic logic n and nt are at least a expressive a autoepistemic logic reflexive autoepistemic logic and default logic in fact each can be regarded a a common generalization of these classic nonmonotonic system we also show that the nonmonotonic logic n and nt have the property of being conservative with respect to adding new definition and prove that computationally they are equivalent to autoepistemic and default logic 
in recent time there ha been an increase in the number of natural language generation system that take into consideration a user s inference the statement generated by these system are typically connected by inferential link which are opportunistic in nature in this paper we describe a discourse structuring mechanism which organizes inferentially linked statement a well a statement connected by certain prescriptive link our mechanism first extract relation and constraint from the output of a discourse planner it then us this information to build a directed graph whose node are rhetorical device and whose link are the relation between these device the mechanism then applies a search procedure to optimize the traversal through the graph this process generates an ordered set of linear discourse sequence where the element of each sequence are maximally connected our mechanism ha been implemented a the discourse organization component of a system called wishful which generates concept explanation 
a semantics for tense modality and aspect in natural language must capture causal and contingent relation between event and state a welt a merely temporal one the paper investigates a non reified dynamic logic based formulation of the situation calculus is a formalism for a computational semantics for a number of temporal category in english and suggests that some recent claim that dynamic logic are inherently unsuitable for this purpose have taken too narrow a view of the situation calculus 
we present an algorithm for automatic word sense disambiguation based on lexical knowledge contained in wordnet and on the result of surface syntactic analysis the algorithm is part of a system that analyzes text in order to acquire knowledge in the presence of a little pre coded semantic knowledge a possible on the other hand we want to make the besl use of public domain information source such a wordnet rather than depend on large amount of hand crafted knowledge or statistical data from large corpus we use syntactic information and information in wordnet and minimize the need for other knowledge source in the word sense disambiguation process we propose to guide disambiguation by semantic similarity between word and heuristic rule based on this similarity the algorithm ha been applied to the canadian income tax guide test result indicate that even on a relatively small text the proposed method produce correct noun meaning more than of the time 
human chess player exhibit a large variation in the amount of time they allocate for each move yet the problem of devising resource allocation strategy for game playing did not receive enough attention in this paper we present a framework for studying resource allocation strategy we define allocation strategy and identify three major type of strategy static semi dynamic and dynamic we then proceed to describe a method for learning semi dynamic strategy from self generated example the method assigns class to the example based on the utility of investing extra resource the method wa implemented in the domain of checker and experimental result show that it is able to learn strategy that improve game playing performance 
we propose a new procedure for proof by induction in conditional theory where case analysis is simulated by term rewriting this technique reduces considerably the number of variable of a conjecture to be considered for applying induction scheme inductive position our procedure is presented a a set of inference rule whose correctness ha been formally proved moreover when the axiom are ground convergent and the defined function are completely defined over free constructor it is possible to apply the system for refuting conjecture the procedure is even refutationally complete for conditional equation with boolean precondition under the same hypothesis the method is entirely implemented in the prover spike this system ha proved interesting example in a completely automatic way that is without interaction with the user and without ad hoc heuristic it ha also proved the challenging gilbreath card trick with only easy lemma 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
agent interacting with an incompletely known dynamic world need to be able to reason about the effect of their action and to gain further information about that world using sensor of some sort unfortunately sensor information is inherently noisy and in general serf only to increase the agent s degree of confidence in various proposition build ing on a general logical theory of action formalized in the sit uation calculus developed by reiter and others we propose a simple axiomatization of the effect on an agent s state of belief of taking a reading from a noisy sensor by exploiting reiter s solution to the frame problem we automatically obtain that these sensor action leave the rest of the world unaffected and further that non sensor action change the state of belief of the agent in appropriate way 
reactive deliberation is a novel robot architecture that ha been designed to overcome some of the problem posed by dynamic robot environment it is argued that the problem of action selection in nontrivial domain cannot be intelligently resolved without attention to detailed planning experimental evidence is provided that the goal and action of a robot must be evaluated at a rate commensurate with change in the environment the goal oriented behaviour of reactive deliberation are a useful abstraction that allow sharing of scarce computational resource and effective goal arbitration through inter behaviour bidding the effectiveness of reactive deliberation ha been demonstrated through a tournament of one on one soccer game between real world robot soccer is a dynamic environment the location of the ball and the robot are constantly changing the result suggest that the architectural element in reactive deliberation are sufficient for real time intelligent control in dynamic environment 
a distributed system of computer play an increasingly important role in society it will be necessary to consider way in which these machine can be made to interact effectively especially when the interacting machine have been independently designed it is essential that the interaction environment be conducive to the aim of their designer these designer might for example wish their machine to behave efficiently and with a minimum of overhead required by the coordination mechanism itself the rule of interaction should satisfy these need and others formal tool and analysis can help in the appropriate design of these rule we here consider how concept from field such a decision theory and game theory can provide standard to be used in the design of appropriate negotiation and interaction environment this design is highly sensitive to the domain in which the interaction is taking place different interaction mechanism are suitable for different domain if attribute like efficiency and stability are to be maintained 
the ai lab at chicago ha begun development of a new set of software agent designed to man age the flood of data colloquially called the in formation superhighway our approach take it lead from case based technology riesbeck and schank hammond kolodner in that we are building system that em phasize the use of example over explicit query or question for communicating with the user 
model of complex physical system often cannot be defined precisely either because of lack of knowledge or because the system parameter change over time according to unknown phenomenon such system can be represented by semi quantitative model that combine both qualitative and quantitative knowledge this paper present numerical interval simulation a method that can produce tight prediction of system involving nonmonotonic function we present a successful application of ni to predict the behavior of a complex process at a brazilian japanese steel company we claim that such capability of simulating nonmonotonic function is fundamental in order to handle real world complex industrial process 
the complex sentence of news wire report contain floating content unit that appear to be opportunistically placed where the form of the surrounding text allows we present a corpus analysis that identified precise semantic and syntactic constraint on where and how such information is realized the result is a set of revision tool that form the rule base for a report generation system allowing incremental generation of complex sentence 
the paper present a framework for a system that describes object in a qualitative fashion a subset of spatial preposition is chosen and an appropriate quantification is applied to each of them that capture their inherent qualitative property the quantification use such object attribute a area center and elongation property the familiar zeroth first and second order moment are used to characterize these attribute the paper detail how and why the particular quantification were chosen since spatial preposition are by their nature rather vague and dependent on context a technique for fuzzifying the definition of the spatial preposition is explained finally an example task is chosen to illustrate the appropriateness of the quantification technique 
we apply reinforcement learning method to learn domain specific heuristic for job shop scheduling a repair based scheduler start with a critical path schedule and incrementally repair constraint violation with the goal of finding a short conflict free schedule the temporal difference algorithm td is applied to tram a neural network to learn a heuristic evaluation function over state this evaluation function is used by a one step lookahead search procedure to find good solution to new scheduling problem we evaluate this approach on synthetic problem and on problem from a nasa space shuttle pay load processing task the evaluation function is trained on problem involving a small number of job and then tested on larger problem the td scheduler performs better than the best known existing algorithm for this task zwehen s iterative repair method based on simulated annealing the result suggest that reinforcement learning can provide a new method for constructing high performance scheduling system 
derivation replay wa first proposed by carbonell a a method of transferring guidance from a previous problemsolving episode to a new one subsequent implementation have used state space planning a the underlying methodology this paper is motivated by the acknowledged superiority of partial order po planner in plan generation and is an attempt to bring derivation replay into the realm of partial order planning here we develop dersnlp a framework for doing replay in snlp a partial order plan space planner and analyze it relative effectiveness we will argue that the decoupling of planning derivation order and the execution order of plan step provided by partial order planner enables dersnlp to exploit the guidance of previous case in a more efficient and straightforward fashion we validate our hypothesis through empirical comparison between dersnlp and two replay system based on state space planner 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
parka a frame based knowledge representation system implemented on the connection machine provides a representation language consisting of concept description frame and binary relation on those description slot the system is designed explicitly to provide extremely fast property inheritance inference capability parka performs fast recognition query of the form find all frame satisfying p property constraint in o d p time proportional only to the depth d of the knowledge base kb and independent of it size for conjunctive query of this type parka s performance is measured in tenth of a second even for kb with frame with similar result for timing on the cyc kb because parka s run time performance is independent of kb size it promise to scale up to arbitrarily larger domain with such run time performance we believe parka is a contender for the title of fastest knowledge representation system in the world 
the output of handwritten word recognizers hwr tends to be very noisy due to variousfactors in order to compensate for thisbehaviour several choice of the hwr mustbe initially considered in the case of handwrittensentence phrase recognition linguisticconstraints may be applied in order to improvethe result of the hwr this paper discussestwo statistical method of applying syntacticconstraints to the output of an hwr on inputconsisting of sentence phrase both methodsare 
we develop a language model using probabilistic context free grammar pcfgs that is pseudo context sensitive in that the probability that a non terminal n expands using a rule r depends on n s parent we derive the equation for estimating the necessary probability using a variant of the inside outside algorithm we give experimental result showing that beginning with a high performance pcfg one can develop a pseudo pcsg that yield significant performance gain analysis show that the benefit from the context sensitive statistic are localized suggesting that we can use them to extend the original pcfg experimental result confirm that this is both feasible and the resulting grammar retains the performance gain this implies that our scheme may be useful a a novel method for pcfg induction 
many researcher have noted the importance of combining inductive and analytical learning yet we still lack combined learning method that are effective in practice we present here a learning method that combine explanation based learning from a previously learned approximate domain theory together with inductive learning from observation this method called explanation based neural network learning ebnn is based on a neural network representation of domain knowledge explanation are constructed by chaining together inference from multiple neural network in contrast with symbolic approach to explanation based learning which extract weakest precondition from the explanation ebnn extract the derivative of the target concept with respect to the training example feature these derivative summarize the dependency within the explanation and are used to bias the inductive learning of the target concept experimental result on a simulated robot control task show that ebnn requires significantly fewer training example than standard inductive learning furthermore the method is shown to be robust to error in the domain theory operating effectively over a broad spectrum from very strong to very weak domain theory 
work on game playing in ai ha typically ignored game of imperfect information such a poker in this paper we present a framework for dealing with such game we point out several important issue that arise only in the context of imperfect information game particularly the insufficiency of a simple game tree model to represent the player information state and the need for randomization in the player optimal strategy we describe gala an implemented system that provides the user with a very natural and expressive language for describing game from a game description gala creates an augmented game tree with information set which can be used by various algorithm in order to find optimal strategy for that game in particular gala implement the first practical algorithm for finding optimal randomized strategy in two player imperfect information competitive game koller et al the running time of this algorithm is palinomial in the size of the game tree whereas previous algorithm were exponential we present experimental result showing that this algorithm is also efficient in practice and can therefore form the basis for a game playing system 
we draw a simple correspondence between kernelrules and prime implicants kernel minimal rule play an important role in many inductiontechniques prime implicants were previouslyused to formally model many other problemdomains including boolean circuit minimizationand such classical ai problem a diagnosis truthmaintenance and circumscription this correspondence allows computing kernelrules using any of a number of prime implicantgeneration algorithm it also lead u to an 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
abstract inthis report we discus the development of an integrated problem solving 
in this paper we define and empirically evaluate new heuristic for solving the job shop scheduling problem with non relaxable time window the hypothesis underlying our approach is that by approaching the problem a one of establishing sequencing constraint between pair of operation requiring the same resource a opposed to a problem of assigning start time to each operation and by exploiting previously developed analysis technique for limiting search through the space of possible sequencing decision simple localized look ahead technique can yield problem solving performance comparable to currently dominating technique that rely on more sophisticated analysis of resource contention we define a series of attention focusing heuristic based on simple analysis of the temporal flexibility associated with different sequencing decision and a similarly motivated heuristic for determining how to sequence a given operation pair performance result are reported on a suite of benchmark problem previously investigated by two advanced approach and our simplified look ahead analysis technique are shown to provide comparable problem solving leverage at reduced computational cost 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion is unit refutable there is a polynomial time algorithm for finding minimal explanation a test for unit refutability of clausal theory is presented based on the topology of the connection graph of the theory 
this video first summarizes current research at the university of massachusetts on mobile vehicle navigation using landmark recognition and a partial d world model we then show how landmark and world model might be automatically acquired and updated over time a fundamental goal in robot navigation is to determine the pose of the robot that is the position and orientation of the robot with respect to a d world model such a a hallway in order to determine it pose the robot identifies modeled d landmark such a door and baseboard in a d image of the hallway identifying landmark involves determining correspondence of extracted image line segment with predicted landmark line projected into the image model matching is achieved by a combinatorial optimization technique local search which minimizes the error in the model to data fit from the modeldata feature correspondence thus obtained the d pose of the robot is computed via a non linear optimization procedure the best pose requires that line in the d image lie on the plane formed by the corresponding d landmark line and the camera center robust statistical method are employed to detect outlier extension of the initial partial model over time is achieved by determining the camera pose over a sequence of image while simultaneously tracking new unmodeled feature triangulation is then used to determine the depth of these new feature allowing them to be incorporated into the d model 
constraint satisfaction problem where value are sought for problem variable subject to restriction on which combination of value are acceptable have many application in artificial intelligence conventional learning method acquire individual tuples of inconsistent value these learning experience can be generalized we propose a model of generalized learning based on inconsistency preserving mapping which is sufficiently focused so a to be computationally cost effective rather than recording an individual inconsistency that led to a failure and looking for that specific inconsistency to recur we observe the context of a failure and then look for a related context in which to apply our experience opportunistically a a result we leverage our learning power this model is implemented extended and evaluated using two simple but important class of constraint problem 
determining whether a propositional theory is satisfiable is a prototypical example of an np complete problem further a large number of problem that occur in knowledge representation learning planning and other area of ai are essentially satisfiability problem this paper report on a series of experiment to determine the location of the crossover point the point at which half the randomly generated propositional theory with a given number of variable and given number of clause are satisfiable and to ass the relationship of the crossover point to the difficulty of determining satisfiability we have found empirically that for sat the number of clause at the crossover point is a linear function of the number of variable this result is of theoretical interest since it is not clear why such a linear relationship should exist but it is also of practical interest since recent experiment mitchell et al cheeseman et al indicate that the most computationally difficult problem tend to be found near the crossover point we have also found that for random sat problem below the crossover point the average time complexity of satisfiability problem seems empirically to grow linearly with problem size at and above the crossover point the complexity seems to grow exponentially but the rate of growth seems to be greatest near the crossover point 
intelligent artificial agent need to be able to explain and justify their action they must therefore understand the rationale for their own action this paper describes a technique for acquiring this understanding implemented in a multimedia explanation system the system determines the motivation for a decision by recalling the situation in which the decision wa made and replaying the decision under variant of the original situation through experimentation the agent is able to discover what factor led to the decision and what alternative might have been chosen had the situation been slightly different the agent learns to recognize similar situation where the same decision would be made for the same reason this approach is implemented in an artificial fighter pilot that can explain the motivation for it action situation assessment and belief 
we present a theory of a modeler s problem decomposition skill in the context of optimal reasonzng the use of qualitative modeling to strategically guide numerical exploration of objective space our technique called activity analysis applies to the pervasive family of linear and non linear constrained optimization problem and easily integrates with any existing numerical approach activity analysis draw from the power of two seemingly divergent perspective the global conflict based approach of combinatorial satisficing search and the local gradientbased approach of continuous optimization combined with the underlying insight of engineering moriotonicity analysis the result is an approach that strategically cut away subspace that it can quickly rule out a suboptimal and then guide the numerical method to the remaining subspace 
although plan space planner have been shown to be flexible and efficient in plan generation they do suffer from the problem of looping that is they may spend an inordinate amount of time doing locally seemingly useful but globally useless refinement in this paper i review the anatomy of looping and argue that looping is intimately tied to the production of non minimal solution i then propose two class of admissible pruning technique based on the notion of plan minimality i show that the first one is admissible for planner which do not protect their establishment but allow a precondition to be reestablished any number of time the second one is admissible for planner which protect their establishment through causal link i also discus the complexity of the proposed pruning strategy and then potential application 
neural network were evolved through genetic algorithm to focus minimax search in the game of othello at each level of the search tree the focus network decide which move are promising enough to be explored further the network effectively hide problem state from minimax based on the knowledge they have evolved about the limitation of minimax and the evaluation function focus network were encoded in marker based chromosome and were evolved against a full width minimax opponent that used the same evaluation function the network were able to guide the search away from poor information resulting in stronger play while examining fewer state when evolved with a highly sophisticated evaluation function of the bill program the system wa able to match bill s performance while only searching a subset of the move 
the purpose of this paper is to discus and compare two type of method for reasoning with inconsistent belief base coherence based approach to non monolonic entailment based on the selection and management of consistent subbasis and argumentation system based on the construction and selection of argument in favor of a conclusion we present several argumentation system then we show that most of the associated inference relation can be also defined using well known principle for selecting consistent subbase so we establish a formal correspondence between the so called argumentation paradigm and recent work on nonmonotonic entailment lastly we propose several direction for further research concerning the integration of preference relation into argumentation system 
an overview is given with new result of mathematical model and algorithm for probabilistic logic probabilistic entailment and various extension analytical and numerical solution are considered the former leading to automated generation of theorem in the theory of probability way to restore consistency and relationship with bayesian network are also studied 
it ha recently been shown that local search is surprisingly good at f inding satisfying assignment for certain computationally hard class of cnf formula the performance of basic local search method can be further enhanced by introducing mechanisme for escaping from local minimum in the search space we will compare three such mechanism simulated annealing random noise and a strategy called mixed random walk we show that mixed random walk is the superior strategy we also present result demonstrating the effectiveness of local search with walk for solving circuit bynthebib and circuit diagnosis problem finally we demonstrate that mixed random walk improves upon the best known method for solving max sat problem 
many abductive understanding system explain novel situation by a chaining process that is neutral to explainer need beyond generating some plausible explanation for the event being explained this paper examines the relationship of standard model of abductive understanding to the case based explanation model in case based explanation construction and selection of abductive hypothesis are focused by specific explanation of prior episode and by goal based criterion reflecting current information need the case based method is inspired by observation of human explanation of anomalous event during everyday understanding and this paper focus on the method s contribution to the problem of building good explanation in everyday domain we identify five central issue compare how those issue are addressed in traditional and case based explanation model and discus motivation for using the case based approach to facilitate generation of plausible and useful explanation in domain that are complex and imperfectly understood 
new type of test instance generator have been developed for generating random cnf conjunctive normal form formula with controlled attribute in this paper we use these generator to test the performance of local search based sat algorithm for this purpose the generator which produce formula having exactly one satisfying truth assignment is especially desirable it is shown that i among several different strategy of local search the weighting strategy is overwhelmingly faster than the others and that ii local search work significantly better for instance of larger clause variable ratio which allows u to come up with a new strategy for making local search even faster 
this paper present a method to incorporate knowledge from possibly imperfect model and domain theory into inductive learning of decision tree for classification the approach assumes that a model or domain theory reflects useful prior knowledge of the task thus the default bias should accept the model prediction a accurate even in the face of somewhat contradictory data which may be unrepresentative or noisy however our approach allows the system to abandon the model or domain theory or portion thereof in the fact of sufficiently contradictory data in particular we use c to induce decision tree from data that ha heen augmented by model or domaintheory derived feature we weakly bias the system to select model derived feature during decision tree induction but this preference is not dogmatically applied our experiment very imperfection in a model the representativeness of data and the veracitv with which model demed feature are preferred 
in this paper we propose preferential matrix semantics for nonmonotonic inference system and show how this algebraic framework can be used in methodological study of cumulative inference operation 
degree of belief are formed using observed evidence and statistical background information in this paper we examine the process of how prior degree of belief derived from the evidence are combined with statistical data to form more specific degree of belief a statistical model for this process then is shown to vindicate the cross entropy minimization principle a a rule for probabilistic default inference 
the bulk of previous work on goal and plan recognition may be crudely stereotyped in one of two way neat theory rigorous jus tified but not yet practical scruffy system heuristic domain specific but practical in contrast we describe a goal recognition mod ule that is provably sound and polynomial time and that performs well in a real domain our goal recognizer observes action executed by a human and repeatedly prune inconsistent ac tions and goal from a graph representation of the domain we report on experiment on hu man subject in the unix domain that demon strate our algorithm to be fast in practice the average time to process an observed action with an initial set of goal schema and action schema wa cpu second on a sparc 
this paper present a qualitative analysis that relates stable structure in visual motion field to property of corresponding three dimensional environment such an analysis is fundamental in the development of method for recovering useful information from dynamic visual data without the need for highly accurate and precise sensing methodologically the technique of singularity theory are used to describe the mapping from image space to velocity space and to relate this mapping to the three dimensional environment the specific result of this paper address situation where an optical sensor is undergoing pure rotational or pure translational motion through it environment for the case of pure rotational motion it is shown that the qualitative structure of visual motion provides information about the ax and relative magnitude of rotation for the case of pure translational motion it is shown that the qualitative structure of visual motion provides information about the shape and orientation of viewed surface a well a information about the translation itself further the temporal evolution of the visual motion field is described these result suggest that valuable information regarding three dimensional environmental structure and motion can be recovered from qualitative consideration of visual motion field 
by looping over a set of behavior reactive system use repetition and feedback to deal with error and environmental uncertainty their robust fault tolerant performance make reactive system desirable for executing plan however most planning system cannot reason about the loop that characterize reactive system in this paper we show how the structured application of abstraction and nondeterminism can map complex planning problem requiring loop plan into a simpler representation amenable to standard planning technology in the process we illustrate key recipe for automatically building predictable reactive system that are guaranteed to achieve their goal 
this paper investigates an algorithm for the construction of decision tree comprised of linear threshold umts and also present a novel algorithm for the learning of nonlinearly separable boolean function using madaline style network which are isomorphic to decision tree the construction of such network is discussed and their performance in learning is compared with standard back propagation on a sample problem in which many irrelevant attribute are introduced littlestone s winnow algorithm is also explored within this architecture a a mean of learning in the presence of many irrelevant attribute the learning ability of this madaline style architecture on non optimal larger than necessary network is also explored 
plan recognition technique frequently make rigid assumption about the student s plan and invest substantial effort to infer unobservable property of the student the pedagogical benefit of plan recognition analysis are not always obvious we claim that these difficulty can be overcome if greater attention is paid to the situational context of the student s activity and the pedagogical task which plan recognition is intended to support this paper describes an approach to plan recognition called situated plan attribution that take these factor into account it devotes varying amount of effort to the interpretation process focusing the greatest effort on interpreting impasse point i e point where the student encounter some difficulty completing the task this approach ha been implemented and evaluated in the context of the react tutor a trainer for operator of deep space communication station appear to be based on an analysis of whether it is appropriate to intervene this paper describes an approach to plan recognition called situated plan attribution that take these factor into account situated plan attribution analyzes both the student s action and the environmental situation attention to the situation is important because it allows the plan recognizer to recognize when the student must deviate from the usual plan a well a alternative way of achieving the goal of the plan this flexibility avoids the rigidity problem of other technique such a kautz and allen s deductive approach kautz allen which assumes that all possible way of performing an action are known and every action is a step in a known plan 
knowledge based natural language processing system have achieved good success with certain task but they are often criticized because they depend on a domain specific dictionary that requires a great deal of manual knowledge engineering this knowledge engineering bottleneck make knowledge based nlp system impractical for real world application because they cannot be easily scaled up or ported to new domain in response to this problem we developed a system called autoslog that automatically build a domain specificdictionary of concept for extracting information from text using autoslog we constructed a dictionary for the domain of terrorist event description in only person hour we then compared the autoslog dictionary with a hand crafted dictionary that wa built by two highly skilled graduate student and required approximately person hour of effort we evaluated the two dictionary using two blind test set of text each overall the autoslog dictionary achieved of the performance of the hand crafted dictionary on the first test set the autoslog dictionary obtained of the performance of the hand crafted dictionary on the second test set the overall score were virtually indistinguishable with the autoslog dictionary achieving of the performance of the handcrafted dictionary 
widespread access to the internet ha led to the formation of geographically dispersed scientific community collaborating through the network the tool supporting such collaboration currently are based primarily on electronic mail through mailing list server and access to archive of research report through ftp gopher and world wide web however electronic communication can support the knowledge process of scientific community more directly through overtly represented knowledge structure this paper describes some experiment in the use of knowledge acquisition ka and representation kr tool to define and analyze major policy and technical issue in an international research community responsible for one of the test case in the intelligent manufacturing system ims research program it is concluded that distributed knowledge support system in routine use by world class scientific community collaborating through the internet will provide a major impetus to artificial intelligence research 
most research in computer vision ha been directed towards minimalistic approach in which problem are addressed on how property of the environment can be computed from a little information a possible although such approach may be scientifically well motivated they have only resulted in limited progress towards our understanding of seeing system ballard bajcsy and others have pointed out the importance of vision being an active process which is tightly connected to behavior we support this thought and also propose that utilizing that the world is rich on information is essential we develop this idea to show how attention and figure ground segmentation by an active observer using multiple cue can be separated from analyzing and recognizing what is seen in a consistent way continuous operation over time and early use of three dimensional cue are important in this context we illustrate our proposed approach by some experiment on a real time active system 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
localization is a general purpose representational technique for partitioning a problem into subproblems a localized problem solver search several smaller search space one for each subproblem unlike most method of partitioning however localization allows for subproblems that overlap e multiple search space may be involved in constructing shared piece of the overall plan in this paper we focus on two criterion for forming localization scope and abstraction we describe a method for automatically generating such localization and provide empirical result that contrast their use in an office building construction domain 
on the basis of it optimal asymptotic time complexity ac is often considered the best algorithm for establishing arc consistency in constraint satisfaction problem csps in the present work ac wa found to be much more efficient than ac for csps with a variety of feature variable pair were in lexical order and in ac they were added to the end of the list of pair this is supported by argument for the superiority of ac over most of the range of constraint satisfiabilities and for the unlikelihood of condition leading to worst case performance the efficiency of ac is affected by the order of variable testing in phase setting up phase performance in this phase can thus be enhanced and this establishes initial condition for phase that improve it performance but since ac is improved by the same ordering it still outperforms ac in most case 
this paper present a novel approach to diagnosis which address the two problem computational complexity of abduction and device model that have prevented model based diagnostic technique from being widely used the experience aided diagnosis ead model is defined that combine deduction to rule out hypothesis abduction to generate hypothesis and induction to recall past experience and account for potential error in the device model a detailed analysis of the relationship between case based reasoning and induction is also provided the ead model yield a practical method for solving hard diagnostic problem and provides a theoretical basis for overcoming the problem of partially incorrect device model 
one important issue of automated theorem proving is the complexity of the inference rule used in theorem provers if krishnamurty s general symmetry rule is to be used then one should provide some way of computing non trivial symmetry we show that this problem is np complete but this general rule can be simplified by restricting it to what we call symmetry yielding the well known symmetry rule we show that computing symmetry is in the same complexity class a the graph isomorphism problem which appears to be neither polynomial nor np complete however it is sufficient to compute the set of all symmetry at the beginning of the proof search and since it is a permutation group there exist some efficient technique from computational group theory to represent this set we also show how these technique can be used for applying the s symmetry rule in polynomial time 
the difference between bayesian conditioning and lewis imaging is somewhat similar to the one between gardenfors belief revision and katsuno and mendelzon updating in the logical framework counterpart in possibility theory of these two operation are presented including the case of conditioning upon an uncertain observation possibilistic conditioning satisfies all the postulate for belief revision and possibilistic imaging all the updating postulate lastly a third operation called focusing is naturally introduced in the setting of belief and plausibility function 
graphical presentation can he used to communicate information in relational data set suffciently and effectively however novel graphical presentation about numerous attribute and their relationship are often difficult to understand completely until explained automatically generated graphical presenlations must therefore either be limited to simple conventional one or risk incornprehensibility one way of alleviating this problem is to design graphical presentation system that can work in conjunction with a natural language generator to produce explanatory caption this paper present three strategy for generating explanatory caption to accompany information graphic based on a representation of the structure of the graphical presentation a framework for identifying the perceptual complexity of graphical element and the structure of the data expressed in the graphic we describe an implemented system and illustrate how it is used to generate explanatory caption for a range of graphic from a data set about real estate transaction in pittsburgh 
considerable progress ha been made in recent yearsin understanding and solving propositional satisfiabilityproblems much of this work ha been basedon experiment on randomly generated sat problem one generally accepted shortcoming of thiswork is that it is not clear how the result and algorithmsdeveloped will carry over to quot real quot constraintsatisfactionproblems this paper report on a seriesof experiment applying satisfiability algorithmsto scheduling problem we have found 
lifschitz introduced a logic of minimal belief and negation a failure called mbnf in order to provide a theory of epistemic query to non monotonic database we present a feasible subsystem of mbnf which can be translated into a logic built on first order logic and negation a failure called fonf we give a semantics for fonf along with an extended connection calculus in particular we demonstrate that the obtained system is still more expressive than other approach 
uncertainty processing method are analysed from the viewpoint of their sensitivity to small variation of certainty factor the analysis make use of the algebraic theory which defines the function for combining partial certainty factor by mean of a group operation of the ordered abelian group over the interval of uncertainty two approach are introduced a sensitivity analysis of the inference network and b calculation of second order probability sensitivity function are defined a partial derivative of the combining function with respect to their argument based on the sensitivity function we define the path sensitivity which measure the sensitivity of a larger part of the inference network if a set of sample of certainty factor is available instead of a single value the second order probability distribution can be approximated by the distribution of an average value it is shown that the parametric form of the distribution is completely determined by the combining function 
we present an abductive semantics for general propositional logic program which defines the meaning of a logic program in term of it extension this approach extends the stable model semantics for normal logic program in a natural way the new semantics is equivalent to stable semantics for a logic program p whenever p is normal and ha a stable model the abductive semantics can also be applied to generalize default logic and autoepistemic logic in a like manner our approach is based on an idea recently proposed by konolige for causal reasoning instead of maximizing the set of hypothesis alone we maximize the union of the hypothesis along with possible hypothesis that are excused or refuted by the theory 
an extension of the concept description language acc used in kl one like terminological reasoning is presented the extension includes multi modal operator that can either stand for the usual role quantification or for modality such a belief time etc the modal operator can be used at all level of the concept term and they can be used to modify both concept and role this is an instance of a new kind of combination of modal logic where the modal operator of one logic may operate directly on the operator of the other logic 
the bankxx system model the process of perusing and gathering information for argument a a heuristic best first search for relevant case theory and other domain specific information a bankxx search it heterogeneous and highly interconnected network of domain knowledge information is incrementally analyzed and amalgamated into a dozen desirable ingredient for argument called argument piece such a citation to case application of legal theory and reference to prototypical factual scenario at the conclusion of the search bankxx output the set of argument piece filled with harvested material relevant to the input problem situation this research explores the appropriateness of the search paradigm a a framework for harvesting and mining information needed to make legal argument we discus how we tackled the problem of evaluation of bankxx from both the case based reasoning cbr and task performance perspective in particular we discus how various system parametersstart node evaluation function resource limit affected bankxx from the cbr perspective and how well bankxx performs it assigned task of gathering information useful for legal argumentation by running bankxx on real legal case and comparing it output with the published court opinion for those case 
two observation motivate our work a modelbased diagnosis program are powerful but do not learn from experience and b one of the long term trend in learning research ha been the increasing use of knowledge to guide and inform the process of induction we have developed a knowledge guided learning method based in ebl that allows a model based diagnosis program to selectively accumulate and generalize it experience our work is novel in part because it produce several different kind of generalization from a single example where previous work in learning ha for the most part intensively explored one or another specific kind of generalization our work ha focused on accumulating and using multiple different ground for generalization i e multiple domain theory a a result our system not only learns from a single example a in all ebl it can learn multiple thing from a single example simply saying there ought to be multiple ground for generalization only open up the possibility of exploring more than one domain theory we provide some guidance in determining which ground to explore by demonstrating that in the domain of physical device causal model are a ric source of useful domain theory we also caution that adding more knowledge can sometimes degrade performance hence we need to select the ground for generalization carefully and analyze the resulting rule to ensure that they improve performance we illustrate one such quantitative analysis in the context of a model based troubleshooting program measuring and analyzing the gain resulting from the generalization produced 
human improve their performance by mean of a variety of learning strategy including both gradual statistical induction from experience and rapid incorporation of advice in many learning environment these strategy may interact in complementary way the focus of this work is on cognitively plausible model of multistrategy learning involving the integration of inductive generalization and learning by being told such model might be developed by starting with an architecture for which advice taking is relatively easy such a one based upon a sentential knowledge representation and subsequently adding some form of inductive learning mechanism alternatively such model might be grounded in a statistical learning framework appropriately extended to operationalize instruction this latter approach is taken here specifically connectionist back propagation network rumelhart mcclelland the pdp research group are made to instantaneously modify their behavior in response to quasi linguistic advice many of the previous approach to the instruction of connectionist network have involved the encoding of symbolic rule a initial connection weight which may be later refined by inductive learning giles omlin tresp hollatz ahmad a major drawback of this approach is that advice may only be given before inductive training begin this is an unreasonable constraint for a cognitive model of instructed learning instead a connectionist network is needed which may have it behavior altered by a stream of encoded instruction without a delay period for lengthy retraining the approach which is examined here focus on encoding the receipt of instruction a motion in a network s activation space in short advice is presented to such an instructable network a a temporal sequence of instruction token where each token is encoded a an input activation pattern the network is trained to appropriately modulate it behavior based on input of such advice sequence the correct interpretation and operationalization of input instruction sequence is learned inductively but once this initial learning is complete instruction following proceeds at the speed of activation propagation this focus on activation space dynamic allows instructional learning and standard connectionist inductive learning to function in tandem this strategy ha been successfully applied to a simple discrete mapping task and to the learning of natural number arithmetic in this latter domain the connectionist adder of cottrell and tsung cottrell tsung which is capable of systematically operating on arbitrarily large natural number wa augmented to receive instruction in various method of addition and subtraction the resulting network tackle arithmetic problem by examining one column of digit at a time and sequentially performing action such a writing a resultant digit for the column announcing a carry or borrow and shifting attention to the next digit column the network s behavior is determined by the most recently presented sequence of instruction token future experiment will extend these multistrategy learner to include auto associative memory containing articulated attractor in activation space which will facilitate systematic generalization to novel advice sequence these later experiment will abandon arithmetic and will focus instead on simple planning task in a block world environment 
we discus the indexing of case for use in precedent based argument our focus is on how multiple related index into a case base of legal precedent are exploited by an argument generation program called bankxx this system s architecture and control scheme are rooted in a conceptualization of legal argument a heuristic search although our framing argument a search is not discussed in detail in this paper we describe the main feature of this view to provide context for a discussion of an indexing scheme that facilitates argument creation we describe five inter related index type citation prototypical story factor family resemblance and legal theory index and show how they can be used to access view widen or filter a set of case the application domain is a u s federal statute that governs the approval of bankruptcy plan 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
many work have been carried out to improve search efficiency in csps but few of them treated the semantics of the constraint in this paper we expose some property of two class of constraint functional and bijective constraint we first present condition under which arc and path consistency are sufficient to guarantee the existence of a bactrack free solution we then exhibit class of polynomial problem and finally we propose a new method of decomposition for problem containing functional or bijective constraint an interesting point in this method is that the resolution complexity is known prior to the search 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
this paper review the concept of metagame and discus the implementation ofmetagamer a program which play metagame in the class of symmetric chess like game which includes chess chinese chess checker draught and shoji the program take asinput just the rule of any game in this class including game unknown to it programmer and play the game against opponent without further human intervention using an evaluationfunction for the entire class of game the program applies 
we present a new property called constraint looseness and show how it can be used to estimate thelevel of local consistency of a binary constraint network specifically we present a relationship betweenthe looseness of the constraint the size of the domain and the inherent level of local consistency of aconstraint network the result we present are usefulin two way first a common method for finding solutionsto a constraint network is to first preprocess thenetwork by enforcing 
the residue driven architecture presented here is a model of auditory stream segregation from input sound a subsystem to extract auditory stream by using some sound attribute is called an agency and the design of each agency is based on the residue driven architecture this architecture consists of three kind of agent an event detector a tracer generator and tracer the event detector calculates a residue by subtracting the predicted input from the actual input when a residue exceeds a threshold value tracer generator generates a tracerthat extract an auditory stream from the residue and return a predicted input of the next time frame to the event detector this aproach improves the performance of segregation and the resulting system can segregate a woman s voiced stream a man s voiced stream and a noise stream from a mixture of these sound binaural segregation is also designed by the architecture 
in this paper we describe the partially observable markov decision process sc pomdp approach to finding optimal or near optimal control strategy for partially observable stochastic environment given a complete model of the environment the sc pomdp approach wa originally developed in the operation research community and provides a formal basis for planning problem that have been of interest to the ai community we found the existing algorithm for computing optimal control strategy to be highly computationally inefficient and have developed a new algorithm that is empirically more efficient we sketch this algorithm and present preliminary result on several small problem that illustrate important property of the sc pomdp approach 
in system which learn a large number of rule production it is important to match therules efficiently in order to avoid the machine learning utility problem if the learned rulesslow down the matcher the quot learning quot can slow the whole system down to a crawl sowe need match algorithm that scale well with the number of production in the system doorenbos introduced right unlinking a a way to improve the scalability of the retematch algorithm in this paper we build on 
this paper is in two part in the first part the outline of an emotion reasoning architecture embodied in a simulation program called the affective reasoner is presented and a rudimentary personality representation for simulated agent is introduced in the second part an exercise is reviewed in which the affective reasoner is given the task of representing agent with different personality type in such a way a to allow the user to engage in a simulated interaction with them representational issue pertaining to the unique appraisal and behavioral style of the different personality type are addressed conclusion are drawn about the usefulness of the affective reasoner in such a paradigm 
the statistical basis for sense resolution decision is arrived at by the application of a process to a corpus of instance in general once the process ha been applied to the corpus the system contains both some residual representation of the instance and some explicit augmentation of that representation with information that wa implicit in the corpus for example part of the residual representation of he feel happy on friday might be the word sense pair happy feel a emotion and part of the augmentation might be the probability of happy co occurring with the sense of feel a an emotion we show that for the simple residual representation of word sense pair the existence of such a representation in and of itself capture much of the regularity inherent in the data we also demonstrate that augmenting the residual representation with the actual number of time each pair occurs in the training corpus provides most of the remainder of the power of probabalistic approach finally we show how viewing this residual representation a a form of episodic memory can enable symbolic knowledge rich system to take advantage of this source of regularity in performing sense resolution 
many search domain are non deterministic although real time search method have traditionally been studied in deterministic domain they are well suited for searching nondeterministic domain since they do not have to plan for every contingency they can react to the actual outcome of action in this paper we introduce the min max lrta algorithm a simple extension of korf s learning real time a algorithm lrta to nondeterministic domain we describe which nondeterministic domain min max lrta can solve and analyze it performance for these domain we also give tight bound on it worst case performance and show how this performance depends on property of both the domain and the heuristic function used to encode prior information about the domain 
gsat is a randomized local search procedure for solving propositional satisfiability problem gsat can solve hard randomly generated problem that are an order of magnitude larger than those that can be handled by more traditional approach such a the davisputnam procedure this paper present the result of numerous experiment we have performed with gsat in order to improve our understanding of it capability and limitation we first characterize the space traversed by gsat we will see that for nearly all problem class we have encountered the space consists of a steep descent followed by broad flat plateau we then compare gsat with simulated annealing and show how gsat can be viewed a an efficient method for executing the lowtemperature tail of an annealing schedule finally we report on extension to the basic gsat procedure we discus two general domain independent extension that dramatically improve gsat s performance on structured problem the use of clause weight and a way to average in near solution when initializing lhe procedure before each try 
we develop a formal tool for representing and analyzing informational aspect of robotic task based on the formal concept of knowledge specifically we adopt the notion of knowledge based protocol from distributed system and define the notion of knowledge complexity of a robotic task and knowledge capability of a robot the resulting formalism naturally capture previous work in the area of robot information management but is sufficiently rigorous and natural to allow many extension in this paper we show one novel application the automated distribution of robotic task 
in this paper we describe an algorithm which exploit the error distribution generatedby a learning algorithm in order to break up the domain which is being approximatedinto piecewise learnable partition traditionally the error distribution ha beenneglected in favor of a lump error measure such a rms by doing this however welose a lot of important information the error distribution tell u where the algorithmis doing badly and if there exists a quot ridge quot of error also tell u 
it ha been shown that there exists a transition in the average case complexity of searching a random tree from exponential to polynomial in the search depth we develop a state space transformation method called e transformation that make use of this complexity transition to find a suboptimal solution the expected number of random tree node expanded by branch and bound bnb using e transformation is cubic in the search depth and the relative error of the solution cost compared to the optimal solution cost is bounded by a small constant we also present an iterative version of e transformation that can be used to find both optimal and suboptimal solution depthfirst bnb dfbnb using iterative e transformation significantly improves upon truncated dfbnb on random tree with large branching factor and deep goal node finding better solution sooner on average on the asymmetric traveling salesman problem dfbnb using e transformation outperforms a well known local search method and dfbnb using iterative etransformation is superior to truncated dfbnb 
most case based reasoning system have used a single best or most similar case a the basis for a solution for many problem however there is no single exact solution rather there is a range of acceptable answer we use case not only a a basis for a solution but also to indicate the boundary within which a solution can be found we solve problem by choosing some point within those boundary in this paper i discus this use of case with illustration from chiron a system t have implemented in the domain of personal income tax planning 
this paper describes a method for the development of dialogue manager for natural language interface a dialogue manager is presented designed on the basis of both a theoretical investigation of model for dialogue management and an analysis of empirical material it is argued that for natural language interface many of the human interaction phenomenon accounted for in for instance plan based model of dialogue do not occur instead for many application dialogue in natural language interface can be managed from information on the functional role of an utterance a conveyed in the linguistic structure this is modelled in a dialogue grammar which control the interaction focus structure is handled using dialogue object recorded in a dialogue tree which can be accessed through a scoreboard by the various module for interpretation generation and background system access a sublanguage approach is proposed for each new application the dialogue manager is customized to meet the need of the application this requires empirical data which are collected through wizard of oz simulation the corpus is used when updating the dierent knowledge source involved in the natural language interface in this paper the customization of the dialogue manager for database information retrieval application is also described 
constraint relaxation is a frequently used technique for managing over determined constraint satisfaction problem a problem in constraint relaxation is the selection of the appropriate constraint we show that method developed in model based diagnosis solve this problem the resulting method doc an abbreviation for diagnosis of over determined constraint satisfaction problem identifies the set of least important constraint that should be relaxed to solve the remaining constraint satisfaction problem if the solution is not acceptable for a user doc selects next best set of least important constraint until an acceptable solution ha been generated the power of doc is illustrated by a case study of scheduling the dutch major league soccer competition the current schedule is made using human insight and operation research method using doc the schedule ha been improved by reducing the number and importance of the violated constraint by the case study revealed that efficiency improvement is a major issue in order to apply this method to large scale over determined scheduling and constraint satisfaction problem 
many corpus based method for natural language processing are based on supervised training requiring expensive manual annotation of training corpus this paper investigates reducing annotation cost by selective sampling in this approach the learner examines many unlabeled example and selects for labeling only those that are most informative at each stage of training in this way it is possible to avoid redundantly annotating example that contribute little new information the paper first analyzes the issue that need to be addressed when constructing a selective sampling algorithm arguing for the attractiveness of committee based sampling method we then focus on selective sampling for training probabilistic classifier which are commonly applied to problem in statistical natural language processing we report experimental result of applying a specific type of committee based sampling during training of a stochastic part of speech tagger and demonstrate substantially improved learning rate over sequential training using all of the text we are currently implementing and evaluating other variant of committee based sampling a discussed in the paper in order to obtain further insight on optimal design of selective sampling method 
this paper advocate a microfeature based approach towards developing computational model for metaphor interpretation it is argued that the existing model based on semantic network and mapping of complex symbolic structure are insufficient and inappropriate for modeling metaphor a connectionist model of metaphor interpretation based on microfeatures is presented which try to take into account some important issue such a accurate capturing of similarity automatic formation of feature contextual effect elimination of long path in conceptual hierarchy salience imbalance and feature enhancement some of these issue have broad implication in cognitive modeling 
including robotics artificial life and artificial ecosystem we present a new approach to human computer interaction called so z interaction it main characteristic are summarized by the following three point first interaction are realized a multimodal verbal and nonverbal conversation using spoken language facial expression and so on second the conversants are a group of human and social agent that are autonomous and social autonomy is an important property that allows agent to decide how to act in an ever changing environment socialness is also an important property that allows agent to behave both cooperatively and collaboratively generally conversation is a joint work and ill structured it participant are required to be social a well a autonomous third conversants often encounter communication mismatch misunderstanding others intention and belief and fail to achieve their joint goal the social agent therefore are always concerned with detecting communication mismatch we realize a social agent that hears human to human conversation and informs what is causing the misunderstanding it can also interact with human by voice with facial display and head and eye movement however is autonomy itself sufficient for social service although autonomy is vital to survive in the real world it is only concerned with self it is selfish by nature it seems that it doe not work well in human society since it includes socially constructed artifact such a law custom culture social service provided by computer system have to incorporate with these artifact socialness is a higher level concept defined above the concept of an individual and is the style of interaction between the individual in a group socialness can be applied to the interaction between human and computer and possibly to that between multiple computer in this paper we study socialness of conversational interaction between human and computer conversation is no doubt a social activity especially when more than two participant are involved in it however conversation research to date ha been biased to problem solving question answering system are typical example all conversation research based on this view ha the following feature 
traditional best first search for optimal solution quickly run out of space even for problem instance of moderate size and linear space search ha unnecessarily long running time since it cannot make use of available memory for using available memory effectively we developed a new generic approach to heuristie search it integrates various strategy and includes idea from bidirectional search due to insight into different utilization of available memory it allows the search to use limited memory effectively instantiation of this approach for two different benchmark domain showed excellent result that are statistically significant improvement over previously reported result for finding optimal solution in the puzzle we achieved the fastest search of all those using the manhattan distance heuristic a the only knowledge source and for a scheduling domain our approach can solve much more difficult problem than the best competitor the most important lesson we learned from the experiment are first that also in domain with symmetric graph topology selecting the right search direction can be very important and second that memory can under certain condition be used much more effectively than by traditional best first search 
in this paper a method of theorem proving dual to resolution method is presented in brief the investigated method is called backward dual resolution or bd re solution for short the main idea of bd resolution consists in proving validity of a formula in disjunctive normal form by generating an empty tautology formula from it it is shown that the initial formula is a logical consequence of the obtained tautology an idea of the theorem proving method is outlined and it application to checking completeness of rule based system is investigated a formal definition of completeness and specific completeness are stated and an algorithm for completeness verification is proposed moreover a generalized bd resolution aimed at proving completeness under intended interpretation is defined 
