in recent year social medium have become indispensable tool for information dissemination operating in tandem with traditional medium outlet such a newspaper and it ha become critical to understand the interaction between the new and old source of news although social medium a well a traditional medium have attracted attention from several research community most of the prior work ha been limited to a single medium in addition temporal analysis of these source can provide an understanding of how information spread and evolves modeling temporal dynamic while considering multiple source is a challenging research problem in this paper we address the problem of modeling text stream from two news source twitter and yahoo news our analysis address both their individual property including temporal dynamic and their inter relationship this work extends standard topic model by allowing each text stream to have both local topic and shared topic for temporal modeling we associate each topic with a time dependent function that characterizes it popularity over time by integrating the two model we effectively model the temporal dynamic of multiple correlated text stream in a unified framework we evaluate our model on a large scale dataset consisting of text stream from both twitter and news feed from yahoo news besides overcoming the limitation of existing model we show that our work achieves better perplexity on unseen data and identifies more coherent topic we also provide analysis of finding real world event from the topic obtained by our model 
a key challenge in privacy preserving data mining is ensuring that a data mining result doe not inherently violate privacy differential privacy appears to provide a solution to this problem however there are no clear guideline on how to set to satisfy a privacy policy we given an alternate formulation differential identifiability parameterized by the probability of individual identification this provides the strong privacy guarantee of differential privacy while letting policy maker set parameter based on the established privacy concept of individual identifiability 
throughout life the cell in every individual accumulate many change in the dna inherited from his or her parent certain combination of change lead to cancer during the last decade the cost of dna sequencing ha been dropping by a factor of every two year making it now possible to read most of the three billion base genome from a patient s cancer tumor and to try to determine all of the thousand of dna change in it under the auspex of nci s cancer genome atlas project tumor will be sequenced in this manner in the next few year soon cancer genome sequencing will be a widespread clinical practice and million of tumor will be sequenced a massive computational problem loom in interpreting these data first because we can only read short piece of dna we have the enormous problem of assembling a coherent and reliable representation of the tumor genome from massive amount of incomplete and error prone evidence this is the first challenge second every human genome is unique from birth and every tumor a unique variant there is no single route to cancer we must learn to read the varied signature of cancer within the tumor genome and associate these with optimal treatment already there are hundred of molecularly targeted treatment for cancer available each known to be more or le effective depending on specific genetic variant however targeting a single gene with one treatment rarely work the second challenge is to tackle the combinatorics of personalized targeted combination therapy in cancer 
we investigate a class of method that we call social sampling where participant in a poll respond with a summary of their friend putative response to the poll social sampling lead to a novel trade off question the saving in the number of sample roughly the average degree of the network of participant v the systematic bias in the poll due to the network structure we provide precise analysis of estimator that result from this idea with non uniform sampling of node and non uniform weighting of neighbor response we devise an ideal unbiased estimator we show that the variance of this estimator is controlled by the second eigenvalue of the normalized laplacian of the network the network structure penalty and the correlation between node degree and the property being measured the effective saving factor in addition we present a sequence of approximate estimator that are simpler or more realistic or both and analyze their performance experiment on large real world network show that social sampling is a powerful paradigm in obtaining accurate estimate with very few sample at the same time our result urge caution in interpreting recent result about expectation v intent polling 
dipole represent long distance connection between the pressure anomaly of two distant region that are negatively correlated with each other such dipole have proven important for understanding and explaining the variability in climate in many region of the world e g the el nino climate phenomenon is known to be responsible for precipitation and temperature anomaly over large part of the world systematic approach for dipole detection generate a large number of candidate dipole but there exists no method to evaluate the significance of the candidate teleconnections in this paper we present a novel method for testing the statistical significance of the class of spatio temporal teleconnection pattern called a dipole one of the most important challenge in addressing significance testing in a spatio temporal context is how to address the spatial and temporal dependency that show up a high autocorrelation we present a novel approach that us the wild bootstrap to capture the spatio temporal dependency in the special use case of teleconnections in climate data our approach to find the statistical significance take into account the autocorrelation the seasonality and the trend in the time series over a period of time this framework is applicable to other problem in spatio temporal data mining to ass the significance of the pattern 
we address the problem of estimating the difference between two probability density a naive approach is a two step procedure of first estimating two density separately and then computing their difference however this procedure doe not necessarily work well because the first step is performed without regard to the second step and thus a small estimation error incurred in the first stage can cause a big error in the second stage in this letter we propose a single shot procedure for directly estimating the density difference without separately estimating two density we derive a nonparametric finite sample error bound for the proposed single shot density difference estimator and show that it achieves the optimal convergence rate we then show how the proposed density difference estimator can be used in l distance approximation finally we experimentally demonstrate the usefulness of the proposed method in robust distribution comparison such a class prior estimation and change point detection 
crowdsourcing is an effective method for collecting labeled data for various data mining task it is critical to ensure the veracity of the produced data because response collected from different user may be noisy and unreliable previous work solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in each task individually in this case each single task need large amount of data to provide accurate estimation however in practice budget provided by customer for a given target task may be limited and hence each question can be presented to only a few user where each user can answer only a few question this data sparsity problem can cause previous approach to perform poorly due to the overfitting problem on rare data and eventually damage the data veracity fortunately in real world application user can answer question from multiple historical task for example one can annotate image a well a label the sentiment of a given title in this paper we employ transfer learning which borrows knowledge from auxiliary historical task to improve the data veracity in a given target task the motivation is that user have stable characteristic across different crowdsourcing task and thus data from different task can be exploited collectively to estimate user ability in the target task we propose a hierarchical bayesian model tlc transfer learning for crowdsourcing to implement this idea by considering the overlapping user a a bridge in addition to avoid possible negative impact tlc introduces task specific factor to model task difference the experimental result show that tlc significantly improves the accuracy over several state of the art non transfer learning approach under very limited budget in various labeling task 
data describing network communication network transaction network disease transmission network collaboration network etc is becoming increasingly ubiquitous while this observational data is useful it often only hint at the actual underlying social or technological structure which give rise to the interaction for example an email communication network provides useful insight but is not the same a the real social network among individual in this paper we introduce the problem of graph identification i e the discovery of the true graph structure underlying an observed network we cast the problem a a probabilistic inference task in which we must infer the node edge and node label of a hidden graph based on evidence provided by the observed network this in turn corresponds to the problem of performing entity resolution link prediction and node labeling to infer the hidden graph while each of these problem have been studied separately they have never been considered together a a coherent task we present a simple yet novel approach to address all three problem simultaneously our approach called c consists of coupled collective classifier that are iteratively applied to propagate information among solution to the problem we empirically demonstrate that c is superior in term of both predictive accuracy and runtime to state of the art probabilistic approach on three real world problem 
with the explosive growth of social network many application are increasingly harnessing the pulse of online crowd for a variety of task such a marketing advertising and opinion mining an important example is the wisdom of crowd effect that ha been well studied for such task when the crowd is non interacting however these study don t explicitly address the network effect in social network a key difference in this setting is the presence of social influence that arise from these interaction and can undermine the wisdom of the crowd using a natural model of opinion formation we analyze the effect of these interaction on an individual s opinion and estimate her propensity to conform we then propose efficient sampling algorithm incorporating these conformity value to arrive at a debiased estimate of the wisdom of a crowd we analyze the trade off between the sample size and estimation error and validate our algorithm using both real data obtained from online user experiment and synthetic data 
in this paper we develop density estimation tree dets the natural analog of classification tree and regression tree for the task of density estimation we consider the estimation of a joint probability density function of a d dimensional random vector x and define a piecewise constant estimator structured a a decision tree the integrated squared error is minimized to learn the tree we show that the method is nonparametric under standard condition of nonparametric density estimation dets are shown to be asymptotically consistent in addition being decision tree dets perform automatic feature selection they empirically exhibit the interpretability adaptability and feature selection property of supervised decision tree while incurring slight loss in accuracy over other nonparametric density estimator hence they might be able to avoid the curse of dimensionality if the true density is sparse in dimension we believe that density estimation tree provide a new tool for exploratory data analysis with unique capability 
the goal of dimensionality reduction is to embed high dimensional data in a low dimensional space while preserving structure in the data relevant to exploratory data analysis such a cluster however existing dimensionality reduction method often either fail to separate cluster due to the crowding problem or can only separate cluster at a single resolution we develop a new approach to dimensionality reduction tree preserving embedding our approach us the topological notion of connectedness to separate cluster at all resolution we provide a formal guarantee of cluster separation for our approach that hold for finite sample our approach requires no parameter and can handle general type of data making it easy to use in practice and suggesting new strategy for robust data visualization 
multi task learning mtl aim at improving the generalization performance by utilizing the intrinsic relationship among multiple related task a key assumption in most mtl algorithm is that all task are related which however may not be the case in many real world application in this paper we propose a robust multi task learning rmtl algorithm which learns multiple task simultaneously a well a identifies the irrelevant outlier task specifically the proposed rmtl algorithm capture the task relationship using a low rank structure and simultaneously identifies the outlier task using a group sparse structure the proposed rmtl algorithm is formulated a a non smooth convex unconstrained optimization problem we propose to adopt the accelerated proximal method apm for solving such an optimization problem the key component in apm is the computation of the proximal operator which can be shown to admit an analytic solution we also theoretically analyze the effectiveness of the rmtl algorithm in particular we derive a key property of the optimal solution to rmtl moreover based on this key property we establish a theoretical bound for characterizing the learning performance of rmtl our experimental result on benchmark data set demonstrate the effectiveness and efficiency of the proposed algorithm 
we consider the problem of adaptively routing a fleet of cooperative vehicle within a road network in the presence of uncertain and dynamic congestion condition to tackle this problem we first propose a gaussian process dynamic congestion model that can effectively characterize both the dynamic and the uncertainty of congestion condition our model is efficient and thus facilitates real time adaptive routing in the face of uncertainty using this congestion model we develop an efficient algorithm for non myopic adaptive routing to minimize the collective travel time of all vehicle in the system a key property of our approach is the ability to efficiently reason about the long term value of exploration which enables collectively balancing the exploration exploitation trade off for entire fleet of vehicle we validate our approach based on traffic data from two large asian city we show that our congestion model is effective in modeling dynamic congestion condition we also show that our routing algorithm generates significantly faster route compared to standard baseline and achieves near optimal performance compared to an omniscient routing algorithm we also present the result from a preliminary field study which showcase the efficacy of our approach 
the analysis of multimedia application trace can reveal important information to enhance program execution comprehension however typical size of trace can be in gigabyte which hinders their effective exploitation by application developer in this paper we study the problem of finding a set of sequence of event that allows a reduced size rewriting of the original trace these sequence of event that we call block can simplify the exploration of large execution trace by allowing application developer to see an abstraction instead of low level event the problem of computing such set of block is np hard and naive approach lead to prohibitive running time that prevent analysing real world trace we propose a novel algorithm that directly mine the set of block our experiment show that our algorithm can analyse real trace of up to two hour of video we also show experimentally the quality of the set of block proposed and the interest of the rewriting to understand actual trace data 
chronic disease constitute the leading cause of mortality in the western world have a major impact on the patient quality of life and comprise the bulk of healthcare cost nowadays healthcare data management system integrate large amount of medical information on patient including diagnosis medical procedure lab test result and more sophisticated analysis method are needed for utilizing these data to assist in patient management and to enhance treatment quality at reduced cost in this study we take a first step towards better disease management of diabetic patient by applying state of the art method to anticipate the patient s future health condition and to identify patient at high risk two relevant outcome measure are explored the need for emergency care service and the probability of the treatment producing a sub optimal result a defined by domain expert by identifying the high risk patient our prediction system can be used by healthcare provider to prepare both financially and logistically for the patient need to demonstrate a potential downstream application for the identified high risk patient we explore the association between the physician treating these patient and the treatment outcome and propose a system that can assist healthcare provider in optimizing the match between a patient and a physician our work formulates the problem and examines the performance of several learning model on data from several thousand of patient we further describe a pilot system built on the result of this analysis we show that the risk for the two considered outcome can be evaluated from patient characteristic and that feature of the patient physician match improve the prediction accuracy for the treatment s success these result suggest that personalized medicine can be valuable for high risk patient and raise interesting question for future improvement 
in order to satisfy and positively surprise the user a recommender system need to recommend item the user will like and most probably would not have found on their own this requires the recommender system to recommend a broader range of item including niche item a well such an approach also support online store that often offer more item than traditional store and need recommender system to enable user to find the not so popular item a well however popular item that hold a lot of usage data are more easy to recommend and thus niche item are often excluded from the recommendation in this paper we propose a new collaborative filtering approach that is based on the item usage context the approach increase the rating prediction for niche item with fewer usage data available and improves the aggragate diversity of the recommendation 
probabilistic topic model have shown remarkable success in many application domain however a probabilistic conditional topic model can be extremely inefficient when considering a rich set of feature because it need to define a normalized distribution which usually involves a hard to compute partition function this paper present conditional topical coding ctc a novel formulation of conditional topic model which is non probabilistic ctc relaxes the normalization constraint a in probabilistic model and learns non negative document code and word code ctc doe not need to define a normalized distribution and can efficiently incorporate a rich set of feature for improved topic discovery and prediction task moreover ctc can directly control the sparsity of inferred representation by using appropriate regularization we develop an efficient and easy to implement coordinate descent learning algorithm of which each coding substep ha a closed form solution finally we demonstrate the advantage of ctc on online review analysis datasets our result show that conditional topical coding can achieve state of the art prediction performance and is much more efficient in training one order of magnitude faster and testing two order of magnitude faster than probabilistic conditional topic model 
we present several exact and highly scalable local pattern sampling algorithm they can be used a an alternative to exhaustive local pattern discovery method e g frequent set mining or optimistic estimator based subgroup discovery and can substantially improve efficiency a well a controllability of pattern discovery process while previous sampling approach mainly rely on the markov chain monte carlo method our procedure are direct i e non process simulating sampling algorithm the advantage of these direct method are an almost optimal time complexity per pattern a well a an exactly controlled distribution of the produced pattern namely the proposed algorithm can sample item set according to frequency area squared frequency and a class discriminativity measure experiment demonstrate that these procedure can improve the accuracy of pattern based model similar to frequent set and often also lead to substantial gain in term of scalability 
statistical machine learning knowledge discovery technique tend to fail when faced with an adaptive adversary attempting to evade detection in the data human do an excellent job of correctly spotting adaptive adversary given a good way to digest the data on the other hand human are glacially slow and error prone when it come to moving through very large volume of data a task best left to the machine fighting complex fraud and cyber security threat requires a symbiosis between the computer and team of human analyst the computer use algorithmic analysis heuristic and or statistical characterization to find interesting simple pattern in the data these candidate event are then queued for in depth human analysis in rich expressive interactive analysis environment in this talk we ll take a look at case study of three different system using a partnership of automation and human analysis on large scale data to find the clandestine human behavior that these datasets hold including a discussion of the backend system architecture and a demo of the interactive analysis environment the backend system architecture is a mix of open source technology like cassandra lucene and hadoop and some new component that bind them all together the interactive analysis environment allows seamless pivoting between semantic geospatial and temporal analysis with a powerful gui interface that s usable by non data scientist the system are real system currently in use by commercial bank pharmaceutical company and government 
dagger is a clustering algorithm for uncertain data in contrast to prior work dagger can work on arbitrarily correlated data and can compute both exact and approximate clustering with error guarantee we demonstrate dagger using a real world scenario in which partial discharge data from uk power network is clustered to predict asset failure in the energy network 
the problem of efficiently finding the best match for a query in a given set with respect to the euclidean distance or the cosine similarity ha been extensively studied however the closely related problem of efficiently finding the best match with respect to the inner product ha never been explored in the general setting to the best of our knowledge in this paper we consider this problem and contrast it with the previous problem considered first we propose a general branch and bound algorithm based on a single tree data structure subsequently we present a dual tree algorithm for the case where there are multiple query our proposed branch and bound algorithm are based on novel inner product bound finally we present a new data structure the cone tree for increasing the efficiency of the dual tree algorithm we evaluate our proposed algorithm on a variety of data set from various application and exhibit up to five order of magnitude improvement in query time over the naive search technique in some case 
modeling the dynamic of online social network over time not only help u understand the evolution of network structure and user behavior but also improves the performance of other analysis task such a link prediction and community detection nowadays user engage in multiple network and form a composite social network by considering common user a the bridge state of the art network dynamic analysis is performed in isolation for individual network but user interaction in one network can influence their behavior in other network and in an individual network different type of user interaction also affect each other without considering the influence across network one may not be able to model the dynamic in a given network correctly due to the lack of information in this paper we study the problem of modeling the dynamic of composite network where the evolution process of different network are jointly considered however due to the difference in network property simply merging multiple network into a single one is not ideal because individual evolution pattern may be ignored and network difference may bring negative impact the proposed solution is a nonparametric bayesian model which model each user s common latent feature to extract the cross network influence and use network specific factor to describe different network evolution pattern empirical study on large scale dynamic composite social network demonstrate that the proposed approach improves the performance of link prediction over several state of the art baseline and unfolds the network evolution accurately 
temporal dependency between multiple sensor data source link two type of event if the occurrence of one is repeatedly followed by the appearance of the other in a certain time interval teddy algorithm aim at discovering such dependency identifying the statically significant time interval with a chi test we present how these dependency can be used within the grizzly project to tackle an environmental and technical issue the deicing of the road this project aim to wisely organize the deicing operation of an urban area based on several sensor network measure of local atmospheric phenomenon a spatial and temporal dependency based model is built from these data to predict freezing alert 
online social stream such a twitter facebook timeline and forum discussion have emerged a prevalent channel for information dissemination a these social stream surge quickly information overload ha become a huge problem existing keyword search engine on social stream like twitter search are not successful in overcoming the problem because they merely return an overwhelming list of post with little aggregation or semantics in this demo we provide a new solution called keysee by grouping post into event and track the evolution pattern of event a new post stream in and old post fade out noise and redundancy problem are effectively addressed in our system our demo support refined keyword query on evolving event by allowing user to specify the time span and designated evolution pattern for each event result we provide various analytic view such a frequency curve word cloud and gps distribution we deploy keysee on real twitter stream and the result show that our demo outperforms existing keyword search engine on both quality and usability 
we propose a scalable approach for making inference about latent space of large network with a succinct representation of network a a bag of triangular motif a parsimonious statistical model and an efficient stochastic variational inference algorithm we are able to analyze real network with over a million vertex and hundred of latent role on a single machine in a matter of hour a setting that is out of reach for many existing method when compared to the state of the art probabilistic approach our method is several order of magnitude faster with competitive or improved accuracy for latent space recovery and link prediction 
integrating new knowledge source into various learning task to improve their performance ha recently become an interesting topic in this paper we propose a novel semi supervised learning ssl approach called semi supervised learning with mixed knowledge information ssl mki which can simultaneously handle both sparse labeled data and additional pairwise constraint together with unlabeled data specifically we first construct a unified ssl framework to combine the manifold assumption and the pairwise constraint assumption for classification task then we present a modified fixed point continuation mfpc algorithm with an eigenvalue thresholding evt operator to learn the enhanced kernel matrix finally we develop a two stage optimization strategy and provide an efficient ssl approach that take advantage of laplacian spectral regularization semi supervised learning with enhanced spectral kernel esk experimental result on a variety of synthetic and real world datasets demonstrate the effectiveness of the proposed esk approach 
partitioning large graph is difficult especially when performed in the limited model of computation afforded to modern large scale computing system in this work we introduce restreaming graph partitioning and develop algorithm that scale similarly to streaming partitioning algorithm yet empirically perform a well a fully offline algorithm in streaming partitioning graph are partitioned serially in a single pas restreaming partitioning is motivated by scenario where approximately the same dataset is routinely streamed making it possible to transform streaming partitioning algorithm into an iterative procedure this combination of simplicity and powerful performance allows restreaming algorithm to be easily adapted to efficiently tackle more challenging partitioning objective in particular we consider the problem of stratified graph partitioning where each of many node attribute stratum are balanced simultaneously a such stratified partitioning is well suited for the study of network effect on social network where it is desirable to isolate disjoint dense subgraphs with representative user demographic to demonstrate we partition a large social network such that each partition exhibit the same degree distribution in the original graph a novel achievement for non regular graph a part of our result we also observe a fundamental difference in the ease with which social graph are partitioned when compared to web graph namely the modular structure of web graph appears to motivate full offline optimization whereas the locally dense structure of social graph precludes significant gain from global manipulation 
friending recommendation ha successfully contributed to the explosive growth of online social network most friending recommendation service today aim to support passive friending where a user passively selects friending target from the recommended candidate in this paper we advocate a recommendation support for active friending where a user actively specifies a friending target to the best of our knowledge a recommendation designed to provide guidance for a user to systematically approach his friending target ha not been explored for existing online social networking service to maximize the probability that the friending target would accept an invitation from the user we formulate a new optimization problem namely acceptance probability maximization apm and develop a polynomial time algorithm called selective invitation with tree and in node aggregation sitina to find the optimal solution we implement an active friending service with sitina on facebook to validate our idea our user study and experimental result reveal that sitina outperforms manual selection and the baseline approach in solution quality efficiently 
in recent year social medium have become indispensable tool for information dissemination operating in tandem with traditional medium outlet such a newspaper and it ha become critical to understand the interaction between the new and old source of news although social medium a well a traditional medium have attracted attention from several research community most of the prior work ha been limited to a single medium in addition temporal analysis of these source can provide an understanding of how information spread and evolves modeling temporal dynamic while considering multiple source is a challenging research problem in this paper we address the problem of modeling text stream from two news source twitter and yahoo news our analysis address both their individual property including temporal dynamic and their inter relationship this work extends standard topic model by allowing each text stream to have both local topic and shared topic for temporal modeling we associate each topic with a time dependent function that characterizes it popularity over time by integrating the two model we effectively model the temporal dynamic of multiple correlated text stream in a unified framework we evaluate our model on a large scale dataset consisting of text stream from both twitter and news feed from yahoo news besides overcoming the limitation of existing model we show that our work achieves better perplexity on unseen data and identifies more coherent topic we also provide analysis of finding real world event from the topic obtained by our model 
a key challenge in privacy preserving data mining is ensuring that a data mining result doe not inherently violate privacy differential privacy appears to provide a solution to this problem however there are no clear guideline on how to set to satisfy a privacy policy we given an alternate formulation differential identifiability parameterized by the probability of individual identification this provides the strong privacy guarantee of differential privacy while letting policy maker set parameter based on the established privacy concept of individual identifiability 
throughout life the cell in every individual accumulate many change in the dna inherited from his or her parent certain combination of change lead to cancer during the last decade the cost of dna sequencing ha been dropping by a factor of every two year making it now possible to read most of the three billion base genome from a patient s cancer tumor and to try to determine all of the thousand of dna change in it under the auspex of nci s cancer genome atlas project tumor will be sequenced in this manner in the next few year soon cancer genome sequencing will be a widespread clinical practice and million of tumor will be sequenced a massive computational problem loom in interpreting these data first because we can only read short piece of dna we have the enormous problem of assembling a coherent and reliable representation of the tumor genome from massive amount of incomplete and error prone evidence this is the first challenge second every human genome is unique from birth and every tumor a unique variant there is no single route to cancer we must learn to read the varied signature of cancer within the tumor genome and associate these with optimal treatment already there are hundred of molecularly targeted treatment for cancer available each known to be more or le effective depending on specific genetic variant however targeting a single gene with one treatment rarely work the second challenge is to tackle the combinatorics of personalized targeted combination therapy in cancer 
we investigate a class of method that we call social sampling where participant in a poll respond with a summary of their friend putative response to the poll social sampling lead to a novel trade off question the saving in the number of sample roughly the average degree of the network of participant v the systematic bias in the poll due to the network structure we provide precise analysis of estimator that result from this idea with non uniform sampling of node and non uniform weighting of neighbor response we devise an ideal unbiased estimator we show that the variance of this estimator is controlled by the second eigenvalue of the normalized laplacian of the network the network structure penalty and the correlation between node degree and the property being measured the effective saving factor in addition we present a sequence of approximate estimator that are simpler or more realistic or both and analyze their performance experiment on large real world network show that social sampling is a powerful paradigm in obtaining accurate estimate with very few sample at the same time our result urge caution in interpreting recent result about expectation v intent polling 
dipole represent long distance connection between the pressure anomaly of two distant region that are negatively correlated with each other such dipole have proven important for understanding and explaining the variability in climate in many region of the world e g the el nino climate phenomenon is known to be responsible for precipitation and temperature anomaly over large part of the world systematic approach for dipole detection generate a large number of candidate dipole but there exists no method to evaluate the significance of the candidate teleconnections in this paper we present a novel method for testing the statistical significance of the class of spatio temporal teleconnection pattern called a dipole one of the most important challenge in addressing significance testing in a spatio temporal context is how to address the spatial and temporal dependency that show up a high autocorrelation we present a novel approach that us the wild bootstrap to capture the spatio temporal dependency in the special use case of teleconnections in climate data our approach to find the statistical significance take into account the autocorrelation the seasonality and the trend in the time series over a period of time this framework is applicable to other problem in spatio temporal data mining to ass the significance of the pattern 
we address the problem of estimating the difference between two probability density a naive approach is a two step procedure of first estimating two density separately and then computing their difference however this procedure doe not necessarily work well because the first step is performed without regard to the second step and thus a small estimation error incurred in the first stage can cause a big error in the second stage in this letter we propose a single shot procedure for directly estimating the density difference without separately estimating two density we derive a nonparametric finite sample error bound for the proposed single shot density difference estimator and show that it achieves the optimal convergence rate we then show how the proposed density difference estimator can be used in l distance approximation finally we experimentally demonstrate the usefulness of the proposed method in robust distribution comparison such a class prior estimation and change point detection 
crowdsourcing is an effective method for collecting labeled data for various data mining task it is critical to ensure the veracity of the produced data because response collected from different user may be noisy and unreliable previous work solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in each task individually in this case each single task need large amount of data to provide accurate estimation however in practice budget provided by customer for a given target task may be limited and hence each question can be presented to only a few user where each user can answer only a few question this data sparsity problem can cause previous approach to perform poorly due to the overfitting problem on rare data and eventually damage the data veracity fortunately in real world application user can answer question from multiple historical task for example one can annotate image a well a label the sentiment of a given title in this paper we employ transfer learning which borrows knowledge from auxiliary historical task to improve the data veracity in a given target task the motivation is that user have stable characteristic across different crowdsourcing task and thus data from different task can be exploited collectively to estimate user ability in the target task we propose a hierarchical bayesian model tlc transfer learning for crowdsourcing to implement this idea by considering the overlapping user a a bridge in addition to avoid possible negative impact tlc introduces task specific factor to model task difference the experimental result show that tlc significantly improves the accuracy over several state of the art non transfer learning approach under very limited budget in various labeling task 
data describing network communication network transaction network disease transmission network collaboration network etc is becoming increasingly ubiquitous while this observational data is useful it often only hint at the actual underlying social or technological structure which give rise to the interaction for example an email communication network provides useful insight but is not the same a the real social network among individual in this paper we introduce the problem of graph identification i e the discovery of the true graph structure underlying an observed network we cast the problem a a probabilistic inference task in which we must infer the node edge and node label of a hidden graph based on evidence provided by the observed network this in turn corresponds to the problem of performing entity resolution link prediction and node labeling to infer the hidden graph while each of these problem have been studied separately they have never been considered together a a coherent task we present a simple yet novel approach to address all three problem simultaneously our approach called c consists of coupled collective classifier that are iteratively applied to propagate information among solution to the problem we empirically demonstrate that c is superior in term of both predictive accuracy and runtime to state of the art probabilistic approach on three real world problem 
with the explosive growth of social network many application are increasingly harnessing the pulse of online crowd for a variety of task such a marketing advertising and opinion mining an important example is the wisdom of crowd effect that ha been well studied for such task when the crowd is non interacting however these study don t explicitly address the network effect in social network a key difference in this setting is the presence of social influence that arise from these interaction and can undermine the wisdom of the crowd using a natural model of opinion formation we analyze the effect of these interaction on an individual s opinion and estimate her propensity to conform we then propose efficient sampling algorithm incorporating these conformity value to arrive at a debiased estimate of the wisdom of a crowd we analyze the trade off between the sample size and estimation error and validate our algorithm using both real data obtained from online user experiment and synthetic data 
in this paper we develop density estimation tree dets the natural analog of classification tree and regression tree for the task of density estimation we consider the estimation of a joint probability density function of a d dimensional random vector x and define a piecewise constant estimator structured a a decision tree the integrated squared error is minimized to learn the tree we show that the method is nonparametric under standard condition of nonparametric density estimation dets are shown to be asymptotically consistent in addition being decision tree dets perform automatic feature selection they empirically exhibit the interpretability adaptability and feature selection property of supervised decision tree while incurring slight loss in accuracy over other nonparametric density estimator hence they might be able to avoid the curse of dimensionality if the true density is sparse in dimension we believe that density estimation tree provide a new tool for exploratory data analysis with unique capability 
the goal of dimensionality reduction is to embed high dimensional data in a low dimensional space while preserving structure in the data relevant to exploratory data analysis such a cluster however existing dimensionality reduction method often either fail to separate cluster due to the crowding problem or can only separate cluster at a single resolution we develop a new approach to dimensionality reduction tree preserving embedding our approach us the topological notion of connectedness to separate cluster at all resolution we provide a formal guarantee of cluster separation for our approach that hold for finite sample our approach requires no parameter and can handle general type of data making it easy to use in practice and suggesting new strategy for robust data visualization 
multi task learning mtl aim at improving the generalization performance by utilizing the intrinsic relationship among multiple related task a key assumption in most mtl algorithm is that all task are related which however may not be the case in many real world application in this paper we propose a robust multi task learning rmtl algorithm which learns multiple task simultaneously a well a identifies the irrelevant outlier task specifically the proposed rmtl algorithm capture the task relationship using a low rank structure and simultaneously identifies the outlier task using a group sparse structure the proposed rmtl algorithm is formulated a a non smooth convex unconstrained optimization problem we propose to adopt the accelerated proximal method apm for solving such an optimization problem the key component in apm is the computation of the proximal operator which can be shown to admit an analytic solution we also theoretically analyze the effectiveness of the rmtl algorithm in particular we derive a key property of the optimal solution to rmtl moreover based on this key property we establish a theoretical bound for characterizing the learning performance of rmtl our experimental result on benchmark data set demonstrate the effectiveness and efficiency of the proposed algorithm 
we consider the problem of adaptively routing a fleet of cooperative vehicle within a road network in the presence of uncertain and dynamic congestion condition to tackle this problem we first propose a gaussian process dynamic congestion model that can effectively characterize both the dynamic and the uncertainty of congestion condition our model is efficient and thus facilitates real time adaptive routing in the face of uncertainty using this congestion model we develop an efficient algorithm for non myopic adaptive routing to minimize the collective travel time of all vehicle in the system a key property of our approach is the ability to efficiently reason about the long term value of exploration which enables collectively balancing the exploration exploitation trade off for entire fleet of vehicle we validate our approach based on traffic data from two large asian city we show that our congestion model is effective in modeling dynamic congestion condition we also show that our routing algorithm generates significantly faster route compared to standard baseline and achieves near optimal performance compared to an omniscient routing algorithm we also present the result from a preliminary field study which showcase the efficacy of our approach 
the analysis of multimedia application trace can reveal important information to enhance program execution comprehension however typical size of trace can be in gigabyte which hinders their effective exploitation by application developer in this paper we study the problem of finding a set of sequence of event that allows a reduced size rewriting of the original trace these sequence of event that we call block can simplify the exploration of large execution trace by allowing application developer to see an abstraction instead of low level event the problem of computing such set of block is np hard and naive approach lead to prohibitive running time that prevent analysing real world trace we propose a novel algorithm that directly mine the set of block our experiment show that our algorithm can analyse real trace of up to two hour of video we also show experimentally the quality of the set of block proposed and the interest of the rewriting to understand actual trace data 
chronic disease constitute the leading cause of mortality in the western world have a major impact on the patient quality of life and comprise the bulk of healthcare cost nowadays healthcare data management system integrate large amount of medical information on patient including diagnosis medical procedure lab test result and more sophisticated analysis method are needed for utilizing these data to assist in patient management and to enhance treatment quality at reduced cost in this study we take a first step towards better disease management of diabetic patient by applying state of the art method to anticipate the patient s future health condition and to identify patient at high risk two relevant outcome measure are explored the need for emergency care service and the probability of the treatment producing a sub optimal result a defined by domain expert by identifying the high risk patient our prediction system can be used by healthcare provider to prepare both financially and logistically for the patient need to demonstrate a potential downstream application for the identified high risk patient we explore the association between the physician treating these patient and the treatment outcome and propose a system that can assist healthcare provider in optimizing the match between a patient and a physician our work formulates the problem and examines the performance of several learning model on data from several thousand of patient we further describe a pilot system built on the result of this analysis we show that the risk for the two considered outcome can be evaluated from patient characteristic and that feature of the patient physician match improve the prediction accuracy for the treatment s success these result suggest that personalized medicine can be valuable for high risk patient and raise interesting question for future improvement 
in order to satisfy and positively surprise the user a recommender system need to recommend item the user will like and most probably would not have found on their own this requires the recommender system to recommend a broader range of item including niche item a well such an approach also support online store that often offer more item than traditional store and need recommender system to enable user to find the not so popular item a well however popular item that hold a lot of usage data are more easy to recommend and thus niche item are often excluded from the recommendation in this paper we propose a new collaborative filtering approach that is based on the item usage context the approach increase the rating prediction for niche item with fewer usage data available and improves the aggragate diversity of the recommendation 
probabilistic topic model have shown remarkable success in many application domain however a probabilistic conditional topic model can be extremely inefficient when considering a rich set of feature because it need to define a normalized distribution which usually involves a hard to compute partition function this paper present conditional topical coding ctc a novel formulation of conditional topic model which is non probabilistic ctc relaxes the normalization constraint a in probabilistic model and learns non negative document code and word code ctc doe not need to define a normalized distribution and can efficiently incorporate a rich set of feature for improved topic discovery and prediction task moreover ctc can directly control the sparsity of inferred representation by using appropriate regularization we develop an efficient and easy to implement coordinate descent learning algorithm of which each coding substep ha a closed form solution finally we demonstrate the advantage of ctc on online review analysis datasets our result show that conditional topical coding can achieve state of the art prediction performance and is much more efficient in training one order of magnitude faster and testing two order of magnitude faster than probabilistic conditional topic model 
we present several exact and highly scalable local pattern sampling algorithm they can be used a an alternative to exhaustive local pattern discovery method e g frequent set mining or optimistic estimator based subgroup discovery and can substantially improve efficiency a well a controllability of pattern discovery process while previous sampling approach mainly rely on the markov chain monte carlo method our procedure are direct i e non process simulating sampling algorithm the advantage of these direct method are an almost optimal time complexity per pattern a well a an exactly controlled distribution of the produced pattern namely the proposed algorithm can sample item set according to frequency area squared frequency and a class discriminativity measure experiment demonstrate that these procedure can improve the accuracy of pattern based model similar to frequent set and often also lead to substantial gain in term of scalability 
statistical machine learning knowledge discovery technique tend to fail when faced with an adaptive adversary attempting to evade detection in the data human do an excellent job of correctly spotting adaptive adversary given a good way to digest the data on the other hand human are glacially slow and error prone when it come to moving through very large volume of data a task best left to the machine fighting complex fraud and cyber security threat requires a symbiosis between the computer and team of human analyst the computer use algorithmic analysis heuristic and or statistical characterization to find interesting simple pattern in the data these candidate event are then queued for in depth human analysis in rich expressive interactive analysis environment in this talk we ll take a look at case study of three different system using a partnership of automation and human analysis on large scale data to find the clandestine human behavior that these datasets hold including a discussion of the backend system architecture and a demo of the interactive analysis environment the backend system architecture is a mix of open source technology like cassandra lucene and hadoop and some new component that bind them all together the interactive analysis environment allows seamless pivoting between semantic geospatial and temporal analysis with a powerful gui interface that s usable by non data scientist the system are real system currently in use by commercial bank pharmaceutical company and government 
dagger is a clustering algorithm for uncertain data in contrast to prior work dagger can work on arbitrarily correlated data and can compute both exact and approximate clustering with error guarantee we demonstrate dagger using a real world scenario in which partial discharge data from uk power network is clustered to predict asset failure in the energy network 
the problem of efficiently finding the best match for a query in a given set with respect to the euclidean distance or the cosine similarity ha been extensively studied however the closely related problem of efficiently finding the best match with respect to the inner product ha never been explored in the general setting to the best of our knowledge in this paper we consider this problem and contrast it with the previous problem considered first we propose a general branch and bound algorithm based on a single tree data structure subsequently we present a dual tree algorithm for the case where there are multiple query our proposed branch and bound algorithm are based on novel inner product bound finally we present a new data structure the cone tree for increasing the efficiency of the dual tree algorithm we evaluate our proposed algorithm on a variety of data set from various application and exhibit up to five order of magnitude improvement in query time over the naive search technique in some case 
modeling the dynamic of online social network over time not only help u understand the evolution of network structure and user behavior but also improves the performance of other analysis task such a link prediction and community detection nowadays user engage in multiple network and form a composite social network by considering common user a the bridge state of the art network dynamic analysis is performed in isolation for individual network but user interaction in one network can influence their behavior in other network and in an individual network different type of user interaction also affect each other without considering the influence across network one may not be able to model the dynamic in a given network correctly due to the lack of information in this paper we study the problem of modeling the dynamic of composite network where the evolution process of different network are jointly considered however due to the difference in network property simply merging multiple network into a single one is not ideal because individual evolution pattern may be ignored and network difference may bring negative impact the proposed solution is a nonparametric bayesian model which model each user s common latent feature to extract the cross network influence and use network specific factor to describe different network evolution pattern empirical study on large scale dynamic composite social network demonstrate that the proposed approach improves the performance of link prediction over several state of the art baseline and unfolds the network evolution accurately 
temporal dependency between multiple sensor data source link two type of event if the occurrence of one is repeatedly followed by the appearance of the other in a certain time interval teddy algorithm aim at discovering such dependency identifying the statically significant time interval with a chi test we present how these dependency can be used within the grizzly project to tackle an environmental and technical issue the deicing of the road this project aim to wisely organize the deicing operation of an urban area based on several sensor network measure of local atmospheric phenomenon a spatial and temporal dependency based model is built from these data to predict freezing alert 
online social stream such a twitter facebook timeline and forum discussion have emerged a prevalent channel for information dissemination a these social stream surge quickly information overload ha become a huge problem existing keyword search engine on social stream like twitter search are not successful in overcoming the problem because they merely return an overwhelming list of post with little aggregation or semantics in this demo we provide a new solution called keysee by grouping post into event and track the evolution pattern of event a new post stream in and old post fade out noise and redundancy problem are effectively addressed in our system our demo support refined keyword query on evolving event by allowing user to specify the time span and designated evolution pattern for each event result we provide various analytic view such a frequency curve word cloud and gps distribution we deploy keysee on real twitter stream and the result show that our demo outperforms existing keyword search engine on both quality and usability 
we propose a scalable approach for making inference about latent space of large network with a succinct representation of network a a bag of triangular motif a parsimonious statistical model and an efficient stochastic variational inference algorithm we are able to analyze real network with over a million vertex and hundred of latent role on a single machine in a matter of hour a setting that is out of reach for many existing method when compared to the state of the art probabilistic approach our method is several order of magnitude faster with competitive or improved accuracy for latent space recovery and link prediction 
integrating new knowledge source into various learning task to improve their performance ha recently become an interesting topic in this paper we propose a novel semi supervised learning ssl approach called semi supervised learning with mixed knowledge information ssl mki which can simultaneously handle both sparse labeled data and additional pairwise constraint together with unlabeled data specifically we first construct a unified ssl framework to combine the manifold assumption and the pairwise constraint assumption for classification task then we present a modified fixed point continuation mfpc algorithm with an eigenvalue thresholding evt operator to learn the enhanced kernel matrix finally we develop a two stage optimization strategy and provide an efficient ssl approach that take advantage of laplacian spectral regularization semi supervised learning with enhanced spectral kernel esk experimental result on a variety of synthetic and real world datasets demonstrate the effectiveness of the proposed esk approach 
partitioning large graph is difficult especially when performed in the limited model of computation afforded to modern large scale computing system in this work we introduce restreaming graph partitioning and develop algorithm that scale similarly to streaming partitioning algorithm yet empirically perform a well a fully offline algorithm in streaming partitioning graph are partitioned serially in a single pas restreaming partitioning is motivated by scenario where approximately the same dataset is routinely streamed making it possible to transform streaming partitioning algorithm into an iterative procedure this combination of simplicity and powerful performance allows restreaming algorithm to be easily adapted to efficiently tackle more challenging partitioning objective in particular we consider the problem of stratified graph partitioning where each of many node attribute stratum are balanced simultaneously a such stratified partitioning is well suited for the study of network effect on social network where it is desirable to isolate disjoint dense subgraphs with representative user demographic to demonstrate we partition a large social network such that each partition exhibit the same degree distribution in the original graph a novel achievement for non regular graph a part of our result we also observe a fundamental difference in the ease with which social graph are partitioned when compared to web graph namely the modular structure of web graph appears to motivate full offline optimization whereas the locally dense structure of social graph precludes significant gain from global manipulation 
friending recommendation ha successfully contributed to the explosive growth of online social network most friending recommendation service today aim to support passive friending where a user passively selects friending target from the recommended candidate in this paper we advocate a recommendation support for active friending where a user actively specifies a friending target to the best of our knowledge a recommendation designed to provide guidance for a user to systematically approach his friending target ha not been explored for existing online social networking service to maximize the probability that the friending target would accept an invitation from the user we formulate a new optimization problem namely acceptance probability maximization apm and develop a polynomial time algorithm called selective invitation with tree and in node aggregation sitina to find the optimal solution we implement an active friending service with sitina on facebook to validate our idea our user study and experimental result reveal that sitina outperforms manual selection and the baseline approach in solution quality efficiently 
we propose to compress weighted graph network motivated by the observation that large network of social biological or other relation can be complex to handle and visualize in the process also known a graph simplification node and unweighted edge are grouped to supernodes and superedges respectively to obtain a smaller graph we propose model and algorithm for weighted graph the interpretation i e decompression of a compressed weighted graph is that a pair of original node is connected by an edge if their supernodes are connected by one and that the weight of an edge is approximated to be the weight of the superedge the compression problem now consists of choosing supernodes superedges and superedge weight so that the approximation error is minimized while the amount of compression is maximized in this paper we formulate this task a the simple weighted graph compression problem we then propose a much wider class of task under the name of generalized weighted graph compression problem the generalized task extends the optimization to preserve longer range connectivity between node not just individual edge weight we study the property of these problem and propose a range of algorithm to solve them with different balance between complexity and quality of the result we evaluate the problem and algorithm experimentally on real network the result indicate that weighted graph can be compressed efficiently with relatively little compression error 
in this talk i introduce cloud computing based cross medium knowledge discovery we propose a framework for cross medium semantic understanding which contains discriminative modeling generative modeling and cognitive modeling in cognitive modeling a new model entitled cam is proposed which is suitable for cross medium semantic understanding we develop an agent aid model for load balance in cloud computing environment for quality of service we present a utility function to evaluate the cloud performance a cross medium intelligent retrieval system cmirs which is managed by ontology based knowledge system kmsphere will be illustrated finally the direction for further research on cloud computing based cross medium knowledge discovery will be pointed out and discussed 
interdisciplinary collaboration have generated huge impact to society however it is often hard for researcher to establish such cross domain collaboration what are the pattern of cross domain collaboration how do those collaboration form can we predict this type of collaboration cross domain collaboration exhibit very different pattern compared to traditional collaboration in the same domain sparse connection cross domain collaboration are rare complementary expertise cross domain collaborator often have different expertise and interest topic skewness cross domain collaboration topic are focused on a subset of topic all these pattern violate fundamental assumption of traditional recommendation system in this paper we analyze the cross domain collaboration data from research publication and confirm the above pattern we propose the cross domain topic learning ctl model to address these challenge for handling sparse connection ctl consolidates the existing cross domain collaboration through topic layer instead of at author layer which alleviates the sparseness issue for handling complementary expertise ctl model topic distribution from source and target domain separately a well a the correlation across domain for handling topic skewness ctl only model relevant topic to the cross domain collaboration we compare ctl with several baseline approach on large publication datasets from different domain ctl outperforms baseline significantly on multiple recommendation metric beyond accurate recommendation performance ctl is also insensitive to parameter tuning a confirmed in the sensitivity analysis 
this paper introduces a nonlinear logistic regression model for classification the main idea is to map the data to a feature space based on kernel density estimation a discriminative model is then learned to optimize the feature weight a well a the bandwidth of a nadaraya watson kernel density estimator we then propose a hierarchical optimization algorithm for learning the coefficient and kernel bandwidth in an integrated way compared to other nonlinear model such a kernel logistic regression klr and svm our approach is far more efficient since it solves an optimization problem with a much smaller size two other major advantage are that it can cope with categorical attribute in a unified fashion and naturally handle multi class problem moveover our approach inherits from logistic regression good interpretability of the model which is important for clinical application but not offered by klr and svm extensive result on real datasets including a clinical prediction application currently under deployment in a major hospital show that our approach not only achieves superior classification accuracy but also drastically reduces the computing time a compared to other leading method 
we analyze the convergence of gradient based optimization algorithm that base their update on delayed stochastic gradient information the main application of our result is to gradient based distributed optimization algorithm where a master node performs parameter update while worker node compute stochastic gradient based on local information in parallel which may give rise to delay due to asynchrony we take motivation from statistical problem where the size of the data is so large that it cannot fit on one computer with the advent of huge datasets in biology astronomy and the internet such problem are now common our main contribution is to show that for smooth stochastic problem the delay are asymptotically negligible and we can achieve order optimal convergence result we show n node architecture whose optimization error in stochastic problem in spite of asynchronous delay scale asymptotically a o nt after t iteration this rate is known to be optimal for a distributed system with n node even in the absence of delay we additionally complement our theoretical result with numerical experiment on a logistic regression task 
in this talk i will review several real world application developed at the university of waikato over the past year these include the use of near infrared spectroscopy coupled with data mining a an alternate laboratory technique for predicting compound concentration in soil and plant sample and the analysis of gas chromatography mass spectrometry gcms data a technique used to determine in environmental application for example the petroleum content in soil and water sample i will then briefly discus how experience with these application ha led to the development of an open source framework for application development 
recent study have suggested using relative distance comparison a constraint to represent domain knowledge a natural extension to relative comparison is the combination of two comparison defined on the same set of three instance constraint in this form termed relative constraint provide a unified knowledge representation for both partitional and hierarchical clustering but many key property of relative constraint remain unknown in this paper we answer the following important question that enable the broader application of relative constraint in general clustering problem feasibility doe there exist a clustering that satisfies a given set of relative constraint consistency of constraint completeness given a set of consistent relative constraint how can one derive a complete clustering without running into dead end informativeness how can one extract the most informative relative constraint from given knowledge source we show that any hierarchical domain knowledge can be easily represented by relative constraint we further present a hierarchical algorithm that find a clustering satisfying all given constraint in polynomial time experiment showed that our algorithm achieves significantly higher accuracy than the existing metric learning approach based on relative comparison 
recently there have been many collection of relational data in diverse area such a the internet social network customer shopping record bioinformatics etc the main goal of the relational data analysis is to discover latent structure from the data the conventional data mining algorithm based on exhaustive enumeration have an inherent limitation for this purpose because of the combinatorial nature of the method in contrast in machine learning a lot of statistical model have been proposed for the relational data analysis in this talk first i will review the statistical approach especially bayesian approach for the relational data analysis with recent advancement in machine learning literature then a a future research i will also talk about a statistical approach for combining multiple relational data 
in recent year some spectral feature selection method are proposed to choose those feature with high power of preserving sample similarity however when there exist lot of irrelevant or noisy feature in data the similarity matrix constructed from all the unweighted feature may be not reliable which then misleads existing spectral feature selection method to select wrong feature to solve this problem we propose that feature importance should be evaluated according to their impact on similarity matrix which mean feature with high impact on similarity matrix should be chosen a important one since graph laplacian luxbury is defined on the similarity matrix then the impact of each feature on similarity matrix can be reflected on the change of graph laplacian especially on it eigen system based on this point of view we propose an eigenvalue sensitive criterion evsc for feature selection which aim at seeking those feature with high impact on graph laplacian s eigenvalue empirical analysis demonstrates our proposed method outperforms some traditional spectral feature selection method copyright by the author s owner s 
genome wide association study gwas have not been able to discover strong association between many complex human disease and single genetic locus mapping these phenotype to pair of genetic locus is hindered by the huge number of candidate leading to enormous computational and statistical problem in gwas on single nucleotide polymorphism snp one ha to consider in the order of to pair which is infeasible in practice in this article we give the first algorithm for locus genome wide association study that is subquadratic in the number n of snp the running time of our algorithm is data dependent but large experiment over real genomic data suggest that it scale empirically a n a a result our algorithm can easily cope with n i e it can efficiently search all pair of snp in the human genome 
global state network provide a powerful mechanism to model the increasing heterogeneity in data generated by current system such a network comprises of a series of network snapshot with dynamic local state at node and a global network state indicating the occurrence of an event mining discriminative subgraphs from global state network allows u to identify the influential sub network that have maximum impact on the global state and unearth the complex relationship between the local entity of a network and their collective behavior in this paper we explore this problem and design a technique called mind to mine minimally discriminative subgraphs from large global state network to combat the exponential subgraph search space we derive the concept of an edit map and perform metropolis hastings sampling on it to compute the answer set furthermore we formulate the idea of network constrained decision tree to learn prediction model that adhere to the underlying network structure extensive experiment on real datasets demonstrate excellent accuracy in term of prediction quality additionally mind achieves a speed up of at least four order of magnitude over baseline technique 
keeping in pace with the increasing importance of commerce conducted over the web several e commerce website now provide admirable facility for helping consumer decide what product to buy and where to buy it however since the price of durable and high tech product generally fall over time a buyer of such product is often faced with a dilemma should she buy the product now or wait for cheaper price we present the design and implementation of prodcast an experimental system whose goal is to help consumer decide when to buy a product the system make use of forecast of future price based on price history of the product incorporating feature such a sale volume seasonality and competition in making it recommendation we describe technique that are well suited for this task and present a comprehensive evaluation of their relative merit using retail sale data for electronic product our back testing of the system indicates that the system is capable of helping consumer time their purchase resulting in significant saving to them 
active search is an increasingly important learning problem in which we use a limited budget of label query to discover a many member of a certain class a possible numerous real world application may be approached in this manner including fraud detection product recommendation and drug discovery active search ha model learning and exploration exploitation feature similar to those encountered in active learning and bandit problem but algorithm for those problem do not fit active search previous work on the active search problem showed that the optimal algorithm requires a lookahead evaluation of expected utility that is exponential in the number of selection to be made and proposed a truncated lookahead heuristic inspired by the success of myopic method for active learning and bandit problem we propose a myopic method for active search on graph we suggest selecting point by maximizing a score considering the potential impact of selecting a node meant to emulate lookahead while avoiding exponential search we test the proposed algorithm empirically on real world graph and show that it outperforms popular approach for active learning and bandit problem a well a truncated lookahead of a few step 
we propose a robust framework to jointly perform two key modeling task involving high dimensional data i learning a sparse functional mapping from multiple predictor to multiple response while taking advantage of the coupling among response and ii estimating the conditional dependency structure among response while adjusting for their predictor the traditional likelihood based estimator lack resilience with respect to outlier and model misspecification this issue is exacerbated when dealing with high dimensional noisy data in this work we propose instead to minimize a regularized distance criterion which is motivated by the minimum distance functionals used in nonparametric method for their excellent robustness property the proposed estimate can be obtained efficiently by leveraging a sequential quadratic programming algorithm we provide theoretical justification such a estimation consistency for the proposed estimator additionally we shed light on the robustness of our estimator through it linearization which yield a combination of weighted lasso and graphical lasso with the sample weight providing an intuitive explanation of the robustness we demonstrate the merit of our framework through simulation study and the analysis of real financial and genetics data 
content personalization is a key tool in creating attractive website synergy can be obtained by integrating personalization between several internet property in this paper we propose a hierarchical bayesian model to address these issue our model allows the integration of multiple property without changing the overall structure which make it easily extensible across large internet portal it relies at it lowest level on latent dirichlet allocation while making use of latent side feature for cross property integration we demonstrate the efficiency of our approach by analyzing data from several property of a major internet portal 
a the number of scientific publication soar even the most enthusiastic reader can have trouble staying on top of the evolving literature it is easy to focus on a narrow aspect of one s field and lose track of the big picture information overload is indeed a major challenge for scientist today and is especially daunting for new investigator attempting to master a discipline and scientist who seek to cross disciplinary border in this paper we propose metric of influence coverage and connectivity for scientific literature we use these metric to create structured summary of information which we call metro map most importantly metro map explicitly show the relation between paper in a way which capture development in the field pilot user study demonstrate that our method help researcher acquire new knowledge efficiently map user achieved better precision and recall score and found more seminal paper while performing fewer search 
in this paper we study the problem of mining partially annotated image we first define what the problem of mining partially annotated image is and argue that in many real world application annotated image are typically partially annotated and thus that the problem of mining partially annotated image exists in many situation we then propose an effective solution to this problem based on a statistical model we have developed called the semi supervised correspondence hierarchical dirichlet process sschdp the main idea of this model lie in exploiting the information pertaining to partially annotated image or even unannotated image to achieve semi supervised learning under the hdp structure we apply this model to completing the annotation appropriately for partially annotated image in the training data and then to predicting the annotation appropriately and completely for all the unannotated image either in the training data or in any unseen data beyond the training process experiment show that ssc hdp is superior to the peer model from the recent literature when they are applied to solving the problem of mining partially annotated image 
multi label learning arises in many real world task where an object is naturally associated with multiple concept it is well accepted that in order to achieve a good performance the relationship among label should be exploited most existing approach require the label relationship a prior knowledge or exploit by counting the label co occurrence in this paper we propose the mahr approach which is able to automatically discover and exploit label relationship our basic idea is that if two label are related the hypothesis generated for one label can be helpful for the other label mahr implement the idea a a boosting approach with a hypothesis reuse mechanism in each boosting round the base learner for a label is generated by not only learning on it own task but also reusing the hypothesis from other label and the amount of reuse across label provides an estimate of the label relationship extensive experimental result validate that mahr is able to achieve superior performance and discover reasonable label relationship moreover we disclose that the label relationship is usually asymmetric 
location based service have become widely available on mobile device existing method employ a pull model or user initiated model where a user issue a query to a server which reply with location aware answer to provide user with instant reply a push model or server initiated model is becoming an inevitable computing model in the next generation location based service in the push model subscriber register spatio textual subscription to capture their interest and publisher post spatio textual message this call for a high performance location aware publish subscribe system to deliver publisher message to relevant subscriber in this paper we address the research challenge that arise in designing a location aware publish subscribe system we propose an rtree based index structure by integrating textual description into rtree node we devise efficient filtering algorithm and develop effective pruning technique to improve filtering efficiency experimental result show that our method achieves high performance for example our method can filter tweet in a second for million registered subscription on a commodity computer 
this talk cover technique for analyzing data set with up to trillion of example with billion of feature using thousand of computer to operate at this scale requires an understanding of an increasing complex hardware hierarchy e g cache memory ssd another machine in the rack disk a machine in another data center a model for recovering from inevitable hardware and software failure a machine learning model that allows for efficient computation over large continuously updated data set and a way to visualize and share the result 
locality sensitive hashing lsh is a basic primitive in several large scale data processing application including nearest neighbor search de duplication clustering etc in this paper we propose a new and simple method to speed up the widely used euclidean realization of lsh at the heart of our method is a fast way to estimate the euclidean distance between two d dimensional vector this is achieved by the use of randomized hadamard transforms in a non linear setting this decrease the running time of a k l parameterized lsh from o dkl to o dlog d kl our experiment show that using the new lsh in nearest neighbor application can improve their running time by significant amount to the best of our knowledge this is the first running time improvement to lsh that is both provable and practical 
given a sequence database can we have a non trivial upper bound on the number of sequential pattern the problem of bounding sequential pattern is very challenging in theory due to the combinatorial complexity of sequence even given some inspiring result on bounding itemsets in frequent itemset mining moreover the problem is highly meaningful in practice since the upper bound can be used in many application such a space allocation in building sequence data warehouse in this paper we tackle the problem of bounding sequential pattern by presenting for the first time in the field of sequential pattern mining strong combinatorial result on computing the number of possible sequential pattern that can be generated at a given length k we introduce a a case study two novel technique to estimate the number of candidate sequence an extensive empirical study on both real data and synthetic data verifies the effectiveness of our method 
machine and people have complementary skill in knowledge discovery automated technique can process enormous amount of data to find new relationship but generally these are represented by fairly simple model on the other hand people are endlessly inventive in creating model to explain data at hand but have problem developing consistent overall model to explain all the data that might occur in a domain and the larger the model the more difficult it becomes to maintain consistency ripple down rule is a technique that ha been developed to allow people to make real time update to a model whenever they notice some data that the model doe not yet explain while at the same time maintaining consistency this allows an entire knowledge base to be built while it is already in use by making update there are now s of ripple down rule knowledge base in use and this paper present some observation from log file tracking how people build these system and also outline some recent research on how such technique can be used to add greater specificity to the simpler model developed by automated technique 
recent year have seen an exponential increase in the number of user of social medium site a the number of user of these site continues to grow at an extraordinary rate the amount of data produced follows in magnitude with this deluge of social medium data the need for comprehensive tool to analyze user interaction is ever increasing in this paper we present a novel tool navigating information facet on twitter nif t which help user to explore data generated on social medium site using the three dimension or facet time location and topic a an example of the many possible facet we enable the user to explore large social medium datasets with the help of a large corpus of tweet collected from the occupy wall street movement on the twitter platform we show how our system can be used to identify important aspect of the event along these facet 
recent research effort have made notable progress in improving the performance of exhaustive maximal clique enumeration mce however existing algorithm still suffer from exploring the huge search space of mce furthermore their result are often undesirable a many of the returned maximal clique have large overlapping part this redundancy lead to problem in both computational efficiency and usefulness of mce in this paper we aim at providing a concise and complete summary of the set of maximal clique which is useful to many application we propose the notion of visible mce to achieve this goal and design algorithm to realize the notion based on the refined output space we further consider application including an efficient computation of the top k result with diversity and an interactive clique exploration process our experimental result demonstrate that our approach is capable of producing output of high usability and our algorithm achieve superior efficiency over classic mce algorithm 
we present spine an efficient algorithm for finding the backbone of an influence network given a social graph and a log of past propagation we build an instance of the independent cascade model that describes the propagation we aim at reducing the complexity of that model while preserving most of it accuracy in describing the data we show that the problem is inapproximable and we present an optimal dynamic programming algorithm whose search space albeit exponential is typically much smaller than that of the brute force exhaustive search approach seeking a practical scalable approach to sparsification we devise spine a greedy efficient algorithm with practically little compromise in quality we claim that sparsification is a fundamental data reduction operation with many application ranging from visualization to exploratory and descriptive data analysis a a proof of concept we use spine on real world datasets revealing the backbone of their influence propagation network moreover we apply spine a a pre processing step for the influence maximization problem showing that computation on sparsified model give up little accuracy but yield significant improvement in term of scalability 
an ideal outcome of pattern mining is a small set of informative pattern containing no redundancy or noise that identifies the key structure of the data at hand standard frequent pattern miner do not achieve this goal a due to the pattern explosion typically very large number of highly redundant pattern are returned we pursue the ideal for sequential data by employing a pattern set mining approach an approach where instead of ranking pattern individually we consider result a a whole pattern set mining ha been successfully applied to transactional data but ha been surprisingly understudied for sequential data in this paper we employ the mdl principle to identify the set of sequential pattern that summarises the data best in particular we formalise how to encode sequential data using set of serial episode and use the encoded length a a quality score a search strategy we propose two approach the first algorithm selects a good pattern set from a large candidate set while the second is a parameter free any time algorithm that mine pattern set directly from the data experimentation on synthetic and real data demonstrates we efficiently discover small set of informative pattern 
activity analysis disaggregates utility consumption from smart meter into specific usage that associate with human activity it can not only help resident better manage their consumption for sustainable lifestyle but also allow utility manager to devise conservation program existing research effort on disaggregating consumption focus on analyzing consumption feature with high sample rate mainly between hz mhz however many smart meter deployment support sample rate at most hz which challenge activity analysis with occurrence of parallel activity difficulty of aligning event and lack of consumption feature we propose a novel statistical framework for disaggregation on coarse granular smart meter reading by modeling fixture characteristic household behavior and activity correlation this framework ha been implemented into two approach for different application scenario and ha been deployed to serve over pilot household in dubuque ia interesting activity level consumption pattern have been identified and the evaluation on both real and synthetic datasets ha shown high accuracy on discovering washer and shower 
sequence datasets are encountered in a plethora of application spanning from web usage analysis to healthcare study and ubiquitous computing disseminating such datasets offer remarkable opportunity for discovering interesting knowledge pattern but may lead to serious privacy violation if sensitive pattern such a business secret are disclosed in this work we consider how to sanitize data to prevent the disclosure of sensitive pattern during sequential pattern mining while ensuring that the nonsensitive pattern can still be discovered first we re define the problem of sequential pattern hiding to capture the information loss incurred by sanitization in term of both event modification distortion and lost nonsensitive knowledge pattern side effect second we model sequence a graph and propose two algorithm to solve the problem by operating on the graph the first algorithm attempt to sanitize data with minimal distortion whereas the second focus on reducing the side effect extensive experiment show that our algorithm outperform the existing solution in term of data distortion and side effect and are more efficient 
we introduce a classifier based on the l infinity norm this classifier called chirp is an iterative sequence of three stage projecting binning and covering that are designed to deal with the curse of dimensionality computational complexity and nonlinear separability chirp is not a hybrid or modification of existing classifier it employ a new covering algorithm the accuracy of chirp on widely used benchmark datasets exceeds the accuracy of competitor it computational complexity is sub linear in number of instance and number of variable and subquadratic in number of class 
the main aim of this paper is to design a co ranking scheme for object and relation in multi relational data it ha many important application in data mining and information retrieval however in the literature there is a lack of a general framework to deal with multi relational data for co ranking the main contribution of this paper is to i propose a framework multirank to determine the importance of both object and relation simultaneously based on a probability distribution computed from multi relational data ii show the existence and uniqueness of such probability distribution so that it can be used for co ranking for object and relation very effectively and iii develop an efficient iterative algorithm to solve a set of tensor multivariate polynomial equation to obtain such probability distribution extensive experiment on real world data suggest that the proposed framework is able to provide a co ranking scheme for object and relation successfully experimental result have also shown that our algorithm is computationally efficient and effective for identification of interesting and explainable co ranking result 
in this work we develop a simple algorithm for semi supervised regression the key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled example a the basis function and learn the prediction function by a simple linear regression we show that under appropriate assumption about the integral operator this approach is able to achieve an improved regression error bound better than existing bound of supervised learning we also verify the effectiveness of the proposed algorithm by an empirical study copyright by the author s owner s 
social medium response to news have increasingly gained in importance a they can enhance a consumer s news reading experience promote information sharing and aid journalist in assessing their readership s response to a story given that the number of response to an online news article may be huge a common challenge is that of selecting only the most interesting response for display this paper address this challenge by casting message selection a an optimization problem we define an objective function which jointly model the message utility score and their entropy we propose a near optimal solution to the underlying optimization problem which leverage the submodularity property of the objective function our solution first learns the utility of individual message in isolation and then produce a diverse selection of interesting message by maximizing the defined objective function the intuition behind our work are that an interesting selection of message contains diverse informative opinionated and popular message referring to the news article written mostly by user that have authority on the topic our intuition are embodied by a rich set of content social and user feature capturing the aforementioned aspect we evaluate our approach through both human and automatic experiment and demonstrate it outperforms the state of the art additionally we perform an in depth analysis of the annotated interesting response shedding light on the subjectivity around the selection process and the perception of interestingness 
the electronic road pricing erp system wa implemented by the land transport authority of singapore to control traffic by road pricing since to better understand the traffic condition and improve the pricing scheme the government initiated the next generation erp erp project which aim to use the global navigation satellite system gnss collecting positional data from vehicle for analysis however most driver fear of being monitored once the government installs the device in their vehicle to collect gps data the existing data stream management system dsms centralize both data management and privacy control at server site this framework assumes dsms server is secure and trustable and protects provider data from illegal access by data user in erp the dsms server is maintained by the government i e data user thus the existing framework is not adoptable we propose a novel framework in which privacy protection is pushed to data provider site by doing this the system could be safer and more efficient our framework can be used for the situation such a erp i e data provider would like to control their own privacy policy and or the workload of dsms server need to be reduced 
we consider the problem of a user navigating an unfamiliar corpus of text document where document metadata is limited or unavailable the domain is specialized and the user base is small these challenging condition may hold for example within an organization such a a business or government agency we propose to augment standard keyword search with user feedback on latent topic these topic are automatically learned from the corpus in an unsupervised manner and presented alongside search result user feedback is then used to reformulate the original query resulting in improved information retrieval performance in our experiment 
this paper describes a new technique and analysis for using on line learning algorithm to solve active learning problem our algorithm is called active vote and it work by actively selecting instance that force several perturbed copy of an on line algorithm to make mistake the main intuition for our result is based on the fact that the number of mistake made by the optimal on line algorithm is a lower bound on the number of label needed for active learning we provide performance bound for active vote in both a batch and on line model of active learning these performance bound depend on the algorithm having a set of unlabeled instance in which the various perturbed on line algorithm disagree the motivating application for active vote is an internet advertisement rating program we conduct experiment using data collected for this advertisement problem along with experiment using standard datasets we show active vote can achieve an order of magnitude decrease in the number of labeled instance over various passive learning algorithm such a support vector machine 
if you re still recovering from the barrage of ad news email facebook post and newspaper article that were giving you the latest poll number asking you to volunteer donate money and vote this talk will give you a look behind the scene on why you were seeing what you were seeing i will talk about how machine learning and data mining along with randomized experiment were used to target and influence ten of million of people beyond the presidential election these methodology for targeting and influence have the power to solve big problem in education healthcare energy transportation and related area i will talk about some recent work we re doing at the university of chicago data science for social good summer fellowship program working with non profit and government organization to tackle some of these challenge 
given a set of entity the all pair similarity search aim at identifying all pair of entity that have similarity greater than or distance smaller than some user defined threshold in this article we propose a parallel framework for solving this problem in metric space novel element of our solution include i flexible support for multiple metric of interest ii an autonomic approach to partition the input dataset with minimal redundancy to achieve good load balance in the presence of limited computing resource iii an on thefly lossless compression strategy to reduce both the running time and the final output size we validate the utility scalability and the effectiveness of the approach on hundred of machine using real and synthetic datasets 
the problem of large scale online matrix completion is addressed via a bayesian approach the proposed method learns a factor analysis fa model for large matrix based on a small number of observed matrix element and leverage the statistical model to actively select which new matrix entry observation would be most informative if they could be acquired to improve the model the model inference and active learning are performed in an online setting in the context of online learning a greedy fast and provably near optimal algorithm is employed to sequentially maximize the mutual information between past and future observation taking advantage of submodularity property additionally a simpler procedure which directly us the posterior parameter learned by the bayesian approach is shown to achieve slightly lower estimation quality with far le computational effort inference is performed using a computationally efficient online variational bayes vb procedure competitive result are obtained in a very large collaborative filtering problem namely the yahoo music rating dataset 
classification of sequence drawn from a finite alphabet using a family of string kernel with inexact matching e g spectrum or mismatch ha shown great success in machine learning however selection of optimal mismatch kernel for a particular task is severely limited by inability to compute such kernel for long substring k mers with potentially many mismatch m in this work we introduce a new method that allows u to exactly evaluate kernel for large k m and arbitrary alphabet size the task can be accomplished by first solving the more tractable problem for small alphabet and then trivially generalizing to any alphabet using a small linear system of equation this make it possible to explore a larger set of kernel with a wide range of kernel parameter opening a possibility to better model selection and improved performance of the string kernel to investigate the utility of large k m string kernel we consider several sequence classification problem including protein remote homology detection fold prediction and music classification our result show that increased k mer length with larger substitution can improve classification performance 
this paper present a cloud based system computing customized and practically fast driving route for an end user using historical and real time traffic condition and driver behavior in this system gps equipped taxicab are employed a mobile sensor constantly probing the traffic rhythm of a city and taxi driver intelligence in choosing driving direction in the physical world meanwhile a cloud aggregate and mine the information from these taxi and other source from the internet like web map and weather forecast the cloud build a model incorporating day of the week time of day weather condition and individual driving strategy both of the taxi driver and of the end user for whom the route is being computed using this model our system predicts the traffic condition of a future time when the computed route is actually driven and performs a self adaptive driving direction service for a particular user this service gradually learns a user s driving behavior from the user s gps log and customizes the fastest route for the user with the help of the cloud we evaluate our service using a real world dataset generated by over taxi over a period of month in beijing a a result our service accurately estimate the travel time of a route for a user hence finding the fastest route customized for the user 
there ha recently been a great deal of work focused on developing statistical model of graph structure with the goal of modeling probability distribution over graph from which new similar graph can be generated by sampling from the estimated distribution although current graph model can capture several important characteristic of social network graph e g degree path length many of them do not generate graph with sufficient variation to reflect the natural variability in real world graph domain one exception is the mixed kronecker product graph model mkpgm a generalization of the kronecker product graph model which us parameter tying to capture variance in the underlying distribution the enhanced representation of mkpgms enables them to match both the mean graph statistic and their spread a observed in real network population but unfortunately to date the only method to estimate mkpgms involves an exhaustive search over the parameter in this work we present the first learning algorithm for mkpgms the o e algorithm search over the continuous parameter space using constrained line search and is based on simulated method of moment where the objective function minimizes the distance between the observed moment in the training graph and the empirically estimated moment of the model we evaluate the mkpgm learning algorithm by comparing it to several different graph model including kpgms we use multi dimensional k distance to compare the generated graph to the observed graph and the result show mkpgms are able to produce a closer match to real world graph reduction in k distance while still providing natural variation in the generated graph 
crowdsourcing ha recently become popular among machine learning researcher and social scientist a an effective way to collect large scale experimental data from distributed worker to extract useful information from the cheap but potentially unreliable answer to task a key problem is to identify reliable worker a well a unambiguous task although for objective task that have one correct answer per task previous work can estimate worker reliability and task clarity based on the single gold standard assumption for task that are subjective and accept multiple reasonable answer that worker may be grouped into a phenomenon called school of thought existing model cannot be trivially applied in this work we present a statistical model to estimate worker reliability and task clarity without resorting to the single gold standard assumption this is instantiated by explicitly characterizing the grouping behavior to form school of thought with a rank factorization of a worker task groupsize matrix instead of performing an intermediate inference step which can be expensive and unstable we present an algorithm to analytically compute the size of different group we perform extensive empirical study on real data collected from amazon mechanical turk our method discovers the school of thought show reasonable estimation of worker reliability and task clarity and is robust to hyperparameter change furthermore our estimated worker reliability can be used to improve the gold standard prediction for objective task 
in recent year both hashing based similarity search and multimodal similarity search have aroused much research interest in the data mining and other community while hashing based similarity search seek to address the scalability issue multimodal similarity search deal with application in which data of multiple modality are available in this paper our goal is to address both issue simultaneously we propose a probabilistic model called multimodal latent binary embedding mlbe to learn hash function from multimodal data automatically mlbe regard the binary latent factor a hash code in a common hamming space given data from multiple modality we devise an efficient algorithm for the learning of binary latent factor which corresponds to hash function learning experimental validation of mlbe ha been conducted using both synthetic data and two realistic data set experimental result show that mlbe compare favorably with two state of the art model 
display advertising ha been a significant source of revenue for publisher and ad network in online advertising ecosystem one important business model in online display advertising is ad exchange marketplace also called non guaranteed delivery ngd in which advertiser buy targeted page view and audience on a spot market through real time auction in this paper we describe a bid landscape forecasting system in ngd marketplace for any advertiser campaign specified by a variety of targeting attribute in the system the impression that satisfy the campaign targeting attribute are partitioned into multiple mutually exclusive sample each sample is one unique combination of quantified attribute value we develop a divide and conquer approach that break down the campaign level forecasting problem first utilizing a novel star tree data structure we forecast the bid for each sample using non linear regression by gradient boosting decision tree then we employ a mixture of log normal model to generate campaign level bid distribution based on the sample level forecasted distribution the experiment result of a system developed with our approach show that it can accurately forecast the bid distribution for various campaign running on the world s largest ngd advertising exchange system outperforming two baseline method in term of forecasting error 
commercial building are significant consumer of electricity the first step towards better energy management in commercial building is monitoring consumption however instrumenting every electrical panel in a large commercial building is expensive and wasteful in this paper we propose a greedy meter sensor placement algorithm based on maximization of information gained subject to a cost constraint the algorithm provides a near optimal solution guarantee furthermore to identify power saving opportunity we use an unsupervised anomaly detection technique based on a low dimensional embedding further to better manage resource such a lighting and hvac we propose a semi supervised approach combining hidden markov model hmm and a standard classifier to model occupancy based on readily available port level network statistic 
the area of constrained clustering ha been actively pursued for the last decade a more recent extension that will be the focus of this paper is constrained hierarchical clustering which allows building user constrained dendrograms tree like all form of constrained clustering previous work on hierarchical constrained clustering us simple constraint that are typically implemented in a procedural language however there exists mature result and package in the field of constraint satisfaction language and solver that the constrained clustering field ha yet to explore this work mark the first step towards introducing constraint satisfaction language solver into hierarchical constrained clustering we make several significant contribution we show how many existing and new constraint for hierarchical clustering can be modeled a a horn sat problem that is easily solvable in polynomial time and which allows their implementation in any number of declarative language or efficient solver we implement our own solver for efficiency reason we then show how to formulate constrained hierarchical clustering in a flexible manner so that any number of algorithm whose output is a dendrogram can make use of the constraint 
disambiguating entity reference by annotating them with unique id from a catalog is a critical step in the enrichment of unstructured content in this paper we show that topic model such a latent dirichlet allocation lda and it hierarchical variant form a natural class of model for learning accurate entity disambiguation model from crowd sourced knowledge base such a wikipedia our main contribution is a semi supervised hierarchical model called wikipedia based pachinko allocation model wpam that exploit all word in the wikipedia corpus to learn word entity association unlike existing approach that only use word in a small fixed window around annotated entity reference in wikipedia page wikipedia annotation to appropriately bias the assignment of entity label to annotated and co occurring unannotated word during model learning and wikipedia s category hierarchy to capture co occurrence pattern among entity we also propose a scheme for pruning spurious node from wikipedia s crowd sourced category hierarchy in our experiment with multiple real life datasets we show that wpam outperforms state of the art baseline by a much a in term of disambiguation accuracy 
trust network where people leave trust and distrust feedback are becoming increasingly common these network may be regarded a signed graph where a positive edge weight capture the degree of trust while a negative edge weight capture the degree of distrust analysis of such signed network ha become an increasingly important research topic one important analysis task is that of sign inference i e infer unknown or future trust or distrust relationship given a partially observed signed network most state of the art approach consider the notion of structural balance in signed network building inference algorithm based on information about link triad and cycle in the network in this paper we first show that the notion of weak structural balance in signed network naturally lead to a global low rank model for the network under such a model the sign inference problem can be formulated a a low rank matrix completion problem we show that we can perfectly recover missing relationship under certain condition using state of the art matrix completion algorithm we also propose the use of a low rank matrix factorization approach with generalized loss function a a practical method for sign inference this approach yield high accuracy while being scalable to large signed network for instance we show that this analysis can be performed on a synthetic graph with million node and million edge in minute we further show that the low rank model can be used for other analysis task on signed network such a user segmentation through signed graph clustering with theoretical guarantee experiment on synthetic a well a real data show that our low rank model substantially improves accuracy of sign inference a well a clustering a an example on the largest real dataset available to u epinions data with k node and k edge our matrix factorization approach yield accuracy on the sign inference task a compared to accuracy using a state of the art cycle based method moreover our method run in second a compared to second for the cycle based method 
standard generalized additive model gam usually model the dependent variable a a sum of univariate model although previous study have shown that standard gam can be interpreted by user their accuracy is significantly le than more complex model that permit interaction in this paper we suggest adding selected term of interacting pair of feature to standard gam the resulting model which we call ga m model for generalized additive model plus interaction consist of univariate term and a small number of pairwise interaction term since these model only include oneand two dimensional component the component of ga m model can be visualized and interpreted by user to explore the huge quadratic number of pair of feature we develop a novel computationally efficient method called fast for ranking all possible pair of feature a candidate for inclusion into the model in a large scale empirical study we show the effectiveness of fast in ranking candidate pair of feature in addition we show the surprising result that ga m model have almost the same performance a the best full complexity model on a number of real datasets thus this paper postulate that for many problem ga m model can yield model that are both intelligible and accurate 
user daily activity such a dining and shopping inherently reflect their habit intent and preference thus provide invaluable information for service such a personalized information recommendation and targeted advertising user activity information although ubiquitous on social medium ha largely been unexploited this paper address the task of user activity classification in microblogs where user can publish short message and maintain social network online we identify the importance of modeling a user s individuality and that of exploiting opinion of the user s friend for accurate activity classification in this light we propose a novel collaborative boosting framework comprising a text to activity classifier for each user and a mechanism for collaboration between classifier of user having social connection the collaboration between two classifier includes exchanging their own training instance and their dynamically changing labeling decision we propose an iterative learning procedure that is formulated a gradient descent in learning function space while opinion exchange between classifier is implemented with a weighted voting in each learning iteration we show through experiment that on real world data from sina weibo our method outperforms existing off the shelf algorithm that do not take user individuality or social connection into account 
most existing approach for non parametric kernel learning npkl suffer from expensive computation which would limit their application to large scale problem to address the scalability problem of npkl we propose a novel algorithm called bcdnpkl which is very efficient and scalable superior to most existing approach bcdnpkl keep away from semidefinite programming sdp and eigen decomposition which benefit from two finding the original sdp framework of npkl can be reduced into a far smaller sized counterpart which is corresponding to the sub kernel referred to a boundary kernel learning the sub kernel learning can be efficiently solved by using the proposed block coordinate descent bcd technique we provide a formal proof of global convergence for our bcdnpkl algorithm the extensive experiment verify the scalability and effectiveness of bcdnpkl compared with the state of the art algorithm copyright by the author s owner s 
precise click prediction is one of the key component in the sponsored search system previous study usually took advantage of two major kind of information for click prediction i e relevance information representing the similarity between ad and query and historical click through information representing user previous preference on the ad these existing work mainly focused on interpreting ad click in term of what user seek i e relevance information and how user choose to click historically clicked through information however few of them attempted to understand why user click the ad in this paper we aim at answering this why question in our opinion user click those ad that can convince them to take further action and the critical factor is if those ad can trigger user desire in their heart our data analysis on a commercial search engine reveals that specific text pattern e g official site x off and guaranteed return in x day are very effective in triggering user desire and therefore lead to significant difference in term of click through rate ctr these observation motivate u to systematically model user psychological desire in order for a precise prediction on ad click to this end we propose modeling user psychological desire in sponsored search according to maslow s desire theory which categorizes psychological desire into five level and each one is represented by a set of textual pattern automatically mined from ad text we then construct novel feature for both ad and user based on our definition on psychological desire and incorporate them into the learning framework of click prediction large scale evaluation on the click through log from a commercial search engine demonstrate that this approach can result in significant improvement in term of click prediction accuracy for both the ad with rich historical data and those with rare one further analysis reveals that specific pattern combination are especially effective in driving click through rate which provides a good guideline for advertiser to improve their ad textual description 
a high quality hierarchical organization of the concept in a dataset at different level of granularity ha many valuable application such a search summarization and content browsing in this paper we propose an algorithm for recursively constructing a hierarchy of topic from a collection of content representative document we characterize each topic in the hierarchy by an integrated ranked list of mixed length phrase our mining framework is based on a phrase centric view for clustering extracting and ranking topical phrase experiment with datasets from three different domain illustrate our ability to generate hierarchy of high quality topic represented by meaningful phrase 
graph ranking play an important role in many application such a page ranking on web graph and entity ranking on social network in application besides graph structure rich information on node and edge and explicit or implicit human supervision are often available in contrast conventional algorithm e g pagerank and hit compute ranking score by only resorting to graph structure information a natural question arises here that is how to effectively and efficiently leverage all the information to more accurately calculate graph ranking score than the conventional algorithm assuming that the graph is also very large previous work only partially tackled the problem and the proposed solution are also not satisfying this paper address the problem and proposes a general framework a well a an efficient algorithm for graph ranking specifically we define a semi supervised learning framework for ranking of node on a very large graph and derive within our proposed framework an efficient algorithm called semi supervised pagerank in the algorithm the objective function is defined based upon a markov random walk on the graph the transition probability and the reset probability of the markov model are defined a parametric model based on feature on node and edge by minimizing the objective function subject to a number of constraint derived from supervision information we simultaneously learn the optimal parameter of the model and the optimal ranking score of the node finally we show that it is possible to make the algorithm efficient to handle a billion node graph by taking advantage of the sparsity of the graph and implement it in the mapreduce logic experiment on real data from a commercial search engine show that the proposed algorithm can outperform previous algorithm on several task 
high dimensional similarity search in large scale database becomes an important challenge due to the advent of internet for such application specialized data structure are required to achieve computational efficiency traditional approach relied on algorithmic construction that are often data independent such a locality sensitive hashing or weakly dependent such a kd tree mean tree while supervised learning algorithm have been applied to related problem those proposed in the literature mainly focused on learning hash code optimized for compact embedding of the data rather than search efficiency consequently such an embedding ha to be used with linear scan or another search algorithm hence learning to hash doe not directly address the search efficiency issue this paper considers a new framework that applies supervised learning to directly optimize a data structure that support efficient large scale search our approach take both search quality and computational cost into consideration specifically we learn a boosted search forest that is optimized using pair wise similarity labeled example the output of this search forest can be efficiently converted into an inverted indexing data structure which can leverage modern text search infrastructure to achieve both scalability and efficiency experimental result show that our approach significantly outperforms the start of the art learning to hash method such a spectral hashing a well a state of the art high dimensional search algorithm such a lsh and mean tree 
given an unknown set of object embedded in the euclidean plane and a nearest neighbor oracle how to estimate the set size and other property of the object in this paper we address this problem we propose an efficient method that us the voronoi partitioning of the space by the object and a nearest neighbor oracle our method can be used in the hidden web database context where the goal is to estimate the number of certain object of interest here we assume that each object ha a geographic location and the nearest neighbor oracle can be realized by application such a map local or store locator apis we illustrate the performance of our method on several real world datasets 
we study pool based active learning of half space we revisit the aggressive approach for active learning in the realizable case and show that it can be made efficient and practical while also having theoretical guarantee under reasonable assumption we further show both theoretically and experimentally that it can be preferable to mellow approach our efficient aggressive active learner of half space ha formal approximation guarantee that hold when the pool is separable with a margin while our analysis is focused on the realizable setting we show that a simple heuristic allows using the same algorithm successfully for pool with low error a well we further compare the aggressive approach to the mellow approach and prove that there are case in which the aggressive approach result in significantly better label complexity compared to the mellow approach we demonstrate experimentally that substantial improvement in label complexity can be achieved using the aggressive approach for both realizable and low error setting 
recent year have witnessed the explosive growth of online social medium weibo a famous chinese twitter ha attracted over billion user in le than four year with more than tweet generated in every second these tweet are informative but very fragmented and thus would be better archived from an event perspective a done by weibo itself in the micro topic program this effort however is yet far from satisfaction for not providing enough analytical power to event in light of this in this demo paper we propose sea a system for event analysis on chinese tweet in general sea is an event centric multi functional platform that conduct panoramic analysis on weibo event from various aspect including the semantic information of the event the temporal and spatial trend the public sentiment the hidden sub event the key user in the event diffusion and their preference etc these function are enabled by the integration of various analytical model and by the nosql technique adopted purposefully for massive tweet management finally a case study on the spring festival event demonstrates the effectiveness of sea to our best knowledge sea is the first third party system that provides panoramic analysis to weibo event 
we address the problem of predicting new drug target interaction from three input known interaction similarity over drug and those over target this setting ha been considered by many method which however have a common problem of allowing to have only one similarity matrix over drug and that over target the key idea of our approach is to use more than one similarity matrix over drug a well a those over target where weight over the multiple similarity matrix are estimated from data to automatically select similarity which are effective for improving the performance of predicting drug target interaction we propose a factor model named multiple similarity collaborative matrix factorization mscmf which project drug and target into a common low rank feature space which is further consistent with weighted similarity matrix over drug and those over target these two low rank matrix and weight over similarity matrix are estimated by an alternating least square algorithm our approach allows to predict drug target interaction by the two low rank matrix collaboratively and to detect similarity which are important for predicting drug target interaction this approach is general and applicable to any binary relation with similarity over element being found in many application such a recommender system in fact mscmf is an extension of weighted low rank approximation for one class collaborative filtering we extensively evaluated the performance of mscmf by using both synthetic and real datasets experimental result showed nice property of mscmf on selecting similarity useful in improving the predictive performance and the performance advantage of mscmf over six state of the art method for predicting drug target interaction 
we propose a parameter server system for distributed ml which follows a stale synchronous parallel ssp model of computation that maximizes the time computational worker spend doing useful work on ml algorithm while still providing correctness guarantee the parameter server provides an easy to use shared interface for read write access to an ml model s value parameter and variable and the ssp model allows distributed worker to read older stale version of these value from a local cache instead of waiting to get them from a central storage this significantly increase the proportion of time worker spend computing a opposed to waiting furthermore the ssp model ensures ml algorithm correctness by limiting the maximum age of the stale value we provide a proof of correctness under ssp a well a empirical result demonstrating that the ssp model achieves faster algorithm convergence on several different ml problem compared to fully synchronous and asynchronous scheme 
we present novel efficient model based kernel for time series data rooted in the reservoir computation framework the kernel are implemented by fitting reservoir model sharing the same fixed deterministically constructed state transition part to individual time series the proposed kernel can naturally handle time series of different length without the need to specify a parametric model class for the time series compared with most time series kernel our kernel are computationally efficient we show how the model distance used in the kernel can be calculated analytically or efficiently estimated the experimental result on synthetic and benchmark time series classification task confirm the efficiency of the proposed kernel in term of both generalization accuracy and computational speed this paper also investigates on line reservoir kernel construction for extremely long time series 
time series classification ha been an active area of research in the data mining community for over a decade and significant progress ha been made in the tractability and accuracy of learning however virtually all work assumes a one time training session in which labeled example of all the concept to be learned are provided this assumption may be valid in a handful of situation but it doe not hold in most medical and scientific application where we initially may have only the vaguest understanding of what concept can be learned based on this observation we propose a never ending learning framework for time series in which an agent examines an unbounded stream of data and occasionally asks a teacher which may be a human or an algorithm for a label we demonstrate the utility of our idea with experiment in domain a diverse a medicine entomology wildlife monitoring and human behavior analysis 
this paper study the problem of prominent streak discovery in sequence data given a sequence of value a prominent streak is a long consecutive subsequence consisting of only large small value for finding prominent streak we make the observation that prominent streak are skyline point in two dimensionsstreak interval length and minimum value in the interval our solution thus hinge upon the idea to separate the two step in prominent streak discovery candidate streak generation and skyline operation over candidate streak for candidate generation we propose the concept of local prominent streak lp we prove that prominent streak are a subset of lp and the number of lp is le than the length of a data sequence in comparison with the quadratic number of candidate produced by a brute force baseline method we develop efficient algorithm based on the concept of lp the non linear lp based method nlp considers a superset of lp a candidate and the linear lp based method llps further guarantee to consider only lp the result of experiment using multiple real datasets verified the effectiveness of the proposed method and showed order of magnitude performance improvement against the baseline method 
challenge faced in organizing impromptu activity are the requirement of making timely invitation in accordance with the location of candidate attendee and the social relationship among them it is desirable to find a group of attendee close to a rally point and ensure that the selected attendee have a good social relationship to create a good atmosphere in the activity therefore this paper proposes socio spatial group query ssgq to select a group of nearby attendee with tight social relation efficient processing of ssgq is very challenging due to the tradeoff in the spatial and social domain we show that the problem is np hard via a proof and design an efficient algorithm ssgselect which includes effective pruning technique to reduce the running time for finding the optimal solution we also propose a new index structure social r tree to further improve the efficiency user study and experimental result demonstrate that ssgselect significantly outperforms manual coordination in both solution quality and efficiency 
the junction tree approach with application in artificial intelligence computer vision machine learning and statistic is often used for computing posterior distribution in probabilistic graphical model one of the key challenge associated with junction tree is computational and several parallel computing technology including many core processor have been investigated to meet this challenge many core processor including gpus are now programmable unfortunately their complexity make it hard to manually tune their parameter in order to optimize software performance in this paper we investigate a machine learning approach to minimize the execution time of parallel junction tree algorithm implemented on a gpu by carefully allocating a gpu s thread to different parallel computing opportunity in a junction tree and treating this thread allocation problem a a machine learning problem we find in experiment that regression specifically support vector regression can substantially outperform manual optimization 
convex optimization ha emerged a useful tool for application that include data analysis and model fitting resource allocation engineering design network design and optimization finance and control and signal processing after an overview the talk will focus on two extreme real time embedded convex optimization and distributed convex optimization code generation can be used to generate extremely efficient and reliable solver for small problem that can execute in millisecond or microsecond and are ideal for embedding in real time system at the other extreme we describe method for large scale distributed optimization which coordinate many solver to solve enormous problem 
malicious uniform resource locator url detection is an important problem in web search and mining which play a critical role in internet security in literature many existing study have attempted to formulate the problem a a regular supervised binary classification task which typically aim to optimize the prediction accuracy however in a real world malicious url detection task the ratio between the number of malicious url and legitimate url is highly imbalanced making it very inappropriate for simply optimizing the prediction accuracy besides another key limitation of the existing work is to assume a large amount of training data is available which is impractical a the human labeling cost could be potentially quite expensive to solve these issue in this paper we present a novel framework of cost sensitive online active learning csoal which only query a small fraction of training data for labeling and directly optimizes two cost sensitive measure to address the class imbalance issue in particular we propose two csoal algorithm and analyze their theoretical performance in term of cost sensitive bound we conduct an extensive set of experiment to examine the empirical performance of the proposed algorithm for a large scale challenging malicious url detection task in which the encouraging result showed that the proposed technique by querying an extremely small sized labeled data about out of million instance can achieve better or highly comparable classification performance in comparison to the state of the art cost insensitive and cost sensitive online classification algorithm using a huge amount of labeled data 
many real world graph have complex label on the node and edge mining only exact pattern yield limited insight since it may be hard to find exact match however in many domain it is relatively easy to define a cost or distance between different label using this information it becomes possible to mine a much richer set of approximate subgraph pattern which preserve the topology but allow bounded label mismatch we present novel and scalable method to efficiently solve the approximate isomorphism problem we show that approximate mining yield interesting pattern in several real world graph ranging from it and protein interaction network to protein structure 
traditional co clustering method identify block structure from static data matrix however the data matrix in many application are dynamic that is they evolve smoothly over time consequently the hidden block structure embedded into the matrix are also expected to vary smoothly along the temporal dimension it is therefore desirable to encourage smoothness between the block structure identified from temporally adjacent data matrix in this paper we propose an evolutionary co clustering formulation for identifying co cluster structure from time varying data the proposed formulation encourages smoothness between temporally adjacent block by employing the fused lasso type of regularization our formulation is very flexible and allows for imposing smoothness constraint over only one dimension of the data matrix thereby enabling it applicability to a large variety of setting the optimization problem for the proposed formulation is non convex non smooth and non separable we develop an iterative procedure to compute the solution each step of the iterative procedure involves a convex but non smooth and non separable problem we propose to solve this problem in it dual form which is convex and smooth this lead to a simple gradient descent algorithm for computing the dual optimal solution we evaluate the proposed formulation using the allen developing mouse brain atlas data result show that our formulation consistently outperforms method without the temporal smoothness constraint 
frequent episode mining fem is an interesting research topic in data mining with wide range of application however the traditional framework of fem treat all event a having the same importance utility and assumes that a same type of event appears at most once at any time point these simplifying assumption do not reflect the characteristic of scenario in real application and thus the useful information of episode in term of utility such a profit is lost furthermore most study on fem focused on mining episode in simple event sequence and few considered the scenario of complex event sequence where different event can occur simultaneously to address these issue in this paper we incorporate the concept of utility into episode mining and address a new problem of mining high utility episode from complex event sequence which ha not been explored so far in the proposed framework the importance utility of different event is considered and multiple event can appear simultaneously several novel feature are incorporated into the proposed framework to resolve the challenge raised by this new problem such a the absence of anti monotone property and the huge set of candidate episode moreover an efficient algorithm named up span utility episode mining by spanning prefix is proposed for mining high utility episode with several strategy incorporated for pruning the search space to achieve high efficiency experimental result on real and synthetic datasets show that up span ha excellent performance and serf a an effective solution to the new problem of mining high utility episode from complex event sequence 
active learning method are used to improve the classification accuracy when little labeled data is available most traditional active learning method pose a very specific query to the oracle i e they ask for the label of an unlabeled example this paper proposes a novel active learning method called riqy rule induced active learning query it can construct generic active learning query based on rule induction from multiple unlabeled instance these query are shorter and more readable for the oracle and encompass many similar case also the learning algorithm can achieve higher accuracy rate by asking fewer query we evaluate our algorithm on different real datasets our result show that we can achieve higher accuracy rate using fewer query compared to the traditional active learning method 
incomplete data present serious problem when integrating largescale brain imaging data set from different imaging modality in the alzheimer s disease neuroimaging initiative adni for example over half of the subject lack cerebrospinal fluid csf measurement an independent half of the subject do not have fluorodeoxyglucose positron emission tomography fdg pet scan many lack proteomics measurement traditionally subject with missing measure are discarded resulting in a severe loss of available information we address this problem by proposing two novel learning method where all the sample with at least one available data source can be used in the first method we divide our sample according to the availability of data source and we learn shared set of feature with state of the art sparse learning method our second method learns a base classifier for each data source independently based on which we represent each source using a single column of prediction score we then estimate the missing prediction score which combined with the existing prediction score are used to build a multi source fusion model to illustrate the proposed approach we classify patient from the adni study into group with alzheimer s disease ad mild cognitive impairment mci and normal control based on the multi modality data at baseline adni s participant ad mci normal have at least one of four data type magnetic resonance imaging mri fdg pet csf and proteomics these data are used to test our algorithm comprehensive experiment show that our proposed method yield stable and promising result 
a the use of mobile device increase a location based service lb becomes increasingly popular because it provides more convenient context aware service however lb introduces problematic issue for location privacy due to the nature of the service location privacy protection method based on k anonymity and l diversity have been proposed to provide anonymized use of lb however the k anonymity and l diversity method still can endanger the user s privacy because location semantic information could easily be breached while using lb this paper present a novel location privacy protection technique which protects the location semantics from an adversary in our scheme location semantics are first learned from location data then the trusted anonymization server performs the anonymization using the location semantic information by cloaking with semantically heterogeneous location thus the location semantic information is kept secure a the cloaking is done with semantically heterogeneous location and the true location information is not delivered to the lb application this paper proposes algorithm for learning location semantics and achieving semantically secure cloaking 
understanding measuring and leveraging the similarity of binary executable code is a foundational challenge in software engineering we present a notion of similarity based on provenance two binary are similar if they are compiled from the same or very similar source code with the same or similar compiler empirical evidence suggests that provenance similarity account for a significant portion of variation in existing binary particularly in malware we propose and evaluate the applicability of classification to detect provenance similarity we evaluate a variety of classifier and different type of attribute and similarity labeling scheme on two benchmark derived from open source software and malware respectively we present encouraging result indicating that classification is a viable approach for automated provenance similarity detection and a an aid for malware analyst in particular 
a a well known semantic repository wordnet is widely used in many application however due to costly edit and maintenance wordnet s capability of keeping up with the emergence of new concept is poor compared with on line encyclopedia such a wikipedia to keep wordnet current with folk wisdom we propose a method to enhance wordnet automatically by merging wikipedia entity into wordnet and construct an enriched ontology named a workinet workinet keep the desirable structure of wordnet at the same time it capture abundant information from wikipedia we also propose a learning approach which is able to generate a tailor made semantic concept collection for a given document collection the learning process take the characteristic of the given document collection into consideration and the semantic concept in the tailor made collection can be used a new feature for document representation the experimental result show that the adaptively generated feature space can outperform a static one significantly in text mining task and workinet dominates wordnet most of the time due to it high coverage 
transfer learning ha been proposed to address the problem of scarcity of labeled data in the target domain by leveraging the data from the source domain in many real world application data is often represented from different perspective which correspond to multiple view for example a web page can be described by it content and it associated link however most existing transfer learning method fail to capture the multi view nature and might not be best suited for such application to better leverage both the labeled data from the source domain and the feature from different view this paper proposes a general framework multi view transfer learning with a large margin approach mvtl lm on one hand labeled data from the source domain is effectively utilized to construct a large margin classifier on the other hand the data from both domain is employed to impose consistency among multiple view a an instantiation of this framework we propose an efficient optimization method which is guaranteed to converge to precision in o step furthermore we analyze it error bound which improves over existing result of related method an extensive set of experiment are conducted to demonstrate the advantage of our proposed method over state of the art technique 
with the wide deployment of smart card automated fare collection scafc system public transit agency have been benefiting from huge volume of transit data a kind of sequential data collected every day yet improper publishing and use of transit data could jeopardize passenger privacy in this paper we present our solution to transit data publication under the rigorous differential privacy model for the soci t de transport de montr al stm we propose an efficient data dependent yet differentially private transit data sanitization approach based on a hybrid granularity prefix tree structure moreover a a post processing step we make use of the inherent consistency constraint of a prefix tree to conduct constrained inference which lead to better utility our solution not only applies to general sequential data but also can be seamlessly extended to trajectory data to our best knowledge this is the first paper to introduce a practical solution for publishing large volume of sequential data under differential privacy we examine data utility in term of two popular data analysis task conducted at the stm namely count query and frequent sequential pattern mining extensive experiment on real life stm datasets confirm that our approach maintains high utility and is scalable to large datasets 
an ever growing amount of information relevant for early detection of certain threat can be extracted from on line news this led to an emergence of news mining tool to help analyst to digest the overflow of information and to extract valuable knowledge from on line news source this paper give an overview of the fully operational real time news event extraction framework developed for frontex the eu border agency to facilitate the process of extracting structured information on border security related event from on line news in particular a hybrid event extraction system ha been constructed which is applied to the stream of news article continuously gathered and pre processed by the europe medium monitor a large scale multilingual news aggregation engine the framework consists also of an earth browser in which event are visualized and an event moderation tool which allows to access the database of automatically extracted event description and to clean validate group enhance and export them into other knowledge repository 
the development of disastrous flood forecasting technique able to provide warning at a long lead time day is of great importance to society extreme flood is usually a consequence of a sequence of precipitation event occurring over from several day to several week though precise short term forecasting the magnitude and extent of individual precipitation event is still beyond our reach long term forecasting of precipitation cluster can be attempted by identifying persistent atmospheric regime that are conducive for the precipitation cluster however such forecasting will suffer from overwhelming number of relevant feature and high imbalance of sample set in this paper we propose an integrated data mining framework for identifying the precursor to precipitation event cluster and use this information to predict extended period of extreme precipitation and subsequent flood we synthesize a representative feature set that describes the atmosphere motion and apply a streaming feature selection algorithm to online identify the precipitation precursor from the enormous feature space a hierarchical re sampling approach is embedded in the framework to deal with the imbalance problem an extensive empirical study is conducted on historical precipitation and associated flood data collected in the state of iowa utilizing our framework a few physically meaningful precipitation cluster precursor set are identified from million of feature more than of extreme precipitation event are captured by the proposed prediction model using precipitation cluster precursor with a lead time of more than day 
spatial analysis of disease risk or disease mapping typically relies on information about the residence and health status of individual from population under study however residence information ha it limitation because people are exposed to numerous disease risk a they spend time outside of their residence thanks to the wide spread use of mobile phone and gps enabled device it is becoming possible to obtain a detailed record about the movement of human population availability of movement information open up an opportunity to improve the accuracy of disease mapping starting with an assumption that an individual s disease risk is a weighted average of risk at the location which were visited we show that disease mapping can be accomplished by spatially regularized logistic regression due to the inherent sparsity of movement data the proposed approach can be applied to large population and over large spatial grid in our experiment we were able to map disease for a simulated population with million people and a spatial grid with thousand location in several minute the result indicate that movement information can improve the accuracy of disease mapping a compared to residential data only we also studied a privacy preserving scenario in which only the aggregate statistic are available about the movement of the overall population while detailed movement information is available only for individual with disease the result indicate that the accuracy of disease mapping remains satisfactory when learning from movement data sanitized in this way 
we present tourviz a system that help it user to interactively visualize and make sense in large network datasets in particular it take a input a set of node the user specifies a of interest and present the user with a visualization of connection subgraphs around these input node each connection subgraph contains good pathway that highlight succinct connection among a close by group of input node tourviz combine visualization with rich user interaction to engage and help the user to further understand the relation among the node of interest by exploring their neighborhood on demand a well a modifying the set of interest node we demonstrate tourviz s usage and benefit using the dblp graph consisting of author and their co authorship relation while our system is designed generally to work with any kind of graph data we will invite the audience to experiment with our system and comment on it usability usefulness and how our system can help with their research and improve the understanding of data in other domain 
the popularity of collaborative tagging site ha created new challenge and opportunity for designer of web item such a electronics product travel itinerary popular blog etc an increasing number of people are turning to online review and user specified tag to choose from among competing item this creates an opportunity for designer to build item that are likely to attract desirable tag when published in this paper we consider a novel optimization problem given a training dataset of existing item with their user submitted tag and a query set of desirable tag design the k best new item expected to attract the maximum number of desirable tag we show that this problem is np complete even if simple naive bayes classifier are used for tag prediction we present two principled algorithm for solving this problem a an exact two tier algorithm based on top k querying technique which performs much better than the naive brute force algorithm and work well for moderate problem instance and b a novel polynomial time approximation algorithm with provable error bound for larger problem instance we conduct detailed experiment on synthetic and real data crawled from the web to evaluate the efficiency and quality of our proposed algorithm 
it ha been recently recognized that heterogeneous information network composed of multiple type of node and link are prevalent in the real world both classification and ranking of the node or data object in such network are essential for network analysis however so far these approach have generally been performed separately in this paper we combine ranking and classification in order to perform more accurate analysis of a heterogeneous information network our intuition is that highly ranked object within a class should play more important role in classification on the other hand class membership information is important for determining a quality ranking over a dataset we believe it is therefore beneficial to integrate classification and ranking in a simultaneous mutually enhancing process and to this end propose a novel ranking based iterative classification framework called rankclass specifically we build a graph based ranking model to iteratively compute the ranking distribution of the object within each class at each iteration according to the current ranking result the graph structure used in the ranking algorithm is adjusted so that the sub network corresponding to the specific class is emphasized while the rest of the network is weakened a our experiment show integrating ranking with classification not only generates more accurate class than the state of art classification method on networked data but also provides meaningful ranking of object within each class serving a a more informative view of the data than traditional classification 
even though human movement and mobility pattern have a high degree of freedom and variation they also exhibit structural pattern due to geographic and social constraint using cell phone location data a well a data from two online location based social network we aim to understand what basic law govern human motion and dynamic we find that human experience a combination of periodic movement that is geographically limited and seemingly random jump correlated with their social network short ranged travel is periodic both spatially and temporally and not effected by the social network structure while long distance travel is more influenced by social network tie we show that social relationship can explain about to of all human movement while periodic behavior explains to based on our finding we develop a model of human mobility that combine periodic short range movement with travel due to the social network structure we show that our model reliably predicts the location and dynamic of future human movement and give an order of magnitude better performance than present model of human mobility 
due to the ever growing presence of data stream there ha been a considerable amount of research on stream mining algorithm while many algorithm have been introduced that tackle the problem of clustering on evolving data stream hardly any attention ha been paid to appropriate evaluation measure measure developed for static scenario namely structural measure and ground truth based measure cannot correctly reflect error attributable to emerging splitting or moving cluster these situation are inherent to the streaming context due to the dynamic change in the data distribution in this paper we develop a novel evaluation measure for stream clustering called cluster mapping measure cmm cmm effectively indicates different type of error by taking the important property of evolving data stream into account we show in extensive experiment on real and synthetic data that cmm is a robust measure for stream clustering evaluation 
digital data explosion mandate the development of scalable tool to organize the data in a meaningful and easily accessible form clustering is a commonly used tool for data organization however many clustering algorithm designed to handle large data set assume linear separability of data and hence do not perform well on real world data set while kernel based clustering algorithm can capture the non linear structure in data they do not scale well in term of speed and memory requirement when the number of object to be clustered exceeds ten of thousand we propose an approximation scheme for kernel k mean termed approximate kernel k mean that reduces both the computational complexity and the memory requirement by employing a randomized approach we show both analytically and empirically that the performance of approximate kernel k mean is similar to that of the kernel k mean algorithm but with dramatically reduced run time complexity and memory requirement 
we demonstrate a system that support the visual exploration of collaboration network the system leverage the notion of fractional core introduced in earlier work to rank vertex in a collaboration network and filter vertex neighborhood fractional core build on the idea of graph degeneracy a captured by the notion of k core in graph theory and extend it to undirected edge weighted graph in a co authorship network for instance the fractional core index of an author intuitively reflects the degree of collaboration with equally or higher ranked author our system ha been deployed on a real world co authorship network derived from dblp demonstrating that the idea of fractional core can be applied even to large scale network the system provides an easy to use interface to query for the fractional core index of an author to see who the closest equally or higher ranked co author are and explore the entire co authorship network in an incremental manner 
online advertising is becoming more and more performance oriented where the decision to show an advertisement to a user is made based on the user s propensity to respond to the ad in a positive manner e g purchasing a product subscribing to an email list the user response depends on how well the ad campaign match to the user s interest a well a the amount of user s past exposure to the campaign a factor shown to be impactful in controlled experimental study past exposure build brand awareness and familiarity with the user which in turn lead to a higher propensity of the user to buy convert on the ad impression in this paper we propose a model of the user response to an ad campaign a a function of both the interest match and the past exposure where the interest match is estimated using historical search browse activity of the user the goal of this paper is two fold first we demonstrate the role played by the user interest and the past exposure in modeling user response by jointly estimating the parameter of these factor we test this response model over hundred of real ad campaign second we use the finding from this joint model to identify more relevant target user for ad campaign in particular we show that on real advertising data this model combine past exposure together with the user profile to identify better target user over the conventional targeting model 
micro blogging service such a twitter and location based social network application have generated short text message associated with geographic information posting time and user id the availability of such data received from user offer a good opportunity to study the user s spatial temporal behavior and preference in this paper we propose a probabilistic model w short for who where when what to exploit such data to discover individual user mobility behavior from spatial temporal and activity aspect to the best of our knowledge our work offer the first solution to jointly model individual user s mobility behavior from the three aspect our model ha a variety of application such a user profiling and location prediction it can be employed to answer question such a can we infer the location of a user given a tweet posted by the user and the posting time experimental result on two real world datasets show that the proposed model is effective in discovering user spatial temporal topic and outperforms state of the art baseline significantly for the task of location prediction for tweet 
collaborative prediction is a powerful technique useful in domain from recommender system to guiding the scientific discovery process low rank matrix factorization is one of the most powerful tool for collaborative prediction this work present a general approach for active collaborative prediction with the probabilistic matrix factorization model using variational approximation or markov chain monte carlo sampling to estimate the posterior distribution over model we can choose query point to maximize our understanding of the model to best predict unknown element of the data matrix or to find a many positive data point a possible we evaluate our method on simulated data and also show their applicability to movie rating prediction and the discovery of drug target interaction 
the canonical technique for analyzing functional magnetic resonance imaging fmri data statistical parametric mapping produce map of brain location that are more active during performance of a task than during a control condition in recent year there ha been increasing awareness of the fact that there is information in the entire pattern of brain activation and not just in saliently active location classifier have been the tool of choice for capturing this information and used to make prediction ranging from what kind of object a subject is thinking about to what decision they will make such classifier are usually trained on a selection of voxels from the d grid that make up the activation pattern often this mean the best accuracy is obtained using few voxels from all across the brain and that different voxels will be chosen in different cross validation fold making the classifier hard to interpret the increasing commonality of datasets with ten to hundred of class make this problem even more acute in this paper we introduce a method for identifying informative subset of adjacent voxels corresponding to brain patch that distinguish subset of class these patch can then be used to train classifier for the distinction they support and used a pattern feature for a meta classifier we show that this method permit classification at a higher accuracy than that obtained with traditional voxel selection and that the set of voxels used are more reproducible across cross validation fold than those identified with voxel selection and lie in plausible brain location 
policy gradient is a useful model free reinforcement learning approach but it tends to suffer from instability of gradient estimate in this paper we analyze and improve the stability of policy gradient method we first prove that the variance of gradient estimate in the pgpe policy gradient with parameter based exploration method is smaller than that of the classical reinforce method under a mild assumption we then derive the optimal baseline for pgpe which contributes to further reducing the variance we also theoretically show that pgpe with the optimal baseline is more preferable than reinforce with the optimal baseline in term of the variance of gradient estimate finally we demonstrate the usefulness of the improved pgpe method through experiment 
we address the problem of inferring road map from large scale gps trace that have relatively low resolution and sampling frequency unlike past published work that requires high resolution trace with dense sampling we focus on situation with coarse granularity data such a that obtained from thousand of taxi in shanghai which transmit their location a seldom a once per minute such data source can be made available inexpensively a byproduct of existing process rather than having to drive every road with high quality gps instrumentation just for map building and having to re drive road for periodic update although the challenge in using opportunistic probe data are significant successful mining algorithm could potentially enable the creation of continuously updated map at very low cost in this paper we compare representative algorithm from two approach working with individual reported location v segment between consecutive location we ass their trade offs and effectiveness in both qualitative and quantitative comparison for region of shanghai and chicago 
given a network intuitively two node belong to the same role if they have similar structural behavior role should be automatically determined from the data and could be for example clique member periphery node etc role enable numerous novel and useful network mining task such a sense making searching for similar node and node classification this paper address the question given a graph how can we automatically discover role for node we propose rolx role extraction a scalable linear in the number of edge unsupervised learning approach for automatically extracting structural role from general network data we demonstrate the effectiveness of rolx on several network mining task from exploratory data analysis to network transfer learning moreover we compare network role discovery with network community discovery we highlight fundamental difference between the two e g role generalize across disconnected network community do not and show that the two approach are complimentary in nature 
cascade are ubiquitous in various network environment such a epidemic network traffic network water distribution network and social network the outbreak of cascade will often bring bad or even devastating effect how to accurately predict the cascading outbreak in early stage is of paramount importance for people to avoid these bad effect although there have been some pioneering work on cascading outbreak detection how to predict rather than detect the cascading outbreak is still an open problem in this paper we attempt harnessing historical cascade data propose a novel data driven approach to select important node a sensor and predict the outbreak based on the cascading behavior of these sensor in particular we propose orthogonal sparse logistic regression oslor method to jointly optimize node selection and outbreak prediction where the prediction loss are combined with an orthogonal regularizer and l regularizer to guarantee good prediction accuracy a well a the sparsity and low redundancy of selected sensor we evaluate the proposed method on a real online social network dataset including million information cascade the experimental result show that the proposed oslor significantly and consistently outperform topological measure based method and other data driven method in prediction performance 
this paper report on method and result of an applied research project by a team consisting of saic and four university to develop integrate and evaluate new approach to detect the weak signal characteristic of insider threat on organization information system our system combine structural and semantic information from a real corporate database of monitored activity on their user computer to detect independently developed red team insert of malicious insider activity we have developed and applied multiple algorithm for anomaly detection based on suspected scenario of malicious insider behavior indicator of unusual activity high dimensional statistical pattern temporal sequence and normal graph evolution algorithm and representation for dynamic graph processing provide the ability to scale a needed for enterprise level deployment on real time data stream we have also developed a visual language for specifying combination of feature baseline peer group time period and algorithm to detect anomaly suggestive of instance of insider threat behavior we defined over data feature in seven category based on approximately million action per day from approximately user we have achieved area under the roc curve value of up to and lift value of on the top user day identified on two month of real data 
in recent year information trustworthiness ha become a serious issue when user generated content prevail in our information world in this paper we investigate the important problem of estimating information trustworthiness from the perspective of correlating and comparing multiple data source to a certain extent the consistency degree is an indicator of information reliability information unanimously agreed by all the source is more likely to be reliable based on this principle we develop an effective computational approach to identify consistent information from multiple data source particularly we analyze vast amount of information collected from multiple review platform multiple source in which people can rate and review the item they have purchased the major challenge is that different platform attract diverse set of user and thus information cannot be compared directly at the surface however latent reason hidden in user rating are mostly shared by multiple source and thus inconsistency about an item only appears when some source provides rating deviating from the common latent reason therefore we propose a novel two step procedure to calculate information consistency degree for a set of item which are rated by multiple set of user on different platform we first build a multi source deep belief network msdbn to identify the common reason hidden in multi source rating data and then calculate a consistency score for each item by comparing individual source with the reconstructed data derived from the latent reason we conduct experiment on real user rating collected from orbitz priceline and tripadvisor on all the hotel in la vega and new york city experimental result demonstrate that the proposed approach successfully find the hotel that receive inconsistent and possibly unreliable rating 
in the cyber security domain we have been collecting big data for almost two decade the volume and variety of our data is extremely large but understanding and capturing the semantics of the data is even more of a challenge finding the needle in the proverbial haystack ha been attempted from many different angle in this talk we will have a look at what approach have been explored what ha worked and what ha not we will see that there is still a large amount of work to be done and data mining is going to play a central role we ll try to motivate that in order to successfully find bad guy we will have to embrace a solution that not only leverage clever data mining but employ the right mix between human computer interface data mining and scalable data platform traditionally cyber security ha been having it challenge with data mining we are different we will explore how to adopt data mining algorithm to the security domain some approach like predictive analytics are extremely hard if not impossible how would you predict the next cyber attack others need to be tailored to the security domain to make them work visualization and visual analytics seem to be extremely promising to solve cyber security issue situational awareness large scale data exploration knowledge capture and forensic investigation are four top use case we will discus visualization alone however doe not solve security problem we need algorithm that support the visualization for example to reduce the amount of data so an analyst can deal with it in both volume and semantics 
we present our experience of using machine learning technique over data originating from advanced meter infrastructure ami system for water consumption in a medium size city we focus on two new use case that are of special importance to city authority one use case is the automatic identification of malfunctioning meter with a focus on distinguishing them from legitimate non consumption such a during period when the household resident are on vacation the other use case is the identification of leak or theft in the unmetered common area of apartment building these two use case are highly important to city authority both because of the lost revenue they imply and because of the hassle to the resident in case of delayed identification both case are inherently complex to analyze and require advanced data mining technique in order to achieve high level of correct identification our result provide for faster and more accurate detection of malfunctioning meter a well a leak in the common area this result in significant tangible value to the authority in term of increase in technician efficiency and a decrease in the amount of wasted non revenue water 
biased and unbiased approach to develop predictive biomarkers of response to drug treatment will be introduced and their utility demonstrated for cell cycle inhibitor opportunity to leverage the growing knowledge of tumor characterized by modern method to measure dna and rna will be shown including the use of appropriate preclinical model and selection of patient furthermore technique to identify mechanism of resistance prior to clinical treatment will be discussed prospect for systematic data mining and current barrier to the application of precision medicine in cancer will be reviewed along with potential solution 
with the rapid development of advanced data acquisition technique such a high throughput biological experiment and wireless sensor network large amount of graph structured data graph data for short have been collected in a wide range of application discovering knowledge from graph data ha witnessed a number of application and received a lot of research attention recently it is observed that uncertainty are inherent in the structure of some graph data for example protein protein interaction ppi data can be represented a a graph where vertex represent protein and edge represent ppi s due to the limit of ppi detection method it is uncertain that a detected ppi exist in practice other example of uncertain graph data include topology of wireless sensor network social network and so on managing and mining such large scale uncertain graph data is of both theoretical and practical significance many solid work have been conducted on uncertain graph mining from the aspect of model semantics methodology and algorithm in last few year a number of research paper on managing and mining uncertain graph data have been published in the database and data mining conference such a vldb icde kdd cikm and edbt this talk focus on the data model semantics computational complexity and algorithm of uncertain graph mining in the talk some typical research work in the field of uncertain graph mining will also be introduced including frequent subgraph pattern mining dense subgraph detection reliable subgraph discovery and clustering on uncertain graph data 
graph clustering often addressed a community detection is a prominent task in the domain of graph data mining with dozen of algorithm proposed in recent year in this paper we focus on several popular community detection algorithm with low computational complexity and with decent performance on the artificial benchmark and we study their behaviour on real world network motivated by the observation that there is a class of network for which the community detection method fail to deliver good community structure we examine the assortativity coefficient of ground truth community and show that assortativity of a community structure can be very different from the assortativity of the original network we then examine the possibility of exploiting the latter by weighting edge of a network with the aim to improve the community detection output for network with assortative community structure the evaluation show that the proposed weighting can significantly improve the result of community detection method on network with assortative community structure 
online user review play a central role in the decision making process of user for a variety of task ranging from entertainment and shopping to medical service a user generated review proliferate it becomes critical to have a mechanism for helping the user information consumer deal with the information overload and presenting them with a small comprehensive set of review that satisfies their information need this is particularly important for mobile phone user who need to make decision quickly and have a device with limited screen real estate for displaying the review previous approach have addressed the problem by ranking review according to their estimated helpfulness however such approach do not account for the fact that the top few high quality review may be highly redundant repeating the same information or presenting the same positive or negative perspective in this work we focus on the problem of selecting a comprehensive set of few high quality review that cover many different aspect of the reviewed item we formulate the problem a a maximum coverage problem and we present a generic formalism that can model the different variant of review set selection we describe algorithm for the different variant we consider and whenever possible we provide approximation guarantee with respect to the optimal solution we also perform an experimental evaluation on real data in order to understand the value of coverage for user 
online review provide consumer with valuable information that guide their decision on a variety of front from entertainment and shopping to medical service although the proliferation of online review give insight about different aspect of a product it can also prove a serious drawback consumer cannot and will not read thousand of review before making a purchase decision this need to extract useful information from large review corpus ha spawned considerable prior work but so far all have drawback review summarization generating statistical description of review set sacrifice the immediacy and narrative structure of review likewise review selection identifying a subset of helpful or important review lead to redundant or non representative summary in this paper we fill the gap between existing review summarization and review selection method by selecting a small subset of review that together preserve the statistical property of the entire review corpus we formalize this task a a combinatorial optimization problem and show that it np hard both tosolve and approximate we also design effective algorithm that prove to work well in practice our experiment with real review corpus on different type of product demonstrate the utility of our method and our user study indicate that our method provide a better summary than prior approach 
display ad on the internet are often sold in bundle of thousand or million of impression over a particular time period typically week or month ad serving system that assign ad to page on behalf of publisher must satisfy these contract but at the same time try to maximize overall quality of placement this is usually modeled in the literature a an online allocation problem where contract are represented by overall delivery constraint over a finite time horizon however this model miss an important aspect of ad delivery time homogeneity advertiser who buy these package expect their ad to be shown smoothly throughout the purchased time period in order to reach a wider audience to have a sustained impact and to support the ad they are running on other medium e g television in this paper we formalize this problem using several nested packing constraint and develop a tight e competitive online algorithm for this problem our algorithm and analysis require novel technique a they involve online computation of multiple dual variable per ad we then show the effectiveness of our algorithm through exhaustive simulation study on real data set 
the n point correlation function npcf are powerful spatial statistic capable of fully characterizing any set of multidimensional point these function are critical in key data analysis in astronomy and material science among other field for example to test whether two point set come from the same distribution and to validate physical model and theory for example the npcf ha been used to study the phenomenon of dark energy considered one of the major breakthrough in recent scientific discovery unfortunately directly estimating the continuous npcf at a single value requires o nn time for n point and n may be or even higher depending on the sensitivity required in order to draw useful conclusion about real scientific problem we must repeat this expensive computation both for many different scale in order to derive a smooth estimate and over many different subsamples of our data in order to bound the variance we present the first comprehensive approach to the entire n point correlation function estimation problem including fast algorithm for the computation at multiple scale and for many subsamples we extend the current state of the art tree based approach with these two algorithm we show an order of magnitude speedup over the current best approach with each of our new algorithm and show that they can be used together to obtain over x speedup over the state of the art in order to enable much larger datasets and more accurate scientific analysis than were possible previously 
we describe a real time bidding algorithm for performance based display ad allocation a central issue in performance display advertising is matching campaign to ad impression which can be formulated a a constrained optimization problem that maximizes revenue subject to constraint such a budget limit and inventory availability the current practice is to solve the optimization problem offline at a tractable level of impression granularity e g the page level and to serve ad online based on the precomputed static delivery scheme although this offline approach take a global view to achieve optimality it fails to scale to ad allocation at the individual impression level therefore we propose a real time bidding algorithm that enables fine grained impression valuation e g targeting user with real time conversion data and adjusts value based bid according to real time constraint snapshot e g budget consumption level theoretically we show that under a linear programming lp primal dual formulation the simple real time bidding algorithm is indeed an online solver to the original primal problem by taking the optimal solution to the dual problem a input in other word the online algorithm guarantee the offline optimality given the same level of knowledge an offline optimization would have empirically we develop and experiment with two real time bid adjustment approach to adapting to the non stationary nature of the marketplace one adjusts bid against real time constraint satisfaction level using control theoretic method and the other adjusts bid also based on the statistically modeled historical bidding landscape finally we show experimental result with real world ad delivery data that support our theoretical conclusion 
temporal datasets in which data evolves continuously exist in a wide variety of application and identifying anomalous or outlying object from temporal datasets is an important and challenging task different from traditional outlier detection which detects object that have quite different behavior compared with the other object temporal outlier detection try to identify object that have different evolutionary behavior compared with other object usually object form multiple community and most of the object belonging to the same community follow similar pattern of evolution however there are some object which evolve in a very different way relative to other community member and we define such object a evolutionary community outlier this definition represents a novel type of outlier considering both temporal dimension and community pattern we investigate the problem of identifying evolutionary community outlier given the discovered community from two snapshot of an evolving dataset to tackle the challenge of community evolution and outlier detection we propose an integrated optimization framework which conduct outlier aware community matching across snapshot and identification of evolutionary outlier in a tightly coupled way a coordinate descent algorithm is proposed to improve community matching and outlier detection performance iteratively experimental result on both synthetic and real datasets show that the proposed approach is highly effective in discovering interesting evolutionary community outlier 
networked data extracted from social medium web page and bibliographic database can contain entity of multiple class interconnected through different type of link in this paper we focus on the problem of performing multi label classification on networked data where the instance in the network can be assigned multiple label in contrast to traditional content only classification method relational learning succeeds in improving classification performance by leveraging the correlation of the label between linked instance however instance in a network can be linked for various causal reason hence treating all link in a homogeneous way can limit the performance of relational classifier in this paper we propose a multi label iterative relational neighbor classifier that employ social context feature scrn our classifier incorporates a class propagation probability distribution obtained from instance social feature which are in turn extracted from the network topology this class propagation probability capture the node s intrinsic likelihood of belonging to each class and serf a a prior weight for each class when aggregating the neighbor class label in the collective inference procedure experiment on several real world datasets demonstrate that our proposed classifier boost classification performance over common benchmark on networked multi label data 
three major factor govern the intricacy of community extraction in network the application domain includes a wide variety of network of fundamentally different nature the literature offer a multitude of disparate community detection algorithm and there is no consensus characterizing how to discriminate community from non community in this paper we present a comprehensive analysis of community property through a class separability framework our approach enables the assessement of the structural dissimilarity among the output of multiple community detection algorithm and between the output of algorithm and community that arise in practice to demostrate this concept we furnish our method with a large set of structural property and multiple community detection algorithm applied to a diverse collection of large scale network datasets the analysis reveals that the different detection algorithm extract fundamentally different structure the structure of community that arise in practice is closest to that of community that random walk based algorithm extract although still siginificantly different from that of the output of all the algorithm and a small subset of the property are nearly a discriminative a the full set while making explicit the way in which the algorithm produce bias our framework enables an informed choice of the most suitable community detection method for a given purpose and network and allows for a comparison of existing community detection algorithm while guiding the design of new one 
the competitive business climate and the complexity of it environment dictate efficient and cost effective service delivery and support of it service these are largely achieved by automating routine maintenance procedure including problem detection determination and resolution system monitoring provides an effective and reliable mean for problem detection coupled with automated ticket creation it ensures that a degradation of the vital sign defined by acceptable threshold or monitoring condition is flagged a a problem candidate and sent to supporting personnel a an incident ticket this paper describes an integrated framework for minimizing false positive ticket and maximizing the monitoring coverage for system fault in particular the integrated framework defines monitoring condition and the optimal corresponding delay time based on an off line analysis of historical alert and incident ticket potential monitoring condition are built on a set of predictive rule which are automatically generated by a rule based learning algorithm with coverage confidence and rule complexity criterion these condition and delay time are propagated a configuration into run time monitoring system moreover a part of misconfigured monitoring condition can be corrected according to false negative ticket that are discovered by another text classification algorithm in this framework this paper also provides implementation detail of a program product that us this framework and show some illustrative example of successful result 
in matrix completion we are given a matrix where the value of only some of the entry are present and we want to reconstruct the missing one much work ha focused on the assumption that the data matrix ha low rank we propose a more general assumption based on denoising so that we expect that the value of a missing entry can be predicted from the value of neighboring point we propose a nonparametric version of denoising based on local iterated averaging with meanshift possibly constrained to preserve local low rank manifold structure the few user parameter required the denoising scale number of neighbor and local dimensionality and the number of iteration can be estimated by cross validating the reconstruction error using our algorithm a a postprocessing step on an initial reconstruction provided by e g a low rank method we show consistent improvement with synthetic image and motion capture data 
suppose we have two competing idea product virus that propagate over a social or other network suppose that they are strong virulent enough so that each if left alone could lead to an epidemic what will happen when both operate on the network earlier model assume that there is perfect competition if a user buy product a or get infected with virus x she will never buy product b or virus y this is not always true for example a user could install and use both firefox and google chrome a browser similarly one type of flu may give partial immunity against some other similar disease in the case of full competition it is known that winner take all that is the weaker virus product will become extinct in the case of no competition both virus survive ignoring each other what happens in between these two extreme we show that there is a phase transition if the competition is harsher than a critical level then winner take all otherwise the weaker virus survives these are the contribution of this paper a the problem definition which is novel even in epidemiology literature b the phase transition result and c experiment on real data illustrating the suitability of our result 
in the last decade advance in data collection and storage technology have led to an increased interest in designing and implementing large scale parallel algorithm for machine learning and data mining ml dm existing programming paradigm for expressing large scale parallelism such a mapreduce mr and the message passing interface mpi have been the de facto choice for implementing these ml dm algorithm the mr programming paradigm ha been of particular interest a it gracefully handle large datasets and ha built in resilience against failure however the existing parallel programming paradigm are too low level and ill suited for implementing ml dm algorithm to address this deficiency we present nimble a portable infrastructure that ha been specifically designed to enable the rapid implementation of parallel ml dm algorithm the infrastructure allows one to compose parallel ml dm algorithm using reusable serial and parallel building block that can be efficiently executed using mr and other parallel programming model it currently run on top of hadoop which is an open source mr implementation we show how nimble can be used to realize scalable implementation of ml dm algorithm and present a performance evaluation 
personalization is ubiquitous in modern online application a it provides significant improvement in user experience by adapting it to inferred user preference however there are increasing concern related to issue of privacy and control of the user data that is aggregated by online system to power personalized experience these concern are particularly significant for user profile aggregation in online advertising this paper describes a practical learning driven client side personalization approach for keyword advertising platform an emerging application previously not addressed in literature our approach relies on storing user specific information entirely within the user s control in a browser cookie or browser local storage thus allowing the user to view edit or purge it at any time e g via a dedicated webpage we develop a principled utility based formulation for the problem of iteratively updating user profile stored client side which relies on calibrated prediction of future user activity while optimal profile construction is np hard for pay per click advertising with bid increment it can be efficiently solved via a greedy approximation algorithm guaranteed to provide a near optimal solution due to the fact that keyword profile utility is submodular it exhibit the property of diminishing return with increasing profile size we empirically evaluate client side keyword profile for keyword advertising on a large scale dataset from a major search engine experiment demonstrate that predictive client side personalization allows ad platform to retain almost all of the revenue gain from personalization even if they give user the freedom to opt out of behavior tracking backed by server side storage additionally we show that advertiser can potentially increase their return on investment significantly by utilizing bid increment for keyword profile in their ad campaign 
nearest neighbor search over time series ha received vast research attention a a basic data mining task still none of the hitherto proposed method scale well with increasing time series length this is due to the fact that all method provide an one off pruning capacity only in particular traditional method utilize an index to search in a reduced dimensionality feature space however for high time series length search with such an index yield many false hit that need to be eliminated by accessing the full record an attempt to reduce false hit by indexing more feature exacerbates the curse of dimensionality and vice versa a recently proposed alternative isax us symbolic approximate representation accessed by a simple file system directory a an index still isax also encounter false hit which are again eliminated by accessing record in full once a false hit is generated by the index there is no second chance to prune it thus the pruning capacity isax provides is also one off this paper proposes an alternative approach to time series knn search following a nontraditional pruning style instead of navigating through candidate record via an index we access their feature obtained by a multi resolution transform in a stepwise sequential scan manner one level of resolution at a time over a vertical representation most candidate are progressively eliminated after a few of their term are accessed using pre computed information and an unprecedentedly tight double bounding scheme involving not only lower but also upper distance bound our experimental study with large high length time series data confirms the advantage of our approach over both the current state of the art method isax and classical index based method 
information about urban air quality e g the concentration of pm is of great importance to protect human health and control air pollution while there are limited air quality monitor station in a city air quality varies in urban space non linearly and depends on multiple factor such a meteorology traffic volume and land us in this paper we infer the real time and fine grained air quality information throughout a city based on the historical and real time air quality data reported by existing monitor station and a variety of data source we observed in the city such a meteorology traffic flow human mobility structure of road network and point of interest poi we propose a semi supervised learning approach based on a co training framework that consists of two separated classifier one is a spatial classifier based on an artificial neural network ann which take spatially related feature e g the density of poi and length of highway a input to model the spatial correlation between air quality of different location the other is a temporal classifier based on a linear chain conditional random field crf involving temporally related feature e g traffic and meteorology to model the temporal dependency of air quality in a location we evaluated our approach with extensive experiment based on five real data source obtained in beijing and shanghai the result show the advantage of our method over four category of baseline including linear gaussian interpolation classical dispersion model well known classification model like decision tree and crf and ann 
shopping can be decomposed into three basic question what where and when to buy in this talk i ll describe how we utilize advanced data mining and text mining technique at decide com and earlier at farecast to solve these problem for on line shopper our algorithm have predicted price utilizing billion of data point and ranked product based on million of review 
online social network information promise to increase recommendation accuracy beyond the capability of purely rating feedback driven recommender system r a to better serve user activity across different domain many online social network now support a new feature of friend circle which refines the domain oblivious friend concept r should also benefit from domain specific trust circle intuitively a user may trust different subset of friend regarding different domain unfortunately in most existing multi category rating datasets a user s social connection from all category are mixed together this paper present an effort to develop circle based r we focus on inferring category specific social trust circle from available rating data combined with social network data we outline several variant of weighting friend within circle based on their inferred expertise level through experiment on publicly available data we demonstrate that the proposed circle based recommendation model can better utilize user s social trust information resulting in increased recommendation accuracy 
financial market are quite sensitive to unanticipated news and event identifying the effect of news on the market is a challenging task in this demo we present forex foreteller ff which mine news article and make forecast about the movement of foreign currency market the system us a combination of language model topic clustering and sentiment analysis to identify relevant news article these article along with the historical stock index and currency exchange value are used in a linear regression model to make forecast the system ha an interactive visualizer designed specifically for touch sensitive device which depicts forecast along with the chronological news event and financial data used for making the forecast 
sale representative must have access to meaningful and actionable intelligence about potential customer to be effective in their role historically ge capital america sale rep identified lead by manually searching through news report and financial statement either in print or online here we describe a system built to automate the collection and aggregation of information on company which is then mined to identify actionable sale lead the financing lead trigger system is comprised of three core component that perform information fusion knowledge discovery and information visualization together these component extract raw data from disparate source fuse that data into information and then automatically mine that information for actionable sale lead driven by a combination of expert defined and statistically derived trigger a web based interface provides sale rep access to the company information and sale lead in a single location the use of the lead trigger system ha significantly improved the performance of the sale rep providing them with actionable intelligence that ha improved their productivity by in lead trigger provided lead on opportunity that represented over b in new deal commitment for ge capital 
recommending interesting content to engage user is important for web portal e g aol msn yahoo and many others existing approach typically recommend article to optimize for a single objective i e number of click however a click is only the starting point of a user s journey and subsequent downstream utility such a time spent and revenue are important in this paper we call the problem of recommending link to jointly optimize for click and post click downstream utility click shaping we propose a multi objective programming approach in which multiple objective are modeled in a constrained optimization framework such a formulation can naturally incorporate various application driven requirement we study several variant that model different requirement a constraint and discus some of the subtlety involved we conduct our experiment on a large dataset from a real system by using a newly proposed unbiased evaluation methodology through extensive experiment we quantify the tradeoff between different objective under various constraint our experimental result show interesting characteristic of different formulation and our finding may provide valuable guidance to the design of recommendation engine for web portal 
social microblogs such a twitter and weibo are experiencing an explosive growth with billion of global user sharing their daily observation and thought beyond public interest e g sport music microblogs can provide highly detailed information for those interested in public health homeland security and financial analysis however the language used in twitter is heavily informal ungrammatical and dynamic existing data mining algorithm require extensive manually labeling to build and maintain a supervised system this paper present sted a semi supervised system that help user to automatically detect and interactively visualize event of a targeted type from twitter such a crime civil unrest and disease outbreak our model first applies transfer learning and label propagation to automatically generate labeled data then learns a customized text classifier based on mini clustering and finally applies fast spatial scan statistic to estimate the location of event we demonstrate sted s usage and benefit using twitter data collected from latin america country and show how our system help to detect and track example event such a civil unrest and crime 
historical price are important information that can help consumer decide whether the time is right to buy a product they provide both a context to the user and facilitate the use of prediction algorithm for forecasting future price to produce a representative price history one need to consider all offer for the product however matching offer to a product is a challenging problem and mismatch could lead to glaring error in price history we propose a principled approach to filter out erroneous match based on a probabilistic model of price we give an efficient algorithm for performing inference that take advantage of the structure of the problem we evaluate our result empirically using merchant offer collected from a search engine and measure the proximity of the price history generated by our approach to the true price history our method outperforms alternative based on robust statistic both in tracking the true price level and the true price trend 
in history the moore s law effect ha been used to describe phenomenon of exponential improvement in technology when it ha a virtuous cycle that make technology improvement proportional to technology itself for example chip performance had doubled every month because better processor support the development of better layout tool that support the development of even better processor i will describe a new moore s law effect that is being created in knowledge engineering and is driven by the self reinforcing nature of three trend and technical advancement big data machine learning and internet economics i will explain how we can take advantage of this new effect to develop a new generation of semantic and knowledge based search engine specifically my presentation will cover the following three area knowledge acquisition our goal is to build a comprehensive entity graph and knowledge graph to complement the web and social graph i will introduce technique for entity extraction and knowledgebase construction through automatic and interactive mining and crowdsourcing managing knowledge our goal is to support advanced analytical query by combining probabilistic knowledge with a distributed platform i will focus on technology for both online knowledge serving and offline knowledge inference knowledge empowered search and application the knowledge we have acquired and curated enables application like query understanding entity centric search experience and answer to natural language query 
the various kind of booming social medium not only provide a platform where people can communicate with each other but also spread useful domain information such a career and job market information for example linkedin publishes a large amount of message either about people who want to seek job or company who want to recruit new member by collecting information we can have a better understanding of the job market and provide insight to job seeker company and even decision maker in this paper we analyze the job information from the social network point of view we first collect the job related information from various social medium source then we construct an inter company job hopping network with the vertex denoting company and the edge denoting flow of personnel between company we subsequently employ graphmining technique to mine influential company and related company group based on the job hopping network model demonstration on linkedin data show that our system jobminer can provide a better understanding of the dynamic process and a more accurate identification of important entity in the job market 
in this paper we introduce a trial and error model to study information diffusion in a social network specifically in every discrete period all individual in the network concurrently try a new technology or product with certain respective probability if it turn out that an individual observes a better utility he will then adopt the trial otherwise the individual continues to choose his prior selection we first demonstrate that the trial and error behavior of individual characterizes certain global community structure of a social network from which we are able to detect macro community through the observation of micro behavior of individual we run simulation on classic benchmark testing graph and quite surprisingly the result show that the trial and error dynamic even outperforms the louvain method a popular modularity maximization approach if individual have dense connection within community this give a solid justification of the model we then study the influence maximization problem in the trial and error dynamic we give a heuristic algorithm based on community detection and provide experiment on both testing and large scale collaboration network simulation result show that our algorithm significantly outperforms several well studied heuristic including degree centrality and distance centrality in almost all of the scenario our result reveal the relation between the budget that an advertiser invests and marketing strategy and indicate that the mixing parameter a benchmark evaluating network community structure play a critical role for information diffusion 
the advance in location acquisition technology have led to a myriad of spatial trajectory these trajectory are usually generated at a low or an irregular frequency due to application characteristic or energy saving leaving the route between two consecutive point of a single trajectory uncertain called an uncertain trajectory in this paper we present a route inference framework based on collective knowledge abbreviated a rick to construct the popular route from uncertain trajectory explicitly given a location sequence and a time span the rick is able to construct the top k route which sequentially pas through the location within the specified time span by aggregating such uncertain trajectory in a mutual reinforcement way i e uncertain uncertain certain our work can benefit trip planning traffic management and animal movement study the rick comprises two component routable graph construction and route inference first we explore the spatial and temporal characteristic of uncertain trajectory and construct a routable graph by collaborative learning among the uncertain trajectory second in light of the routable graph we propose a routing algorithm to construct the top k route according to a user specified query we have conducted extensive experiment on two real datasets consisting of foursquare check in datasets and taxi trajectory the result show that rick is both effective and efficient 
we propose a novel collapsed variational bayes cvb inference for the hierarchical dirichlet process hdp while the existing cvb inference for the hdp variant of latent dirichlet allocation lda is more complicated and harder to implement than that for lda the proposed algorithm is simple to implement doe not require variance count to be maintained doe not need to set hyper parameter and ha good predictive performance 
online social network become a bridge to connect our physical daily life and the virtual web space which not only provides rich data for mining but also brings many new challenge in this paper we present a novel social analytic engine sae for large online social network the key issue we pursue in the analytic engine are concerned with the following problem at the micro level how do people form different type of social tie and how people influence each other at the meso level how do people group into community at the macro level what are the hottest topic in a social network and how the topic evolve over time we propose method to address the above question the method are general and can be applied to various social networking data we have deployed and validated the proposed analytic engine over multiple different network and validated the effectiveness and efficiency of the proposed method 
privacy preserving data publishing address the problem of disclosing sensitive data when mining for useful information among the existing privacy model differential privacy provides one of the strongest privacy guarantee and ha no assumption about an adversary s background knowledge most of the existing solution that ensure differential privacy are based on an interactive model where the data miner is only allowed to pose aggregate query to the database in this paper we propose the first anonymization algorithm for the non interactive setting based on the generalization technique the proposed solution first probabilistically generalizes the raw data and then add noise to guarantee differential privacy a a sample application we show that the anonymized data can be used effectively to build a decision tree induction classifier experimental result demonstrate that the proposed non interactive anonymization algorithm is scalable and performs better than the existing solution for classification analysis 
data analysis is an inherently iterative process that is what we know about the data greatly determines our expectation and hence what result we would find the most interesting with this in mind we introduce a well founded approach for succinctly summarizing data with a collection of itemsets using a probabilistic maximum entropy model we iteratively find the most interesting itemset and in turn update our model of the data accordingly a we only include itemsets that are surprising with regard to the current model the summary is guaranteed to be both descriptive and non redundant the algorithm that we present can either mine the top k most interesting itemsets or use the bayesian information criterion to automatically identify the model containing only the itemsets most important for describing the data or in other word it will tell you what you need to know experiment on synthetic and benchmark data show that the discovered summary are succinct and correctly identify the key pattern in the data the model they form attain high likelihood and inspection show that they summarize the data well with increasingly specific yet non redundant itemsets 
prior work on computing semantic relatedness of word focused on representing their meaning in isolation effectively disregarding inter word affinity we propose a large scale data mining approach to learning word word relatedness where known pair of related word impose constraint on the learning process we learn for each word a low dimensional representation which strives to maximize the likelihood of a word given the context in which it appears our method called clear is shown to significantly outperform previously published approach the proposed method is based on first principle and is generic enough to exploit diverse type of text corpus while having the flexibility to impose constraint on the derived word similarity we also make publicly available a new labeled dataset for evaluating word relatedness algorithm which we believe to be the largest such dataset to date 
given a very large moderate to high dimensionality dataset how could one cluster it point for datasets that don t fit even on a single disk parallelism is a first class option in this paper we explore mapreduce for clustering this kind of data the main question are a how to minimize the i o cost taking into account the already existing data partition e g on disk and b how to minimize the network cost among processing node either of them may be a bottleneck thus we propose the best of both world bow method that automatically spot the bottleneck and chooses a good strategy our main contribution are we propose bow and carefully derive it cost function which dynamically choose the best strategy we show that bow ha numerous desirable feature it can work with most serial clustering method a a plugged in clustering subroutine it balance the cost for disk access and network access achieving a very good tradeoff between the two it us no user defined parameter thanks to our reasonable default it match the clustering quality of the serial algorithm and it ha near linear scale up and finally we report experiment on real and synthetic data with billion of point using up to core in parallel to the best of our knowledge our yahoo web is the largest real dataset ever reported in the database subspace clustering literature spanning tb of multi dimensional data it took only minute to be clustered using core 
recent advance in search user click modeling consider both user search query and click skip behavior on document to infer the user s perceived relevance most of these model including dynamic bayesian network dbn and user browsing model ubm use probabilistic model to understand user click behavior based on individual query the user behavior is more complex when her action to satisfy her information need form a search session which may include multiple query and subsequent click behavior on various item on search result page previous research is limited to treating each query within a search session in isolation without paying attention to their dynamic interaction with other query in a search session investigating this problem we consider the sequence of query and their click in a search session a a task and propose a task centric click model tcm tcm characterizes user behavior related to a task a a collective whole specifically we identify and consider two new bias in tcm a the basis for user modeling the first indicates that user tend to express their information need incrementally in a task and thus perform more click a their need become clearer the other illustrates that user tend to click fresh document that are not included in the result of previous query using these bias tcm is more accurately able to capture user search behavior extensive experimental result demonstrate that by considering all the task information collectively tcm can better interpret user click behavior and achieve significant improvement in term of ranking metric of ndcg and perplexity 
large collection of electronic clinical record today provide u with a vast source of information on medical practice however the utilization of those data for exploratory analysis to support clinical decision is still limited extracting useful pattern from such data is particularly challenging because it is longitudinal sparse and heterogeneous in this paper we propose a nonnegative matrix factorization nmf based framework using a convolutional approach for open ended temporal pattern discovery over large collection of clinical record we call the method one sided convolutional nmf osc nmf our framework can mine common a well a individual shift invariant temporal pattern from heterogeneous event over different patient group and handle sparsity a well a scalability problem well furthermore we use an event matrix based representation that can encode quantitatively all key temporal concept including order concurrency and synchronicity we derive efficient multiplicative update rule for osc nmf and also prove theoretically it convergence finally the experimental result on both synthetic and real world electronic patient data are presented to demonstrate the effectiveness of the proposed method 
personalize pagerank ppr is an effective relevance proximity measure in graph mining the goal of this paper is to efficiently compute single node relevance and top k highly relevant node without iteratively computing the relevance of all node based on a random surfer model ppr iteratively computes the relevance of all node in a graph until convergence for a given user preference distribution the problem with this iterative approach is that it cannot compute the relevance of just one or a few node the heart of our solution is to compute single node relevance accurately in non iterative manner based on sparse matrix representation and to compute top k highly relevant node exactly by pruning unnecessary relevance computation based on upper lower relevance estimation our experiment show that our approach is up to seven order of magnitude faster than the existing alternative 
with the advance and increasing sophistication in data collection technique we are facing with large amount of data collected from multiple heterogeneous source in many application for example in the study of alzheimer s disease ad different type of measurement such a neuroimages gene protein expression data genetic data etc are often collected and analyzed together for improved predictive power it is believed that a joint learning of multiple data source is beneficial a different data source may contain complementary information and feature pruning and data source selection are critical for learning interpretable model from high dimensional data very often the collected data come with block wise missing entry for example a patient without the mri scan will have no information in the mri data block making his her overall record incomplete there ha been a growing interest in the data mining community on expanding traditional technique for single source complete data analysis to the study of multi source incomplete data the key challenge is how to effectively integrate information from multiple heterogeneous source in the presence of block wise missing data in this paper we first investigate the situation of complete data and present a unified bi level learning model for multi source data then we give a natural extension of this model to the more challenging case with incomplete data our major contribution are threefold the proposed model handle both feature level and source level analysis in a unified formulation and include several existing feature learning approach a special case the model for incomplete data avoids direct imputation of the missing element and thus provides superior performance moreover it can be easily generalized to other application with block wise missing data source efficient optimization algorithm are presented for both the complete and incomplete model we have performed comprehensive evaluation of the proposed model on the application of ad diagnosis our proposed model compare favorably against existing approach 
user in online social network play a variety of social role and status for example user in twitter can be represented a advertiser content contributor information receiver etc user in linkedin can be in different professional role such a engineer salesperson and recruiter previous research work mainly focus on using categorical and textual information to predict the attribute of user however it cannot be applied to a large number of user in real social network since much of such information is missing outdated and non standard in this paper we investigate the social role and status that people act in online social network in the perspective of network structure since the uniqueness of social network is connecting people we quantitatively analyze a number of key social principle and theory that correlate with social role and status we systematically study how the network characteristic reflect the social situation of user in an online society we discover pattern of homophily the tendency of user to connect with user with similar social role and status in addition we observe that different factor in social theory influence the social role status of an individual user to various extent since these social principle represent different aspect of the network we then introduce an optimization framework based on factor conditioning symmetry and we propose a probabilistic model to integrate the optimization framework on local structural information a well a network influence to infer the unknown social role and status of online user we will present experiment result to show the effectiveness of the inference 
the improvement of crisis management and disaster recovery technique are national priority in the wake of man made and nature inflicted calamity of the last decade our prior work ha demonstrated that the efficiency of sharing and managing information play an important role in business recovery effort after disaster event with the proliferation of smart phone and wireless tablet professional who have an operational responsibility in disaster situation are relying on such device to maintain communication further with the rise of social medium technology savvy consumer are also using these device extensively for situational update in this paper we address several critical task which can facilitate information sharing and collaboration between both private and public sector participant for major disaster recovery planning and management we design and implement an all hazard disaster situation browser adsb system that run on apple s mobile operating system io and iphone and ipad mobile device our proposed technique create a collaborative solution on a mobile platform using advanced data mining and information retrieval technique for disaster preparedness and recovery that help impacted community better understand the current disaster situation and how the community is recovering specifically hierarchical summarization technique are used to generate brief review from a large collection of report at different granularity probabilistic model are proposed to dynamically generate query form based on user s feedback and recommendation technique are adapted to help user identify potential contact for report sharing and community organization furthermore the developed technique are designed to be all hazard capable so that they can be used in earthquake terrorism or other unanticipated disaster situation 
network interconnection information interoperability and crowd interaction on the internet could inspire better computation model than turing machine since that human play an important factor in internet computing so that the human machine and machine machine interaction have evolved to be the kernel of internet computing internet ha not been simply equivalent to a virtual huge computer or a set of computer on the internet human s behavior are uncertain the interaction and influence among people are also uncertain these uncertainty cannot be described by turing machine and traditional interaction machine a a new computation platform internet computing requires new theory and method by combining topology in mathematics with the field theory in physic we propose the topological potential approach which set up a virtual field by the topological space to reflect individual activity local effect and preferential attachment this approach can be used to research the emergence of collective intelligence here we introduce three case study to illustrate the analysis on the collective intelligence on the internet and discus some potential application of the topological potential approach 
combining correlated information from multiple context can significantly improve predictive accuracy in recommender problem such information from multiple context is often available in the form of several incomplete matrix spanning a set of entity like user item feature and so on existing method simultaneously factorize these matrix by sharing a single set of factor for entity across all context we show that such a strategy may introduce significant bias in estimate and propose a new model that ameliorates this issue by positing local context specific factor for entity to avoid over fitting in context with sparse data the local factor are connected through a shared global model this sharing of parameter allows information to flow across context through multivariate regression among local factor instead of enforcing exactly the same factor for an entity everywhere model fitting is done in an em framework we show that the e step can be fitted through a fast multi resolution kalman filter algorithm that ensures scalability experiment on benchmark and real world yahoo datasets clearly illustrate the usefulness of our approach our model significantly improves predictive accuracy especially in cold start scenario 
this paper demonstrates a system that exploit graph mining social network analysis and agent based crowd simulation technique to investigate the evacuation dynamic during fire emergency we create a novel evacuation planning system evaplanner to deal with three task first the system identifies the preferable location to establish the exit to facilitate efficient evacuation from the dangerous area second it determines the most effective position to place the emergency sign such that panic crowd can quickly find the exit third it faithfully simulates the evacuation dynamic of crowd considering not only the individual movement kinetics but also the social connection between people evaplanner provides a flexible experimental platform for investigating the evacuation dynamic under a variety of setting and can further be utilized for animation and movie production in addition it can serve a a tool to assist architect address the safety concern during the planning phase the demo system can be found in the link http mslab csie ntu edu tw evaplanner 
caesar entertainment the largest provider of branded casino entertainment capture a wealth of data for million customer through it total reward program in depth data analysis ha helped caesar weather the economic downturn by prioritizing marketing spend expense saving target and identifying new revenue opportunity this talk will describe how closed loop marketing state of the art user segmentation and ongoing experimentation via test and control group have enabled caesar entertainment to achieve all time high customer satisfaction score and outperform the competition in a challenging economic climate the lesson learned are generic and apply across multiple industry insight will also be provided on the next wave of challenge to be answered analytically 
detecting multiple clustering solution is an emerging research field while data is often multi faceted in it very nature traditional clustering method are restricted to find just a single grouping to overcome this limitation method aiming at the detection of alternative and multiple clustering solution have been proposed in this work we present a bayesian framework to tackle the problem of multi view clustering we provide multiple generalization of the data by using multiple mixture model each mixture describes a specific view on the data by using a mixture of beta distribution in subspace projection since a mixture summarizes the cluster located in similar subspace projection each view highlight specific aspect of the data in addition our model handle overlapping view where the mixture component compete against each other in the data generation process for efficiently learning the distribution we propose the algorithm mvgen that exploit the icm principle and us bayesian model selection to trade off the cluster model s complexity against it goodness of fit with experiment on various real world data set we demonstrate the high potential of mvgen to detect multiple overlapping clustering view in subspace projection of the data 
spontaneous devaluation in preference is ubiquitous where yesterday s hit is today s affliction despite technological advance facilitating access to a wide range of medium commodity finding engaging content is a major enterprise with few principled solution system tracking spontaneous devaluation in user preference can allow prediction of the onset of boredom in user potentially catering to their changed need in this work we study the music listening history of last fm user focusing on the change in their preference based on their choice for different artist at different point in time a hazard function commonly used in statistic for survival analysis is used to capture the rate at which a user return to an artist a a function of exposure to the artist the analysis provides the first evidence of spontaneous devaluation in preference of music listener better understanding of the temporal dynamic of this phenomenon can inform solution to the similarity diversity dilemma of recommender system 
outlier detection and ensemble learning are well established research direction in data mining yet the application of ensemble technique to outlier detection ha been rarely studied here we propose and study subsampling a a technique to induce diversity among individual outlier detector we show analytically and experimentally that an outlier detector based on a subsample per se besides inducing diversity can under certain condition already improve upon the result of the same outlier detector on the complete dataset building an ensemble on top of several subsamples is further improving the result while in the literature so far the intuition that ensemble improve over single outlier detector ha just been transferred from the classification literature here we also justify analytically why ensemble are also expected to work in the unsupervised area of outlier detection a a side effect running an ensemble of several outlier detector on subsamples of the dataset is more efficient than ensemble based on other mean of introducing diversity and depending on the sample rate and the size of the ensemble can be even more efficient than just the single outlier detector on the complete data 
supervised local pattern discovery aim to find subset of a database with a high statistical unusualness in the distribution of a target attribute local pattern discovery is often used to generate a human understandable representation of the most interesting dependency in a data set hence the more crisp and concise the output is the better unfortunately standard algorithm often produce very large and redundant output in this paper we introduce delta relevance a definition of a more strict criterion of relevance it will allow u to significantly reduce the output space while being able to guarantee that every local pattern ha a delta relevant representative which is almost a good in a clearly defined sense we show empirically that delta relevance lead to a considerable reduction of the amount of returned pattern we also demonstrate that in a top k setting the removal of not delta relevant pattern improves the quality of the result set 
in order to minimize redundancy and optimize coverage of multiple user interest search engine and recommender system aim to diversify their set of result to date these diversification mechanism are largely hand coded or relied on expensive training data provided by expert to overcome this problem we propose an online learning model and algorithm for learning diversified recommendation and retrieval function from implicit feedback in our model the learning algorithm present a ranking to the user at each step and us the set of document from the presented ranking which the user read a feedback even for imperfect and noisy feedback we show that the algorithm admit theoretical guarantee for maximizing any submodular utility measure under approximately rational user behavior in addition to the theoretical result we find that the algorithm learns quickly accurately and robustly in empirical evaluation on two datasets 
we consider the problem of finding a suitable common low dimensional subspace for accurately representing a given set of covariance matrix with one covariance matrix this is principal component analysis pca for multiple covariance matrix we term the problem common component analysis cca while cca can be posed a a tensor decomposition problem standard approach to tensor decomposition have two critical issue i tensor decomposition method are iterative and rely on the initialization ii for a given level of approximation error it is difficult to choose a suitable low dimensionality in this paper we present a detailed analysis of cca that yield an effective initialization and iterative algorithm for the problem the proposed methodology ha provable approximation guarantee w r t the global maximum and also allows one to choose the dimensionality for a given level of approximation error we also establish condition under which the methodology will achieve the global maximum we illustrate the effectiveness of the proposed method through extensive experiment on synthetic data a well a on two real stock market datasets where major financial event can be visualized in low dimension 
support vector machine svms are a leading tool in machine learning and have been used with considerable success for the task of time series forecasting however a key challenge when using svms for time series is the question of how to deeply integrate time element into the learning process to address this challenge we investigated the distribution of error in the forecast delivered by standard svms once we identified the sample that produced the largest error we observed their correlation with distribution shift that occur in the time series this motivated u to propose a time dependent loss function which allows the inclusion of the information about the distribution shift in the series directly into the svm learning process we present experimental result which indicate that using a time dependent loss function is highly promising reducing the overall variance of the error a well a delivering more accurate prediction 
we consider composite loss function for multiclass prediction comprising a proper i e fisher consistent loss over probability distribution and an inverse link function we establish condition for their strong convexity and explore the implication we also show how the separation of concern afforded by using this composite representation allows for the design of family of loss with the same bayes risk copyright by the author s owner s 
mining high utility itemsets from database is an emerging topic in data mining which refers to the discovery of itemsets with utility higher than a user specified minimum utility threshold min util although several study have been carried out on this topic setting an appropriate minimum utility threshold is a difficult problem for user if min util is set too low too many high utility itemsets will be generated which may cause the mining algorithm to become inefficient or even run out of memory on the other hand if min util is set too high no high utility itemset will be found setting appropriate minimum utility threshold by trial and error is a tedious process for user in this paper we address this problem by proposing a new framework named top k high utility itemset mining where k is the desired number of high utility itemsets to be mined an efficient algorithm named tku top k utility itemsets mining is proposed for mining such itemsets without setting min util several feature were designed in tku to solve the new challenge raised in this problem like the absence of anti monotone property and the requirement of lossless result moreover tku incorporates several novel strategy for pruning the search space to achieve high efficiency result on real and synthetic datasets show that tku ha excellent performance and scalability 
in stanford university offered three online course which anyone in the world could enroll in and take for free together these three course had enrollment of around student making this one of the largest experiment in online education ever performed since the beginning of we have transitioned this effort into a new venture coursera a social entrepreneurship company whose mission is to make high quality education accessible to everyone by allowing the best university to offer course to everyone around the world for free coursera class provide a real course experience to student including video content interactive exercise with meaningful feedback using both auto grading and peer grading and a rich peer to peer interaction around the course material currently coursera ha university partner and over million student enrolled in it over course these course span a range of topic including computer science business medicine science humanity social science and more in this talk i ll report on this far reaching experiment in education and why we believe this model can provide both an improved classroom experience for our on campus student via a flipped classroom model a well a a meaningful learning experience for the million of student around the world who would otherwise never have access to education of this quality 
from it s beginning a a framework for building web crawler for small scale search engine to being one of the most promising technology for building datacenter scale distributed computing and storage platform apache hadoop ha come far in the last seven year in this talk i will reminisce about the early day of hadoop and will give an overview of the current state of the hadoop ecosystem and some real world use case of this open source platform i will conclude with some crystal gazing in the future of hadoop and associated technology 
the data mining literature is rich in problem asking to ass the importance of entity in a given dataset at a high level existing work identifies important entity either by ranking or by selection ranking method assign a score to every entity in the population and then use the assigned score to create a ranked list the major shortcoming of such approach is that they ignore the redundancy between high ranked entity which may in fact be very similar or even identical therefore in scenario where diversity is desirable such method perform poorly selection method overcome this drawback by evaluating the importance of a group of entity collectively to achieve this they typically adopt a set cover formulation which identifies the entity in the minimum set cover a the important one however this dichotomy of entity conceals the fact that even though an entity may not be in the reported cover it may still participate in many other optimal or near optimal solution in this paper we propose a framework that overcomes the above drawback by integrating the ranking and selection paradigm our approach assigns importance score to entity based on both the number and the quality of set cover solution that they participate our algorithmic contribution lie with the design of an efficient algorithm for approximating the number of high quality set cover that each entity participates our methodology applies to a wide range of application in a user study and an experimental evaluation on real data we demonstrate that our framework is efficient and provides useful and intuitive result 
given a graph how can we extract good feature for the node for example given two large graph from the same domain how can we use information in one to do classification in the other i e perform across network classification or transfer learning on graph also if one of the graph is anonymized how can we use information in one to de anonymize the other the key step in all such graph mining task is to find effective node feature we propose refex recursive feature extraction a novel algorithm that recursively combine local node based feature with neighborhood egonet based feature and output regional feature capturing behavioral information we demonstrate how these powerful regional feature can be used in within network and across network classification and de anonymization task without relying on homophily or the availability of class label the contribution of our work are a follows a refex is scalable and b it is effective capturing regional behavioral information in large graph we report experiment on real graph from various domain with over m edge where refex outperforms it competitor on typical graph mining task like network classification and de anonymization 
the classic stochastic approximation sa method achieves optimal rate under the black box model this optimality doe not rule out better algorithm when more information about function and data is available we present a family of noise adaptive stochastic approximation nasa algorithm for online convex optimization and stochastic convex optimization nasa is an adaptive variant of mirror descent stochastic approximation it is novel in it practical variation dependent stepsizes and better theoretical guarantee we show that comparing with state of the art adaptive and non adaptive sa method lower regret and faster rate can be achieved under low variation assumption 
traditionally academic machine learning and data mining researcher focus on proposing new algorithm the task of implementing these method is often left to company that are developing software package however the gap between the two side ha caused some problem first the practical deployment of new algorithm still involves some challenging issue that need to be studied by researcher second without further investigation after publishing their paper researcher have neither the opportunity to work with real problem nor see how their method are used we discus the experience in developing two machine learning package libsvm and liblinear that are widely used in both academia and industry we demonstrate that the interaction with user lead u to identify some important research problem for example the decision to study and then support multi class svm wa essential in the early stage of developing libsvm the birth of liblinear wa driven by the need to classify large scale document in internet company for fast training of large scale problem we had to create new algorithm other than those used in libsvm for kernel svm we present some practical use of liblinear for internet application finally we give lesson learned and future perspective for developing industry strength machine learning and data mining software 
the community of a social network are set of vertex with more connection inside the set than outside we theoretically demonstrate that two commonly observed property of social network heavy tailed degree distribution and large clustering coefficient imply the existence of vertex neighborhood also known a egonets that are themselves good community we evaluate these neighborhood community on a range of graph what we find is that the neighborhood community can exhibit conductance score that are a good a the fiedler cut also the conductance of neighborhood community show similar behavior a the network community profile computed with a personalized pagerank community detection method neighborhood community give u a simple and powerful heuristic for speeding up local partitioning method since finding good seed for the pagerank clustering method is difficult most approach involve an expensive sweep over a great many starting vertex we show how to use neighborhood community to quickly generate a small set of seed 
the concern of privacy ha become an important issue for online social network in service such a foursquare com whether a person like an article is considered private and therefore not disclosed only the aggregative statistic of article i e how many people like this article is revealed this paper try to answer a question can we predict the opinion holder in a heterogeneous social network without any labeled data this question can be generalized to a link prediction with aggregative statistic problem this paper devise a novel unsupervised framework to solve this problem including two main component a three layer factor graph model and three type of potential function a ranked margin learning and inference algorithm finally we evaluate our method on four diverse prediction scenario using four datasets preference foursquare repost twitter response plurk and citation dblp we further exploit nine unsupervised model to solve this problem a baseline our approach not only win out in all scenario but on the average achieves auc and ndcg improvement over the best competitor the resource are available at http www csie ntu edu tw d aggregative 
a botnets continue to proliferate and grow in sophistication so doe the need for more advanced security solution to effectively detect and defend against such attack in particular botnets such a conficker have been known to encrypt the communication packet exchanged between bot and their command and control server making it costly for existing botnet detection system that rely on deep packet inspection dpi method to identify compromised machine in this paper we argue that even in the face of encrypted traffic flow botnets can still be detected by examining the set of server ip address visited by a client machine in the past however there are several challenge that must be addressed first the set of server ip address visited by client machine may evolve dynamically second the set of client machine used for training and their class label may also change over time to overcome these challenge this paper present a novel incremental l svm algorithm that is adaptive to both change in the feature set and class label of training instance to evaluate the performance of our algorithm we have performed experiment on two large scale datasets including real time data collected from peering router at a large tier isp experimental result showed that the proposed algorithm produce classification accuracy comparable to it batch counterpart while consuming significantly le computational resource 
user location are important to many application such a targeted advertisement and news recommendation in this paper we focus on the problem of profiling user home location in the context of social network twitter the problem is nontrivial because signal which may help to identify a user s location are scarce and noisy we propose a unified discriminative influence model named a udi to solve the problem to overcome the challenge of scarce signal udi integrates signal observed from both social network friend and user centric data tweet in a unified probabilistic framework to overcome the challenge of noisy signal udi capture how likely a user connects to a signal with respect to the distance between the user and the signal and the influence scope of the signal based on the model we develop local and global location prediction method the experiment on a large scale data set show that our method improve the state of the art method by and achieve the best performance 
in targeted display advertising the goal is to identify the best opportunity to display a banner ad to an online user who is most likely to take a desired action such a purchasing a product or signing up for a newsletter finding the best ad impression i e the opportunity to show an ad to a user requires the ability to estimate the probability that the user who see the ad on his or her browser will take an action i e the user will convert however conversion probability estimation is a challenging task since there is extreme data sparsity across different data dimension and the conversion event occurs rarely in this paper we present our approach to conversion rate estimation which relies on utilizing past performance observation along user publisher and advertiser data hierarchy more specifically we model the conversion event at different select hierarchical level with separate binomial distribution and estimate the distribution parameter individually then we demonstrate how we can combine these individual estimator using logistic regression to accurately identify conversion event in our presentation we also discus main practical consideration such a data imbalance missing data and output probability calibration which render this estimation problem more difficult but yet need solving for a real world implementation of the approach we provide result from real advertising campaign to demonstrate the effectiveness of our proposed approach 
multiple kernel learning mkl aim to learn the kernel in an svm from training data many mkl formulation have been proposed and some have proved effective in certain application nevertheless a mkl is a nascent field many more formulation need to be developed to generalize across domain and meet the challenge of real world application however each mkl formulation typically necessitates the development of a specialized optimization algorithm the lack of an efficient general purpose optimizer capable of handling a wide range of formulation present a significant challenge to those looking to take mkl out of the lab and into the real world this problem wa somewhat alleviated by the development of the generalized multiple kernel learning gmkl formulation which admits fairly general kernel parameterizations and regularizers subject to mild constraint however the projected gradient descent gmkl optimizer is inefficient a the computation of the step size and a reasonably accurate objective function value or gradient direction are all expensive we overcome these limitation by developing a spectral projected gradient spg descent optimizer which a take into account second order information in selecting step size b employ a non monotone step size selection criterion requiring fewer function evaluation c is robust to gradient noise and d can take quick step when far away from the optimum we show that our proposed spg gmkl optimizer can be an order of magnitude faster than projected gradient descent on even small and medium sized datasets in some case spg gmkl can even outperform state of the art specialized optimization algorithm developed for a single mkl formulation furthermore we demonstrate that spg gmkl can scale well beyond gradient descent to large problem involving a million kernel or half a million data point our code and implementation are available publically 
social community connect people of similar interest together and play essential role in social network application example of such community include people who like the same object on facebook follow common subject on twitter or join similar group on linkedin among community we notice that some of them are em magnetic to people a em magnet community is such a community that attracts significantly more people s interest and attention than other community of similar topic with the explosive number of self formed community in social network one important demand is to identify magnet community for user this can not only track attractive community but also help improve user experience and increase their engagement e g the login frequency and user generated content quality in this paper we initiate the study of magnet community identification problem first we observe several property of magnet community such a attention flow attention qualify and attention persistence second we formalize these property with the combination of community feature extraction into a graph ranking formulation based on constraint quadratic programming in detail we treat community of a network a super node and their interaction a link among those super node therefore a network of community is defined we extract community s magnet feature from heterogeneous source i e a community s standalone feature and it dependency feature with other community a graph ranking model is formulated given these feature furthermore we define constraint reflecting community magnet property to regularize the model we demonstrate the effectiveness of our framework on real world social network data 
a tree structured multiplicative gamma process tmgp is developed for inferring the depth of a tree based factor analysis model this new model is coupled with the nested chinese restaurant process to nonparametrically infer the depth and width structure of the tree in addition to developing the model theoretical property of the tmgp are addressed and a novel mcmc sampler is developed the structure of the inferred tree is used to learn relationship between high dimensional data and the model is also applied to compressive sensing and interpolation of incomplete image 
to fulfill user search need the search engine must have good performance easy to use functionality and good search result quality search quality evaluation becomes challenging when user satisfaction may not be able to judge by a single search and even within a single search judgment from various source are not consistent in this talk i will discus how user s satisfaction is decomposed into different component in general and how we measure them with various mean human judgment automatic computation with query log and outsourcing and their pro and con with operational implication for an outlook i will postulate potential evaluation approach for a better user s satisfaction 
inferring causal network behind observed data is an active area of research with wide applicability to area such a epidemiology microbiology and social science in particular recent research ha focused on identifying how information propagates through the internet this research ha so far only used temporal feature of observation and while reasonable result have been achieved there is often further information which can be used in this paper we show that additional feature of the observed data can be used very effectively to improve an existing method our particular example is one of inferring an underlying network for how text is reused in the internet although the general approach is applicable to other inference method and information source we develop a method to identify how a piece of text evolves a it move through an underlying network and how substring information can be used to narrow down where in the evolutionary process a particular observation at a node lie hence we narrow down the number of way the node could have acquired the infection text reuse is detected using a suffix tree which is also used to identify the substring relation between chunk of reused text we then use a modification of the netcover method to infer the underlying network experimental result on both synthetic and real life data show that using more information than just timing lead to greater accuracy in the inferred network 
we describe method for continual prediction of manufactured product quality prior to final testing in our most expansive modeling approach an estimated final characteristic of a product is updated after each manufacturing operation our initial application is for the manufacture of microprocessor and we predict final microprocessor speed using these prediction early corrective manufacturing action may be taken to increase the speed of expected slow wafer a collection of microprocessor or reduce the speed of fast wafer such prediction may also be used to initiate corrective supply chain management action developing statistical learning model for this task ha many complicating factor a a temporally unstable population b missing data that is a result of sparsely sampled measurement and c relatively few available measurement prior to corrective action opportunity in a real manufacturing pilot application our automated model selected fast wafer in real time a predicted those wafer were significantly faster than average during manufacture downstream corrective processing restored nominally unacceptable wafer to normal operation 
social medium ha become a major source of information for many application numerous technique have been proposed to analyze network structure and text content in this paper we focus on fine grained mining of contention in discussion debate forum contention are perhaps the most important feature of forum that discus social political and religious issue our goal is to discover contention and agreement indicator expression and contention point or topic both at the discussion collection level and also at each individual post level to the best of our knowledge limited work ha been done on such detailed analysis this paper proposes three model to solve the problem which not only model both contention agreement expression and discussion topic but also more importantly model the intrinsic nature of discussion debate i e interaction among discussant or debater and topic sharing among post through quoting and replying relation evaluation result using real life discussion debate post from several domain demonstrate the effectiveness of the proposed model 
sequential pattern discovery is a well studied field in data mining episode are sequential pattern describing event that often occur in the vicinity of each other episode can impose restriction to the order of the event which make them a versatile technique for describing complex pattern in the sequence most of the research on episode deal with special case such a serial parallel and injective episode while discovering general episode is understudied in this paper we extend the definition of an episode in order to be able to represent case where event often occur simultaneously we present an efficient and novel miner for discovering frequent and closed general episode such a task present unique challenge firstly we cannot define closure based on frequency we solve this by computing a more conservative closure that we use to reduce the search space and discover the closed episode a a postprocessing step secondly episode are traditionally presented a directed acyclic graph we argue that this representation ha drawback leading to redundancy in the output we solve these drawback by defining a subset relationship in such a way that allows u to remove the redundant episode we demonstrate the efficiency of our algorithm and the need for using closed episode empirically on synthetic and real world datasets 
the brief history of knowledge discovery is filled with product that promised to bring bi to the mass but how do you build a product that truly bridge the gap between the conceptual simplicity of question and answer and the structure needed to query traditional data store in this talk chris neumann will discus how datahero applied the principle of user centric design and development over a year and a half to create a product with which more than of new user can get answer on their first attempt he ll demonstrate the process datahero us to determine the best combination of algorithm and user interface concept needed to create intuitive solution to potentially complex interaction including determining the structure of file uploaded by user accurately identifying data type within file presenting user with an optimal visualization for any combination of data helping user to ask question of data when they don t know what to do chris will also talk about what it s like to start a big data company and how he applied lesson from his time a the first engineer at aster data system to datahero 
relational learning is becoming increasingly important in many area of application here we present a novel approach to relational learning based on the factorization of a three way tensor we show that unlike other tensor approach our method is able to perform collective learning via the latent component of the model and provide an efficient algorithm to compute the factorization we substantiate our theoretical consideration regarding the collective learning capability of our model by the mean of experiment on both a new dataset and a dataset commonly used in entity resolution furthermore we show on common benchmark datasets that our approach achieves better or on par result if compared to current state of the art relational learning solution while it is significantly faster to compute 
the use of new technology such a rfid sensor provides scientist with novel way of doing experimental research a scientist become more technologically savvy and use these technique the traditional approach to data analysis fail given the huge amount of data produced by these method in this paper we describe an experiment in which colony of naked mole rat were tagged with rfid transponder rfid sensor were strategically placed in the mole rat caging system the goal of this experiment wa to document and analyze the interaction between animal the huge amount of data produced by the sensor wa not analyzable using the traditional method employed by behavioral neuroscience researcher computational method used by data miner such a cluster analysis association rule mining and graphical model were able to scale to the data and produce knowledge and insight that wa previously unknown this paper describes in detail the experimental setup and the computational method used 
multi task learning mtl learns multiple related task simultaneously to improve generalization performance alternating structure optimization aso is a popular mtl method that learns a shared low dimensional predictive structure on hypothesis space from multiple related task it ha been applied successfully in many real world application a an alternative mtl approach clustered multi task learning cmtl assumes that multiple task follow a clustered structure i e task are partitioned into a set of group where task in the same group are similar to each other and that such a clustered structure is unknown a priori the objective in aso and cmtl differ in how multiple task are related interestingly we show in this paper the equivalence relationship between aso and cmtl providing significant new insight into aso and cmtl a well a their inherent relationship the cmtl formulation is non convex and we adopt a convex relaxation to the cmtl formulation we further establish the equivalence relationship between the proposed convex relaxation of cmtl and an existing convex relaxation of aso and show that the proposed convex cmtl formulation is significantly more efficient especially for high dimensional data in addition we present three algorithm for solving the convex cmtl formulation we report experimental result on benchmark datasets to demonstrate the efficiency of the proposed algorithm 
the gordon data intensive computing system wa designed to handle problem with large memory requirement that cannot easily be solved using standard workstation or distributed memory supercomputer we describe the unique feature of gordon that make it ideally suited for data mining and knowledge discovery application memory aggregation using the vsmp software solution from scalemp i o node containing tb of low latency flash memory and a high performance parallel file system with pb capacity we also demonstrate how a number of standard data mining tool e g matlab weka r can be used effectively on dash an early prototype of the full gordon system 
matching entity from different information source is a very important problem in data analysis and data integration it is however challenging due to the number and diversity of information source involved and the significant editorial effort required to collect sufficient training data in this paper we present an approach that leverage user click during web search to automatically generate training data for entity matching the key insight of our approach is that web page clicked for a given query are likely to be about the same entity we use random walk with restart to reduce data sparseness rely on co clustering to group query and web page and exploit page similarity to improve matching precision experimental result show that i with k page from major travel website we obtain k matchings of k page that refer to the same entity with an average precision of ii the quality of matching obtained from a classifier trained on the resulted seed data is promising the performance match that of editorial data at small size and improves with size 
named entity disambiguation is the task of disambiguating named entity mention in natural language text and link them to their corresponding entry in a knowledge base such a wikipedia such disambiguation can help enhance readability and add semantics to plain text it is also a central step in constructing high quality information network or knowledge graph from unstructured text previous research ha tackled this problem by making use of various textual and structural feature from a knowledge base most of the proposed algorithm assume that a knowledge base can provide enough explicit and useful information to help disambiguate a mention to the right entity however the existing knowledge base are rarely complete likely will never be thus leading to poor performance on short query with not well known context in such case we need to collect additional evidence scattered in internal and external corpus to augment the knowledge base and enhance their disambiguation power in this work we propose a generative model and an incremental algorithm to automatically mine useful evidence across document with a specific modeling of background topic and unknown entity our model is able to harvest useful evidence out of noisy information experimental result show that our proposed method outperforms the state of the art approach significantly boosting the disambiguation accuracy from baseline to on short query derived from tweet 
we formalize the data mining process a a process of information exchange defined by the following key component the data miner s state of mind is modeled a a probability distribution called the background distribution which represents the uncertainty and misconception the data miner ha about the data this model initially incorporates any prior possibly incorrect belief a data miner ha about the data during the data mining process property of the data to which we refer a pattern are revealed to the data miner either in batch one by one or even interactively this acquisition of information in the data mining process is formalized by update to the background distribution to account for the presence of the found pattern the proposed framework can be motivated using concept from information theory and game theory understanding it from this perspective it is easy to see how it can be extended to more sophisticated setting e g where pattern are probabilistic function of the data thus allowing one to account for noise and error in the data mining process and allowing one to study data mining technique based on subsampling the data the framework then model the data mining process using concept from information geometry and i projection in particular the framework can be used to help in designing new data mining algorithm that maximize the efficiency of the information exchange from the algorithm to the data miner 
the problem of distance based outlier detection is difficult to solve efficiently in very large datasets because of potential quadratic time complexity we address this problem and develop sequential and distributed algorithm that are significantly more efficient than state of the art method while still guaranteeing the same outlier by combining simple but effective indexing and disk block accessing technique we have developed a sequential algorithm iorca that is up to an order of magnitude faster than the state of the art the indexing scheme is based on sorting the data point in order of increasing distance from a fixed reference point and then accessing those point based on this sorted order to speed up the basic outlier detection technique we develop two distributed algorithm door and idoor for modern distributed multi core cluster of machine connected on a ring topology the first algorithm pass data block from each machine around the ring incrementally updating the nearest neighbor of the point passed by maintaining a cutoff threshold it is able to prune a large number of point in a distributed fashion the second distributed algorithm extends this basic idea with the indexing scheme discussed earlier in our experiment both distributed algorithm exhibit significant improvement compared to the state of the art distributed method 
the standardization and wider use of electronic medical record emr creates opportunity for better understanding pattern of illness and care within and across medical system our interest is in the temporal history of event code embedded in patient record specifically investigating frequently occurring sequence of event code across patient in studying data from more than million patient history at the university of michigan health system we quickly realized that frequent sequence while providing one level of data reduction still constitute a serious analytical challenge a many involve alternate serialization of the same set of code to further analyze these sequence we designed an approach where a partial order is mined from frequent sequence of code we demonstrate an emr mining system called emrview that enables exploration of the precedence relationship to quickly identify and visualize partial order information encoded in key class of patient we demonstrate some important nugget learned through our approach and also outline key challenge for future research based on our experience 
active learning ha been proven to be effective in reducing labeling effort for supervised learning however existing active learning work ha mainly focused on training model for a single domain in practical application it is common to simultaneously train classifier for multiple domain for example some merchant web site like amazon com may need a set of classifier to predict the sentiment polarity of product review collected from various domain e g electronics book shoe though different domain have their own unique feature they may share some common latent feature if we apply active learning on each domain separately some data instance selected from different domain may contain duplicate knowledge due to the common feature therefore how to choose the data from multiple domain to label is crucial to further reducing the human labeling effort in multi domain learning in this paper we propose a novel multi domain active learning framework to jointly select data instance from all domain with duplicate information considered in our solution a shared subspace is first learned to represent common latent feature of different domain by considering the common and the domain specific feature together the model loss reduction induced by each data instance can be decomposed into a common part and a domain specific part in this way the duplicate information across domain can be encoded into the common part of model loss reduction and taken into account when querying we compare our method with the state of the art active learning approach on several text classification task sentiment classification newsgroup classification and email spam filtering the experiment result show that our method reduces the human labeling effort by and on the three task respectively 
better tool for content based access of video are needed to improve access to time continuous video data particularly information about linear tv broadcast program ha been available in a form limited to program guide that provide short manually described overview of the program content recent development in digitalization of tv broadcasting and emergence of web based service for catch up and on demand viewing bring out new possibility to access data in this paper we introduce our data mining system and accompanying service for summarizing finnish dvb broadcast stream from seven national channel we describe how data mining of novelty concept can be extracted from dvb subtitle to augment web based catch up tv guide and novelty cloud tv service furthermore our system allows accessing medium fragment a picture quote via generated word list and provides content based recommendation to find new program that have content similar to the user selected program our index consists of over program that are used to recommend relevant program the service ha been under development and available online since it ha registered over user session 
since baidu ha set it mission a providing the best way for people to find what they re looking for today the company ha become the world s largest chinese search engine everyday we process billion of search query and serve hundred of million of internet user the huge volume of online text and multimedia content a well a user log data provide u unprecedented opportunity and challenge for further accomplishing our mission in addition we have seen several megatrends cloud computing is becoming a pervasive service infrastructure mobile internet will surpass traditional internet in user time spend and social platform ha demonstrated it great power in response to all these opportunity challenge and megatrends we must think ahead on the major technology focus that may help baidu to serve our user better in this talk i would like to share with the audience the nine area that are the most important and interesting in my mind for each of the area i will describe the challenge and explain why it is important and interesting i hope the research community can get excited and help u provide better service for user 
in this paper we investigate a problem of predicting what image are likely to appear on the web at a future time point given a query word and a database of historical image stream that potentiates learning of uploading pattern of previous user image and associated metadata we address such a web image prediction problem at both a collective group level and an individual user level we develop a predictive framework based on the multivariate point process which employ a stochastic parametric model to solve the relation between image occurrence and the covariates that influence it in a flexible scalable and globally optimal way using flickr datasets of more than ten million image of topic our empirical result show that the proposed algorithm is more successful in predicting unseen web image than other candidate method including forecasting on semantic meaning only a pagerank based image retrieval and a generative author time topic model 
this work combine the central idea from two different area crowd simulation and social network analysis to tackle some existing problem in both area from a new angle we present a novel spatio temporal social crowd simulation framework social flock to revisit three essential research problem a generation of social network b community detection in social network c modeling collective social behavior in crowd simulation our framework produce social network that satisfy the property of high clustering coefficient low average path length and power law degree distribution it can also be exploited a a novel dynamic model for community detection finally our framework can be used to produce real life collective social behavior over crowd including community guided flocking leader following and spatio social information propagation social flock can serve a visualization of simulated crowd for domain expert to explore the dynamic effect of the spatial temporal and social factor on social network in addition it provides an experimental platform of collective social behavior for social gaming and movie animation social flock demo is at http mslab csie ntu edu tw socialflocks 
active learning is a machine learning and data mining technique that selects the most informative sample for labeling and us them a training data it is especially useful when there are large amount of unlabeled data and labeling them is expensive recently batch mode active learning where a set of sample are selected concurrently for labeling based on their collective merit ha attracted a lot of attention the objective of batch mode active learning is to select a set of informative sample so that a classifier learned on these sample ha good generalization performance on the unlabeled data most of the existing batch mode active learning methodology try to achieve this by selecting sample based on varied criterion in this paper we propose a novel criterion which achieves good generalization performance of a classifier by specifically selecting a set of query sample that minimizes the difference in distribution between the labeled and the unlabeled data after annotation we explicitly measure this difference based on all candidate subset of the unlabeled data and select the best subset the proposed objective is an np hard integer programming optimization problem we provide two optimization technique to solve this problem in the first one the problem is transformed into a convex quadratic programming problem and in the second method the problem is transformed into a linear programming problem our empirical study using publicly available uci datasets and a biomedical image dataset demonstrate the effectiveness of the proposed approach in comparison with the state of the art batch mode active learning method we also present two extension of the proposed approach which incorporate uncertainty of the predicted label of the unlabeled data and transfer learning in the proposed formulation our empirical study on uci datasets show that incorporation of uncertainty information improves performance at later iteration while our study on newsgroups dataset show that transfer learning improves the performance of the classifier during initial iteration 
learning temporal dependency between variable over continuous time is an important and challenging task continuous time bayesian network effectively model such process but are limited by the number of conditional intensity matrix which grows exponentially in the number of parent per variable we develop a partition based representation using regression tree and forest whose parameter space grow linearly in the number of node split using a multiplicative assumption we show how to update the forest likelihood in closed form producing efficient model update our result show multiplicative forest can be learned from few temporal trajectory with large gain in performance and scalability 
advanced technology in gps and sensor enables u to track physical event such a human movement and facility usage periodicity analysis from the recorded data is an important data mining task which provides useful insight into the physical event and enables u to report outlier and predict future behavior to mine periodicity in an event we have to face real world challenge of inherently complicated periodic behavior and imperfect data collection problem specifically the hidden temporal periodic behavior could be oscillating and noisy and the observation of the event could be incomplete in this paper we propose a novel probabilistic measure for periodicity and design a practical method to detect period our method ha thoroughly considered the uncertainty and noise in periodic behavior and is provably robust to incomplete observation comprehensive experiment on both synthetic and real datasets demonstrate the effectiveness of our method 
understanding how research theme evolve over time in a research community is useful in many way e g revealing important milestone and discovering emerging major research trend in this paper we propose a novel way of analyzing literature citation to explore the research topic and the theme evolution by modeling article citation relation with a probabilistic generative model the key idea is to represent a research paper by a bag of citation and model such a citation document with a probabilistic topic model we explore the extension of a particular topic model i e latent dirichlet allocation lda for citation analysis and show that such a citation lda can facilitate discovering of individual research topic a well a the theme evolution from multiple related topic both of which in turn lead to the construction of evolution graph for characterizing research theme we test the proposed citation lda on two datasets the acl anthology network aan of natural language research literature and pubmed central pmc archive of biomedical and life science literature and demonstrate that citation lda can effectively discover the evolution of research theme with better formed topic than conventional content lda 
linear classification ha achieved complexity linear to the data size however in many application data contain large amount of sample that doe not help improve the quality of model but still cost much i o and memory to process in this paper we show how a block coordinate descent method based on nearest neighbor index can significantly reduce such cost when learning a dual sparse model in particular we employ truncated loss function to induce a series of convex program with superior dual sparsity and solve each dual using indexed block coordinate descent which make use of approximate nearest neighbor ann search to select active dual variable without i o cost on irrelevant sample we prove that despite the bias and weak guarantee from ann query the proposed algorithm ha global convergence to the solution defined on entire dataset with sublinear complexity each iteration experiment in both sufficient and limited memory condition show that the proposed approach learns many time faster than other state of the art solver without sacrificing accuracy 
online review play a crucial role in today s electronic commerce it is desirable for a customer to read review of product or store before making the decision of what or from where to buy due to the pervasive spam review customer can be misled to buy low quality product while decent store can be defamed by malicious review we observe that in reality a great portion in the data we study of the reviewer write only one review singleton review these review are so enormous in number that they can almost determine a store s rating and impression however existing method did not examine this larger part of the review are most of these singleton review truthful one if not how to detect spam review in singleton review we call this problem singleton review spam detection to address this problem we observe that the normal reviewer arrival pattern is stable and uncorrelated to their rating pattern temporally in contrast spam attack are usually bursty and either positively or negatively correlated to the rating thus we propose to detect such attack via unusually correlated temporal pattern we identify and construct multidimensional time series based on aggregate statistic in order to depict and mine such correlation in this way the singleton review spam detection problem is mapped to a abnormally correlated pattern detection problem we propose a hierarchical algorithm to robustly detect the time window where such attack are likely to have happened the algorithm also pinpoint such window in different time resolution to facilitate faster human inspection experimental result show that the proposed method is effective in detecting singleton review attack we discover that singleton review is a significant source of spam review and largely affect the rating of online store 
in recent time collaborative filtering based recommender system r have become extremely popular while research in recommender system ha mostly focused on improving the accuracy of recommendation in this paper we look at the flip side of a r that is instead of improving existing recommender algorithm we ask whether we can use an existing operational r to launch a targeted marketing campaign to this end we propose a novel problem called recmax that aim to select a set of seed user for a marketing campaign for a new product such that if they endorse the product by providing relatively high rating the number of other user to whom the product is recommended by the underlying r algorithm is maximum we motivate recmax with real world application we show that seeding can make a substantial difference if done carefully we prove that recmax is not only np hard to solve optimally it is np hard to even approximate within any reasonable factor given this hardness we explore several natural heuristic on real world datasets movielens yahoo music and jester joke and report our finding we show that even though recmax is hard to approximate simple natural heuristic may provide impressive gain for targeted marketing using r 
understanding and quantifying the impact of unobserved process is one of the major challenge of analyzing multivariate time series data in this paper we analyze a flexible stochastic process model the generalized linear auto regressive process glarp and identify the condition under which the impact of hidden variable appears a an additive term to the evolution matrix estimated with the maximum likelihood in particular we examine three example including two popular model for count data i e poisson and conwey maxwell poisson vector auto regressive process and one powerful model for extreme value data i e gumbel vector auto regressive process we demonstrate that the impact of hidden factor can be separated out via convex optimization in these three model we also propose a fast greedy algorithm based on the selection of composite atom in each iteration and provide a performance guarantee for it experiment on two synthetic datasets one social network dataset and one climatology dataset demonstrate the the superior performance of our proposed model 
high dimensional regression classification continues to be an important and challenging problem especially when feature are highly correlated feature selection combined with additional structure information on the feature ha been considered to be promising in promoting regression classification performance graph guided fused lasso gflasso ha recently been proposed to facilitate feature selection and graph structure exploitation when feature exhibit certain graph structure however the formulation in gflasso relies on pairwise sample correlation to perform feature grouping which could introduce additional estimation bias in this paper we propose three new feature grouping and selection method to resolve this issue the first method employ a convex function to penalize the pairwise l norm of connected regression classification coefficient achieving simultaneous feature grouping and selection the second method improves the first one by utilizing a non convex function to reduce the estimation bias the third one is the extension of the second method using a truncated l regularization to further reduce the estimation bias the proposed method combine feature grouping and feature selection to enhance estimation accuracy we employ the alternating direction method of multiplier admm and difference of convex function dc programming to solve the proposed formulation our experimental result on synthetic data and two real datasets demonstrate the effectiveness of the proposed method 
social medium is a platform for people to share and vote content from the analysis of the social medium data we found that user are quite inactive in rating voting for example a user on average only vote out of accessed item traditional recommendation method are mostly based on user vote and thus can not cope with this situation based on the observation that the dwell time on an item may reflect the opinion of a user we aim to enrich the user vote matrix by converting the dwell time on item into user pseudo vote and then help improve recommendation performance however it is challenging to correctly interpret the dwell time since many subjective human factor e g user expectation sensitivity to various item quality reading speed are involved into the casual behavior of online reading in psychology it is assumed that people have choice threshold in decision making the time spent on making decision reflects the decision maker s threshold this idea inspires u to develop a view voting model which can estimate how much the user like the viewed item according to her dwell time and thus make recommendation even if there is no voting data available finally our experimental evaluation show that the traditional rate based recommendation s performance is greatly improved with the support of vv model 
user can rarely reveal their information need in full detail to a search engine within word so search engine need to hedge their bet and present diverse result within the precious response slot diversity in ranking is of much recent interest most existing solution estimate the marginal utility of an item given a set of item already in the response and then use variant of greedy set cover others design graph with the item a node and choose diverse item based on visit rate pagerank here we introduce a radically new and natural formulation of diversity a finding center in resistive graph unlike in pagerank we do not specify the edge resistance equivalently conductance and ask for node visit rate instead we look for a sparse set of center node so that the effective conductance from the center to the rest of the graph ha maximum entropy we give a cogent semantic justification for turning pagerank thus on it head in marked deviation from prior work our edge resistance are learnt from training data inference and learning are np hard but we give practical solution in extensive experiment with subtopic retrieval social network search and document summarization our approach convincingly surpasses recently published diversity algorithm like subtopic cover max marginal relevance mmr grasshopper divrank and svmdiv 
identifying genetic variation underlying a complex disease is important many complex disease have heterogeneous phenotype and are product of a variety of genetic and environmental factor acting in concert deriving highly heritable quantitative trait of a complex disease can improve the identification of genetic risk of the disease the most sophisticated method so far perform unsupervised cluster analysis on phenotypic feature and then a quantitative trait is derived based on each resultant cluster heritability is estimated to ass the validity of the derived quantitative trait however none of these method explicitly maximize the heritability of the derived trait we propose a quadratic optimization approach that directly utilizes heritability a an objective during the derivation of quantitative trait of a disease this method maximizes an objective function that is formulated by decomposing the traditional maximum likelihood method for estimating heritability of a quantitative trait we demonstrate the effectiveness of the proposed method on both synthetic data and real world problem we apply our algorithm to identify highly heritable trait of complex human behavior disorder including opioid and cocaine use disorder and highly heritable trait of dairy cattle that are economically important our approach outperforms standard cluster analysis and several previous method 
we present shoppingadvisor a novel recommender system that help user in shopping for technical product shoppingadvisor leverage both user preference and technical product attribute in order to generate it suggestion the system elicits user preference via a tree shaped flowchart where each node is a question to the user at each node shoppingadvisor suggests a ranking of product matching the preference of the user and that get progressively refined along the path from the tree s root to one of it leaf in this paper we show i how to learn the structure of the tree i e which question to ask at each node and ii how to produce a suitable ranking at each node first we adapt the classical top down strategy for building decision tree in order to find the best user attribute to ask at each node differently from decision tree shoppingadvisor partition the user space rather than the product space second we show how to employ a learning to rank approach in order to learn for each node of the tree a ranking of product appropriate to the user who reach that node we experiment with two real world datasets for car and camera and a synthetic one we use mean reciprocal rank to evaluate shoppingadvisor and show how the performance increase by more than along the path from root to leaf we also show how collaborative recommendation algorithm such a k nearest neighbor benefit from feature selection done by the shoppingadvisor tree our experiment show that shoppingadvisor produce good quality interpretable recommendation while requiring le input from user and being able to handle the cold start problem 
alzheimer s disease ad is the most common neurodegenerative disorder associated with aging understanding how the disease progress and identifying related pathological biomarkers for the progression is of primary importance in the clinical diagnosis and prognosis of alzheimer s disease in this paper we develop novel multi task learning technique to predict the disease progression measured by cognitive score and select biomarkers predictive of the progression in multi task learning the prediction of cognitive score at each time point is considered a a task and multiple prediction task at different time point are performed simultaneously to capture the temporal smoothness of the prediction model across different time point specifically we propose a novel convex fused sparse group lasso cfsgl formulation that allows the simultaneous selection of a common set of biomarkers for multiple time point and specific set of biomarkers for different time point using the sparse group lasso penalty and in the meantime incorporates the temporal smoothness using the fused lasso penalty the proposed formulation is challenging to solve due to the use of several non smooth penalty one of the main technical contribution of this paper is to show that the proximal operator associated with the proposed formulation exhibit a certain decomposition property and can be computed efficiently thus cfsgl can be solved efficiently using the accelerated gradient method to further improve the model we propose two non convex formulation to reduce the shrinkage bias inherent in the convex formulation we employ the difference of convex dc programming technique to solve the non convex formulation we have performed extensive experiment using data from the alzheimer s disease neuroimaging initiative adni result demonstrate the effectiveness of the proposed progression model in comparison with existing method for disease progression we also perform longitudinal stability selection to identify and analyze the temporal pattern of biomarkers in disease progression 
social medium is becoming increasingly ubiquitous and popular on the internet due to the huge popularity of social medium website such a facebook twitter youtube and flickr many company or public figure are now active in maintaining page on those website to interact with online user attracting a large number of fan follower by posting interesting object e g product photo video and text message like ha now become a very popular social function by allowing user to express their like of certain object it provides an accurate way of estimating user interest and an effective way of sharing promoting information in social medium in this demo we propose a system called likeminer to mine the power of like in social medium network we introduce a heterogeneous network model for social medium with like and propose like mining algorithm to estimate representativeness and influence of object the implemented prototype system demonstrates the effectiveness of the proposed approach using the large scale facebook data 
most data mining research is concerned with building high quality classification model in isolation in massive production system however the ability to monitor and maintain performance over time while growing in size and scope is equally important many external factor may degrade classification performance including change in data distribution noise or bias in the source data and the evolution of the system itself a well functioning system must gracefully handle all of these this paper lay out a set of design principle for large scale autonomous data mining system and then demonstrates our application of these principle within the m d automated ad targeting system we demonstrate a comprehensive set of quality control process that allow u monitor and maintain thousand of distinct classification model automatically and to add new model take on new data and correct poorly performing model without manual intervention or system disruption 
mining detailed opinion buried in the vast amount of review text data is an important yet quite challenging task with widespread application in multiple domain latent aspect rating analysis lara refers to the task of inferring both opinion rating on topical aspect e g location service of a hotel and the relative weight reviewer have placed on each aspect based on review content and the associated overall rating a major limitation of previous work on lara is the assumption of pre specified aspect by keywords however the aspect information is not always available and it may be difficult to pre define appropriate aspect without a good knowledge about what aspect are actually commented on in the review in this paper we propose a unified generative model for lara which doe not need pre specified aspect keywords and simultaneously mine latent topical aspect rating on each identified aspect and weight placed on different aspect by a reviewer experiment result on two different review data set demonstrate that the proposed model can effectively perform the latent aspect rating analysis task without the supervision of aspect keywords because of it generality the proposed model can be applied to explore all kind of opinionated text data containing overall sentiment judgment and support a wide range of interesting application task such a aspect based opinion summarization personalized entity ranking and recommendation and reviewer behavior analysis 
link prediction system have been largely adopted to recommend new friend in online social network using data about social interaction with the soaring adoption of location based social service it becomes possible to take advantage of an additional source of information the place people visit in this paper we study the problem of designing a link prediction system for online location based social network we have gathered extensive data about one of these service gowalla with periodic snapshot to capture it temporal evolution we study the link prediction space finding that about of new link are added among place friend i e among user who visit the same place we show how this prediction space can be made time smaller while still of future connection can be discovered thus we define new prediction feature based on the property of the place visited by user which are able to discriminate potential future link among them building on these finding we describe a supervised learning framework which exploit these prediction feature to predict new link among friend of friend and place friend our evaluation show how the inclusion of information about place and related user activity offer high link prediction performance these result open new direction for real world link recommendation system on location based social network 
we are concerned with the issue of detecting change of clustering structure from multivariate time series from the viewpoint of the minimum description length mdl principle we propose an algorithm that track change of clustering structure so that the sum of the code length for data and that for clustering change is minimum here we employ a gaussian mixture model gmm a representation of clustering and compute the code length for data sequence using the normalized maximum likelihood nml coding the proposed algorithm enables u to deal with clustering dynamic including merging splitting emergence disappearance of cluster from a unifying view of the mdl principle we empirically demonstrate using artificial data set that our proposed method is able to detect cluster change significantly more accurately than an existing statistical test based method and aic bic based method we further use real customer transaction data set to demonstrate the validity of our algorithm in market analysis we show that it is able to detect change of customer group which correspond to change of real market environment 
in recent year mining frequent itemsets over uncertain data ha attracted much attention in the data mining community unlike the corresponding problem in deterministic data the frequent itemset under uncertain data ha two different definition the expected support based frequent itemset and the probabilistic frequent itemset most existing work only focus on one of the definition and no comprehensive study is conducted to compare the two different definition moreover due to lacking the uniform implementation platform existing solution for the same definition even generate inconsistent result in this demo we present a demonstration called a ufimt underline uncertain frequent itemset mining toolbox which not only discovers frequent itemsets over uncertain data but also compare the performance of different algorithm and demonstrates the relationship between different definition in this demo we firstly present important technique and implementation skill of the mining problem secondly we show the system architecture of ufimt thirdly we report an empirical analysis on extensive both real and synthetic benchmark data set which are used to compare different algorithm and to show the close relationship between two different frequent itemset definition and finally we discus some existing challenge and new finding 
heterogeneous information network that contain multiple type of object and link are ubiquitous in the real world such a bibliographic network cyber physical network and social medium network although researcher have studied various data mining task in information network interactive query based network exploration technique have not been addressed systematically which in fact are highly desirable for exploring large scale information network in this demo we introduce and demonstrate our recent research project on query driven discovery of semantically similar substructure in heterogeneous network given a subgraph query our system search a given large information network and find efficiently a list of subgraphs that are structurally identical and semantically similar since data mining method are used to obtain semantically similar entity node we use discovery a a term to describe this process in order to achieve high efficiency and scalability we design and implement a filter and verification search framework which can first generate promising subgraph candidate using off line index built by data mining result and then verify candidate with a recursive pruning matching process the proposed system demonstrates the effectiveness of our query driven semantic similarity search framework and the efficiency of the proposed methodology on multiple real world heterogeneous information network 
the increasing number of core and the rich instruction set of modern hardware are opening up new opportunity for optimizing many traditional data mining task in this paper we demonstrate how to speed up the performance of the computation of frequent item by almost one order of magnitude over the best published result by matching the algorithm to the underlying hardware architecture we start with the observation that frequent item counting like other data mining task assumes certain amount of skew in the data we exploit this skew to design a new algorithm that us a pre filtering stage that can be implemented in a highly efficient manner through simd instruction using pipelining we then combine this pre filtering stage with a conventional frequent item algorithm space saving that will process the remainder of the data the resulting operator can be parallelized with a small number of core leading to a parallel implementation that doe not suffer any of the overhead of existing parallel solution when querying the result and offer significantly higher throughput 
triangle listing is one of the fundamental algorithmic problem whose solution ha numerous application especially in the analysis of complex network such a the computation of clustering coefficient transitivity triangular connectivity etc existing algorithm for triangle listing are mainly in memory algorithm whose performance cannot scale with the massive volume of today s fast growing network when the input graph cannot fit into main memory triangle listing requires random disk access that can incur prohibitively large i o cost some streaming and sampling algorithm have been proposed but these are approximation algorithm we propose an i o efficient algorithm for triangle listing our algorithm is exact and avoids random disk access our result show that our algorithm is scalable and outperforms the state of the art local triangle estimation algorithm 
given huge collection of time evolving event such a web click log which consist of multiple attribute e g url userid timestamp how do we find pattern and trend how do we go about capturing daily pattern and forecasting future event we need two property a effectiveness that is the pattern should help u understand the data discover group and enable forecasting and b scalability that is the method should be linear with the data size we introduce trimine which performs three way mining for all three attribute namely url user and time specifically trimine discovers hidden topic group of url and group of user simultaneously thanks to it concise but effective summarization it make it possible to accomplish the most challenging and important task namely to forecast future event extensive experiment on real datasets demonstrate that trimine discovers meaningful topic and make long range forecast which are notoriously difficult to achieve in fact trimine consistently outperforms the best state of the art existing method in term of accuracy and execution speed up to x faster 
understanding topic hierarchy in text stream and their evolution pattern over time is very important in many application in this paper we propose an evolutionary multi branch tree clustering method for streaming text data we build evolutionary tree in a bayesian online filtering framework the tree construction is formulated a an online posterior estimation problem which considers both the likelihood of the current tree and conditional prior given the previous tree we also introduce a constraint model to compute the conditional prior of a tree in the multi branch setting experiment on real world news data demonstrate that our algorithm can better incorporate historical tree information and is more efficient and effective than the traditional evolutionary hierarchical clustering algorithm 
we present eventsearch a system for event extraction and retrieval on four type of news related historical data i e web news article newspaper tv news program and micro blog short message the system incorporates over million web page extracted from web infomall the chinese web archive since the newspaper and tv news video clip also span from to the system upon a user query return a list of event snippet from multiple data source a novel burst model is used to discover event from time stamped text in addition to offline event extraction our system also provides online event extraction to further meet the user need eventsearch provides meaningful analytics that synthesize an accurate description of event user interact with the system by ranking the identified event using different criterion scale recency and relevance and submitting their own information need in different input field 
a new hierarchical tree based topic model is developed based on nonparametric bayesian technique the model ha two unique attribute i a child node in the tree may have more than one parent with the goal of eliminating redundant sub topic deep in the tree and ii parsimonious sub topic are manifested by removing redundant usage of word at multiple scale the depth and width of the tree are unbounded within the prior with a retrospective sampler employed to adaptively infer the appropriate tree size based upon the corpus under study excellent quantitative result are manifested on five standard data set and the inferred tree structure is also found to be highly interpretable 
data mining is a crucial tool for identifying risk signal of potential adverse drug reaction adrs however mining of adr signal is currently limited to leveraging a single data source at a time it is widely believed that combining adr evidence from multiple data source will result in a more accurate risk identification system we present a methodology based on empirical bayes modeling to combine adr signal mined from million adverse event report collected by the fda and healthcare data corresponding to million patient the main two type of information source currently employed for signal detection based on four set of test case gold standard we demonstrate that our method lead to a statistically significant and substantial improvement in signal detection accuracy averaging over the use of each source independently and an area under the roc curve of we also compare the method with alternative supervised learning approach and argue that our approach is preferable a it doe not require labeled training sample whose availability is currently limited to our knowledge this is the first effort to combine signal from these two complementary data source and to demonstrate the benefit of a computationally integrative strategy for drug safety surveillance 
associative classification is a predictive modeling technique that construct a classifier based on class association rule also known a predictive association rule par par are association rule where the consequence of the rule is a class label associative classification ha gained substantial research attention because it successfully join the benefit of association rule mining with classification these benefit include the inherent ability of association rule mining to extract high order interaction among the predictor an ability that many modern classifier lack and also the natural interpretability of the individual par associative classification is not without it caveat association rule mining often discovers a combinatorially large number of association rule eroding the interpretability of the rule set extensive effort ha been directed towards developing interestingness measure which filter predictive association rule after they have been generated these interestingness measure albeit very successful at selecting interesting rule lack two feature that are highly valuable in the context of classification first only few of the interestingness measure are rooted in a statistical model given the distinction between a training and a test data set in the classification setting the ability to make statistical inference about the performance of the predictive classification rule on the test set is highly desirable second the unfiltered set of predictive assocation rule par are often redundant we can prove that certain par will not be used to construct a classification model given the presence of other par in this paper we propose a simple statistical model towards making inference on the test set about the various performance metric of predictive association rule we also derive three filtering criterion based on hypothesis testing which are very selective reduce the number of par to be considered by the classifier by several order of magnitude yet do not effect the performance of the classification adversely in the case where the classification model is constructed a a logistic model on top of the par we can mathematically prove that the filtering criterion do not significantly effect the classifier s performance we also demonstrate empirically on three publicly available data set that the vast reduction in the number of par indeed did not come at the cost of reducing the predictive performance 
with the support of the legally grounded methodology of situation testing we tackle the problem of discrimination discovery and prevention from a dataset of historical decision by adopting a variant of k nn classification a tuple is labeled a discriminated if we can observe a significant difference of treatment among it neighbor belonging to a protected by law group and it neighbor not belonging to it discrimination discovery boil down to extracting a classification model from the labeled tuples discrimination prevention is tackled by changing the decision value for tuples labeled a discriminated before training a classifier the approach of this paper overcomes legal weakness and technical limitation of existing proposal 
influential people have an important role in the process of information diffusion however there are several way to be influential for example to be the most popular or the first that adopts a new idea in this paper we present a methodology to find trendsetters in information network according to a specific topic of interest trendsetters are people that adopt and spread new idea influencing other people before these idea become popular at the same time not all early adopter are trendsetters because only few of them have the ability of propagating their idea by their social contact through word of mouth differently from other influence measure a trendsetter is not necessarily popular or famous but the one whose idea spread over the graph successfully other metric such a node in degree or even standard pagerank focus only in the static topology of the network we propose a ranking strategy that focus on the ability of some user to push new idea that will be successful in the future to that end we combine temporal attribute of node and edge of the network with a pagerank based algorithm to find the trendsetters for a given topic to test our algorithm we conduct innovative experiment over a large twitter dataset we show that node with high in degree tend to arrive late for new trend while user in the top of our ranking tend to be early adopter that also influence their social contact to adopt the new trend 
mobile connected device and smartphones in particular are rapidly emerging a a dominant computing and sensing platform this pose several unique opportunity for data collection and analysis a well a new challenge in this tutorial we survey the state of the art in term of mining data from mobile device across different application area such a ad healthcare geosocial public policy etc our tutorial ha three part in part one we summarize data collection in term of various sensing modality in part two we present cross cutting challenge such a real time analysis security and we outline cross cutting method for mobile data mining such a network inference streaming algorithm etc in the last part we specifically overview emerging and fast growing application area such a noted above concluding we briefly highlight the opportunity for joint design of new data collection technique and analysis method suggesting additional direction for future research 
frequent pattern mining often produce an enormous number of frequent pattern which imposes a great challenge on understanding and further analysis of the generated pattern this call for finding a small number of representative pattern to best approximate all other pattern an ideal approach should produce a minimum number of representative pattern restore the support of all pattern with error guarantee and have good efficiency few existing approach can satisfy all the three requirement in this paper we develop two algorithm minrpset and flexrpset for finding minimum representative pattern set both algorithm provide error guarantee minrpset produce the smallest solution that we can possibly have in practice under the given problem setting and it take a reasonable amount of time to finish flexrpset is developed based on minrpset it provides one extra parameter k to allow user to make a trade off between result size and efficiency our experiment result show that minrpset and flexrpset produce fewer representative pattern than rplocal an efficient algorithm that is developed for solving the same problem flexrpset can be slightly faster than rplocal when k is small 
the area under the roc curve auc is a well known performance measure in machine learning and data mining in an increasing number of application however ranging from ranking application to a variety of important bioinformatics application performance is measured in term of the partial area under the roc curve between two specified false positive rate in recent work we proposed a structural svm based approach for optimizing this performance measure narasimhan and agarwal in this paper we develop a new support vector method svmpauctight that optimizes a tighter convex upper bound on the partial auc loss which lead to both improved accuracy and reduced computational complexity in particular by rewriting the empirical partial auc risk a a maximum over subset of negative instance we derive a new formulation where a modified form of the earlier optimization objective is evaluated on each of these subset leading to a tighter hinge relaxation on the partial auc loss a with our previous method the resulting optimization problem can be solved using a cutting plane algorithm but the new method ha better run time guarantee we also discus a projected subgradient method for solving this problem which offer additional computational saving in certain setting we demonstrate on a wide variety of bioinformatics task ranging from protein protein interaction prediction to drug discovery task that the proposed method doe in many case perform significantly better on the partial auc measure than the previous structural svm approach in addition we also develop extension of our method to learn sparse and group sparse model often of interest in biological application 
conventional video search system to find relevant video rely on textual data such a video title annotation and text around the video nowadays video recording device such a ameras smartphones and car blackboxes are equipped with gps sensor and able to capture video with spatiotemporal information such a time location and camera direction we call such video georeferenced video this paper present a georeferenced video retrieval system geosearch which efficiently retrieves video containing a certain point or range in the map to enable a fast search of georeferenced video geosearch adopts a novel data structure mbtr minimum bounding tilted rectangle in the leaf node of r tree new algorithm are developed to build mbtrs from georeferenced video and to efficiently process point and range query on mbtrs we demonstrate our system on real georeferenced video and show that compared to previous method geosearch substantially reduces the index size and also improves the search speed for georeferenced video data our online demo is available at http dm hwanjoyu org geosearch 
ensemble learning ha become a common tool for data stream classification being able to handle large volume of stream data and concept drifting previous study focus on building accurate prediction model from stream data however a linear scan of a large number of base classifier in the ensemble during prediction incurs significant cost in response time preventing ensemble learning from being practical for many real world time critical data stream application such a web traffic stream monitoring spam detection and intrusion detection in these application data stream usually arrive at a speed of gb second and it is necessary to classify each stream record in a timely manner to address this problem we propose a novel ensemble tree e tree for short indexing structure to organize all base classifier in an ensemble for fast prediction on one hand e tree treat ensemble a spatial database and employ an r tree like height balanced structure to reduce the expected prediction time from linear to sub linear complexity on the other hand e tree can automatically update themselves by continuously integrating new classifier and discarding outdated one well adapting to new trend and pattern underneath data stream experiment on both synthetic and real world data stream demonstrate the performance of our approach 
our understanding of how individual mobility pattern shape and impact the social network is limited but is essential for a deeper understanding of network dynamic and evolution this question is largely unexplored partly due to the difficulty in obtaining large scale society wide data that simultaneously capture the dynamical information on individual movement and social interaction here we address this challenge for the first time by tracking the trajectory and communication record of million mobile phone user we find that the similarity between two individual movement strongly correlate with their proximity in the social network we further investigate how the predictive power hidden in such correlation can be exploited to address a challenging problem which new link will develop in a social network we show that mobility measure alone yield surprising predictive power comparable to traditional network based measure furthermore the prediction accuracy can be significantly improved by learning a supervised classifier based on combined mobility and network measure we believe our finding on the interplay of mobility pattern and social tie offer new perspective on not only link prediction but also network dynamic 
in recent year a rich variety of shrinkage prior have been proposed that have great promise in addressing massive regression problem in general these new prior can be expressed a scale mixture of normal but have more complex form and better property than traditional cauchy and double exponential prior we first propose a new class of normal scale mixture through a novel generalized beta distribution that encompasses many interesting prior a special case this encompassing framework should prove useful in comparing competing prior considering property and revealing close connection we then develop a class of variational bayes approximation through the new hierarchy presented that will scale more efficiently to the type of truly massive data set that are now encountered routinely 
for a number of year we have been conducting controlled human subject experiment in distributed social computation in network with only limited and local communication these experiment cast a number of traditional computational economic and sociological problem including graph coloring consensus independent set networked bargaining biased voting and network formation a game of strategic interaction in which subject have financial incentive to collectively compute global solution i will overview and summarize the many behavioral finding from this line of experimentation i will give particular emphasis to the novel data the experiment have generated and the analysis this data ha permitted including quantitative study of subject personality trait such a stubbornness altruism and patience and whether those trait seem helpful or harmful to individual and collective performance 
entity resolution er the problem of extracting matching and resolving entity mention in structured and unstructured data is a long standing challenge in database management information retrieval machine learning natural language processing and statistic accurate and fast entity resolution ha huge practical implication in a wide variety of commercial scientific and security domain despite the long history of work on entity resolution there is still a surprising diversity of approach and lack of guiding theory meanwhile in the age of big data the need for high quality entity resolution is growing a we are inundated with more and more data all of which need to be integrated aligned and matched before further utility can be extracted in this tutorial we bring together perspective on entity resolution from a variety of field including database information retrieval natural language processing and machine learning to provide in one setting a survey of a large body of work we discus both the practical aspect and theoretical underpinnings of er we describe existing solution current challenge and open research problem in addition to giving attendee a thorough understanding of existing er model algorithm and evaluation method the tutorial will cover important research topic such a scalable er active and lightly supervised er and query driven er 
text corpus with document from a range of time epoch are natural and ubiquitous in many field such a research paper newspaper article and a variety of type of recently emerged social medium people not only would like to know what kind of topic can be found from these data source but also wish to understand the temporal dynamic of these topic and predict certain property of term or document in the future topic model are usually utilized to find latent topic from text collection and recently have been applied to temporal text corpus however most proposed model are general purpose model to which no real task are explicitly associated therefore current model may be difficult to apply in real world application such a the problem of tracking trend and predicting popularity of keywords in this paper we introduce a real world task tracking trend of term to which temporal topic model can be applied rather than building a general purpose model we propose a new type of topic model that incorporates the volume of term into the temporal dynamic of topic and optimizes estimate of term volume in existing model trend are either latent variable or not considered at all which limit the potential for practical use of trend information in contrast we combine state space model with term volume with a supervised learning model enabling u to effectively predict the volume in the future even without new document in addition it is straightforward to obtain the volume of latent topic a a by product of our model demonstrating the superiority of utilizing temporal topic model over traditional time series tool e g autoregressive model to tackle this kind of problem the proposed model can be further extended with arbitrary word level feature which are evolving over time we present the result of applying the model to two datasets with long time period and show it effectiveness over non trivial baseline 
in large scale application of undirected graphical model such a social network and biological network similar pattern occur frequently and give rise to similar parameter in this situation it is beneficial to group the parameter for more efficient learning we show that even when the grouping is unknown we can infer these parameter group during learning via a bayesian approach we impose a dirichlet process prior on the parameter posterior inference usually involves calculating intractable term and we propose two approximation algorithm namely a metropolis hastings algorithm with auxiliary variable and a gibbs sampling algorithm with stripped beta approximation gibbs sba simulation show that both algorithm outperform conventional maximum likelihood estimation mle gibbs sba s performance is close to gibbs sampling with exact likelihood calculation model learned with gibbs sba also generalize better than the model learned by mle on real world senate voting data 
multi label classification is prevalent in many real world application where each example can be associated with a set of multiple label simultaneously the key challenge of multi label classification come from the large space of all possible label set which is exponential to the number of candidate label most previous work focus on exploiting correlation among different label to facilitate the learning process it is usually assumed that the label correlation are given beforehand or can be derived directly from data sample by counting their label co occurrence however in many real world multi label classification task the label correlation are not given and can be hard to learn directly from data sample within a moderate sized training set heterogeneous information network can provide abundant knowledge about relationship among different type of entity including data sample and class label in this paper we propose to use heterogeneous information network to facilitate the multi label classification process by mining the linkage structure of heterogeneous information network multiple type of relationship among different class label and data sample can be extracted then we can use these relationship to effectively infer the correlation among different class label in general a well a the dependency among the label set of data example inter connected in the network empirical study on real world task demonstrate that the performance of multi label classification can be effectively boosted using heterogeneous information network 
complex network are ubiquitous in our daily life with the world wide web social network and academic citation network being some of the common example it is well understood that modeling and understanding the network structure is of crucial importance to revealing the network function one important problem known a community detection is to detect and extract the community structure of network more recently the focus in this research topic ha been switched to the detection of overlapping community in this paper based on the matrix factorization approach we propose a method called bounded nonnegative matrix tri factorization bnmtf using three factor in the factorization we can explicitly model and learn the community membership of each node a well a the interaction among community based on a unified formulation for both directed and undirected network the optimization problem underlying bnmtf can use either the squared loss or the generalized kl divergence a it loss function in addition to address the sparsity problem a a result of missing edge we also propose another setting in which the loss function is defined only on the observed edge we report some experiment on real world datasets to demonstrate the superiority of bnmtf over other related matrix factorization method 
we present siren an interactive tool for mining and visualizing geospatial redescriptions redescription mining is a powerful data analysis tool that aim at finding alternative description of the same entity for example in biology an important task is to identify the bioclimatic constraint that allow some specie to survive that is to describe geographical region in term of both the fauna that inhabits them and their bioclimatic condition using siren user can explore geospatial data of their interest by visualizing the redescriptions on a map interactively edit extend and filter them to demonstrate the use of the tool we focus on climatic niche finding over europe a an example task yet siren is by no mean limited to a particular dataset or application 
in stock market an emerging challenge for surveillance is that a group of hidden manipulator collaborate with each other to manipulate the price movement of security recently the coupled hidden markov model chmm based coupled behavior analysis cba ha been proposed to consider the coupling relationship in the above group based behavior for manipulation detection from the modeling perspective however this requires overall aggregation of the behavioral data to cater for the chmm modeling which doe not differentiate the coupling relationship presented in different form within the aggregated behavior and degrade the capability for further anomaly detection thus this paper suggests a general cba framework for detecting group based market manipulation by capturing more comprehensive coupling and proposes two variant implementation which are hybrid coupling hc based and hierarchical grouping hg based respectively the proposed framework consists of three stage the first stage qualitative analysis generates possible qualitative coupling relationship between behavior with or without domain knowledge in the second stage quantitative representation of coupled behavior is learned via proper method for the third stage anomaly detection algorithm are proposed to cater for different application scenario experimental result on data from a major asian stock market show that the proposed framework outperforms the chmm based analysis in term of detecting abnormal collaborative market manipulation additionally the two different implementation are compared with their effectiveness for different application scenario 
identifying the k most influential individual in a social network is a well studied problem the objective is to detect k individual in a social network who will influence the maximum number of people if they are independently convinced of adopting a new strategy product idea etc there are case in real life however where we aim to instigate group instead of individual to trigger network diffusion such case abound e g billboard tv commercial and newspaper ad are utilized extensively to boost the popularity and raise awareness in this paper we generalize the influential node problem namely we are interested to locate the most influential group in a network a the first paper to address this problem we propose a fine grained model of information diffusion for the group based problem show that the process is submodular and present an algorithm to determine the influential group under this model with a precise approximation bound propose a coarse grained model that inspects the network at group level not individual significantly speeding up calculation for large network show that the diffusion function we design here is submodular in general case and propose an approximation algorithm for this coarse grained model and finally by conducting experiment on real datasets demonstrate that seeding member of selected group to be the first adopter can broaden diffusion when compared to the influential individual case moreover we can identify these influential group much faster up to million time speedup delivering a practical solution to this problem 
we propose a novel inference framework for finding maximal clique in a weighted graph that satisfy hard constraint the constraint specify the graph node that must belong to the solution a well a mutual exclusion of graph node i e set of node that cannot belong to the same solution the proposed inference is based on a novel particle filter algorithm with state permeation we apply the inference framework to a challenging problem of learning part based deformable object model two core problem in the learning framework matching of image patch and finding salient part are formulated a two instance of the problem of finding maximal clique with hard constraint our learning framework yield discriminative part based object model that achieve very good detection rate and outperform other method on object class with large deformation 
a one s social network expands a user s privacy protection go beyond his privacy setting and becomes a social networking problem in this research we aim to address some critical issue related to privacy protection would the highest privacy setting guarantee a secure protection given the open nature of social networking site is it possible to manage one s privacy protection with the diversity of one s social medium friend how can one figure out an effective approach to balance between vulnerability and privacy we present a novel way to define a vulnerable friend from an individual user s perspective is dependent on whether or not the user s friend privacy setting protect the friend and the individual s network of friend which includes the user a a single vulnerable friend in a user s social network might place all friend at risk we resort to experiment and observe how much security an individual user can improve by unfriending a vulnerable friend we also show how privacy weakens if newly accepted friend are unguarded or unprotected this work provides a large scale evaluation of new security and privacy index using a facebook dataset we present and discus a new perspective for reasoning about social networking security when a user accepts a new friend the user should ensure that the new friend is not an increased security risk with the potential of negatively impacting the entire friend network additionally by leveraging the index proposed and employing new strategy for unfriending vulnerable friend it is possible to further improve security and privacy without changing the social networking site s existing architecture 
although hashing technique have been popular for the large scale similarity search problem most of the existing method for designing optimal hash function focus on homogeneous similarity assessment i e the data entity to be indexed are of the same type realizing that heterogeneous entity and relationship are also ubiquitous in the real world application there is an emerging need to retrieve and search similar or relevant data entity from multiple heterogeneous domain e g recommending relevant post and image to a certain facebook user in this paper we address the problem of comparing apple to orange under the large scale setting specifically we propose a novel relation aware heterogeneous hashing rahh which provides a general framework for generating hash code of data entity sitting in multiple heterogeneous domain unlike some existing hashing method that map heterogeneous data in a common hamming space the rahh approach construct a hamming space for each type of data entity and learns optimal mapping between them simultaneously this make the learned hash code flexibly cope with the characteristic of different data domain moreover the rahh framework encodes both homogeneous and heterogeneous relationship between the data entity to design hash function with improved accuracy to validate the proposed rahh method we conduct extensive evaluation on two large datasets one is crawled from a popular social medium site tencent weibo and the other is an open dataset of flickr nu wide the experimental result clearly demonstrate that the rahh outperforms several state of the art hashing method with significant performance gain 
in domain such a consumer product or manufacturing amongst others we have problem that warrant the prediction of a continuous target besides the usual set of explanatory attribute we may also have exact or approximate estimate of aggregated target which are the sum of disjoint set of individual target that we are trying to predict hence the question now becomes can we use these aggregated target which are a coarser piece of information to improve the quality of prediction of the individual target in this paper we provide a simple yet provable way of accomplishing this in particular given prediction from any regression model of the target on the test data we elucidate a provable method for improving these prediction in term of mean squared error given exact or accurate enough information of the aggregated target these estimate of the aggregated target may be readily available or obtained through multilevel regression at different level of granularity based on the proof of our method we suggest a criterion for choosing the appropriate level moreover in addition to estimate of the aggregated target if we have exact or approximate estimate of the mean and variance of the target distribution then based on our general strategy we provide an optimal way of incorporating this information so a to further improve the quality of prediction of the individual target we then validate the result and our claim by conducting experiment on synthetic and real industrial data obtained from diverse domain 
in the current social network a user may have hundred of friend and find it very time consuming to categorize and tag every friend manually when a user is going to initiate an activity by issuing a corresponding query he she need to consider the relationship among candidate attendee to find a group of mutually close friend meanwhile he she also need to consider the schedule of candidate attendee to find an activity period available for all attendee it would certainly be desirable if the efficiency of such process is improved in this talk information processing in social network will first be reviewed in three phrase namely i from content to social relationship ii mining on social relationship and iii from social relationship to content organization in addition we shall present an effective procedure which help a user to organize an event with proper attendee with minimum total social distance and commonly available time moreover it is noted that the information retrieved from the social network is also able to facilitate those user dependent and human centric service in light of this we shall explore the quality of recommendation through incorporating the notion of social filtering and collaborative filtering finally it is recognized that the cloud computing ha offered many new capability of storing and processing huge amount of heterogeneous data in social network in view of this we shall also examine how this paradigm shift will affect the information processing in social network 
we present a document classification system that employ lazy learning from labeled phrase and argue that the system can be highly effective whenever the following property hold most of information on document label is captured in phrase we call this property near sufficiency our research contribution is twofold a we quantify the near sufficiency property using the information bottleneck principle and show that it is easy to check on a given dataset b we reveal that in all practical case from small scale to very large scale manual labeling of phrase is feasible the natural language constrains the number of common phrase composed of a vocabulary to grow linearly with the size of the vocabulary both these contribution provide firm foundation to applicability of the phrase based classification pbc framework to a variety of large scale task we deployed the pbc system on the task of job title classification a a part of linkedin s data standardization effort the system significantly outperforms it predecessor both in term of precision and coverage it is currently being used in linkedin s ad targeting product and more application are being developed we argue that pbc excels in high explainability of the classification result a well a in low development and low maintenance cost we benchmark pbc against existing high precision document classification algorithm and conclude that it is most useful in multilabel classification 
diversified ranking on graph is a fundamental mining task and ha a variety of high impact application there are two important open question here the first challenge is the measure how to quantify the goodness of a given top k ranking list that capture both the relevance and the diversity the second challenge lie in the algorithmic aspect how to find an optimal or near optimal top k ranking list that maximizes the measure we defined in a scalable way in this paper we address these challenge from an optimization point of view firstly we propose a goodness measure for a given top k ranking list the proposed goodness measure intuitively capture both a the relevance between each individual node in the ranking list and the query and b the diversity among different node in the ranking list moreover we propose a scalable algorithm linear wrt the size of the graph that generates a provably near optimal solution the experimental evaluation on real graph demonstrate it effectiveness and efficiency 
the development of a personalized approach to medical care is now well recognized a an urgent priority this approach is particularly important in oncology where it is well understood that each cancer diagnosis is unique at the molecular level arising from a particular and specific collection of genetic alteration furthermore taking a personalized approach to oncology may expedite the treatment process pre empting therapeutic decision based on fewer data in favor of treatment targeted to an individual s tumor this directed course may be key to survival for many patient who are terminal or have failed standard therapy 
a the size of data set used to build classifier steadily increase training a linear model efficiently with limited memory becomes essential several technique deal with this problem by loading block of data from disk one at a time but usually take a considerable number of iteration to converge to a reasonable model even the best block minimization technique require many block load since they treat all training example uniformly a disk i o is expensive reducing the amount of disk access can dramatically decrease the training time this paper introduces a selective block minimization sbm algorithm a block minimization method that make use of selective sampling at each step sbm update the model using data consisting of two part new data loaded from disk and a set of informative sample already in memory from previous step we prove that by updating the linear model in the dual form the proposed method fully utilizes the data in memory and converges to a globally optimal solution on the entire data experiment show that the sbm algorithm dramatically reduces the number of block loaded from disk and consequently obtains an accurate and stable model quickly on both binary and multi class classification 
credit reference centre crc of people s bank of china pbc ha built a big data the largest personal credit database in the world with million people s account collected from all commercial bank in china since from june to sept research centre on fictitious economy and data science chinese academy of science casfeds and crc jointly developed china s national personal credit scoring system known a china score which is a unique and advanced kdd application under intelligent knowledge management on this big data the system will be eventually serving all billion population of china for their daily financial activity such a bank account credit card application mortgage personal loan etc it can become one of the most influential event of kdd technique to human kind this talk will introduce the key component of china score project that includes objective modeling process kdd technique used in the project intelligent knowledge management and experience of the project development in addition the talk will also outline a number of policy recommendation based on china score project which ha been potentially impacting chinese government on it strategic decision making for china s economic development 
this paper describes the bid data suite a collection of hardware software and design pattern that enable fast large scale data mining at very low cost by co designing all of these element we achieve single machine performance level that equal or exceed reported cluster implementation for common benchmark problem a key design criterion is rapid exploration of model hence the system is interactive and primarily single user the element of the suite are i the data engine a hardware design pattern that balance storage cpu and gpu acceleration for typical data mining workload ii bidmat an interactive matrix library that integrates cpu and gpu acceleration and novel computational kernel iii bidmach a machine learning system that includes very efficient model optimizers iv butterfly mixing a communication strategy that hide the latency of frequent model update needed by fast optimizers and v design pattern to improve performance of iterative update algorithm we present several benchmark problem to show how the above element combine to yield multiple order of magnitude improvement for each problem 
non guaranteed display advertising ngd is a multi billion dollar business that ha been growing rapidly in recent year advertiser in ngd sell a large portion of their ad campaign using performance dependent pricing model such a cost per click cpc and cost per action cpa an accurate prediction of the probability that user click on ad is a crucial task in ngd advertising because this value is required to compute the expected revenue state of the art prediction algorithm rely heavily on historical information collected for advertiser user and publisher click prediction of new ad in the system is a challenging task due to the lack of such historical data the objective of this paper is to mitigate this problem by integrating multimedia feature extracted from display ad into the click prediction model multimedia feature can help u capture the attractiveness of the ad with similar content or aesthetic in this paper we evaluate the use of numerous multimedia feature in addition to commonly used user advertiser and publisher feature for the purpose of improving click prediction in ad with no history we provide analytical result generated over billion of sample and demonstrate that adding multimedia feature can significantly improve the accuracy of click prediction for new ad compared to a state of the art baseline model 
click through rate ctr prediction play a central role in search advertising one need ctr estimate unbiased by positional effect in order for ad ranking allocation and pricing to be based upon ad relevance or quality in term of click propensity however the observed click through data ha been confounded by positional bias that is user tend to click more on ad shown in higher position than lower one regardless of the ad relevance we describe a probabilistic factor model a a general principled approach to studying these exogenous and often overwhelming phenomenon the model is simple and linear in nature while empirically justified by the advertising domain our experimental result with artificial and real world sponsored search data show the soundness of the underlying model assumption which in turn yield superior prediction accuracy 
natural sound exhibit complex statistical regularity at multiple scale acoustic event underlying speech for example are characterized by precise temporal and frequency relationship but they can also vary substantially according to the pitch duration and other high level property of speech production learning this structure from data while capturing the inherent variability is an important first step in building auditory processing system a well a understanding the mechanism of auditory perception here we develop hierarchical spike coding a two layer probabilistic generative model for complex acoustic structure the first layer consists of a sparse spiking representation that encodes the sound using kernel positioned precisely in time and frequency pattern in the position of first layer spike are learned from the data on a coarse scale statistical regularity are encoded by a second layer spiking representation while fine scale structure is captured by recurrent interaction within the first layer when fit to speech data the second layer acoustic feature include harmonic stack sweep frequency modulation and precise temporal onset which can be composed to represent complex acoustic event unlike spectrogram based method the model give a probability distribution over sound pressure waveform this allows u to use the second layer representation to synthesize sound directly and to perform model based denoising on which we demonstrate a significant improvement over standard method 
in event pattern matching a sequence of input event is matched against a complex query pattern that specifies constraint on extent order value and quantification of matching event in this paper we propose a general pattern matching strategy that consists of a pre processing step and a pattern matching step instead of eagerly matching incoming event the pre processing step buffer event in a match window to apply different pruning technique filtering partitioning and testing for necessary match condition in the second step an event pattern matching algorithm a is called only for match window that satisfy the necessary match condition this two phase strategy with a lazy call of the matching algorithm significantly reduces the number of event that need to be processed by a a well a the number of call to a this is important since pattern matching algorithm tend to be expensive in term of runtime and memory complexity whereas the pre processing can be done very efficiently we conduct extensive experiment using real world data with pattern matching algorithm for respectively automaton and join tree the experimental result confirm the effectiveness of our strategy for both type of pattern matching algorithm 
a common problem with most of the feature selection method is that they often produce feature set model that are not stable with respect to slight variation in the training data different author tried to improve the feature selection stability using ensemble method which aggregate different feature set into a single model however the existing ensemble feature selection method suffer from two main shortcoming i the aggregation treat the feature independently and doe not account for their interaction and ii a single feature set is returned nevertheless in various application there might be more than one feature set potentially redundant with similar information content in this work we address these two limitation we present a general framework in which we mine over different feature model produced from a given dataset in order to extract pattern over the model we use these pattern to derive more complex feature model aggregation strategy that account for feature interaction and identify core and distinct feature model we conduct an extensive experimental evaluation of the proposed framework where we demonstrate it effectiveness over a number of high dimensional problem from the field of biology and text mining 
many machine learning and signal processing problem can be formulated a lin early constrained convex program which could be efficiently solved by the alternating direction method adm however usually the subproblems in adm are easily solvable only when the linear mapping in the constraint are identity to address this issue we propose a linearized adm ladm method by linearizing the quadratic penalty term and adding a proximal term when solving the sub problem for fast convergence we also allow the penalty to change adaptively according a novel update rule we prove the global convergence of ladm with adaptive penalty ladmap a an example we apply ladmap to solve low rank representation lrr which is an important subspace clustering technique yet suffers from high computation cost by combining ladmap with a skinny svd representation technique we are able to reduce the complexity o n of the original adm based method to o rn where r and n are the rank and size of the representation matrix respectively hence making lrr possible for large scale application numerical experiment verify that for lrr our ladmap based method are much faster than state of the art algorithm 
mining probabilistic frequent pattern from uncertain data ha received a great deal of attention in recent year due to the wide application however probabilistic frequent pattern mining suffers from the problem that an exponential number of result pattern are generated which seriously hinders further evaluation and analysis in this paper we focus on the problem of mining probabilistic representative frequent pattern p rfp which is the minimal set of pattern with adequately high probability to represent all frequent pattern observing the bottleneck in checking whether a pattern can probabilistically represent another which involves the computation of a joint probability of the support of two pattern we introduce a novel approximation of the joint probability with both theoretical and empirical proof based on the approximation we propose an approximate p rfp mining apm algorithm which effectively and efficiently compress the set of probabilistic frequent pattern to our knowledge this is the first attempt to analyze the relationship between two probabilistic frequent pattern through an approximate approach our experiment on both synthetic and real world datasets demonstrate that the apm algorithm accelerates p rfp mining dramatically order of magnitude faster than an exact solution moreover the error rate of apm is guaranteed to be very small when the database contains hundred transaction which further affirms apm is a practical solution for summarizing probabilistic frequent pattern 
we consider the characterization of muscle fatigue through noninvasive sensing mechanism such a surface electromyography semg while change in the property of semg signal with respect to muscle fatigue have been reported in the literature the large variation in these signal across different individual make the task of modeling and classification of semg signal challenging indeed the variation in semg parameter from subject to subject creates difference in the data distribution in this paper we propose a transfer learning framework based on the multi source domain adaptation methodology for detecting different stage of fatigue using semg signal that address the distribution difference in the proposed framework the semg data of a subject represent a domain data from multiple subject in the training set form the multiple source domain and the test subject data form the target domain semg signal are predominantly different in conditional probability distribution across subject the key feature of the proposed framework is a novel weighting scheme that address the conditional probability distribution difference across multiple domain subject we have validated the proposed framework on surface electromyogram signal collected from people during a fatigue causing repetitive gripping activity comprehensive experiment on the semg data set demonstrate that the proposed method improves the classification accuracy by to over the case without any domain adaptation method and by to over the existing state of the art domain adaptation method 
large amount of heterogeneous medical data have become available in various healthcare organization payer provider pharmaceutical those data could be an enabling resource for deriving insight for improving care delivery and reducing waste the enormity and complexity of these datasets present great challenge in analysis and subsequent application to a practical clinical environment in this tutorial we introduce the characteristic and related mining challenge on dealing with big medical data many of those insight come from medical informatics community which is highly related to data mining but focus on biomedical specific we survey various related paper from data mining venue a well a medical informatics venue to share with the audience key problem and trend in healthcare analytics research with different application ranging from clinical text mining predictive modeling survival analysis patient similarity genetic data analysis and public health the tutorial will include several case study dealing with some of the important healthcare application 
the proliferation of online social network and the concomitant accumulation of user data give rise to hotly debated issue of privacy security and control one specific challenge is the sharing or public release of anonymized data without accidentally leaking personally identifiable information pii unfortunately it is often difficult to ascertain that sophisticated statistical technique potentially employing additional external data source are unable to break anonymity in this paper we consider an instance of this problem where the object of interest is the structure of a social network i e a graph describing user and their link recent work demonstrates that anonymizing node identity may not be sufficient to keep the network private the availability of node and link data from another domain which is correlated with the anonymized network ha been used to re identify the anonymized node this paper is about condition under which such a de anonymization process is possible we attempt to shed light on the following question can we assume that a sufficiently sparse network is inherently anonymous in the sense that even with unlimited computational power de anonymization is impossible our approach is to introduce a random graph model for a version of the de anonymization problem which is parameterized by the expected node degree and a similarity parameter that control the correlation between two graph over the same vertex set we find simple condition on these parameter delineating the boundary of privacy and show that the mean node degree need only grow slightly faster than log n with network size n for node to be identifiable our result have policy implication for sharing of anonymized network information 
the study of social network evolution ha attracted many attention from both the industry and academia in this paper we demonstrate laft explorer a general toolkit for explaining and reproducing the network growth process based on the friendship propagation laft explorer present multiple perspective for analyzing the network evolution process and structure including laft tree laft trace and laft flow upon that we build laft rec a new visualized interactive friend recommendation service based on the friendship propagation laft rec not only show whom one may make friend with but also tell the user that why you should make friend with him and how you can reach him we demonstrate our system built upon the academic social network of dblp 
the two key challenge in hierarchical classification are to leverage the hierarchical dependency between the class label for improving performance and at the same time maintaining scalability across large hierarchy in this paper we propose a regularization framework for large scale hierarchical classification that address both the problem specifically we incorporate the hierarchical dependency between the class label into the regularization structure of the parameter thereby encouraging class nearby in the hierarchy to share similar model parameter furthermore we extend our approach to scenario where the dependency between the class label are encoded in the form of a graph rather than a hierarchy to enable large scale training we develop a parallel iterative optimization scheme that can handle datasets with hundred of thousand of class and million of instance and learning terabyte of parameter our experiment showed a consistent improvement over other competing approach and achieved state of the art result on benchmark datasets 
spectral clustering is a widely used method for organizing data that only relies on pairwise similarity measurement this make it application to non vectorial data straight forward in principle a long a all pairwise similarity are available however in recent year numerous example have emerged in which the cost of assessing similarity is substantial or prohibitive we propose an active learning algorithm for spectral clustering that incrementally measure only those similarity that are most likely to remove uncertainty in an intermediate clustering solution in many application similarity are not only costly to compute but also noisy we extend our algorithm to maintain running estimate of the true similarity a well a estimate of their accuracy using this information the algorithm update only those estimate which are relatively inaccurate and whose update would most likely remove clustering uncertainty we compare our method on several datasets including a realistic example where similarity are expensive and noisy the result show a significant improvement in performance compared to the alternative 
efficient thermal management is important in modern data center a cooling consumes up to of the total energy unlike previous work we consider proactive thermal management whereby server can predict potential overheating event due to dynamic in data center configuration and workload giving operator enough time to react however such forecasting is very challenging due to data center scale and complexity moreover such a physical system is influenced by cyber effect including workload scheduling in server we propose thermocast a novel thermal forecasting model to predict the temperature surrounding the server in a data center based on continuous stream of temperature and airflow measurement our approach is a capable of capturing cyberphysical interaction and automatically learning them from data b computationally and physically scalable to data center scale c able to provide online prediction with real time sensor measurement the paper s main contribution are i we provide a systematic approach to integrate physical law and sensor observation in a data center ii we provide an algorithm that us sensor data to learn the parameter of a data center s cyber physical system in turn this ability enables u to reduce model complexity compared to full fledged fluid dynamic model while maintaining forecast accuracy iii unlike previous simulation based study we perform experiment in a production data center using real data trace we show that thermocast forecast temperature better than a machine learning approach solely driven by data and can successfully predict thermal alarm minute ahead of time 
a datasets become larger more complex and more available to diverse group of analyst it would be quite useful to be able to automatically and generically ass the quality of estimate much a we are able to automatically train and evaluate predictive model such a classifier however despite the fundamental importance of estimator quality assessment in data analysis this task ha eluded highly automatic solution while the bootstrap provides perhaps the most promising step in this direction it level of automation is limited by the difficulty of evaluating it finite sample performance and even it asymptotic consistency thus we present here a general diagnostic procedure which directly and automatically evaluates the accuracy of the bootstrap s output determining whether or not the bootstrap is performing satisfactorily when applied to a given dataset and estimator we show that our proposed diagnostic is effective via an extensive empirical evaluation on a variety of estimator and simulated and real datasets including a real world query workload from conviva inc involving tb of data i e approximately billion data point 
newly emerged event based online social service such a meetup and plancast have experienced increased popularity and rapid growth from these service we observed a new type of social network event based social network ebsn an ebsn doe not only contain online social interaction a in other conventional online social network but also includes valuable offline social interaction captured in offline activity by analyzing real data collected from meetup we investigated ebsn property and discovered many unique and interesting characteristic such a heavy tailed degree distribution and strong locality of social interaction we subsequently studied the heterogeneous nature co existence of both online and offline social interaction of ebsns on two challenging problem community detection and information flow we found that community detected in ebsns are more cohesive than those in other type of social network e g location based social network in the context of information flow we studied the event recommendation problem by experimenting various information diffusion pattern we found that a community based diffusion model that take into account of both online and offline interaction provides the best prediction power this paper is the first research to study ebsns at scale and pave the way for future study on this new type of social network a sample dataset of this study can be downloaded from http www largenetwork org ebsn 
we study multi class bandit prediction an online learning problem where the learner only receives a partial feedback in each trial indicating whether the predicted class label is correct the exploration v exploitation tradeoff strategy is a well known technique for online learning with incomplete feedback i e bandit setup banditron a multi class online learning algorithm for bandit setting maximizes the run time gain by balancing between exploration and exploitation with a fixed tradeoff parameter the performance of banditron can be quite sensitive to the choice of the tradeoff parameter and therefore effective algorithm to automatically tune this parameter is desirable in this paper we propose three learning strategy to automatically adjust the tradeoff parameter for banditron our extensive empirical study with multiple real world data set verifies the efficacy of the proposed approach in learning the exploration v exploitation tradeoff parameter 
the increasing availability of large scale location trace creates unprecedent opportunity to change the paradigm for knowledge discovery in transportation system a particularly promising area is to extract useful business intelligence which can be used a guidance for reducing inefficiency in energy consumption of transportation sector improving customer experience and increasing business performance however extracting business intelligence from location trace is not a trivial task conventional data analytic tool are usually not customized for handling large complex dynamic and distributed nature of location trace to that end we develop a taxi business intelligence system to explore the massive taxi location trace from different business perspective with various data mining function since we implement the system using the real world taxi gps data this demonstration will help taxi company to improve their business performance by understanding the behavior of both driver and customer in addition several identified technical challenge also motivate data mining people to develop more sophisticate technique in the future 
taxonomy especially the one in specific domain are becoming indispensable to a growing number of application state of the art approach assume there exists a text corpus to accurately characterize the domain of interest and that a taxonomy can be derived from the text corpus using information extraction technique in reality neither assumption is valid especially for highly focused or fast changing domain in this paper we study a challenging problem deriving a taxonomy from a set of keyword phrase a solution can benefit many real life application because i keywords give user the flexibility and ease to characterize a specific domain and ii in many application such a online advertisement the domain of interest is already represented by a set of keywords however it is impossible to create a taxonomy out of a keyword set itself we argue that additional knowledge and context are needed to this end we first use a general purpose knowledgebase and keyword search to supply the required knowledge and context then we develop a bayesian approach to build a hierarchical taxonomy for a given set of keywords we reduce the complexity of previous hierarchical clustering approach from o n log n to o n log n so that we can derive a domain specific taxonomy from one million keyword phrase in le than an hour finally we conduct comprehensive large scale experiment to show the effectiveness and efficiency of our approach a real life example of building an insurance related query taxonomy illustrates the usefulness of our approach for specific domain 
in entity matching a fundamental issue while training a classifier to label pair of entity a either duplicate or non duplicate is the one of selecting informative training example although active learning present an attractive solution to this problem previous approach minimize the misclassification rate loss of the classifier which is an unsuitable metric for entity matching due to class imbalance i e many more non duplicate pair than duplicate pair to address this a recent paper proposes to maximize recall of the classifier under the constraint that it precision should be greater than a specified threshold however the proposed technique requires the label of all n input pair in the worst case our main result is an active learning algorithm that approximately maximizes recall of the classifier while respecting a precision constraint with provably sub linear label complexity under certain distributional assumption our algorithm us a a black box any active learning module that minimizes loss we show that label complexity of our algorithm is at most log n time the label complexity of the black box and also bound the difference in the recall of classifier learnt by our algorithm and the recall of the optimal classifier satisfying the precision constraint we provide an empirical evaluation of our algorithm on several real world matching data set that demonstrates the effectiveness of our approach 
many learning task such a spam filtering and credit card fraud detection face an active adversary that try to avoid detection for learning problem that deal with an active adversary it is important to model the adversary s attack strategy and develop robust learning model to mitigate the attack these are the two objective of this paper we consider two attack model a free range attack model that permit arbitrary data corruption and a restrained attack model that anticipates more realistic attack that a reasonable adversary would devise under penalty we then develop optimal svm learning strategy against the two attack model the learning algorithm minimize the hinge loss while assuming the adversary is modifying data to maximize the loss experiment are performed on both artificial and real data set we demonstrate that optimal solution may be overly pessimistic when the actual attack are much weaker than expected more important we demonstrate that it is possible to develop a much more resilient svm learning model while making loose assumption on the data corruption model when derived under the restrained attack model our optimal svm learning strategy provides more robust overall performance under a wide range of attack parameter 
we study the impact of display advertising on user search behavior using a field experiment in such an experiment the treatment group user are exposed to some display advertising campaign while the control group user are not during the campaign and the post campaign period we monitor the user search query and we label them a relevant or irrelevant to the campaign using technique that leverage the bipartite query url click graph our result indicate that user who are exposed to the advertising campaign submit to more query that are relevant to it compared to the unexposed user using the social graph of the experiment user we also explore how user are affected by their friend who are exposed to ad our result indicate that a user with exposed friend is more likely to submit query relevant to the campaign a compared to a user without exposed friend the result is surprising given that the display advertising campaign that we study doe not include any incentive for social action e g discount for recommending friend 
outlier mining in d dimensional point set is a fundamental and well studied data mining task due to it variety of application most such application arise in high dimensional domain a bottleneck of existing approach is that implicit or explicit assessment on concept of distance or nearest neighbor are deteriorated in high dimensional data following up on the work of kriegel et al kdd we investigate the use of angle based outlier factor in mining high dimensional outlier while their algorithm run in cubic time with a quadratic time heuristic we propose a novel random projection based technique that is able to estimate the angle based outlier factor for all data point in time near linear in the size of the data also our approach is suitable to be performed in parallel environment to achieve a parallel speedup we introduce a theoretical analysis of the quality of approximation to guarantee the reliability of our estimation algorithm the empirical experiment on synthetic and real world data set demonstrate that our approach is efficient and scalable to very large high dimensional data set 
shilling attacker apply biased rating profile to recommender system for manipulating online product recommendation although many study have been devoted to shilling attack detection few of them can handle the hybrid shilling attack that usually happen in practice and the study for real life application are rarely seen moreover little attention ha yet been paid to modeling both labeled and unlabeled user profile although there are often a few labeled but numerous unlabeled user available in practice this paper present a hybrid shilling attack detector or hysad for short to tackle these problem in particular hysad introduces mc relief to select effective detection metric and semi supervised naive bayes snb lambda to precisely separate random filler model attacker and average filler model attacker from normal user thorough experiment on movielens and netflix datasets demonstrate the effectiveness of hysad in detecting hybrid shilling attack and it robustness for various obfuscated strategy a real life case study on product review of amazon cn is also provided which further demonstrates that hysad can effectively improve the accuracy of a collaborative filtering based recommender system and provide interesting opportunity for in depth analysis of attacker behavior these in turn justify the value of hysad for real world application 
he role of big data in addressing the need of the present healthcare system in u and rest of the world ha been echoed by government private and academic sector there ha been a growing emphasis to explore the promise of big data analytics in tapping the potential of the massive healthcare data emanating from private and government health insurance provider while the domain implication of such collaboration are well known this type of data ha been explored to a limited extent in the data mining community the objective of this paper is two fold first we introduce the emerging domain of big healthcare claim data to the kdd community and second we describe the success and challenge that we encountered in analyzing this data using state of art analytics for massive data specifically we translate the problem of analyzing healthcare data into some of the most well known analysis problem in the data mining community social network analysis text mining and temporal analysis and higher order feature construction and describe how advance within each of these area can be leveraged to understand the domain of healthcare each case study illustrates a unique intersection of data mining and healthcare with a common objective of improving the cost care ratio by mining for opportunity to improve healthcare operation and reducing what seems to fall under fraud waste and abuse 
suicide is a major concern in society despite of great attention paid by the community with very substantive medico legal implication there ha been no satisfying method that can reliably predict the future attempted or completed suicide we present an integrated machine learning framework to tackle this challenge our proposed framework consists of a novel feature extraction scheme an embedded feature selection process a set of risk classifier and finally a risk calibration procedure for temporal feature extraction we cast the patient s clinical history into a temporal image to which a bank of one side filter are applied the response are then partly transformed into mid level feature and then selected in l norm framework under the extreme value theory a set of probabilistic ordinal risk classifier are then applied to compute the risk probability and further re rank the feature finally the predicted risk are calibrated together with our australian partner we perform comprehensive study on data collected for the mental health cohort and the experiment validate that our proposed framework outperforms risk assessment instrument by medical practitioner 
advance in tourism economics have enabled u to collect massive amount of travel tour data if properly analyzed this data can be a source of rich intelligence for providing real time decision making and for the provision of travel tour recommendation however tour recommendation is quite different from traditional recommendation because the tourist s choice is directly affected by the travel cost which includes the financial cost and the time to that end in this paper we provide a focused study of cost aware tour recommendation along this line we develop two cost aware latent factor model to recommend travel package by considering both the travel cost and the tourist s interest specifically we first design a cpmf model which model the tourist s cost with a dimensional vector also in this cpmf model the tourist s interest and the travel cost are learnt by exploring travel tour data furthermore in order to model the uncertainty in the travel cost we further introduce a gaussian prior into the cpmf model and develop the gcpmf model where the gaussian prior is used to express the uncertainty of the travel cost finally experiment on real world travel tour data show that the cost aware recommendation model outperform state of the art latent factor model with a significant margin also the gcpmf model with the gaussian prior can better capture the impact of the uncertainty of the travel cost and thus performs better than the cpmf model 
billion of online display advertising spot are purchased on a daily basis through real time bidding exchange rtbs advertising company bid for these spot on behalf of a company or brand in order to purchase these spot to display banner advertisement these bidding decision must be made in fraction of a second after the potential purchaser is informed of what location internet site ha a spot available and who would see the advertisement the entire transaction must be completed in near real time to avoid delay loading the page and maintain a good user experience this paper present a bid optimization approach that is implemented in production at medium degree for bidding on these advertising opportunity at an appropriate price the approach combine several supervised learning algorithm a well a second price auction theory to determine the correct price to ensure that the right message is delivered to the right person at the right time 
social network continue to grow in size and the type of information hosted we witness a growing interest in clustering a social network of people based on both their social relationship and their participation in activity based information network in this paper we present a social influence based clustering framework for analyzing heterogeneous information network with three unique feature first we introduce a novel social influence based vertex similarity metric in term of both self influence similarity and co influence similarity we compute self influence and co influence based similarity based on social graph and it associated activity graph and influence graph respectively second we compute the combined social influence based similarity between each pair of vertex by unifying the self similarity and multiple co influence similarity score through a weight function with an iterative update method third we design an iterative learning algorithm si cluster to dynamically refine the k cluster by continuously quantifying and adjusting the weight on self influence similarity and on multiple co influence similarity score towards the clustering convergence to make si cluster converge fast we transformed a sophisticated nonlinear fractional programming problem of multiple weight into a straightforward nonlinear parametric programming problem of single variable our experiment result show that si cluster not only achieves a better balance between self influence and co influence similarity but also scale extremely well for large graph clustering 
existing fact finding model assume availability of structured data or accurate information extraction however a online data get more unstructured these assumption are no longer valid to overcome this we propose a novel content based trust propagation framework that relies on signal from the textual content to ascertain veracity of free text claim and compute trustworthiness of their source we incorporate the quality of relevant content into the framework and present an iterative algorithm for propagation of trust score we show that existing fact finder on structured data can be modeled a specific instance of this framework using a retrieval based approach to find relevant article we instantiate the framework to compute trustworthiness of news source and article we show that the proposed framework help ascertain trustworthiness of source better we also show that ranking news article based on trustworthiness learned from the content driven framework is significantly better than baseline that ignore either the content quality or the trust framework 
recent year have witnessed the explosive growth of online social medium weibo a twitter like online social network in china ha attracted more than million user in le than three year with more than tweet generated in every second these tweet not only convey the factual information but also reflect the emotional state of the author which are very important for understanding user behavior however a tweet in weibo is extremely short and the word it contains evolve extraordinarily fast moreover the chinese corpus of sentiment is still very small which prevents the conventional keyword based method from being used in light of this we build a system called moodlens which to our best knowledge is the first system for sentiment analysis of chinese tweet in weibo in moodlens emoticon are mapped into four category of sentiment i e angry disgusting joyful and sad which serve a the class label of tweet we then collect over million labeled tweet a the corpus and train a fast naive bayes classifier with an empirical precision of moodlens also implement an incremental learning method to tackle the problem of the sentiment shift and the generation of new word using moodlens for real time tweet obtained from weibo several interesting temporal and spatial pattern are observed also sentiment variation are well captured by moodlens to effectively detect abnormal event in china finally by using the highly efficient naive bayes classifier moodlens is capable of online real time sentiment monitoring the demo of moodlens can be found at http goo gl dq 
mining dense subgraphs such a clique or quasi clique is an important graph mining problem and closely related to the notion of graph clustering in various application graph are enriched by additional information for example we can observe graph representing different type of relation between the vertex these multiple edge type can also be viewed a different layer of the same graph which is denoted a a multi layer graph in this work additionally each edge might be annotated by a label characterizing the given relation in more detail by exploiting all these different kind of information the detection of more interesting cluster in the graph can be supported in this work we introduce the multi layer coherent subgraph mlcs model which defines cluster of vertex that are densely connected by edge with similar label in a subset of the graph layer we avoid redundancy in the result by selecting only the most interesting non redundant cluster for the output based on this model we introduce the best first search algorithm mimag in thorough experiment we demonstrate the strength of mimag in comparison with related approach on synthetic a well a real world datasets 
support vector machine svms are among the most popular and successful classification algorithm kernel svms often reach state of the art accuracy but suffer from the curse of kernelization due to linear model growth with data size on noisy data linear svms have the ability to efficiently learn from truly large data but they are applicable to a limited number of domain due to low representational power to fill the representability and scalability gap between linear and nonlinear svms we propose the adaptive multi hyperplane machine amm algorithm that accomplishes fast training and prediction and ha capability to solve nonlinear classification problem amm model consists of a set of hyperplanes weight each assigned to one of the multiple class and predicts based on the associated class of the weight that provides the largest prediction the number of weight is automatically determined through an iterative algorithm based on the stochastic gradient descent algorithm which is guaranteed to converge to a local optimum since the generalization bound decrease with the number of weight a weight pruning mechanism is proposed and analyzed the experiment on several large data set show that amm is nearly a fast during training and prediction a the state of the art linear svm solver and that it can be order of magnitude faster than kernel svm in accuracy amm is somewhere between linear and kernel svms for example on an ocr task with million highly dimensional training example amm trained in second on a single core processor had error rate which wa significantly lower than error rate of a linear svm trained in the same time and comparable to error rate of a kernel svm trained in day on processor the result indicate that amm could be an attractive option when solving large scale classification problem the software is available at www dabi temple edu vucetic amm html 
the ability to achieve operational efficiency product leadership and customer intimacy still eludes many organization due in large part to the chaos of business inconsistent prioritization and decision making poor visibility between system process that are not well controlled and individual front line decision that seem small but in totality have a huge impact make it difficult for organization to link strategy to execution and back during this presentation we will demonstrate how automating and optimizing decision operational efficiency with business rule and predictive model enables better data driven result across the enterprise and how this is implemented at the point of impact customer intimacy to transform an organization and support market leadership 
we present a max margin nonparametric latent feature relational model which unites the idea of max margin learning and bayesian nonparametrics to discover discriminative latent feature for link prediction and automatically infer the unknown latent social dimension by minimizing a hinge loss using the linear expectation operator we can perform posterior inference efficiently without dealing with a highly nonlinear link likelihood function by using a fully bayesian formulation we can avoid tuning regularization constant experimental result on real datasets appear to demonstrate the benefit inherited from max margin learning and fully bayesian nonparametric inference copyright by the author s owner s 
matrix factorization where a given data matrix is approximated by a product of two or more factor matrix are powerful data mining tool among other task matrix factorization are often used to separate global structure from noise this however requires solving the model order selection problem of determining where fine grained structure stop and noise start i e what is the proper size of the factor matrix boolean matrix factorization bmf where data factor and matrix product are boolean ha received increased attention from the data mining community in recent year the technique ha desirable property such a high interpretability and natural sparsity but so far no method for selecting the correct model order for bmf ha been available in this paper we propose to use the minimum description length mdl principle for this task besides solving the problem this well founded approach ha numerous benefit e g it is automatic doe not require a likelihood function is fast and a experiment show is highly accurate we formulate the description length function for bmf in general making it applicable for any bmf algorithm we extend an existing algorithm for bmf to use mdl to identify the best boolean matrix factorization analyze the complexity of the problem and perform an extensive experimental evaluation to study it behavior 
we study discriminative clustering for market segmentation task the underlying problem setting resembles discriminative clustering however existing approach focus on the prediction of univariate cluster label by contrast market segment encode complex future behavior of the individual which cannot be represented by a single variable in this paper we generalize discriminative clustering to structured and complex output variable that can be represented a graphical model we devise two novel method to jointly learn the classifier and the clustering using alternating optimization and collapsed inference respectively the two approach jointly learn a discriminative segmentation of the input space and a generative output prediction model for each segment we evaluate our method on segmenting user navigation sequence from yahoo news the proposed collapsed algorithm is observed to outperform baseline approach such a mixture of expert we showcase exemplary projection of the resulting segment to display the interpretability of the solution 
how to automatically spot the major trend in large amount of heterogeneous data clustering can help however most existing technique suffer from one or more of the following drawback many technique support only one particular data type most commonly numerical attribute other technique do not support attribute dependency which are prevalent in real data some approach require input parameter which are difficult to estimate most clustering approach lack in interpretability to address these challenge we present the algorithm scenic for dependency clustering across measurement scale our approach seamlessly integrates heterogenous data type measured at different scale most importantly continuous numerical and discrete categorical data scenic cluster by arranging object and attribute in a cluster specific low dimensional space the embedding serf a a compact cluster model allowing to reconstruct the original heterogenous attribute with high accuracy thereby embedding reveals the major cluster specific mixed type attribute dependency following the minimum description length mdl principle the cluster specific embedding serf a a codebook for effective data compression this compression based view automatically balance goodness of fit and model complexity making input parameter redundant finally the embedding serf a a visualization enhancing the interpretability of the clustering result extensive experiment demonstrate the benefit of scenic 
an important problem in the non contractual marketing domain is discovering the customer lifetime and assessing the impact of customer s characteristic variable on the lifetime unfortunately the conventional hierarchical bayes model cannot discern the impact of customer s characteristic variable for each customer to overcome this problem we present a new survival model using a non parametric bayes paradigm with mcmc the assumption of a conventional model logarithm of purchase rate and dropout rate with linear regression is extended to include our assumption of the dirichlet process mixture of regression the extension assumes that each customer belongs probabilistically to different mixture of regression thereby permitting u to estimate a different impact of customer characteristic variable for each customer our model creates several customer group to mirror the structure of the target data set the effectiveness of our proposal is confirmed by a comparison involving a real e commerce transaction dataset and an artificial dataset it generally achieves higher predictive performance in addition we show that preselecting the actual number of customer group doe not always lead to higher predictive performance 
we present innite svm isvm a dirichlet process mixture of large margin kernel machine for multi way classification an isvm enjoys the advantage of both bayesian nonparametrics in handling the unknown number of mixing component and large margin kernel machine in robustly capturing local nonlinearity of complex data we develop an efficient variational learning algorithm for posterior inference of isvm and we demonstrate the advantage of isvm over dirichlet process mixture of generalized linear model and other benchmark on both synthetic and real flickr image classification datasets 
object with multiple numeric attribute can be compared within any subspace subset of attribute in application such a computational journalism user are interested in claim of the form karl malone is one of the only two player in nba history with at least point rebound and assist in one s career one challenge in identifying such one of the k claim k above is ensuring their interestingness a small k is not a good indicator for interestingness a one can often make such claim for many object by increasing the dimensionality of the subspace considered we propose a uniqueness based interestingness measure for one of the few claim that is intuitive for non technical user and we design algorithm for finding all interesting claim across all subspace from a dataset sometimes user are interested primarily in the object appearing in these claim building on our notion of interesting claim we propose a scheme for ranking object and an algorithm for computing the top ranked object using real world datasets we evaluate the efficiency of our algorithm a well a the advantage of our object ranking scheme over popular method such a kemeny optimal rank aggregation and weighted sum ranking 
we consider the problem of estimating rate of rare event obtained through interaction among several categorical variable that are heavy tailed and hierarchical in our previous work we proposed a scalable log linear model called lmmh log linear model for multiple hierarchy that combat data sparsity at granular level through small sample size correction that borrow strength from rate estimate at coarser resolution this paper extends our previous work in two direction first we model excess heterogeneity by fitting local lmmh model to relatively homogeneous subset of the data to ensure scalable computation these subset are induced through a decision tree we call this treed lmmh second the treed lmmh method is coupled with temporal smoothing procedure based on a fast kalman filter style algorithm we show that simultaneously performing hierarchical and temporal smoothing lead to significant improvement in predictive accuracy our method are illustrated on a large scale computational advertising dataset consisting of billion of observation and hundred of million of attribute combination cell 
multi view graph clustering aim to enhance clustering performance by integrating heterogeneous information collected in different domain each domain provides a different view of the data instance leveraging cross domain information ha been demonstrated an effective way to achieve better clustering result despite the previous success existing multi view graph clustering method usually assume that different view are available for the same set of instance thus instance in different domain can be treated a having strict one to one relationship in many real life application however data instance in one domain may correspond to multiple instance in another domain moreover relationship between instance in different domain may be associated with weight based on prior partial knowledge in this paper we propose a flexible and robust framework cgc co regularized graph clustering based on non negative matrix factorization nmf to tackle these challenge cgc ha several advantage over the existing method first it support many to many cross domain instance relationship second it incorporates weight on cross domain relationship third it allows partial cross domain mapping so that graph in different domain may have different size finally it provides user with the extent to which the cross domain instance relationship violates the in domain clustering structure and thus enables user to re evaluate the consistency of the relationship extensive experimental result on uci benchmark data set newsgroup data set and biological interaction network demonstrate the effectiveness of our approach 
singular value decomposition svd and principal component analysis pca is one of the most widely used technique for dimensionality reduction successful and efficiently computable it is nevertheless plagued by a well known well documented sensitivity to outlier recent work ha considered the setting where each point ha a few arbitrarily corrupted component yet in application of svd or pca such a robust collaborative filtering or bioinformatics malicious agent defective gene or simply corrupted or contaminated experiment may effectively yield entire point that are completely corrupted we present an efficient convex optimization based algorithm that we call outlier pursuit which under some mild assumption on the uncorrupted point satisfied e g by the standard generative assumption in pca problem recovers the exact optimal low dimensional subspace and identifies the corrupted point such identification of corrupted point that do not conform to the low dimensional approximation is of paramount interest in bioinformatics financial application and beyond our technique involve matrix decomposition using nuclear norm minimization however our result setup and approach necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition since we develop an approach to recover the correct column space of the uncorrupted matrix rather than the exact matrix itself in any problem where one seek to recover a structure rather than the exact initial matrix technique developed thus far relying on certificate of optimality will fail we present an important extension of these method which allows the treatment of such problem 
multi task learning mtl aim to improve the performance of multiple related task by exploiting the intrinsic relationship among them recently multi task feature learning algorithm have received increasing attention and they have been successfully applied to many application involving high dimensional data however they assume that all task share a common set of feature which is too restrictive and may not hold in real world application since outlier task often exist in this paper we propose a robust multitask feature learning algorithm rmtfl which simultaneously capture a common set of feature among relevant task and identifies outlier task specifically we decompose the weight model matrix for all task into two component we impose the well known group lasso penalty on row group of the first component for capturing the shared feature among relevant task to simultaneously identify the outlier task we impose the same group lasso penalty but on column group of the second component we propose to employ the accelerated gradient descent to efficiently solve the optimization problem in rmtfl and show that the proposed algorithm is scalable to large size problem in addition we provide a detailed theoretical analysis on the proposed rmtfl formulation specifically we present a theoretical bound to measure how well our proposed rmtfl approximates the true evaluation and provide bound to measure the error between the estimated weight of rmtfl and the underlying true weight moreover by assuming that the underlying true weight are above the noise level we present a sound theoretical result to show how to obtain the underlying true shared feature and outlier task sparsity pattern empirical study on both synthetic and real world data demonstrate that our proposed rmtfl is capable of simultaneously capturing shared feature among task and identifying outlier task 
in this paper we present a computationally efficient algorithm based on multiple instance learning for mapping informal settlement slum using very high resolution remote sensing imagery from remote sensing perspective informal settlement share unique spatial characteristic that distinguish them from other urban structure like industrial commercial and formal residential settlement however regular pattern recognition and machine learning method which are predominantly single instance or per pixel classifier often fail to accurately map the informal settlement a they do not capture the complex spatial pattern to overcome these limitation we employed a multiple instance based machine learning approach where group of contiguous pixel image patch are modeled a generated by a gaussian distribution we have conducted several experiment on very high resolution satellite imagery representing four unique geographic region across the world our method showed consistent improvement in accurately identifying informal settlement 
in this paper we discus an interesting and useful property of clickstream data often a visit includes repeated view of the same page we show that in three real datasets sampled from the website of technology and consulting group and a news broadcaster page repetition occur for the majority a a very specific structure namely in the form of nested palindrome this can be explained by the widespread use of feature which are available in any web browser the refresh and back button among the type of pattern which can be mined from sequence data many either stumble if symbol repetition are involved or else fail to capture interesting aspect related to symbol repetition in an attempt to remedy this we characterize the palindromic structure and discus possible way of making use of them one way is to pre process the sequence data by explicitly inserting these structure in order to obtain a richer output from conventional mining algorithm another application we discus is to use the information directly in order to analyze certain aspect of the website under study we also provide the simple linear time algorithm which we developed to identify and extract the structure from our data 
this paper study the problem of accurately recovering a k sparse vector beta star in bbr p from highly corrupted linear measurement y x beta star e star w where e star in bbr n is a sparse error vector whose nonzero entry may be unbounded and w is a stochastic noise term we propose a so called extended lasso optimization which take into consideration sparse prior information of both beta star and e star our first result show that the extended lasso can faithfully recover both the regression a well a the corruption vector our analysis relies on the notion of extended restricted eigenvalue for the design matrix x our second set of result applies to a general class of gaussian design matrix x with i i d row cal n sigma for which we can establish a surprising result the extended lasso can recover exact signed support of both beta star and e star from only omega k log p log n observation even when a linear fraction of observation is grossly corrupted our analysis also show that this amount of observation required to achieve exact signed support is indeed optimal 
we present a generalized framework for active inference the selective acquisition of label for case at prediction time in lieu of using the estimated label of a predictive model we develop technique within this framework for classifying in an online setting for example for classifying the stream of web page where online advertisement are being served stream application present novel complication because i at the time of label acquisition we don t know the set of instance that we will eventually see ii instance repeat based on some unknown and possibly skewed distribution we combine idea from decision theory cost sensitive learning and online density estimation we also introduce a method for on line estimation of the utility distribution which allows u to manage the budget over the stream the resulting model tell which instance to label so that by the end of each budget period the budget is best spent in expectation the main result show that our proposed approach to active inference on stream can indeed reduce error cost substantially over alternative approach more sophisticated online estimation achieve larger reduction in error we next discus simultaneously conducting active inference and active learning we show that our expected utility active inference strategy also selects good example for learning we close by pointing out that our utility distribution estimation strategy can also be applied to convert pool based active learning technique into budget sensitive online active learning technique 
many people share their activity with others through online community these shared activity have an impact on other user activity for example user are likely to become interested in item that are adopted e g liked bought and shared by their friend in this paper we propose a probabilistic model for discovering latent influence from sequence of item adoption event an inhomogeneous poisson process is used for modeling a sequence in which adoption by a user trigger the subsequent adoption of the same item by other user for modeling adoption of multiple item we employ multiple inhomogeneous poisson process which share parameter such a influence for each user and relation between user the proposed model can be used for finding influential user discovering relation between user and predicting item popularity in the future we present an efficient bayesian inference procedure of the proposed model based on the stochastic em algorithm the effectiveness of the proposed model is demonstrated by using real data set in a social bookmark sharing service 
multiple kernel learning mkl generalizes svms to the setting where one simultaneously train a linear classifier and chooses an optimal combination of given base kernel model complexity is typically controlled using various norm regularization on the base kernel mixing coefficient existing method neither regularize nor exploit potentially useful information pertaining to how kernel in the input set interact that is higher order kernel pair relationship that can be easily obtained via unsupervised similarity geodesic supervised correlation in error or domain knowledge driven mechanism which feature were used to construct the kernel we show that by substituting the norm penalty with an arbitrary quadratic function q one can impose a desired covariance structure on mixing weight and use this a an inductive bias when learning the concept this formulation significantly generalizes the widely used and norm mkl objective we explore the model s utility via experiment on a challenging neuroimaging problem where the goal is to predict a subject s conversion to alzheimer s disease ad by exploiting aggregate information from many distinct imaging modality here our new model outperforms the state of the art p value we briefly discus ramification in term of learning bound rademacher complexity 
network science ha emerged over the last year a an interdisciplinary area spanning traditional domain including mathematics computer science sociology biology and economics since complexity in social biological and economical system and more generally in complex system arises through pairwise interaction there exists a surging interest in understanding network in this tutorial we will provide an in depth presentation of the most popular random graph model used for modeling real world network we will then discus efficient algorithmic technique for mining large graph with emphasis on the problem of extracting graph sparsifiers partitioning graph into densely connected component and finding dense subgraphs we will motivate the problem we will discus and the algorithm we will present with real world application our aim is to survey important result in the area of modeling and mining large graph to uncover the intuition behind the key idea and to present future research direction 
more and more technology are taking advantage of the explosion of social medium web search content recommendation service marketing ad targeting etc this paper focus on the problem of automatically constructing user profile which can significantly benefit such technology we describe a general and robust machine learning framework for large scale classification of social medium user according to dimension of interest we report encouraging experimental result on task with different characteristic political affiliation detection ethnicity identification and detecting affinity for a particular business 
abstract much of the world s electronic text is annotated with human interpretable label such a tag on web page and subject code on academic publication effective text mining in this setting requires model that can flexibly account for the textual pattern that underlie the observed label while still discovering unlabeled topic neither supervised classification with it focus on label prediction nor purely unsupervised learning which doe not model the label explicitly is appropriate in this paper we present two new partially supervised generative model of labeled text partially labeled dirichlet allocation plda and the partially labeled dirichlet process pldp these model make use of the unsupervised learning machinery of topic model to discover the hidden topic within each label a well a unlabeled corpus wide latent topic we explore application with qualitative case study of tagged web page from del icio u and phd dissertation abstract demonstrating improved model interpretability over traditional topic model we use the many tag present in our del icio u dataset to quantitatively demonstrate the new model higher correlation with human relatedness score over several strong baseline 
we provide a novel algorithm to approximately factor large matrix with million of row million of column and billion of nonzero element our approach rest on stochastic gradient descent sgd an iterative stochastic optimization algorithm we first develop a novel stratified sgd variant ssgd that applies to general loss minimization problem in which the loss function can be expressed a a weighted sum of stratum loss we establish sufficient condition for convergence of ssgd using result from stochastic approximation theory and regenerative process theory we then specialize ssgd to obtain a new matrix factorization algorithm called dsgd that can be fully distributed and run on web scale datasets using e g mapreduce dsgd can handle a wide variety of matrix factorization we describe the practical technique used to optimize performance in our dsgd implementation experiment suggest that dsgd converges significantly faster and ha better scalability property than alternative algorithm 
social medium is producing massive amount of data on an unprecedented scale here people share their experience and opinion on various topic including personal health issue symptom treatment side effect and so on this make publicly available social medium data an invaluable resource for mining interesting and actionable healthcare insight in this paper we describe a novel real time flu and cancer surveillance system that us spatial temporal and text mining on twitter data the real time analysis result are reported visually in term of u disease surveillance map distribution and timeline of disease type symptom and treatment in addition to overall disease activity timeline on our project website our surveillance system can be very useful not only for early prediction of seasonal disease outbreak such a flu but also for monitoring distribution of cancer patient with different cancer type and symptom in each state and the popularity of treatment used the resulting insight are expected to help facilitate faster response to and preparation for epidemic and also be very useful for both patient and doctor to make more informed decision 
one of the most important feature of the web graph and social network is that they are constantly evolving the classical computational paradigm which assumes a fixed data set a an input to an algorithm that terminates is inadequate for such setting in this paper we study the problem of computing pagerank on an evolving graph we propose an algorithm that at any moment in the time and by crawling a small portion of the graph provides an estimate of the pagerank that is close to the true pagerank of the graph at that moment we will also evaluate our algorithm experimentally on real data set and on randomly generated input under a stylized model of graph evolution we show that our algorithm achieves a provable performance guarantee that is significantly better than the naive algorithm that crawl the node in a round robin fashion 
good textbook are organized in a systematically progressive fashion so that student acquire new knowledge and learn new concept based on known item of information we provide a diagnostic tool for quantitatively assessing the comprehension burden that a textbook imposes on the reader due to non sequential presentation of concept we present a formal definition of comprehension burden and propose an algorithmic approach for computing it we apply the tool to a corpus of high school textbook from india and empirically examine it effectiveness in helping author identify section of textbook that can benefit from reorganizing the material presented 
in digital advertising attribution is the problem of assigning credit to one or more advertisement for driving the user to the desirable action such a making a purchase rather than giving all the credit to the last ad a user see multi touch attribution allows more than one ad to get the credit based on their corresponding contribution multi touch attribution is one of the most important problem in digital advertising especially when multiple medium channel such a search display social mobile and video are involved due to the lack of statistical framework and a viable modeling approach true data driven methodology doe not exist today in the industry while predictive modeling ha been thoroughly researched in recent year in the digital advertising domain the attribution problem focus more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification traditional classification model fail to achieve those goal in this paper we first propose a bivariate metric one measure the variability of the estimate and the other measure the accuracy of classifying the positive and negative user we then develop a bagged logistic regression model which we show achieves a comparable classification accuracy a a usual logistic regression but a much more stable estimate of individual advertising channel contribution we also propose an intuitive and simple probabilistic model to directly quantify the attribution of different advertising channel we then apply both the bagged logistic model and the probabilistic model to a real world data set from a multi channel advertising campaign for a well known consumer software and service brand the two model produce consistent general conclusion and thus offer useful cross validation the result of our attribution model also shed several important insight that have been validated by the advertising team we have implemented the probabilistic model in the production advertising platform of the first author s company and plan to implement the bagged logistic regression in the next product release we believe availability of such data driven multi touch attribution metric and model is a break through in the digital advertising industry 
nonnegative matrix factorization nmf ha been successfully used a a clustering method especially for flat partitioning of document in this paper we propose an efficient hierarchical document clustering method based on a new algorithm for rank nmf when the two block coordinate descent framework of nonnegative least square is applied to computing rank nmf each subproblem requires a solution for nonnegative least square with only two column in the matrix we design the algorithm for rank nmf by exploiting the fact that an exhaustive search for the optimal active set can be performed extremely fast when solving these nnls problem in addition we design a measure based on the result of rank nmf for determining which leaf node should be further split on a number of text data set our proposed method produce high quality tree structure in significantly le time compared to other method such a hierarchical k mean standard nmf and latent dirichlet allocation 
digital storage of personal music collection and cloud based music service e g pandora spotify have fundamentally changed how music is consumed in particular automatically generated playlist have become an important mode of accessing large music collection the key goal of automated playlist generation is to provide the user with a coherent listening experience in this paper we present latent markov embedding lme a machine learning algorithm for generating such playlist in analogy to matrix factorization method for collaborative filtering the algorithm doe not require song to be described by feature a priori but it learns a representation from example playlist we formulate this problem a a regularized maximum likelihood embedding of markov chain in euclidian space and show how the resulting optimization problem can be solved efficiently an empirical evaluation show that the lme is substantially more accurate than adaptation of smoothed n gram model commonly used in natural language processing 
multidimensional distribution are often used in data mining to describe and summarize different feature of large datasets it is natural to look for distinct class in such datasets by clustering the data a common approach entail the use of method like k mean clustering however the k mean method inherently relies on the euclidean metric in the embedded space and doe not account for additional topology underlying the distribution in this paper we propose using earth mover distance emd to compare multidimensional distribution for a n bin histogram the emd is based on a solution to the transportation problem with time complexity o n log n to mitigate the high computational cost of emd we propose an approximation that reduces the cost to linear time given the large size of our dataset a fast approximation is crucial for this application other notion of distance such a the information theoretic kullback leibler divergence and statistical distance account only for the correspondence between bin with the same index and do not use information across bin and are sensitive to bin size a cross bin distance measure like emd is not affected by binning difference and meaningfully match the perceptual notion of nearness our technique is simple efficient and practical for clustering distribution we demonstrate the use of emd on a real world application of analyzing anonymous mobility usage pattern which are defined a distribution over a manifold emd allows u to represent inherent relationship in this space and enables u to successfully cluster even sparse signature 
approximation of non linear kernel using random feature mapping ha been successfully employed in large scale data analysis application accelerating the training of kernel machine while previous random feature mapping run in o ndd time for n training sample in d dimensional space and d random feature map we propose a novel randomized tensor product technique called tensor sketching for approximating any polynomial kernel in o n d d log d time also we introduce both absolute and relative error bound for our approximation to guarantee the reliability of our estimation algorithm empirically tensor sketching achieves higher accuracy and often run order of magnitude faster than the state of the art approach for large scale real world datasets 
in a large online advertising system adversary may attempt to profit from the creation of low quality or harmful advertisement in this paper we present a large scale data mining effort that detects and block such adversarial advertisement for the benefit and safety of our user because both false positive and false negative have high cost our deployed system us a tiered strategy combining automated and semi automated method to ensure reliable classification we also employ strategy to address the challenge of learning from highly skewed data at scale allocating the effort of human expert leveraging domain expert knowledge and independently assessing the effectiveness of our system 
in sponsored search auction the auctioneer operates the marketplace by setting a number of auction parameter such a reserve price for the task of auction optimization the auction parameter may be set for each individual keyword but the optimization problem becomes intractable since the number of keywords is in the million to reduce the dimensionality and generalize well one wish to cluster keywords or query into meaningful group and set parameter at the keyword cluster level for auction optimization keywords shall be deemed a interchangeable commodity with respect to their valuation from advertiser represented a bid distribution or landscape clustering keywords for auction optimization shall thus be based on their bid distribution in this paper we present a formalism of clustering probability distribution and it application to query clustering where each query is represented a a probability density of click through rate ctr weighted bid and distortion is measured by kl divergence we first derive a k mean variant for clustering gaussian density which have a closed form kl divergence we then develop an algorithm for clustering gaussian mixture density which generalize a single gaussian and are typically a more realistic parametric assumption for real world data the kl divergence between gaussian mixture density is no longer analytically tractable hence we derive a variational em algorithm that minimizes an upper bound of the total within cluster kl divergence the clustering algorithm ha been deployed successfully into production yielding significant improvement in revenue and click over the existing production system while motivated by the specific setting of query clustering the proposed clustering method is generally applicable to many real world application where an example is better characterized by a distribution than a finite dimensional feature vector in euclidean space a in the classical k mean 
predicting ad click through rate ctr is a massive scale learning problem that is central to the multi billion dollar online advertising industry we present a selection of case study and topic drawn from recent experiment in the setting of a deployed ctr prediction system these include improvement in the context of traditional supervised learning based on an ftrl proximal online learning algorithm which ha excellent sparsity and convergence property and the use of per coordinate learning rate we also explore some of the challenge that arise in a real world system that may appear at first to be outside the domain of traditional machine learning research these include useful trick for memory saving method for assessing and visualizing performance practical method for providing confidence estimate for predicted probability calibration method and method for automated management of feature finally we also detail several direction that did not turn out to be beneficial for u despite promising result elsewhere in the literature the goal of this paper is to highlight the close relationship between theoretical advance and practical engineering in this industrial setting and to show the depth of challenge that appear when applying traditional machine learning method in a complex dynamic system 
how to realize targeted advertising in digital signage is an interesting question this paper proposed an intelligent advertising framework iaf which pioneer the integration of anonymous viewer analytics ava and data mining technology to achieve targeted and interactive advertising iaf correlate ava viewership information with point of sale po data and establishes a link between the response time to an ad by a certain demographic group and the effect on the sale of the advertised product with the advertising model learned based on this correlation iaf can provide advertiser and retailer with intelligence to show the right ad to right audience in right location at right time preliminary result indicate that iaf will greatly improve the effect and utility of advertising and maximize the return on investment roi of advertiser and retailer the demo show intel s leadership regarding intelligent advertising in the digital signage industry 
matching one set of object to another is a ubiquitous task in machine learning and computer vision that often reduces to some form of the quadratic assignment problem qap the qap is known to be notoriously hard both in theory and in practice here we investigate if this difficulty can be mitigated when some additional piece of information is available a that all qap instance of interest come from the same application and b the correct solution for a set of such qap instance is given we propose a new approach to accelerate the solution of qaps based on learning parameter for a modified objective function from prior qap instance a key feature of our approach is that it take advantage of the algebraic structure of permutation in conjunction with special method for optimizing function over the symmetric group n in fourier space experiment show that in practical domain the new method can outperform existing approach 
clinical study found that early detection and intervention are essential for preventing clinical deterioration in patient for patient both in intensive care unit icu a well a in general ward but under real time data sensing rds in this paper we develop an integrated data mining approach to give early deterioration warning for patient under real time monitoring in icu and rds existing work on mining real time clinical data often focus on certain single vital sign and specific disease in this paper we consider an integrated data mining approach for general sudden deterioration warning we synthesize a large feature set that includes first and second order time series feature detrended fluctuation analysis dfa spectral analysis approximative entropy and cross signal feature we then systematically apply and evaluate a series of established data mining method including forward feature selection linear and nonlinear classification algorithm and exploratory undersampling for class imbalance an extensive empirical study is conducted on real patient data collected between and from a variety of icu result show the benefit of each of the proposed technique and the final integrated approach significantly improves the prediction quality the proposed clinical warning system is currently under integration with the electronic medical record system at barnes jewish hospital in preparation for a clinical trial this work represents a promising step toward general early clinical warning which ha the potential to significantly improve the quality of patient care in hospital 
research on time series forecasting is mostly focused on point prediction model are obtained to estimate the expected value of the target variable for a certain point in future however for several relevant application this type of forecast ha limited utility e g costumer wallet value estimation wind and electricity power production control of water quality etc for these domain it is frequently more important to be able to forecast a range of plausible future value of the target variable a typical example is wind power production where it is of high relevance to predict the future wind variability in order to ensure that supply and demand are balanced this type of prediction will allow timely action to be taken in order to cope with the expected value of the target variable on a certain future time horizon in this paper we study this type of prediction the prediction of a range of expected value for a future time interval we describe some possible approach to this task and propose an alternative procedure that our extensive experiment on both artificial and real world domain show to have clear advantage 
many people use the web a the main source of information in their daily life however most web page contain non informative component such a side bar footer header and advertisement which are undesirable for certain application like printing we demonstrate a system that automatically extract the informative content from newsand blog like web page in contrast to many existing method that are limited to identifying only the text or the bounding rectangular region our system not only identifies the content but also the structural role of various content component such a title paragraph image and caption the structural information enables re layout of the content in a pleasing way besides the article text extraction our system includes the following component print link detection to identify the url link for printing and to use it for more reliable analysis and recognition title detection incorporating both visual cue and html tag image and caption detection utilizing extensive visual cue multiple page and next page url detection the performance of our system ha been thoroughly evaluated using a human labeled ground truth dataset consisting of web page from major web site we show accurate result using such a dataset 
role discovery in graph is an emerging area that allows analysis of complex graph in an intuitive way in contrast to community discovery which find group of highly connected node role discovery find group of node that share similar topological structure in the graph and hence a common role or function such a being a broker or a periphery node however existing work so far is completely unsupervised which is undesirable for a number of reason we provide an alternating least square framework that allows convex constraint to be placed on the role discovery problem which can provide useful supervision in particular we explore supervision to enforce i sparsity ii diversity and iii alternativeness in the role we illustrate the usefulness of this supervision on various data set and application 
in this paper we introduce a methodology for extracting mobility profile of individual from raw digital trace in particular gps trace and study criterion to match individual based on profile we instantiate the profile matching problem to a specific application context namely proactive car pooling service and therefore develop a matching criterion that satisfies various basic constraint obtained from the background knowledge of the application domain in order to evaluate the impact and robustness of the method introduced two experiment are reported which were performed on a massive dataset containing gps trace of private car i the impact of the car pooling application based on profile matching is measured in term of percentage shareable traffic ii the approach is adapted to coarser grained mobility data source that are nowadays commonly available from telecom operator in addition the ensuing loss in precision and coverage of profile match is measured 
a major source of revenue shrink in retail store is the intentional or unintentional failure of proper checking out of item by the cashier more recently a few automated surveillance system have been developed to monitor cashier lane and detect non compliant activity such a fake item checkout or scan done with the intention of deriving monetary benefit these system use data from surveillance video camera and transaction log tlog recorded at the point of sale po in this paper we present a pattern discovery based approach to detect fraudulent event at the po our approach is based on mining time ordered text stream representing retail transaction formed from a combination of visually detected checkout related activity called primitive and barcodes from tlog data pattern representing single item checkout i e anchored around a single barcode are discovered from these text stream using an efficient pattern discovery technique called teiresias the discovered pattern are used to build model for true and fake item scan by retaining or discarding the anchoring barcodes in those pattern respectively a pattern matching and classification scheme is designed to robustly detect non compliant cashier activity in the presence of noise in either the tlog or the video data different weighting scheme for quantifying the relative importance of the discovered pattern are explored frequency support vector machine svm and frequency svm using a large scale dataset recorded from retail store our approach discovers semantically meaningful cashier scan pattern our experiment also suggest that different weighting scheme result in varied false and true positive performance on the task of fake scan detection 
in online advertising response prediction is the problem of estimating the probability that an advertisement is clicked when displayed on a content publisher s webpage in this paper we show how response prediction can be viewed a a problem of matrix completion and propose to solve it using matrix factorization technique from collaborative filtering cf we point out the two crucial difference between standard cf problem and response prediction namely the requirement of predicting probability rather than score and the issue of confidence in matrix entry we address these issue using a matrix factorization analogue of logistic regression and by applying a principled confidence weighting scheme to it objective we show how this factorization can be seamlessly combined with explicit feature or side information for page and ad which let u combine the benefit of both approach finally we combat the extreme sparsity of response prediction data by incorporating hierarchical information about the page and ad into our factorization model experiment on three very large real world datasets show that our model outperforms current state of the art method for response prediction 
the effectiveness of existing top n recommendation method decrease a the sparsity of the datasets increase to alleviate this problem we present an item based method for generating top n recommendation that learns the item item similarity matrix a the product of two low dimensional latent factor matrix these matrix are learned using a structural equation modeling approach wherein the value being estimated is not used for it own estimation a comprehensive set of experiment on multiple datasets at three different sparsity level indicate that the proposed method can handle sparse datasets effectively and outperforms other state of the art top n recommendation method the experimental result also show that the relative performance gain compared to competing method increase a the data get sparser 
this paper proposes an online mixture modeling methodology in which individual component can have different marginal distribution and dependency structure mixture model have been widely studied and applied to various application area including density estimation fraud failure detection image segmentation etc previous research ha been almost exclusively focused on mixture model having component of a single type e g a gaussian mixture model however recent growing need for complicated data modeling necessitate the use of more flexible mixture model e g a mixture of a lognormal distribution for medical cost and a gaussian distribution for blood pressure for medical analytics our key idea include separating marginal distribution and their dependency using copula and online extension of a recently developed expectation minimization of description length which enable u to efficiently learn type of both marginal distribution and copula a well a their parameter the proposed method provides not only good performance in application but also scalable automatic model selection which greatly reduces the intensive modeling cost in data mining process we show that the proposed method outperforms state of the art method in application to density estimation and to anomaly detection 
multi task learning mtl aim to improve generalization performance by learning multiple related task simultaneously and identifying the shared information among task most of existing mtl method focus on learning linear model under the supervised setting we propose a novel semi supervised and nonlinear approach for mtl using vector field a vector field is a smooth mapping from the manifold to the tangent space which can be viewed a a directional derivative of function on the manifold we argue that vector field provide a natural way to exploit the geometric structure of data a well a the shared differential structure of task both of which are crucial for semi supervised multi task learning in this paper we develop multi task vector field learning mtvfl which learns the predictor function and the vector field simultaneously mtvfl ha the following key property the vector field mtvfl learns are close to the gradient field of the predictor function within each task the vector field is required to be a parallel a possible which is expected to span a low dimensional subspace the vector field from all task share a low dimensional subspace we formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non convex problem the experimental result on synthetic and real data demonstrate the effectiveness of our proposed approach 
maximal clique enumeration mce is a long standing problem in graph theory and ha numerous important application though extensively studied most existing algorithm become impractical when the input graph is too large and is disk resident we first propose an efficient partition based algorithm for mce that address the problem of processing large graph with limited memory we then further reduce the high cost of cpu computation of mce by a careful nested partition based on a cost model finally we parallelize our algorithm to further reduce the overall running time we verified the efficiency of our algorithm by experiment in large real world graph 
given the vast amount of information on the world wide web recommender system are increasingly being used to help filter irrelevant data and suggest information that would interest user traditional system make recommendation based on a single domain e g movie or book domain recent work ha examined the correlation in different domain and designed model that exploit user preference on a source domain to predict user preference on a target domain however these method are based on matrix factorization and can only be applied to two dimensional data transferring high dimensional data from one domain to another requires decomposing the high dimensional data to binary relation which result in information loss furthermore this decomposition creates a large number of matrix that need to be transferred and combining them in the target domain is non trivial separately researcher have looked into using social network information to improve recommendation however this social network information ha not been explored in cross domain collaborative filtering in this work we propose a generalized cross domain collaborative filtering framework that integrates social network information seamlessly with cross domain data this is achieved by utilizing tensor factorization with topic based social regularization this framework is able to transfer high dimensional data without the need for decomposition by finding shared implicit cluster level tensor from multiple domain extensive experiment conducted on real world datasets indicate that the proposed framework outperforms state of art algorithm for item recommendation user recommendation and tag recommendation 
learning algorithm that embed object into euclidean space have become the method of choice for a wide range of problem ranging from recommendation and image search to playlist prediction and language modeling probabilistic embedding method provide elegant approach to these problem but can be expensive to train and store a a large monolithic model in this paper we propose a method that train not one monolithic model but multiple local embeddings for a class of pairwise conditional model especially suited for sequence and co occurrence modeling we show that computation and memory for training these multi space model can be efficiently parallelized over many node of a cluster focusing on sequence modeling for music playlist we show that the method substantially speed up training while maintaining high model quality 
when analyzing data that originated from a dynamical system a common practice is to encompass the problem in the well known framework of markov decision process mdps and reinforcement learning rl the state space in these solution is usually chosen in some heuristic fashion and the formed mdp can then be used to simulate and predict data a well a indicate the best possible action in each state the model chosen to characterize the data affect the complexity and accuracy of any further action we may wish to apply yet few method that rely on the dynamic structure to select such a model were suggested in this work we address the problem of how to use time series data to choose from a finite set of candidate discrete state space where these space are constructed by a domain expert we formalize the notion of model selection consistency in the proposed setup we then discus the difference between our proposed framework and the classical maximum likelihood ml framework and give an example where ml fails afterwards we suggest alternative selection criterion and show them to be weakly consistent we then define weak consistency for a model construction algorithm and show a simple algorithm that is weakly consistent finally we test the performance of the suggested criterion and algorithm on both simulated and real world data 
we are interested in the problem of tracking broad topic such a baseball and fashion in continuous stream of short text exemplified by tweet from the microblogging service twitter the task is conceived a a language modeling problem where per topic model are trained using hashtags in the tweet stream which serve a proxy for topic label simple perplexity based classifier are then applied to filter the tweet stream for topic of interest within this framework we evaluate both intrinsically and extrinsically smoothing technique for integrating foreground model to capture recency and background model to combat sparsity a well a different technique for retaining history experiment show that unigram language model smoothed using a normalized extension of stupid backoff and a simple queue for history retention performs well on the task 
time series shapelets are small local pattern in a time series that are highly predictive of a class and are thus very useful feature for building classifier and for certain visualization and summarization task while shapelets were introduced only recently they have already seen significant adoption and extension in the community despite their immense potential a a data mining primitive there are two important limitation of shapelets first their expressiveness is limited to simple binary presence absence question second even though shapelets are computed offline the time taken to compute them is significant in this work we address the latter problem by introducing a novel algorithm that find shapelets in le time than current method by an order of magnitude our algorithm is based on intelligent caching and reuse of computation and the admissible pruning of the search space because our algorithm is so fast it creates an opportunity to consider more expressive shapelet query in particular we show for the first time an augmented shapelet representation that distinguishes the data based on conjunction or disjunction of shapelets we call our novel representation logical shapelets we demonstrate the efficiency of our approach on the classic benchmark datasets used for these problem and show several case study where logical shapelets significantly outperform the original shapelet representation and other time series classification technique we demonstrate the utility of our idea in domain a diverse a gesture recognition robotics and biometrics 
arguably the most effective technique to ensure wide adoption of a concept or product is by repeatedly exposing individual to message that reinforce the concept or promote the product recognizing the role of repeated exposure to a message in this paper we propose a novel framework for the effective placement of content given the navigational pattern of user in a network e g web graph hyperlinked corpus or road network and given a model of the relationship between content adoption and frequency of exposition we define the repetition aware content placement racp problem a that of identifying the set of b node on which content should be placed so that the expected number of user adopting that content is maximized the key contribution of our work is the introduction of memory into the navigation process by making user conversion dependent on the number of her exposure to that content this dependency is captured using a conversion model that is general enough to capture arbitrary dependency our solution to this general problem build upon the notion of absorbing random walk which we extend appropriately in order to address the technicality of our definition although we show the racp problem to be np hard we propose a general and efficient algorithmic solution our experimental result demonstrate the efficacy and the efficiency of our method in multiple real world datasets obtained from different application domain 
the prevalent use of social medium produce mountain of unlabeled high dimensional data feature selection ha been shown effective in dealing with high dimensional data for efficient data mining feature selection for unlabeled data remains a challenging task due to the absence of label information by which the feature relevance can be assessed the unique characteristic of social medium data further complicate the already challenging problem of unsupervised feature selection e g part of social medium data is linked which make invalid the independent and identically distributed assumption bringing about new challenge to traditional unsupervised feature selection algorithm in this paper we study the difference between social medium data and traditional attribute value data investigate if the relation revealed in linked data can be used to help select relevant feature and propose a novel unsupervised feature selection framework lufs for linked social medium data we perform experiment with real world social medium datasets to evaluate the effectiveness of the proposed framework and probe the working of it key component 
question answering q a website are now large repository of valuable knowledge while most q a site were initially aimed at providing useful answer to the question asker there ha been a marked shift towards question answering a a community driven knowledge creation process whose end product can be of enduring value to a broad audience a part of this shift specific expertise and deep knowledge of the subject at hand have become increasingly important and many q a site employ voting and reputation mechanism a centerpiece of their design to help user identify the trustworthiness and accuracy of the content to better understand this shift in focus from one off answer to a group knowledge creation process we consider a question together with it entire set of corresponding answer a our fundamental unit of analysis in contrast with the focus on individual question answer pair that characterized previous work our investigation considers the dynamic of the community activity that shape the set of answer both how answer and voter arrive over time and how this influence the eventual outcome for example we observe significant assortativity in the reputation of co answerer relationship between reputation and answer speed and that the probability of an answer being chosen a the best one strongly depends on temporal characteristic of answer arrival we then show that our understanding of such property is naturally applicable to predicting several important quantity including the long term value of the question and it answer a well a whether a question requires a better answer finally we discus the implication of these result for the design of q a site 
u report is an open source sm platform operated by unicef uganda designed to give community member a voice on issue that impact them data received by the system are either sm response to a poll conducted by unicef or unsolicited report of a problem occurring within the community there are currently u report participant and they send up to unsolicited text message a week the objective of the program in uganda is to understand the data in real time and have issue addressed by the appropriate department in unicef in a timely manner given the high volume and velocity of the data stream manual inspection of all message is no longer sustainable this paper describes an automated message understanding and routing system deployed by ibm at unicef we employ recent advance in data mining to get the most out of labeled training data while incorporating domain knowledge from expert we discus the trade offs design choice and challenge in applying such technique in a real world deployment 
people use various social medium for different purpose the information on an individual site is often incomplete when source of complementary information are integrated a better profile of a user can be built to improve online service such a verifying online information to integrate these source of information it is necessary to identify individual across social medium site this paper aim to address the cross medium user identification problem we introduce a methodology mobius for finding a mapping among identity of individual across social medium site it consists of three key component the first component identifies user unique behavioral pattern that lead to information redundancy across site the second component construct feature that exploit information redundancy due to these behavioral pattern and the third component employ machine learning for effective user identification we formally define the cross medium user identification problem and show that mobius is effective in identifying user across social medium site this study pave the way for analysis and mining across social medium site and facilitates the creation of novel online service across site 
prediction market are virtual stock market used to gain insight and forecast event by leveraging the wisdom of crowd popularly applied in the public to cultural question election result box office return they have recently been applied by corporation to leverage employee knowledge and forecast answer to business question sale volume product and feature release timing determining whether to run a prediction market requires practical experience that is rarely described over the last few year ford motor company obtained practical experience by deploying one of the largest corporate prediction market known business partner in the u europe and south america provided question on new vehicle feature sale volume take rate pricing and macroeconomic trend we describe our experience including both the strong and weak correlation found between prediction and real world result evaluating this methodology go beyond prediction accuracy however since there are many side benefit in addition to the prediction we discus the value of comment stock price change over time the ability to overcome bureaucratic limit and flexibly filling hole in corporate knowledge enabling better decision making we conclude with advice on running prediction market including writing good question market duration motivating trader and protecting confidential information 
this paper introduces a novel image decomposition approach for an ensemble of correlated image using low rank and sparsity constraint each image is decomposed a a combination of three component one common component one condition component which is assumed to be a low rank matrix and a sparse residual for a set of face image of nsubjects the decomposition find n common component one for each subject k low rank component each capturing a different global condition of the set e g different illumination condition and a sparse residual for each input image through this decomposition the proposed approach recovers a clean face image the common component for each subject and discovers the condition the condition component and the sparse residual of the image in the set the set of n k image containing only the common and the low rank component form a compact and discriminative representation for the original image we design a classifier using only these n k image experiment on commonly used face data set demonstrate the effectiveness of the approach for face recognition through comparing with the leading state of the art in the literature the experiment further show good accuracy in classifying the condition of an input image suggesting that the component from the proposed decomposition indeed capture physically meaningful feature of the input 
most existing research about online trust assumes static trust relation between user a we are informed by social science trust evolves a human interact little work exists studying trust evolution in an online world researching online trust evolution face unique challenge because more often than not available data is from passive observation in this paper we leverage social science theory to develop a methodology that enables the study of online trust evolution in particular we propose a framework of evolution trust etrust which exploit the dynamic of user preference in the context of online product review we present technical detail about modeling trust evolution and perform experiment to show how the exploitation of trust evolution can help improve the performance of online application such a rating and trust prediction 
aliexpress is an online e commerce platform for wholesale product credit card is one of it various payment method an online transaction using credit card is called a card not present cnp transaction where the physical card ha not been swiped into a reader it s also the major type of credit card fraud causing a great overhead of the online operation seller and buyer to protect customer on our platform we developed a real time credit card fraud detection system using the machine learning technology which allows u to achieve a precision of at a recall of with the system we can provide the best online shopping experience for our customer without the high risk of online transaction which always result a high operational cost we will briefly share our experience and practice in the expo 
sensor node have limited local storage computational power and battery life a a result of which it is desirable to minimize the storage processing and communication from these node during data collection the problem is further magnified by the large volume of data collected in real application sensor stream are often highly correlated with one another or may have other kind of functional dependency for example a group of sound sensor in a given geographical proximity may pick almost the same set of signal clearly since there are considerable functional dependency between different sensor there are huge redundancy in the data collected by sensor these redundancy may also change a the data evolve over time in this paper we discus real time algorithm for reducing the volume of the data collected in sensor network the broad idea is to determine the functional dependency between sensor stream efficiently in real time and actively collect the data only from a minimal set of sensor the remaining sensor collect the data passively at low sampling rate in order to detect any changing trend in the underlying data we present real time algorithm in order to minimize the power consumption in reducing the data collected and show that the resulting data retains almost the same amount of information at a much lower cost 
collaborative filtering ha been extensively studied in the context of rating prediction however industrial recommender system often aim at predicting a few item of immediate interest to the user typically product that s he is likely to buy in the near future in a collaborative filtering setting the prediction may be based on the user s purchase history rather than rating information which may be unreliable or unavailable in this paper we present an experimental evaluation of various collaborative filtering algorithm on a real world dataset of purchase history from customer in a store of a french home improvement and building supply chain these experiment are part of the development of a prototype recommender system for salesperson in the store we show how different setting for training and applying the model a well a the introduction of domain knowledge may dramatically influence both the absolute and the relative performance of the different algorithm to the best of our knowledge the influence of these parameter on the quality of the prediction of recommender system ha rarely been reported in the literature 
real time interaction which enables live discussion ha become a key feature of most web application in such an environment the ability to automatically analyze user opinion and sentiment a discussion develop is a powerful resource known a real time sentiment analysis however this task come with several challenge including the need to deal with highly dynamic textual content that is characterized by change in vocabulary and it subjective meaning and the lack of labeled data needed to support supervised classifier in this paper we propose a transfer learning strategy to perform real time sentiment analysis we identify a task opinion holder bias prediction which is strongly related to the sentiment analysis task however in constrast to sentiment analysis it build accurate model since the underlying relational data follows a stationary distribution instead of learning textual model to predict content polarity i e the traditional sentiment analysis approach we first measure the bias of social medium user toward a topic by solving a relational learning task over a network of user connected by endorsement e g retweets in twitter we then analyze sentiment by transferring user bias to textual feature this approach work because while new term may arise and old term may change their meaning user bias tends to be more consistent over time a a basic property of human behavior thus we adopted user bias a the basis for building accurate classification model we applied our model to post collected from twitter on two topic the brazilian presidential election and the season of brazilian soccer league our result show that knowing the bias of only of user generates an f accuracy level ranging from to in predicting user sentiment in tweet 
i present some recent work on statistical inference for big data divide and conquer is a natural computational paradigm for approaching big data problem particularly given recent development in distributed and parallel computing but some interesting challenge arise when applying divide and conquer algorithm to statistical inference problem one interesting issue is that of obtaining confidence interval in massive datasets the bootstrap principle suggests resampling data to obtain fluctuation in the value of estimator and thereby confidence interval but this is infeasible with massive data subsampling the data yield fluctuation on the wrong scale which have to be corrected to provide calibrated statistical inference i present a new procedure the bag of little bootstrap which circumvents this problem inheriting the favorable theoretical property of the bootstrap but also having a much more favorable computational profile another issue that i discus is the problem of large scale matrix completion here divide and conquer is a natural heuristic that work well in practice but new theoretical problem arise when attempting to characterize the statistical performance of divide and conquer algorithm here the theoretical support is provided by concentration theorem for random matrix and i present a new approach to this problem based on stein s method 
this paper describes three linear scale incremental and fully automatic semantic mining algorithm that are at the foundation of the new semantic platform being released in the next version of sql server the target workload is large million enterprise document corpus at these scale anything short of linear scale and incremental is costly to deploy these three algorithm give rise to three weighted physical index tag index top keywords in each document document similarity index top closely related document given any document and semantic phrase similarity index top semantically related phrase given any phrase which are then query able through the sql interface the need for specifically creating these three index wa motivated by observing typical stage of document research and gap analysis given current tool and technology at the enterprise we describe the mining algorithm and architecture and also outline some compelling user experience that are enabled by the index 
in many real world application it is becoming common to have data extracted from multiple diverse source known a multi view data multi view learning mvl ha been widely studied in many application but existing mvl method learn a single task individually in this paper we study a new direction of multi view learning where there are multiple related task with multi view data i e multi view multi task learning or mvmt learning in our mvmt learning method we learn a linear mapping for each view in each task in a single task we use co regularization to obtain function that are in agreement with each other on the unlabeled sample and achieve low classification error on the labeled sample simultaneously cross different task additional regularization function are utilized to ensure the function that we learn in each view are similar we also developed two extension of the mvmt learning algorithm one extension handle missing view and the other handle non uniformly related task experimental study on three real world data set demonstrate that our mvmt method significantly outperform the existing state of the art method 
probabilistic model of network growth have been extensively studied a idealized representation of network evolution model such a the kronecker model duplication based model and preferential attachment model have been used for task such a representing null model detecting anomaly algorithm testing and developing an understanding of various mechanistic growth process however developing a new growth model to fit observed property of a network is a difficult task and a new network are studied new model must constantly be developed here we present a framework called growcode for the automatic discovery of novel growth model that match user specified topological feature in undirected graph growcode introduces a set of basic command that are general enough to encode several previously developed model coupling this formal representation with an optimization approach we show that growcode is able to discover model for protein interaction network autonomous system network and scientific collaboration network that closely match property such a the degree distribution the clustering coefficient and assortativity that are observed in real network of these class additional test on simulated network show that the model learned by growcode generate distribution of graph with similar variance a existing model for these class 
the volume includes a set of selected paper extended and revised from the th international conference on knowledge discovery and data mining march macau chin this volume is to provide a forum for researcher educator engineer and government official involved in the general area of knowledge discovery and data mining and learning to disseminate their latest research result and exchange view on the future research direction of these field high quality paper are included in the volume 
in this paper we introduce laicos a social web search engine a a contribution to the growing area of social information retrieval sir social information and personalization are at the heart of laicos on the one hand the social context of document is added a a layer to their textual content traditionally used for indexing to provide personalized social document representation on the other hand the social context of user is used for the query expansion process using the personalized social query expansion framework psqe proposed in our earlier work we describe the different component of the system while relying on social bookmarking system a a source of social information for personalizing and enhancing the ir process we show how the internal structure of index a well a the query expansion process operated using social information 
non convex sparsity inducing penalty have recently received considerable attention in sparse learning recent theoretical investigation have demonstrated their superiority over the convex counterpart in several sparse learning setting however solving the non convex optimization problem associated with non convex penalty remains a big challenge a commonly used approach is the multi stage m convex relaxation or dc programming which relaxes the original non convex problem to a sequence of convex problem this approach is usually not very practical for large scale problem because it computational cost is a multiple of solving a single convex problem in this paper we propose a general iterative shrinkage and thresholding gist algorithm to solve the nonconvex optimization problem for a large class of non convex penalty the gist algorithm iteratively solves a proximal operator problem which in turn ha a closed form solution for many commonly used penalty at each outer iteration of the algorithm we use a line search initialized by the barzilai borwein bb rule that allows finding an appropriate step size quickly the paper also present a detailed convergence analysis of the gist algorithm the efficiency of the proposed algorithm is demonstrated by extensive experiment on large scale data set 
recent advance in smart metering technology enable utility company to have access to tremendous amount of smart meter data from which the utility company are eager to gain more insight about their customer in this paper we aim to detect electric heat pump from coarse grained smart meter data for a heat pump marketing campaign however appliance detection is a challenging task especially given a very low granularity and partial labeled even unlabeled data traditional method install either a high granularity smart meter or sensor at every appliance which is either too expensive or requires technical expertise we propose a novel approach to detect heat pump that utilizes low granularity smart meter data prior sale data and weather data in particular motivated by the characteristic of heat pump consumption pattern we extract novel feature that are highly relevant to heat pump usage from smart meter data and weather data under the constraint that only a subset of heat pump user are available we formalize the problem into a positive and unlabeled data classification and apply biased support vector machine bsvm to our extracted feature our empirical study on a real world data set demonstrates the effectiveness of our method furthermore our method ha been deployed in a real life setting where the partner electric company run a targeted campaign for customer based on the initial feedback our detection algorithm can successfully detect substantial number of non heat pump user who were identified heat pump user with the prior algorithm the company had used 
patenting is one of the most important way to protect company s core business concept and proprietary technology analyzing large volume of patent data can uncover the potential competitive or collaborative relation among company in certain area which can provide valuable information to develop strategy for intellectual property ip r d and marketing in this paper we present a novel topic driven patent analysis and mining system instead of merely searching over patent content we focus on studying the heterogeneous patent network derived from the patent database which is represented by several type of object company inventor and technical content jointly evolving over time we design and implement a general topic driven framework for analyzing and mining the heterogeneous patent network specifically we propose a dynamic probabilistic model to characterize the topical evolution of these object within the patent network based on this modeling framework we derive several patent analytics tool that can be directly used for ip and r d strategy planning including a heterogeneous network co ranking method a topic level competitor evolution analysis algorithm and a method to summarize the search result we evaluate the proposed method on a real world patent database the experimental result show that the proposed technique clearly outperform the corresponding baseline method 
microblogging service such a twitter have become popular channel for people to express their opinion towards a broad range of topic twitter generates a huge volume of instant message i e tweet carrying user sentiment and attitude every minute which both necessitates automatic opinion summarization and pose great challenge to the summarization system in this paper we study the problem of opinion summarization for entity such a celebrity and brand in twitter we propose an entity centric topic based opinion summarization framework which aim to produce opinion summary in accordance with topic and remarkably emphasizing the insight behind the opinion to this end we first mine topic from hashtags the human annotated semantic tag in tweet we integrate the hashtags a weakly supervised information into topic modeling algorithm to obtain better interpretation and representation for calculating the similarity among them and adopt affinity propagation algorithm to group hashtags into coherent topic subsequently we use template generalized from paraphrasing to identify tweet with deep insight which reveal reason express demand or reflect viewpoint afterwards we develop a target i e entity dependent sentiment classification approach to identifying the opinion towards a given target i e entity of tweet finally the opinion summary is generated through integrating information from dimension of topic opinion and insight a well a other factor e g topic relevancy redundancy and language style in an unified optimization framework we conduct extensive experiment on a real life data set to evaluate the performance of individual opinion summarization module a well a the quality of the produced summary the promising experiment result show the effectiveness of the proposed framework and algorithm 
in more and more industry competitive advantage hinge on exploiting the largest quantity of data in the shortest possible time and doing so cost effectively data volume are growing exponentially while business are striving to deploy sophisticated and computationally intensive predictive analytics often massive data is stored in a data warehouse running on dedicated parallel hardware but advanced analytics is performed on a separate compute platform moving data from the data warehouse to the compute environment can constitute a significant bottleneck organization resort to considering only a fraction of their data or refreshing their analysis infrequently to address the data movement bottleneck and take full advantage of parallel data warehouse platform vendor are offering new in database analytics capability they are opening up their platform allowing user to run their own user defined function and statistical model a well a vendorand partner supplied advanced analytics on the database platform close to the data in parallel without transporting the data through a host node or corporate network in this talk we will present the need for in database analytics and discus a number of the new solution available highlighting case study where solution time have been reduced from hour to minute or second 
many practical data mining system such a those for fraud detection and surveillance deal with building classifier that are not autonomous but part of a larger interactive system with an expert in the loop the goal of these system is not just to maximize the performance of the classifier but to make the expert more efficient at performing their task thus maximizing the overall return on investment of the system this paper describes an interactive system for detecting payment error in insurance claim with claim auditor in the loop we describe an interactive claim prioritization component that us an online cost sensitive learning approach more like this to make the system efficient our interactive prioritization component is built on top of a batch classifier that ha been trained to detect payment error in health insurance claim and optimizes the interaction between the classifier and the domain expert who are consuming the result of this system the goal is to make these auditor more efficient and effective a well a improving the classification performance of the system the result is both a reduction in time it take for the auditor to review and label claim a well a improving the precision of the system in finding payment error we show result obtained from applying this system at two major u health insurance company indicating significant reduction in claim audit cost and potential saving of million year making the insurance provider more efficient and lowering their operating cost our system reduces the money being wasted by provider and insurer dealing with incorrectly processed claim and make the healthcare system more efficient 
we study the accuracy of evaluation metric used to estimate the efficacy of predictive model offline evaluation metric are indicator of the expected model performance on real data however in practice we often experience substantial discrepancy between the offline and online performance of the model we investigate the characteristic and behavior of the evaluation metric on offline and online testing both analytically and empirically by experimenting them on online advertising data from the bing search engine one of our finding is that some offline metric like auc the area under the receiver operating characteristic curve and rig relative information gain that summarize the model performance on the entire spectrum of operating point could be quite misleading sometimes and result in significant discrepancy in offline and online metric for example for click prediction model for search advertising error in prediction in the very low range of predicted click score impact the online performance much more negatively than error in other region most of the offline metric we studied including auc and rig however are insensitive to such model behavior we designed a new model evaluation paradigm that simulates the online behavior of predictive model for a set of ad selected by a new prediction model the online user behavior is estimated from the historic user behavior in the search log the experimental result on click prediction model for search advertising are highly promising 
clipping web page namely extracting the informative clip area from web page ha many application such a web printing and e reading on small handheld device although many existing method attempt to address this task most of them can either work only on certain type of web page e g newsand blog like web page or perform semi automatically where extra user effort are required in adjusting the output the problem of clipping any type of web page accurately in a totally automatic way remains pretty much open to this end in this study we harness the wisdom of the crowd to provide accurate recommendation of informative clip on any given web page specifically we leverage the knowledge on how previous user clip similar web page and this knowledge repository can be represented a a transaction database where each transaction contains the clip selected by a user on a certain web page then we formulate a new pattern mining problem mining top qualified pattern on transaction database for this recommendation here the recommendation considers not only the pattern support but also the pattern occupancy proposed in this work high support requires that pattern appear frequently in the database while high occupancy requires that pattern occupy a large portion of the transaction they appear in thus it lead to both precise and complete recommendation additionally we explore the property on occupancy to further prune the search space for high efficient pattern mining finally we show the effectiveness of the proposed algorithm on a human labeled ground truth dataset consisting of web page from major web site and demonstrate it efficiency on large synthetic datasets 
this paper introduces a new method for estimating the local neighborhood and scale of data point to improve the robustness of spectral clustering algorithm we employ a subset of empty region graph the skeleton and non linear diffusion to define a locally adapted affinity matrix which a we demonstrate provides higher quality clustering than conventional approach based on nearest neighbor or global scale parameter moreover we show that the clustering quality is far le sensitive to the choice of and other algorithm parameter and to transformation such a geometric distortion and random perturbation we summarize the result of an empirical study that applies our method to a number of d synthetic data set consisting of cluster of arbitrary shape and scale and to real multi dimensional classification example from benchmark including image segmentation 
entity synonym are critical for many application like information retrieval and named entity recognition in document the current trend is to automatically discover entity synonym using statistical technique on web data prior technique suffer from several limitation like click log sparsity and inability to distinguish between entity of different concept class in this paper we propose a general framework for robustly discovering entity synonym with two novel similarity function that overcome the limitation of prior technique we develop efficient and scalable technique leveraging the mapreduce framework to discover synonym at large scale to handle long entity name with extraneous token we propose technique to effectively map long entity name to short query in query log our experiment on real data from different entity domain demonstrate the superior quality of our synonym a well a the efficiency of our algorithm the entity synonym produced by our system is in production in bing shopping and video search with experiment showing the significance it brings in improving search experience 
in multivariate analysis rank minimization emerges when a low rank structure of matrix is desired a well a a small estimation error rank minimization is nonconvex and generally np hard imposing one major challenge in this paper we consider a nonconvex least square formulation which seek to minimize the least square loss function with the rank constraint computationally we develop efficient algorithm to compute a global solution a well a an entire regularization solution path theoretically we show that our method reconstructs the oracle estimator exactly from noisy data a a result it recovers the true rank optimally against any method and lead to sharper parameter estimation over it counterpart finally the utility of the proposed method is demonstrated by simulation and image reconstruction from noisy background 
the challenge of predicting retail sale on a product by product basis throughout a network of retail store ha been researched intensively by applied econometrician and statistician for decade the principal tool of analysis have been linear regression with bayesian inspired adjustment to stabilize demand curve estimate the scale of such analytics can be challenging a retailer often work with more than product skus and typically operate network of hundred of brick and mortar store department and grocery store are excellent example but fast food restaurant also require such detailed predictive modeling system depending on the objective of the company prediction may be required for block of time spanning a week or more or a in the case of fast food operator prediction are required for each minute time interval of the operating day the author have modernized industry standard approach to such predictive modeling by leveraging advanced data mining technique these technique are more adept in detecting nonlinear response and accommodating interaction and automatically sifting through hundred if not thousand of potential factor influencing sale outcome result show that conventional statistical model miss a substantial fraction of the explainable variance while the new method dominate in term of performance and speed of model development accurate prediction is required for reliable planning and logistics and optimization optimization with respect to pricing promotion and assortment can be asked for relative to a variety of objective e g revenue profit and short term and long term optimization may result in different decision being taken a unique challenge for retailer is the large number of constraint to which complex retail organization are subject contract and special understanding with valued supplier severely constrain a retailer s flexibility for example certain product may not be promotable or discounted in isolation and others say from competitor may not be promoted jointly and the cost of good sold may well depend on the quantity contracted we discus how we have resolved such challenge via a cycle of prediction and simulation to develop a flexible high speed system for handling arbitrary constraint arbitrary objective and achieve new level of predictive accuracy and reliability 
this paper present a fast approximate similarity search method for finding the most similar object to a given query object from an object set with a dissimilarity with a success probability exceeding a given value a a search index the proposed method utilizes a degree reduced k nearest neighbor k dr graph constructed from the object set with the dissimilarity and explores the k dr graph along it edge using a greedy search g algorithm starting from multiple initial vertex with parallel processing in the graph construction stage the structural parameter k of the k dr graph is determined so that the probability with which at least one search trial of those with multiple initial vertex succeeds is more than the given success probability to estimate the greedy search success probability we introduce the concept of a basin in the k dr graph the experimental result on a real data set verify the approximation scheme and high search performance of the proposed method and demonstrate that it is superior to e lsh in term of the expected search cost 
learning of the information diffusion model is a fundamental problem in the study of information diffusion in social network existing approach learn the diffusion model from event in social network however event in social network may have different underlying reason some of them may be caused by the social influence inside the network while others may reflect external trend in the real world most existing work on the learning of diffusion model doe not distinguish the event caused by the social influence from those caused by external trend in this paper we extract social event from data stream in social network and then use the extracted social event to improve the learning of information diffusion model we propose a ladp latent action diffusion path model to incorporate the information diffusion model with the model of external trend and then design an em based algorithm to infer the diffusion probability the external trend and the source of event efficiently 
by helping member to connect discover and share relevant content or find a new career opportunity recommender system have become a critical component of user growth and engagement for social network the multidimensional nature of engagement and diversity of member on large scale social network have generated new infrastructure and modeling challenge and opportunity in the development deployment and operation of recommender system this presentation will address some of these issue focusing on the modeling side for which new research is much needed while describing a recommendation platform that enables real time recommendation update at scale a well a batch computation and cross leverage between different product recommendation topic covered on the modeling side will include optimizing for multiple competing objective solving contradicting business goal modeling user intent and interest to maximize placement and timeliness of the recommendation utility metric beyond ctr that leverage both real time tracking of explicit and implicit user feedback gathering training data for new product recommendation virality preserving online testing and virtual profiling 
the firehose of data generated by user on social networking and microblogging site such a facebook and twitter is enormous real time analytics on such data is challenging with most current effort largely focusing on the efficient querying and retrieval of data produced recently in this paper we present a dynamic pattern driven approach to summarize data produced by twitter feed we develop a novel approach to maintain an in memory summary while retaining sufficient information to facilitate a range of user specific and topic specific temporal analytics we empirically compare our approach with several state of the art pattern summarization approach along the ax of storage cost query accuracy query flexibility and efficiency using real data from twitter we find that the proposed approach is not only scalable but also outperforms existing approach by a large margin 
social recommendation which aim to systematically leverage the social relationship between user a well a their past behavior for automatic recommendation attract much attention recently the belief is that user linked with each other in social network tend to share certain common interest or have similar taste homophily principle such similarity is expected to help improve the recommendation accuracy and quality there have been a few study on social recommendation however they almost completely ignored the heterogeneity and diversity of the social relationship in this paper we develop a joint personal and social latent factor pslf model for social recommendation specifically it combine the state of the art collaborative filtering and the social network modeling approach for social recommendation especially the pslf extract the social factor vector for each user based on the state of the art mixture membership stochastic blockmodel which can explicitly express the variety of the social relationship to optimize the pslf model we develop a scalable expectation maximization em algorithm which utilizes a novel approximate mean field technique for fast expectation computation we compare our approach with the latest social recommendation approach on two real datasets flixter and douban both with large social network with similar training cost our approach ha shown a significant improvement in term of prediction accuracy criterion over the existing approach 
most algorithm work in data mining focus on designing algorithm to address a learning problem here we focus our attention on designing algorithm to determine the ease or difficulty of a problem instance the area of clustering under constraint ha recently received much attention in the data mining community we can view the constraint a restricting either directly or indirectly the search space of a clustering algorithm to just feasible clustering however to our knowledge no work explores method to count the feasible clustering or other measure of difficulty nor the importance of these measure we present two approach to efficiently characterize the difficulty of satisfying must link ml and cannot link cl constraint calculating the fractional chromatic polynomial of the constraint graph using lp and approximately counting the number of feasible clustering using mcmc sampler we show that these measure are correlated to the classical performance measure of constrained clustering algorithm from these insight and our algorithm we construct new method of generating and pruning constraint and empirically demonstrate their usefulness 
time lag is a key feature of hidden temporal dependency within sequential data in many real world application time lag play an essential role in interpreting the cause of discovered temporal dependency traditional temporal mining method either use a predefined time window to analyze the item sequence or employ statistical technique to simply derive the time dependency among item such paradigm cannot effectively handle varied data with special property e g the interleaved temporal dependency in this paper we study the problem of finding lag interval for temporal dependency analysis we first investigate the correlation between the temporal dependency and other temporal pattern and then propose a generalized framework to resolve the problem by utilizing the sorted table in representing time lag among item the proposed algorithm achieves an elegant balance between the time cost and the space cost extensive empirical evaluation on both synthetic and real data set demonstrates the efficiency and effectiveness of our proposed algorithm in finding the temporal dependency with lag interval in sequential data 
causal graphical model are developed to detect the dependence relationship between random variable and provide intuitive explanation for the relationship in complex system most of existing work focus on learning a single graphical model for all the variable however a single graphical model cannot accurately characterize the complicated causal relationship for a relatively large graph in this paper we propose the problem of estimating an overlapping decomposition for gaussian graphical model of a large scale to generate overlapping sub graphical model specifically we formulate an objective function for the overlapping decomposition problem and propose an approximate algorithm for it a key theory of the algorithm is that the problem of solving a node graphical model can be reduced to the problem of solving a one step regularization based on a solved node graphical model based on this theory a greedy expansion algorithm is proposed to generate the overlapping subgraphs we evaluate the effectiveness of our model on both synthetic datasets and real traffic dataset and the experimental result show the superiority of our method 
tweet are the most up to date and inclusive stream of information and commentary on current event but they are also fragmented and noisy motivating the need for system that can extract aggregate and categorize important event previous work on extracting structured representation of event ha focused largely on newswire text twitter s unique characteristic present new challenge and opportunity for open domain event extraction this paper describes twical the first open domain event extraction and categorization system for twitter we demonstrate that accurately extracting an open domain calendar of significant event from twitter is indeed feasible in addition we present a novel approach for discovering important event category and classifying extracted event based on latent variable model by leveraging large volume of unlabeled data our approach achieves a increase in maximum f over a supervised baseline a continuously updating demonstration of our system can be viewed at http statuscalendar com our nlp tool are available at http github com aritter twitter nlp 
in many data mining exercise we see information that appears on the surface to demonstrate a particular conclusion but closer examination of the data reveals that these result are indeed misleading in this session we will examine this notion of misleading result in three area statistical issue statistical issue such a multicollinearity and outlier can impact result dramatically we will first outline how these statistical issue can provide misleading result at the same time we will demonstrate how the data mining practitioner overcomes these issue through data analysis approach that provide both more meaningful and non misleading result to the business community overstating of result from a business standpoint we will also look at result that appear to be too good to be true in other word there appears to be some overstating of result within a given data mining solution initially we will discus how to identify these situation secondly we will outline what cause this overstatement of result and detail our approach on how we would overcome this predicament overfitting another topic for discussion is overfitting of result this is particularly the case when building predictive model in this section of the seminar we will define what overfitting is and why it is becoming more relevant for understanding by the business community once again analytical approach will be discussed in term of how to best handle this issue we present two case study that demonstrate how our principled step approach can be used to solve challenging data mining problem these step are a follows how to identify the problem how we construct the right data environment to conduct our analytics what kind of analytics are employed which include technique such a correlation analysis eda report logistic regression and gain chart more importantly we discus how to interpret the output in term of the actual impact to the business i e increased response rate and ultimately increased roi how do we apply the learning to a future initiative and what were the actual result 
this paper focus on the problem of clustering data from a em hidden or a deep web data source a key characteristic of deep web data source is that data can only be accessed through the limited query interface they support because the underlying data set cannot be accessed directly data mining must be performed based on sampling of the datasets the sample in turn can only be obtained by querying the deep web database with specific input we have developed a new stratified clustering method addressing this problem for a deep web data source specifically we have developed a stratified k mean clustering method in our approach the space of input attribute of a deep web data source is stratified for capturing the relationship between the input and the output attribute the space of output attribute of a deep web data source is partitioned into sub space three representative sampling method are developed in this paper with the goal of achieving a good estimation of the statistic including proportion and center within the sub space of the output attribute we have evaluated our method using two synthetic and two real datasets our comparison show significant gain in estimation accuracy from both the novel aspect of our work i e the use of stratification and our and representative sampling method up to 
in the last year deep learning ha gone from being a special purpose machine learning technique used mainly for image and speech recognition to becoming a general purpose machine learning tool this ha broad implication for all organization that rely on data analysis it represents the latest development in a general trend towards more automated algorithm and away from domain specific knowledge for organization that rely on domain expertise for their competitive advantage this trend could be extremely disruptive for start ups interested in entering established market this trend could be a major opportunity this talk will be a non technical introduction to general purpose deep learning and it potential business impact 
heterogeneous information network are pervasive in application ranging from bioinformatics to e commerce a a result unsupervised learning and clustering method pertaining to such network have gained significant attention recently node in a heterogeneous information network are regarded a object derived from distinct domain such a author and paper in many case feature set characterizing the object are not available hence clustering of the object depends solely on the link and relationship amongst object although several previous study have addressed information network clustering shortcoming remain first the definition of what constitutes an information network cluster varies drastically from study to study second previous algorithm have generally focused on non overlapping cluster while many algorithm are also limited to specific network topology in this paper we introduce a game theoretic framework ghin for defining and mining cluster in heterogeneous information network the clustering problem is modeled a a game wherein each domain represents a player and cluster are defined a the nash equilibrium point of the game adopting the abstraction of nash equilibrium point a cluster allows for flexible definition of reward function that characterize cluster without any modification to the underlying algorithm we prove that well established definition of cluster in domain information network such a formal concept maximal bi clique and noisy binary tile can always be represented a nash equilibrium point moreover experimental result employing a variety of reward function and several real world information network illustrate that the ghin framework produce more accurate and informative cluster than the recently proposed netclus and state of the art mdc algorithm 
data generated by observing the action of web browser across the internet is being used at an ever increasing rate for both building model and making decision in fact a quarter of the industry track paper for kdd in were based on data generated by online action the model analytics and decision they inform all stem from the assumption that observed data capture the intent of user however a large portion of these observed action are not intentional and are effectively polluting the model much of this observed activity is either generated by robot traversing the internet or the result of unintended action of real user these non intentional action observed in the web log severely bias both analytics and the model created from the data in this paper we will show example of how non intentional traffic that is produced by fraudulent activity adversely affect both general analytics and predictive model and propose an approach using co visitation network to identify site that have large amount of this fraudulent traffic we will then show how this approach along with a second stage classifier that identifies non intentional traffic at the browser level is deployed in production at medium degree m d a targeting technology company for display advertising this deployed product act both to filter out the fraudulent traffic from the input data and to insure that we don t serve ad during unintended website visit 
a cyber physical system cps integrates physical i e sensor device with cyber i e informational component to form a context sensitive system that responds intelligently to dynamic change in real world situation the cps ha wide application in scenario such a environment monitoring battlefield surveillance and traffic control one key research problem of cps is called mining line in the sand with a large number of sensor sand deployed in a designated area the cps is required to discover all the trajectory line of passing intruder in real time there are two crucial challenge that need to be addressed the collected sensor data are not trustworthy the intruder do not send out any identification information the system need to distinguish multiple intruder and track their movement in this study we propose a method called lism line in the sand miner to discover trajectory from untrustworthy sensor data lism construct a watching network from sensor data and computes the location of intruder appearance based on the link information of the network the system retrieves a cone model from the historical trajectory and track multiple intruder based on this model finally the system validates the mining result and update the sensor s reliability in a feedback process extensive experiment on big datasets demonstrate the feasibility and applicability of the proposed method 
the history of humankind is intimately connected to insect insect borne disease kill a million people and destroy ten of billion of dollar worth of crop annually however at the same time beneficial insect pollinate the majority of crop specie and it ha been estimated that approximately one third of all food consumed by human is directly pollinated by bee alone given the importance of insect in human affair it is somewhat surprising that computer science ha not had a larger impact in entomology we believe that recent advance in sensor technology are beginning change this and a new field of computational entomology will emerge we will demonstrate an inexpensive sensor that allows u to capture data from flying insect and the software that allows u to analyze the data moreover we will distribute both the sensor and software for free to party willing to take part in a crowdsourcing project on insect classification 
web facing company including amazon ebay etsy facebook google groupon intuit linkedin microsoft netflix shop direct stumbleupon yahoo and zynga use online controlled experiment to guide product development and accelerate innovation at microsoft s bing the use of controlled experiment ha grown exponentially over time with over concurrent experiment now running on any given day running experiment at large scale requires addressing multiple challenge in three area cultural organizational engineering and trustworthiness on the cultural and organizational front the larger organization need to learn the reason for running controlled experiment and the tradeoff between controlled experiment and other method of evaluating idea we discus why negative experiment which degrade the user experience short term should be run given the learning value and long term benefit on the engineering side we architected a highly scalable system able to handle data at massive scale hundred of concurrent experiment each containing million of user classical testing and debugging technique no longer apply when there are billion of live variant of the site so alert are used to identify issue rather than relying on heavy up front testing on the trustworthiness front we have a high occurrence of false positive that we address and we alert experimenter to statistical interaction between experiment the bing experimentation system is credited with having accelerated innovation and increased annual revenue by hundred of million of dollar by allowing u to find and focus on key idea evaluated through thousand of controlled experiment a improvement to revenue equal more than m annually in the u yet many idea impact key metric by and are not well estimated a priori the system ha also identified many negative feature that we avoided deploying despite key stakeholder early excitement saving u similar large amount 
a the public transport infrastructure of large city expands transport operator are diversifying the range and price of ticket that can be purchased for travel however selecting the best fare for each individual traveller s need is a complex process that is left almost completely unaided by examining the relation between urban mobility and fare purchasing habit in large datasets from london england s public transport network we estimate that traveller in the city cumulatively spend per year up to approximately gbp million more than they need to a a result of purchasing the incorrect fare we propose to address these incorrect purchase by leveraging the huge volume of data that traveller create a they move about the city by providing to each of them personalised ticket recommendation based on their estimated future travel pattern in this work we explore the viability of building a fare recommendation system for public transport network by a formalising the problem a two separate prediction problem and b evaluating a number of algorithm that aim to match traveller to the best fare we find that applying data mining technique to public transport data ha the potential to provide traveller with substantial saving 
graph classification ha become an important and active research topic in the last decade current research on graph classification focus on mining discriminative subgraph feature under supervised setting the basic assumption is that a large number of labeled graph are available however labeling graph data is quite expensive and time consuming for many real world application in order to reduce the labeling cost for graph data we address the problem of how to select the most important graph to query for the label this problem is challenging and different from conventional active learning problem because there is no predefined feature vector moreover the subgraph enumeration problem is np hard the active sample selection problem and the feature selection problem are correlated for graph data before we can solve the active sample selection problem we need to find a set of optimal subgraph feature to address this challenge we demonstrate how one can simultaneously estimate the usefulness of a query graph and a set of subgraph feature the idea is to maximize the dependency between subgraph feature and graph label using an active learning framework we propose a branch and bound algorithm to search for the optimal query graph and optimal feature simultaneously empirical study on nine real world task demonstrate that the proposed method can obtain better accuracy on graph data than alternative approach 
in regression analysis of count a lack of simple and efficient algorithm for posterior computation ha made bayesian approach appear unattractive and thus underdeveloped we propose a lognormal and gamma mixed negative binomial nb regression model for count and present efficient closed form bayesian inference unlike conventional poisson model the proposed approach ha two free parameter to include two different kind of random effect and allows the incorporation of prior information such a sparsity in the regression coefficient by placing a gamma distribution prior on the nb dispersion parameter r and connecting a lognormal distribution prior with the logit of the nb probability parameter p efficient gibbs sampling and variational bayes inference are both developed the closed form update are obtained by exploiting conditional conjugacy via both a compound poisson representation and a polya gamma distribution based data augmentation approach the proposed bayesian inference can be implemented routinely while being easily generalizable to more complex setting involving multivariate dependence structure the algorithm are illustrated using real example 
recent study have shown that alzheimer s disease ad is related to alteration in brain connectivity network one type of connectivity called effective connectivity defined a the directional relationship between brain region is essential to brain function however there have been few study on modeling the effective connectivity of ad and characterizing it difference from normal control nc in this paper we investigate the sparse bayesian network bn for effective connectivity modeling specifically we propose a novel formulation for the structure learning of bns which involves one l norm penalty term to impose sparsity and another penalty to ensure the learned bn to be a directed acyclic graph a required property of bns we show through both theoretical analysis and extensive experiment on eleven moderate and large benchmark network with various sample size that the proposed method ha much improved learning accuracy and scalability compared with ten competing algorithm we apply the proposed method to fdg pet image of ad and nc subject and identify the effective connectivity model for ad and nc respectively our study reveals that the effective connectivity of ad is different from that of nc in many way including the global scale effective connectivity intra lobe inter lobe and inter hemispheric effective connectivity distribution a well a the effective connectivity associated with specific brain region these finding are consistent with known pathology and clinical progression of ad and will contribute to ad knowledge discovery 
alzheimer s disease ad is associated with progressive cognitive decline leading to dementia the atrophy loss of brain structure a seen on magnetic resonance imaging mri is strongly correlated with the severity of the cognitive impairment in ad in this paper we set out to find association between predefined region of the brain region of interest roi and the severity of the disease specifically we use these association to address two important issue in ad i typical versus atypical atrophy pattern and ii the origin and direction of progression of atrophy which is currently under debate we observed that each ad related roi is associated with a wide range of severity and that the difference between roi is merely a difference in severity distribution to model difference between the severity distribution of a subpopulation with significant atrophy in certain roi and the severity distribution of the entire population we developed the concept of distributional association rule using the distributional association rule we clustered roi into disease subsystem we define a disease subsystem a a contiguous set of roi that are collectively implicated in ad ad is known to be heterogeneous in the sense that multiple set of roi may be related to the disease in a population we proposed an enhancement to the association rule mining where the algorithm only discovers association rule with roi that form an approximately contiguous volume next we applied these association rule to infer the direction of disease progression based on the support measure of the association rule we also developed a novel statistical test to determine the statistical significance of the discovered direction we evaluated the proposed method on the mayo clinic alzheimer s disease research center adrc prospective patient cohort the key achievement of the methodology is that it accurately identified larger disease subsystem implicated in typical and atypical ad and it successfully mapped the direction of disease progression the wealth of data available in radiology give rise to opportunity for applying this methodology to map out the trajectory of several other disease e g other neuro degenerative disease and cancer most notably breast cancer the applicability of this method is not limited to image data a associating predictor with severity provides valuable information in most area of medicine a well a other industry 
in the era of big data it is increasingly difficult for an analyst to extract meaningful knowledge from a sea of information we present tweetxplorer a system for analyst with little information about an event to gain knowledge through the use of effective visualization technique using tweet collected during hurricane sandy a an example we will lead the reader through a workflow that exhibit the functionality of the system 
unlabeled sample can be intelligently selected for labeling to minimize classification error in many real world application a large number of unlabeled sample arrive in a streaming manner making it impossible to maintain all the data in a candidate pool in this work we focus on binary classification problem and study selective labeling in data stream where a decision is required on each sample sequentially we consider the unbiasedness property in the sampling process and design optimal instrumental distribution to minimize the variance in the stochastic process meanwhile bayesian linear classifier with weighted maximum likelihood are optimized online to estimate parameter in empirical evaluation we collect a data stream of user generated comment on a commercial news portal in consecutive day and carry out offline evaluation to compare various sampling strategy including unbiased active learning biased variant and random sampling experimental result verify the usefulness of online active learning especially in the non stationary situation with concept drift 
classification of time series data is an important problem with application in virtually every scientific endeavor the large research community working on time series classification ha typically used the ucr archive to test their algorithm in this work we argue that the availability of this resource ha isolated much of the research community from the following reality labeled time series data is often very difficult to obtain the obvious solution to this problem is the application of semi supervised learning however a we shall show direct application of off the shelf semi supervised learning algorithm do not typically work well for time series in this work we explain why semi supervised learning algorithm typically fail for time series problem and we introduce a simple but very effective fix we demonstrate our idea on diverse real word problem 
list data is an important source of structured data on the web this paper is concerned with top k page which are web page that describe a list of k instance of a particular topic or concept example include the tallest person in the world and the hit of you don t want to miss compared to normal web list data top k list contain richer information and are easier to understand therefore the extraction of such list can help enrich existing knowledge base about general concept or act a a preprocessing step to produce fact for a fact answering engine we present an efficient system that extract the target list from web page with high accuracy we have used the system to process up to million or of a high frequency web snapshot from bing and obtained over list with precision 
real world multiple typed object are often interconnected forming heterogeneous information network a major challenge for link based clustering in such network is it potential to generate many different result carrying rather diverse semantic meaning in order to generate desired clustering we propose to use meta path a path that connects object type via a sequence of relation to control clustering with distinct semantics nevertheless it is easier for a user to provide a few example seed than a weighted combination of sophisticated meta path to specify her clustering preference thus we propose to integrate meta path selection with user guided clustering to cluster object in network where a user first provides a small set of object seed for each cluster a guidance then the system learns the weight for each meta path that are consistent with the clustering result implied by the guidance and generates cluster under the learned weight of meta path a probabilistic approach is proposed to solve the problem and an effective and efficient iterative algorithm pathselclus is proposed to learn the model where the clustering quality and the meta path weight are mutually enhancing each other our experiment with several clustering task in two real network demonstrate the power of the algorithm in comparison with the baseline 
today s popular web search engine expand the search process beyond crawled web page to specialized corpus vertical like image video news local sport finance shopping etc each with it own specialized search engine search federation deal with problem of the selection of search engine to query and merging of their result into a single result set despite a few recent advance the problem is still very challenging first due to the heterogeneous nature of different vertical how the system merges the vertical result with the web document to serve the user s information need is still an open problem moreover the scale of the search engine and the increasing number of vertical property requires a solution which is efficient and scaleable in this paper we propose a unified framework for the search federation problem we model the search federation a a contextual bandit problem the system us reward a a proxy for user satisfaction given a query our system predicts the expected reward for each vertical then organizes the search result page serp in a way which maximizes the total reward instead of relying on human judge our system leverage implicit user feedback to learn the model the method is efficient to implement and can be applied to vertical of different nature we have successfully deployed the system to three different market and it handle multiple vertical in each market the system is now serving hundred of million of query live each day and ha improved user metric considerably 
professional sport is a roughly billion dollar industry that is increasingly data driven in this paper we show how machine learning can be applied to generate a model that could lead to better on field decision by manager of professional baseball team specifically we show how to use regularized linear regression to learn pitcher specific predictive model that can be used to help decide when a starting pitcher should be replaced a key step in the process is our method of converting categorical variable e g the venue in which a game is played into continuous variable suitable for the regression another key step is dealing with situation in which there is an insufficient amount of data to compute measure such a the effectiveness of a pitcher against specific batter for each season we trained on the first of the game and tested on the rest the result suggest that using our model could have led to better decision than those made by major league manager applying our model would have led to a different decision of the time for those game in which a manager left a pitcher in that our model would have removed the pitcher ended up performing poorly of the time 
blockmodelling is an important technique for decomposing graph into set of role vertex playing the same role have similar pattern of interaction with vertex in other role these role along with the role to role interaction can succinctly summarise the underlying structure of the studied graph a the underlying graph evolve with time it is important to study how their blockmodels evolve too this will enable u to detect role change across time detect different pattern of interaction for example weekday and weekend behaviour and allow u to study how the structure in the underlying dynamic graph evolves to date there ha been limited research on studying dynamic blockmodels they focus on smoothing role change between adjacent time instance however this approach can overfit during stationary period where the underling structure doe not change but there is random noise in the graph therefore an approach to a find blockmodels across span of time and b to find the stationary period is needed in this paper we propose an information theoretic framework seqibloc combined with a change point detection approach to achieve a and b in addition we propose new vertex equivalence definition that include time and show how they relate back to our information theoretic approach we demonstrate their usefulness and superior accuracy over existing work on synthetic and real datasets 
accurate prediction of user behavior is important for many social medium application including social marketing personalization and recommendation etc a major challenge lie in that the available behavior data or interaction between user and item in a given social network are usually very limited and sparse e g empty many previous work model user behavior from only historical user log we observe that many people are member of several social network in the same time such a facebook twitter and tencent s qq importantly their behavior and interest in different network influence one another this give u an opportunity to leverage the knowledge of user behavior in different network in order to alleviate the data sparsity problem and enhance the predictive performance of user modeling combining different network simply and naively doe not work well instead we formulate the problem to model multiple network a composite network knowledge transfer we first select the most suitable network inside a composite social network via a hierarchical bayesian model parameterized for individual user and then build topic model for user behavior prediction using both the relationship in the selected network and related behavior data to handle big data we have implemented the algorithm using map reduce we demonstrate that the proposed composite network based user behavior model significantly improve the predictive accuracy over a number of existing approach on several real world application such a a very large social networking dataset from tencent inc 
social medium website are currently central hub on the internet major online social medium platform are not only place for individual user to socialize but are increasingly more important a channel for company to advertise public figure to engage etc in order to optimize such advertising and engaging effort there is an emerging challenge for knowledge discovery on today s internet the goal of knowledge discovery is to understand the entire online social landscape instead of merely summarizing the statistic to answer this challenge we have created voxsup a a unified social engagement framework unlike most existing tool voxsup not only aggregate and filter social data from the internet but also provides what we call voxsupian knowledge discovery vkd vkd consists of an almost human level understanding of social conversation at any level of granularity from a single comment sentiment to multi lingual inter platform user demographic here we describe the technology that are crucial to vkd and subsequently go beyond experimental verification and present case study from our live voxsup system 
the problem of time series classification tsc where we consider any real valued ordered data a time series present a specific machine learning challenge a the ordering of variable is often crucial in finding the best discriminating feature one of the most promising recent approach is to find shapelets within a data set a shapelet is a time series subsequence that is identified a being representative of class membership the original research in this field embedded the procedure of finding shapelets within a decision tree we propose disconnecting the process of finding shapelets from the classification algorithm by proposing a shapelet transformation we describe a mean of extracting the k best shapelets from a data set in a single pas and then use these shapelets to transform data by calculating the distance from a series to each shapelet we demonstrate that transformation into this new data space can improve classification accuracy whilst retaining the explanatory power provided by shapelets 
in recent year social medium site have provided a large amount of information recipient of such information need mechanism to know more about the received information including the provenance previous research ha shown that some attribute related to the received information provide additional context so that a recipient can ass the amount of value trust and validity to be placed in the received information personal attribute of a user including name location education ethnicity gender and political and religious affiliation can be found in social medium site in this paper we present a novel web based tool for collecting the attribute of interest associated with a particular social medium user related to the received information this tool provides a way to combine different attribute available at different social medium site into a single user profile using different type of twitter user we also evaluate the performance of the tool in term of number of attribute value collected validity of these value and total amount of retrieval time 
in many application such a image and video processing the data matrix often posse simultaneously a low rank structure capturing the global information and a sparse component capturing the local information how to accurately extract the low rank and sparse component is a major challenge robust principal component analysis rpca is a general framework to extract such structure it is well studied that under certain assumption convex optimization using the trace norm and l norm can be an effective computation surrogate of the difficult rpca problem however such convex formulation is based on a strong assumption which may not hold in real world application and the approximation error in these convex relaxation often cannot be neglected in this paper we present a novel non convex formulation for the rpca problem using the capped trace norm and the capped l norm in addition we present two algorithm to solve the non convex optimization one is based on the difference of convex function dc framework and the other attempt to solve the sub problem via a greedy approach our empirical evaluation on synthetic and real world data show that both of the proposed algorithm achieve higher accuracy than existing convex formulation furthermore between the two proposed algorithm the greedy algorithm is more efficient than the dc programming while they achieve comparable accuracy 
exceptional model mining emm is an exploratory data analysis technique that can be regarded a a generalization of subgroup discovery in emm we look for subgroup of the data for which a model fitted to the subgroup differs substantially from the same model fitted to the entire dataset in this paper we develop method to mine for exceptional regression model we propose a measure for the exceptionality of regression model cook s distance and explore the possibility to avoid having to fit the regression model to each candidate subgroup the algorithm is evaluated on a number of real life datasets these datasets are also used to illustrate the result of the algorithm we find interesting subgroup with deviating model on datasets from several different domain we also show that under certain circumstance one can forego fitting regression model on up to of the subgroup and these are the relatively expensive regression model to compute 
the recent explosion in the adoption of search engine and new medium such a blog and twitter have facilitated faster propagation of news and rumor how quickly doe a piece of news spread over these medium how doe it popularity diminish over time doe the rising and falling pattern follow a simple universal law in this paper we propose spikem a concise yet flexible analytical model for the rise and fall pattern of influence propagation our model ha the following advantage a unification power it generalizes and explains earlier theoretical model and empirical observation b practicality it match the observed behavior of diverse set of real data c parsimony it requires only a handful of parameter and d usefulness it enables further analytics task such a forecasting spotting anomaly and interpretation by reverseengineering the system parameter of interest e g quality of news count of interested blogger etc using spikem we analyzed gb of real data most of which were collected from the public domain we have shown that our spikem model accurately and succinctly describes all the pattern of the rise and fall spike in these real datasets 
real world physical and abstract data object are interconnected forming gigantic interconnected network by structuring these data object into multiple type such network become semi structured heterogeneous information network most real world application that handle big data including interconnected social medium and social network scientific engineering or medical information system online e commerce system and most database system can be structured into heterogeneous information network for example in a medical care network object of multiple type such a patient doctor disease medication and link such a visit diagnosis and treatment are intertwined together providing rich information and forming heterogeneous information network effective analysis of large scale heterogeneous information network pose an interesting but critical challenge in this talk we present a set of data mining scenario in heterogeneous information network and show that mining heterogeneous information network is a new and promising research frontier in data mining research departing from many existing network model that view data a homogeneous graph or network the semi structured heterogeneous information network model leverage the rich semantics of typed node and link in a network and can uncover surprisingly rich knowledge from interconnected data this heterogeneous network modeling will lead to the discovery of a set of new principle and methodology for mining interconnected data the example to be used in this discussion include meta path based similarity search rank based clustering rank based classification meta path based link relationship prediction relation strength aware mining a well a a few other recent development we will also point out some promising research direction and provide convincing argument on that mining heterogeneous information network is the next frontier in data mining 
ranking is one of the key problem in information retrieval recently there ha been significant interest in a class of ranking algorithm based on the assumption that data is sampled from a low dimensional manifold embedded in a higher dimensional euclidean space in this paper we study a popular graph laplacian based ranking algorithm using an analytical method which provides theoretical insight into the ranking algorithm going beyond the intuitive idea of diffusion our analysis show that the algorithm is sensitive to a commonly used parameter due to the use of symmetric normalized graph laplacian we also show that the ranking function may diverge to infinity at the query point in the limit of infinite sample to address these issue we propose an improved ranking algorithm on manifold using green s function of an iterated unnormalized graph laplacian which is more robust and density adaptive a well a pointwise continuous in the limit of infinite sample we also for the first time in the ranking literature empirically explore two variant from a family of twice normalized graph laplacians experimental result on text and image data support our analysis which also suggest the potential value of twice normalized graph laplacians in practice 
optimization tool are vital to data analysis and learning the optimization perspective ha provided valuable insight and optimization formulation have led to practical algorithm with good theoretical property in turn the rich collection of problem in learning and data analysis is providing fresh perspective on optimization algorithm and is driving new fundamental research in the area we discus research on several area in this domain including signal reconstruction manifold learning and regression classification describing in each case recent research in which optimization algorithm have been developed and applied successfully a particular focus is asynchronous parallel algorithm for optimization and linear algebra and their application in data analysis and learning 
group play an essential role in many social website which promote user interaction and accelerate the diffusion of information recommending group that user are really interested to join is significant for both user and social medium while traditional group recommendation problem ha been extensively studied we focus on a new type of the problem i e event based group recommendation unlike the other form of group user join this type of group mainly for participating offline event organized by group member or inviting other user to attend event sponsored by them these characteristic determine that previously proposed approach for group recommendation cannot be adapted to the new problem easily a they ignore the geographical influence and other explicit feature of group and user in this paper we propose a method called pairwise tag enhanced and feature based matrix factorization for group recommendation ptarmigan which considers location feature social feature and implicit pattern simultaneously in a unified model more specifically we exploit matrix factorization to model interaction between user and group meanwhile we incorporate their profile information into pairwise enhanced latent factor respectively we also utilize the linear model to capture explicit feature due to the reinforcement between explicit feature and implicit pattern our approach can provide better group recommendation we conducted a comprehensive performance evaluation on real word data set and the experimental result demonstrate the effectiveness of our method 
with the explosion of mobile device with camera online search ha moved beyond text to other modality like image voice and writing for many application like fashion image based search offer a compelling interface a compared to text form by better capturing the visual attribute in this paper we present a simple and fast search algorithm that us color a the main feature for building visual search we show that low level cue such a color can be used to quantify image similarity and also to discriminate among product with different visual appearance we demonstrate the effectiveness of our approach through a mobile shopping application footnote ebay fashion app available at http itunes apple com u app ebay fashion id mt and ebay image swatch is the feature indexing million of real world fashion image our approach outperforms several other state of the art image retrieval algorithm for large scale image data 
the necessity to analyze subspace projection of complex data is a well known fact in the clustering community while the full space may be obfuscated by overlapping pattern and irrelevant dimension only certain subspace are able to reveal the clustering structure subspace clustering discard irrelevant dimension and allows object to belong to multiple overlapping cluster due to individual subspace projection for each set of object a we will demonstrate the observation which originate the need to consider subspace projection for traditional clustering also apply for the task of correlation analysis in this work we introduce the novel paradigm of subspace correlation clustering we analyze subspace projection to find subset of object showing linear correlation among this subset of dimension in contrast to existing technique which determine correlation based on the full space our method is able to exclude locally irrelevant dimension enabling more precise detection of the correlated feature since we analyze subspace projection each object can contribute to several correlation our model allows multiple overlapping cluster in general but simultaneously avoids redundant cluster deducible from already known correlation we introduce the algorithm sscc that exploit different pruning technique to efficiently generate a subspace correlation clustering in thorough experiment we demonstrate the strength of our novel paradigm in comparison to existing method 
the advent of big data era drive data analyst from different domain to use data mining technique for data analysis however performing data analysis in a specific domain is not trivial it often requires complex task configuration onerous integration of algorithm and efficient execution in distributed environment few effort have been paid on developing effective tool to facilitate data analyst in conducting complex data analysis task in this paper we design and implement fiu miner a fast integrated and user friendly system to ease data analysis fiu miner allows user to rapidly configure a complex data analysis task without writing a single line of code it also help user conveniently import and integrate different analysis program further it significantly balance resource utilization and task execution in heterogeneous environment a case study of a real world application demonstrates the efficacy and effectiveness of our proposed system 
traditionally feature construction and feature selection are two important but separate process in data mining however many real world application require an integrated approach for creating refining and selecting feature to address this problem we propose feafiner short for feature refiner an efficient formulation that simultaneously generalizes low level feature into higher level concept and then selects relevant concept based on the target variable specifically we formulate a double sparsity optimization problem that identifies group in the low level feature generalizes higher level feature using the group and performs feature selection since in many clinical research nonoverlapping group are preferred for better interpretability we further improve the formulation to generalize feature using mutually exclusive feature group the proposed formulation is challenging to solve due to the orthogonality constraint non convexity objective and non smoothness penalty we apply a recently developed augmented lagrangian method to solve this formulation in which each subproblem is solved by a non monotone spectral projected gradient method our numerical experiment show that this approach is computationally efficient and also capable of producing solution of high quality we also present a generalization bound showing the consistency and the asymptotic behavior of the learning process of our proposed formulation finally the proposed feafiner method is validated on alzheimer s disease neuroimaging initiative dataset where low level biomarkers are automatically generalized into robust higher level concept which are then selected for predicting the disease status measured by mini mental state examination and alzheimer s disease assessment scale cognitive subscore compared to existing predictive modeling method feafiner provides intuitive and robust feature concept and competitive predictive accuracy 
in the vast majority of recent work on sparse estimation algorithm performance ha been evaluated using ideal or quasi ideal dictionary e g random gaussian or fourier characterized by unit norm incoherent column or feature but in reality these type of dictionary represent only a subset of the dictionary that are actually used in practice largely restricted to idealized compressive sensing application in contrast herein sparse estimation is considered in the context of structured dictionary possibly exhibiting high coherence between arbitrary group of column and or row sparse penalized regression model are analyzed with the purpose of finding to the extent possible regime of dictionary invariant performance in particular a type ii bayesian estimator with a dictionarydependent sparsity penalty is shown to have a number of desirable invariance property leading to provable advantage over more conventional penalty such a the norm especially in area where existing theoretical recovery guarantee no longer hold this can translate into improved performance in application such a model selection with correlated feature source localization and compressive sensing with constrained measurement direction 
today there is a strong interest in publishing set valued data in a privacy preserving manner such data associate individual to set of value e g preference shopping item symptom query log in addition an individual can be associated with a sensitive label e g marital status religious or political conviction anonymizing such data implies ensuring that an adversary should not be able to identify an individual s record and infer a sensitive label if such exists existing research on this problem either perturbs the data publishes them in disjoint group disassociated from their sensitive label or generalizes their value by assuming the availability of a generalization hierarchy in this paper we propose a novel alternative our publication method also put data in a generalized form but doe not require that published record form disjoint group and doe not assume a hierarchy either instead it employ generalized bitmap and recasts data value in a nonreciprocal manner formally the bipartite graph from original to anonymized record doe not have to be composed of disjoint complete subgraphs we configure our scheme to provide popular privacy guarantee while resisting attack proposed in recent research and demonstrate experimentally that we gain a clear utility advantage over the previous state of the art 
extracting knowledge by performing computation on graph is becoming increasingly challenging a graph grow in size a standard approach distributes the graph over a cluster of node but performing computation on a distributed graph is expensive if large amount of data have to be moved without partitioning the graph communication quickly becomes a limiting factor in scaling the system up existing graph partitioning heuristic incur high computation and communication cost on large graph sometimes a high a the future computation itself observing that the graph ha to be loaded into the cluster we ask if the partitioning can be done at the same time with a lightweight streaming algorithm we propose natural simple heuristic and compare their performance to hashing and metis a fast offline heuristic we show on a large collection of graph datasets that our heuristic are a significant improvement with the best obtaining an average gain of the heuristic are scalable in the size of the graph and the number of partition using our streaming partitioning method we are able to speed up pagerank computation on spark a distributed computation system by to for large social network 
latent topic analysis ha emerged a one of the most effective method for classifying clustering and retrieving textual data however existing model such a latent dirichlet allocation lda were developed for static corpus of relatively large document in contrast much of the textual content on the web and especially social medium is temporally sequenced and come in short fragment including microblog post on site such a twitter and weibo status update on social networking site such a facebook and linkedin or comment on content sharing site such a youtube in this paper we propose a novel topic model temporal lda or tm lda for efficiently mining text stream such a a sequence of post from the same author by modeling the topic transition that naturally arise in these data tm lda learns the transition parameter among topic by minimizing the prediction error on topic distribution in subsequent posting after training tm lda is thus able to accurately predict the expected topic distribution in future post to make these prediction more efficient for a realistic online setting we develop an efficient updating algorithm to adjust the topic transition parameter a new document stream in our empirical result over a corpus of over million microblog post show that tm lda significantly outperforms state of the art static lda model for estimating the topic distribution of new document over time we also demonstrate that tm lda is able to highlight interesting variation of common topic transition such a the difference in the work life rhythm of city and factor associated with area specific problem and complaint 
how to extract the truly relevant information from a large relational data set the answer of this paper is a technique integrating graph summarization graph clustering link prediction and the discovery of the hidden structure on the basis of data compression our novel algorithm scminer for summarization compression miner reduces a large bipartite input graph to a highly compact representation which is very useful for different data mining task clustering the compact summary graph contains the truly relevant cluster of both type of node of a bipartite graph link prediction the compression scheme of scminer reveals suspicious edge which are probably erroneous a well a missing edge i e pair of node which should be connected by an edge discovery of the hidden structure unlike traditional co clustering method the result of scminer is not limited to rowand column cluster besides the cluster the summary graph also contains the essential relationship between both type of cluster and thus reveals the hidden structure of the data extensive experiment on synthetic and real data demonstrate that scminer outperforms state of the art technique for clustering and link prediction moreover scminer discovers the hidden structure and report it in an interpretable way to the user based on data compression our technique doe not rely on any input parameter which are difficult to estimate 
there ha been an explosion in the amount of digital text information available in recent year leading to challenge of scale for traditional inference algorithm for topic model recent advance in stochastic variational inference algorithm for latent dirichlet allocation lda have made it feasible to learn topic model on very large scale corpus but these method do not currently take full advantage of the collapsed representation of the model we propose a stochastic algorithm for collapsed variational bayesian inference for lda which is simpler and more efficient than the state of the art method in experiment on large scale text corpus the algorithm wa found to converge faster and often to a better solution than previous method human subject experiment also demonstrated that the method can learn coherent topic in second on small corpus facilitating the use of topic model in interactive document analysis software 
a critical step in bridging the knowledge base with the huge corpus of semi structured web list data is to link the entity mention that appear in the web list with the corresponding real world entity in the knowledge base which we call list linking task this task can facilitate many different task such a knowledge base population entity search and table annotation however the list linking task is challenging because a web list ha almost no textual context and the only input for this task is a list of entity mention extracted from the web page in this paper we propose liege the first general framework to link the entity in web list with the knowledge base to the best of our knowledge our assumption is that entity mentioned in a web list can be any collection of entity that have the same conceptual type that people have in mind to annotate the list item in a web list with entity that they likely mention we leverage the prior probability of an entity being mentioned and the global coherence between the type of entity in the web list the interdependence between different entity assignment in a web list make the optimization of this list linking problem np hard accordingly we propose a practical solution based on the iterative substitution to jointly optimize the identification of the mapping entity for the web list item we extensively evaluated the performance of our proposed framework over both manually annotated real web list extracted from the web page and two public data set and the experimental result show that our framework significantly outperforms the baseline method in term of accuracy 
signal theft can be defined a the interdiction consumption or usage of carrier signal from a provider s network without payment or payment of an amount le than the level of service consumed high level of signal theft can potentially reflect open technical network issue failure of electronic countermeasure or operational gap that are estimated to cost the cable industry provider more than billion annually this session will discus the business challenge associated with the quantification of signal theft related loss outline some of the countermeasure taken by msos and then provide view on the development of predictive model to help identify the potential likelihood of signal theft in a given environment we will examine the performance of certain machine learning algorithm a well a data challenge associated with both the architecture construction and analytical effort and conclude with a lesson learned discussion and view on future approach 
online controlled experiment are often utilized to make data driven decision at amazon microsoft ebay facebook google yahoo zynga and at many other company while the theory of a controlled experiment is simple and date back to sir ronald a fisher s experiment at the rothamsted agricultural experimental station in england in the s the deployment and mining of online controlled experiment at scale thousand of experiment now ha taught u many lesson these exemplify the proverb that the difference between theory and practice is greater in practice than in theory we present our learning a they happened puzzling outcome of controlled experiment that we analyzed deeply to understand and explain each of these took multiple person week to month to properly analyze and get to the often surprising root cause the root cause behind these puzzling result are not isolated incident these issue generalized to multiple experiment the heightened awareness should help reader increase the trustworthiness of the result coming out of controlled experiment at microsoft s bing it is not uncommon to see experiment that impact annual revenue by million of dollar thus getting trustworthy result is critical and investing in understanding anomaly ha tremendous payoff reversing a single incorrect decision based on the result of an experiment can fund a whole team of analyst the topic we cover include the oec overall evaluation criterion click tracking effect trend experiment length and power and carryover effect 
twitter ha become an increasingly important source of information with more than million tweet posted per day the task to link the named entity mention detected from tweet with the corresponding real world entity in the knowledge base is called tweet entity linking this task is of practical importance and can facilitate many different task such a personalized recommendation and user interest discovery the tweet entity linking task is challenging due to the noisy short and informal nature of tweet previous method focus on linking entity in web document and largely rely on the context around the entity mention and the topical coherence between entity in the document however these method cannot be effectively applied to the tweet entity linking task due to the insufficient context information contained in a tweet in this paper we propose kauri a graph based framework to collectively link all the named entity mention in all tweet posted by a user via modeling the user s topic of interest our assumption is that each user ha an underlying topic interest distribution over various named entity kauri integrates the intra tweet local information with the inter tweet user interest information into a unified graph based framework we extensively evaluated the performance of kauri over manually annotated tweet corpus and the experimental result show that kauri significantly outperforms the baseline method in term of accuracy and kauri is efficient and scale well to tweet stream 
in multiple instance learning mil each entity is normally expressed a a set of instance most of the current mil method only deal with the case when each instance is represented by one type of feature however in many real world application entity are often described from several different information source view for example when applying mil to image categorization the characteristic of each image can be derived from both it rgb feature and sift feature previous research work ha shown that in traditional learning method leveraging the consistency between different information source could improve the classification performance drastically out of a similar motivation to incorporate the consistency between different information source into mil we propose a novel research framework multi instance learning from multiple information source mi l based on this framework an algorithm fast mi l fmi l is designed which combine concave convex constraint programming cccp method and an adapted stoachastic gradient descent sgd method some theoretical analysis on the optimality of the adapted sgd method and the generalized error bound of the formulation are given based on the proposed method experimental result on document classification and a novel application insider threat detection itd clearly demonstrate the superior performance of the proposed method over state of the art mil method 
exploring pubmed to find relevant information is challenging and time consuming because pubmed typically return a long list of article a a result of query semantic network help user to explore a large document collection and to capture key concept and relationship among the concept the semantic network also serf to broaden the user s knowledge and extend query keyword by detecting and visualizing new related concept or relation hidden in the retrieved document the problem of existing semantic network technique is that they typically produce many redundant relationship which prevents user from quickly capturing the underlying relationship among concept this paper develops an online pubmed search system which display semantic network having no redundant relationship in real time a a result of query to do so we propose an efficient semantic network construction algorithm which prevents producing redundant relationship during the network construction our extensive experiment on actual pubmed data show that the proposed method is significantly faster than the method removing redundant relationship afterward our method is implemented and integrated into a relevance feedback pubmed search engine called refmed http dm postech ac kr refmed and will be demonstrated through the website 
several data mining algorithm use iterative optimization method for learning predictive model it is not easy to determine upfront which optimization method will perform best or converge fast for such task in this paper we analyze meta algorithm ma which work by adaptively combining iterates from a pool of base optimization algorithm we show that the performance of ma are competitive with the best convex combination of the iterates from the base algorithm for online a well a batch convex optimization problem we illustrate the effectiveness of ma on the problem of portfolio selection in the stock market and use several existing idea for portfolio selection a base algorithm using daily s p data for the past year and a benchmark nyse dataset we show that ma outperform existing portfolio selection algorithm with provable guarantee by several order of magnitude and match the performance of the best heuristic in the pool 
the amount and variety of data being collected in the enterprise is growing at a staggering pace the default now is to capture and store any and all data in anticipation of potential future strategic value and vast amount of data are being generated by instrumenting key customer and system touch point until recently data wa gathered for well defined objective such a auditing forensics reporting and line of business operation now exploratory and predictive analysis is becoming ubiquitous these difference in data heterogeneity scale and usage are leading to a new generation of data management and analytic system where the emphasis is on supporting a wide range of large datasets to be stored uniformly and analyzed seamlessly using whatever technique are most appropriate including traditional tool like sql and bi and newer tool e g for machine learning these new system are necessarily based on scale out architecture for both storage and computation the term big data and data science are often used to refer to this class of system and application hadoop ha become a key building block in the new generation of scale out system early version of analytic tool over hadoop such a hive and pig for sql like query were implemented by translation into map reduce computation this approach ha inherent limitation and the emergence of resource manager such a yarn and mesos ha opened the door for newer analytic tool to bypass the map reduce layer this trend is especially significant for iterative computation such a graph analytics and machine learning for which map reduce is widely recognized to be a poor fit in fact the website of the machine learning toolkit apache mahout explicitly warns about the slow performance of some of the algorithm on hadoop in this talk i will examine this architectural trend and argue that resource manager are a first step in re factoring the early implementation of map reduce and that more work is needed if we wish to support a variety of analytic tool on a common scale out computational fabric i will then present reef which run on top of resource manager like yarn and provides support for task monitoring and restart data movement and communication and distributed state management finally i will illustrate the value of using reef to implement iterative algorithm for graph analytics and machine learning 
recovering a large matrix from a small subset of it entry is a challenging problem arising in many real world application such a recommender system and image in painting these problem can be formulated a a general matrix completion problem the singular value thresholding svt algorithm is a simple and efficient first order matrix completion method to recover the missing value when the original data matrix is of low rank svt ha been applied successfully in many application however svt is computationally expensive when the size of the data matrix is large which significantly limit it applicability in this paper we propose an accelerated singular value thresholding asvt algorithm which improves the convergence rate from o n for svt to o n where n is the number of iteration during optimization specifically the dual problem of the nuclear norm minimization problem is derived and an adaptive line search scheme is introduced to solve this dual problem consequently the optimal solution of the primary problem can be readily obtained from that of the dual problem we have conducted a series of experiment on a synthetic dataset a distance matrix dataset and a large movie rating dataset the experimental result have demonstrated the efficiency and effectiveness of the proposed algorithm 
yahoo answer is currently one of the most popular question answering system we claim however that it user experience could be significantly improved if it could route the right question to the right user indeed while some user would rush answering a question such a what should i wear at the prom others would be upset simply being exposed to it we argue here that community question answering site in general and yahoo answer in particular need a mechanism that would expose user to question they can relate to and possibly answer we propose here to address this need via a multi channel recommender system technology for associating question with potential answerer on yahoo answer one novel aspect of our approach is exploiting a wide variety of content and social signal user regularly provide to the system and organizing them into channel content signal relate mostly to the text and category of question and associated answer while social signal capture the various user interaction with question such a asking answering voting etc we fuse and generalize known recommendation approach within a single symmetric framework which incorporates and properly balance multiple type of signal according to channel tested on a large scale dataset our model exhibit good performance clearly outperforming standard baseline 
online review have been popularly adopted in many application since they can either promote or harm the reputation of a product or a service buying and selling fake review becomes a profitable business and a big threat in this paper we introduce a very simple but powerful review spamming technique that could fail the existing feature based detection algorithm easily it us one truthful review a a template and replaces it sentence with those from other review in a repository fake review generated by this mechanism are extremely hard to detect both the state of the art computational approach and human reader acquire an error rate of just slightly better than a random guess while it is challenging to detect such fake review we have made solid progress in suppressing them a novel defense method that leverage the difference of semantic flow between synthetic and truthful review is developed which is able to reduce the detection error rate to approximately a significant improvement over the performance of existing approach nevertheless it is still a challenging research task to further decrease the error rate synthetic review spamming demo www c ucsb edu alex morale reviewspam 
the number of patent filed each year ha increased dramatically in recent year raising concern that patent of questionable validity are restricting the issuance of truly innovative patent for this reason there is a strong demand to develop an objective model to quantify patent quality and characterize the attribute that lead to higher quality patent in this paper we develop a latent graphical model to infer patent quality from related measurement in addition we extract advanced lexical feature via natural language processing technique to capture the quality measure such a clarity of claim originality and importance of cited prior art we demonstrate the effectiveness of our approach by validating it prediction with previous court decision of litigated patent 
we study the problem of topic level social network search which aim to find who are the most influential user in a network on a specific topic and how the influential user connect with each other we employ a topic model to find topical aspect of each user and a retrieval method to identify influential user by combining the language model and the topic model an influence maximization algorithm is then presented to find the sub network that closely connects the influential user two demonstration system have been developed and are online available empirical analysis based on the user s viewing time and the number of click validates the proposed methodology 
differential privacy is a cryptographically motivated definition of privacy which ha gained significant attention over the past few year differentially private solution enforce privacy by adding random noise to a function computed over the data and the challenge in designing such algorithm is to control the added noise in order to optimize the privacy accuracy sample size tradeoff this work study differentially private statistical estimation and show upper and lower bound on the convergence rate of differentially private approximation to statistical estimator our result reveal a formal connection between differential privacy and the notion of gross error sensitivity ge in robust statistic by showing that the convergence rate of any differentially private approximation to an estimator that is accurate over a large class of distribution ha to grow with the ge of the estimator we then provide an upper bound on the convergence rate of a differentially private approximation to an estimator with bounded range and bounded ge we show that the bounded range condition is necessary if we wish to ensure a strict form of differential privacy 
i will review concept principle and mathematical tool that were found useful in application involving causal and counterfactual relationship this semantical framework enriched with a few idea from logic and graph theory give rise to a complete coherent and friendly calculus of causation that unifies the graphical and counterfactual approach to causation and resolve many long standing problem in several of the science these include question of causal effect estimation policy analysis and the integration of data from diverse study of special interest to kdd researcher would be the following topic the mediation formula and what it tell u about direct and indirect effect what mathematics can tell u about external validity or generalizing from experiment what can graph theory tell u about recovering from sample selection bias 
most email application devote a significant part of their real estate to organization mechanism such a folder yet we verified on the yahoo mail service that of email user have never defined a single folder this implies that one of the most well known email feature is underexploited we propose here to revive the feature by providing a method for generating a lighter form of folder or tag benefiting even the most passive user the method automatically associate whenever possible an appropriate semantic tag with a given email this give rise to an alternate mechanism for organizing and searching email we advocate a novel modeling approach that exploit the overall population of user thereby learning from the wisdom of crowd how to categorize message given our massive user base it is enough to learn from a minority of the user who label certain message in order to label that kind of message for the general population we design a novel cascade classification approach which cope with the severe scalability and accuracy constraint we are facing significant efficiency gain are achieved by working within a low dimensional latent space and by using a novel hierarchical classifier precision level is controlled by separating the task into a two phase classification process we performed an extensive empirical study covering three different time period over million message and thousand of candidate tag per message the result are encouraging and compare favorably with alternative approach our method successfully tag of incoming email traffic performance wise the computational overhead even on surge large traffic is sufficiently low for our approach to be applicable in production on any large web mail service 
the network inference problem consists of reconstructing the edge set of a network given trace representing the chronology of infection time a epidemic spread through the network this problem is a paradigmatic representative of prediction task in machine learning that require deducing a latent structure from observed pattern of activity in a network which often require an unrealistically large number of resource e g amount of available data or computational time a fundamental question is to understand which property we can predict with a reasonable degree of accuracy with the available resource and which we cannot we define the trace complexity a the number of distinct trace required to achieve high fidelity in reconstructing the topology of the unobserved network or more generally some of it property we give algorithm that are competitive with while being simpler and more efficient than existing network inference approach moreover we prove that our algorithm are nearly optimal by proving an information theoretic lower bound on the number of trace that an optimal inference algorithm requires for performing this task in the general case given these strong lower bound we turn our attention to special case such a tree and bounded degree graph and to property recovery task such a reconstructing the degree distribution without inferring the network we show that these problem require a much smaller and more realistic number of trace making them potentially solvable in practice 
a nonparametric bayesian contextual focused topic model cftm is proposed the cftm infers a sparse focused set of topic for each document while also leveraging contextual information about the author s and document venue the hierarchical beta process coupled with a bernoulli process is employed to infer the focused set of topic associated with each author and venue the same construction is also employed to infer those topic associated with a given document that are unusual termed random effect relative to topic that are inferred a probable for the associated author s and venue to leverage statistical strength and infer latent interrelationship between author and venue the dirichlet process is utilized to cluster author and venue the cftm automatically infers the number of topic needed to represent the corpus the number of author and venue cluster and the probabilistic importance of the author venue and random effect information on word assignment for a given document efficient mcmc inference is presented example result and interpretation are presented for two real datasets demonstrating promising performance with comparison to other state of the art method 
alzheimer s disease ad the most common type of dementia is a severe neurodegenerative disorder identifying marker that can track the progress of the disease ha recently received increasing attention in ad research a definitive diagnosis of ad requires autopsy confirmation thus many clinical cognitive measure including mini mental state examination mmse and alzheimer s disease assessment scale cognitive subscale ada cog have been designed to evaluate the cognitive status of the patient and used a important criterion for clinical diagnosis of probable ad in this paper we propose a multi task learning formulation for predicting the disease progression measured by the cognitive score and selecting marker predictive of the progression specifically we formulate the prediction problem a a multi task regression problem by considering the prediction at each time point a a task we capture the intrinsic relatedness among different task by a temporal group lasso regularizer the regularizer consists of two component including an l norm penalty on the regression weight vector which ensures that a small subset of feature will be selected for the regression model at all time point and a temporal smoothness term which ensures a small deviation between two regression model at successive time point we have performed extensive evaluation using various type of data at the baseline from the alzheimer s disease neuroimaging initiative adni database for predicting the future mmse and ada cog score our experimental study demonstrate the effectiveness of the proposed algorithm for capturing the progression trend and the cross sectional group difference of ad severity result also show that most marker selected by the proposed algorithm are consistent with finding from existing cross sectional study 
online recruiting system have gained immense attention in the wake of more and more job seeker searching job and enterprise finding candidate on the internet a critical problem in a recruiting system is how to maximally satisfy the desire of both job seeker and enterprise with reasonable recommendation or search result in this paper we investigate and compare various online recruiting system from a product perspective we then point out several key function that help achieve a win win situation between job seeker and enterprise for a successful recruiting system based on the observation and key function we design implement and deploy a web based application of recruiting system named ihr for xiamen talent service center the system utilizes the latest advance in data mining and recommendation technology to create a user oriented service for a myriad of audience in job marketing community empirical evaluation and online user study demonstrate the efficacy and effectiveness of our proposed system currently ihr ha been deployed at http i xmrc com cn xmrcintel 
newly emerging location based and event based social network service provide u with a new platform to understand user preference based on their activity history a user can only visit a limited number of venue event and most of them are within a limited distance range so the user item matrix is very sparse which creates a big challenge for traditional collaborative filtering based recommender system the problem becomes more challenging when people travel to a new city where they have no activity history in this paper we propose lcars a location content aware recommender system that offer a particular user a set of venue e g restaurant or event e g concert and exhibition by giving consideration to both personal interest and local preference this recommender system can facilitate people s travel not only near the area in which they live but also in a city that is new to them specifically lcars consists of two component offline modeling and online recommendation the offline modeling part called lca lda is designed to learn the interest of each individual user and the local preference of each individual city by capturing item co occurrence pattern and exploiting item content the online recommendation part automatically combine the learnt interest of the querying user and the local preference of the querying city to produce the top k recommendation to speed up this online process a scalable query processing technique is developed by extending the classic threshold algorithm ta we evaluate the performance of our recommender system on two large scale real data set doubanevent and foursquare the result show the superiority of lcars in recommending spatial item for user especially when traveling to new city in term of both effectiveness and efficiency 
personalization is a ubiquitous phenomenon in our daily online experience while such technology is critical for helping u combat the overload of information we face in many case we may not even realize that our result are being tailored to our personal taste and preference worse yet when such a system make a mistake we have little recourse to correct it in this work we propose a framework for addressing this problem by developing a new user interpretable feature set upon which to base personalized recommendation these feature which we call badge represent fundamental trait of user e g vegetarian or apple fanboy inferred by modeling the interplay between a user s behavior and self reported identity specifically we consider the microblogging site twitter where user provide short description of themselves in their profile a well a perform action such a tweeting and retweeting our approach is based on the insight that we can define badge using high precision low recall rule e g twitter profile contains the phrase apple fanboy and with enough data generalize to other user by observing shared behavior we develop a fully bayesian generative model that describes this interaction while allowing u to avoid the pitfall associated with having positive only data experiment on real twitter data demonstrate the effectiveness of our model at capturing rich and interpretable user trait that can be used to provide transparency for personalization 
in an it service delivery environment the speedy dispatch of a ticket to the correct resolution group is the crucial first step in the problem resolution process the size and complexity of such environment make the dispatch decision challenging and incorrect routing by a human dispatcher can lead to significant delay that degrade customer satisfaction and also have adverse financial implication for both the customer and the it vendor in this paper we present smartdispatch a learning based tool that seek to automate the process of ticket dispatch while maintaining high accuracy level smartdispatch come with two classification approach the well known svm method and a discriminative term based approach that we designed to address some of the issue in svm classification that were empirically observed using a combination of these approach smartdispatch is able to automate the dispatch of a ticket to the correct resolution group for a large share of the ticket while for the rest it is able to suggest a short list of group that contain the correct resolution group with a high probability empirical evaluation of smartdispatch on data from large service engagement project in ibm demonstrate the efficacy and practical utility of the approach 
with the growth of location based service and social service lowsampling rate trajectory from check in data or photo with geotag information becomes ubiquitous in general most detailed moving information in low sampling rate trajectory are lost prior work have elaborated on distant time location prediction in highsampling rate trajectory however existing prediction model are pattern based and thus not applicable due to the sparsity of data point in low sampling rate trajectory to address the sparsity in low sampling rate trajectory we develop a reachability based prediction model on time constrained mobility graph rtmg to predict location for distant time query specifically we design an adaptive temporal exploration approach to extract effective supporting trajectory that are temporally close to the query time based on the supporting trajectory a time constrained mobility graph tg is constructed to capture mobility information at the given query time in light of tg we further derive the reachability probability among location in tg thus a location with maximum reachability from the current location among all possible location in supporting trajectory is considered a the prediction result to efficiently process query we proposed the index structure sorted interval tree soit to organize location record extensive experiment with real data demonstrated the effectiveness and efficiency of rtmg first rtmg with adaptive temporal exploration significantly outperforms the existing pattern based prediction model hpm over varying data sparsity in term of higher accuracy and higher coverage also the proposed index structure soit can efficiently speedup rtmg in large scale trajectory dataset in the future we could extend rtmg by considering more factor e g staying duration in location application usage in smart phone to further improve the prediction accuracy 
reputable user are valuable asset of a web site we focus on user reputation in a comment rating environment where user make comment about content item and rate the comment of one another intuitively a reputable user post high quality comment and is highly rated by the user community to our surprise we find that the quality of a comment judged editorially is almost uncorrelated with the rating that it receives but can be predicted using standard text feature achieving accuracy a high a the agreement between two editor however extracting a pure reputation signal from rating is difficult because of data sparseness and several confounding factor in user voting behavior to address these issue we propose a novel bias smoothed tensor model and empirically show that our model significantly outperforms a number of alternative based on yahoo news yahoo buzz and epinions datasets 
we present a system called assocexplorer to support exploratory data analysis via association rule visualization and exploration assocexplorer is designed by following the visual information seeking mantra overview first zoom and filter then detail on demand it effectively us coloring to deliver information so that user can easily detect thing that are interesting to them if user find a rule interesting they can explore related rule for further analysis which allows user to find interesting phenomenon that are difficult to detect when rule are examined separately our system also allows user to compare rule and inspect rule with similar item composition but different statistic so that the key factor that contribute to the difference can be isolated 
we consider the problem of building compact unsupervised representation of large high dimensional non negative data using sparse coding and dictionary learning scheme with an emphasis on executing the algorithm in a map reduce environment the proposed algorithm may be seen a parallel optimization procedure for constructing sparse non negative factorization of large sparse matrix our approach alternate between a parallel sparse coding phase implemented using greedy or convex l regularized risk minimization procedure and a sequential dictionary learning phase where we solve a set of l optimization problem exactly these two fold sparsity constraint lead to better statistical performance on text analysis task and at the same time make it possible to implement each iteration in a single map reduce job we detail our implementation and optimization that lead to the ability to factor matrix with more than million row and billion of non zero entry in just a few hour on a small commodity cluster 
an e commerce catalog typically comprises of specification for million of product the search engine receives million of sale offer from thousand of independent merchant that must be matched to the right product we describe the challenge that a system for matching unstructured offer to structured product description must address drawing upon our experience from building such a system for bing shopping the heart of our system is a data driven component that learns the matching function off line which is then applied at run time for matching offer to product we provide the design of this and other critical component of the system a well a the detail of the extensive experiment we performed to ass the readiness of the system this system is currently deployed in an experimental commerce search engine and is used to match all the offer received by bing shopping to the bing product catalog 
advance in biotechnology have made available multitude of heterogeneous proteomic and genomic data integrating these heterogeneous data source to automatically infer the function of protein is a fundamental challenge in computational biology several approach represent each data source with a kernel similarity function the resulting kernel are then integrated to determine a composite kernel which is used for developing a function prediction model protein are also found to have multiple role and function a such several approach cast the protein function prediction problem within a multi label learning framework in our work we develop an approach that take advantage of several unlabeled protein along with multiple data source and multiple function of protein we develop a graph based transductive multi label classifier tmc that is evaluated on a composite kernel and also propose a method for data integration using the ensemble framework called transductive multi label ensemble classifier tmec the tmec approach train a graph based multi label classifier for each individual kernel and then combine the prediction of the individual model our contribution is the use of a bi relational directed graph that capture relationship between pair of protein between pair of function and between protein and function we evaluate the ability of tmc and tmec to predict the function of protein by using two yeast datasets we show that our approach performs better than recently proposed protein function prediction method on composite and multiple kernel 
finding dense subgraphs is an important graph mining task with many application given that the direct optimization of edge density is not meaningful a even a single edge achieves maximum density research ha focused on optimizing alternative density function a very popular among such function is the average degree whose maximization lead to the well known densest subgraph notion surprisingly enough however densest subgraphs are typically large graph with small edge density and large diameter in this paper we define a novel density function which give subgraphs of much higher quality than densest subgraphs the graph found by our method are compact dense and with smaller diameter we show that the proposed function can be derived from a general framework which includes other important density function a subcases and for which we show interesting general theoretical property to optimize the proposed function we provide an additive approximation algorithm and a local search heuristic both algorithm are very efficient and scale well to large graph we evaluate our algorithm on real and synthetic datasets and we also devise several application study a variant of our original problem when compared with the method that find the subgraph of the largest average degree our algorithm return denser subgraphs with smaller diameter finally we discus new interesting research direction that our problem leaf open 
the need for highly scalable and accurate detection and filtering of misbehaving user and obscene content in online video chat service ha grown a the popularity of these service ha exploded in popularity this is a challenging problem because processing large amount of video is compute intensive decision about whether a user is misbehaving or not must be made online and quickly and moreover these video chat are characterized by low quality video poorly lit scene diversity of user and their behavior diversity of the content and typically short session this paper present emerald a highly scalable system for accurately detecting and filtering misbehaving user in online video chat application emerald substantially improves upon the state of the art filtering mechanism by achieving much lower computational cost and higher accuracy we demonstrate emerald s improvement via experimental evaluation on real world data set obtained from chatroulette com 
a an integral part of electronic health record ehrs clinical note pose special challenge for analyzing ehrs due to their unstructured nature in this paper we present a general mining framework sympgraph for modeling and analyzing symptom relationship in clinical note a sympgraph ha symptom a node and co occurrence relation between symptom a edge and can be constructed automatically through extracting symptom over sequence of clinical note for a large number of patient we present an important clinical application of sympgraph symptom expansion which can expand a given set of symptom to other related symptom by analyzing the underlying sympgraph structure we further propose a matrix update algorithm which provides a significant computational saving for dynamic update to the graph comprehensive evaluation on million longitudinal clinical note over k patient show that static symptom expansion can successfully expand a set of known symptom to a disease with high agreement rate with physician input average precision a improvement over baseline co occurrence based method the experimental result also show that the expanded symptom can serve a useful feature for improving auc measure for disease diagnosis prediction thus confirming the potential clinical value of our work 
we present apolo a system that us a mixed initiative approach to help people interactively explore and make sense of large network datasets it combine visualization rich user interaction and machine learning to engage the user in bottom up sensemaking to gradually build up an understanding over time by starting small rather than starting big and drilling down apolo help user find relevant information by specifying exemplar and then using a machine learning method called belief propagation to infer which other node may be of interest we demonstrate apolo s usage and benefit using a google scholar citation graph consisting of article node and citation relationship a demo video of apolo is available at http www c cmu edu dchau apolo apolo mp 
in scientific research it is often difficult to express information need a simple keyword query we present a more natural way of searching for relevant scientific literature rather than a string of keywords we define a query a a small set of paper deemed relevant to the research task at hand by optimizing an objective function based on a fine grained notion of influence between document our approach efficiently selects a set of highly relevant article moreover a scientist trust some author more than others result are personalized to individual preference in a user study researcher found the paper recommended by our method to be more useful trustworthy and diverse than those selected by popular alternative such a google scholar and a state of the art topic modeling approach 
metric like disk activity and network traffic are widespread source of diagnosis and monitoring information in datacenters and network however a the scale of these system increase examining the raw data yield diminishing insight we present rainmon a novel end to end approach for mining timeseries monitoring data designed to handle it size and unique characteristic our system is able to a mine large bursty real world monitoring data b find significant trend and anomaly in the data c compress the raw data effectively and d estimate trend to make forecast furthermore rainmon integrates the full analysis process from data storage to the user interface to provide accessible long term diagnosis we apply rainmon to three real world datasets from production system and show it utility in discovering anomalous machine and time period 
p p lending a a novel economic lending model ha imposed new challenge about how to make effective investment decision indeed a key challenge along this line is how to align the right information with the right people for a long time people have made tremendous effort in establishing credit record for the borrower however information from investor is still under explored for improving investment decision in p p lending to that end we propose a data driven investment decision making framework which exploit the investor composition of each investment for enhancing decision making in p p lending specifically we first build investor profile based on quantitative analysis of past performance risk preference and investment experience of investor then based on investor profile we develop an investor composition analysis model which can be used to select valuable investment and improve the investment decision to validate the proposed model we perform extensive experiment on the real world data from the world s largest p p lending marketplace experimental result reveal that investor composition can help u evaluate the profit potential of an investment and the decision model based on investor composition can help investor make better investment decision 
the standard assumption of identically distributed training and test data is violated when test data are generated in response to a predictive model this becomes apparent for example in the context of email spam filtering where an email service provider employ a spam filter and the spam sender can take this filter into account when generating new email we model the interaction between learner and data generator a a stackelberg competition in which the learner play the role of the leader and the data generator may react on the leader s move we derive an optimization problem to determine the solution of this game and present several instance of the stackelberg prediction game we show that the stackelberg prediction game generalizes existing prediction model finally we explore property of the discussed model empirically in the context of email spam filtering 
the great east japan earthquake and the fukushima nuclear accident cause large human population movement and evacuation understanding and predicting these movement is critical for planning effective humanitarian relief disaster management and long term societal reconstruction in this paper we construct a large human mobility database that store and manages gps record from mobile device used by approximately million people throughout japan from august to july by mining this enormous set of auto gps mobile sensor data the short term and long term evacuation behavior for individual throughout japan during this disaster are able to be automatically discovered to better understand and simulate human mobility during the disaster we develop a probabilistic model that is able to be effectively trained by the discovered evacuation via machine learning technique based on our training model population mobility in various city impacted by the disaster throughout the country is able to be automatically simulated or predicted on the basis of the whole database developed model and experimental result it is easy for u to find some new feature or population mobility pattern after the recent severe earthquake tsunami and release of radioactivity in japan which are likely to play a vital role in future disaster relief and management worldwide 
discovering community from social medium and collaboration system ha been of great interest in recent year existing work show prospect of modeling content and social link aiming at discovering social community whose definition varies by application we believe that a community depends not only on the group of people who actively participate but also the topic they communicate about or collaborate on this is especially true for workplace email communication within an organization it is not uncommon that employee multifunction and group of employee collaborate on multiple project at the same time in this paper we aim to automatically discovering and profiling user community by taking into account both the contact and the topic more specifically we propose a community profiling model called cocomp where the community label are latent and each social document corresponds to an information sharing activity among the most probable community member regarding the most relevant community issue experiment result on several social communication datasets including email and twitter message demonstrate that the model can discover user community effectively and provide concrete semantics 
we present an algorithm for language identification in particular of short document for the case of an internet domain with site in multiple country with differing language the algorithm is significantly faster than standard language identification method while providing state of the art identification we bootstrap the algorithm based on the language identification based on the site alone a methodology suitable for any supervised language identification algorithm we demonstrate the bootstrapping and algorithm on ebay email data and on twitter status update data the algorithm is deployed at ebay a part of the back office development data repository 
sequential pattern mining play an important role in many application such a bioinformatics and consumer behavior analysis however the classic frequency based framework often lead to many pattern being identified most of which are not informative enough for business decision making in frequent pattern mining a recent effort ha been to incorporate utility into the pattern selection framework so that high utility frequent or infrequent pattern are mined which address typical business concern such a dollar value associated with each pattern in this paper we incorporate utility into sequential pattern mining and a generic framework for high utility sequence mining is defined an efficient algorithm uspan is presented to mine for high utility sequential pattern in uspan we introduce the lexicographic quantitative sequence tree to extract the complete set of high utility sequence and design concatenation mechanism for calculating the utility of a node and it child with two effective pruning strategy substantial experiment on both synthetic and real datasets show that uspan efficiently identifies high utility sequence from large scale data with very low minimum utility 
the benefit of crowdsourcing are well recognized today for an increasingly broad range of problem meanwhile the rapid development of social medium make it possible to seek the wisdom of a crowd of targeted user however it is not trivial to implement the crowdsourcing platform on social medium specifically to make social medium user a worker we need to address the following two challenge how to motivate user to participate in task and how to choose user for a task in this paper we present wise market a an effective framework for crowdsourcing on social medium that motivates user to participate in a task with care and correctly aggregate their opinion on pairwise choice problem the wise market consists of a set of investor each with an associated individual confidence in his her prediction and after the investment only the one whose choice are the same a the whole market are granted reward therefore a social medium user ha to give his her best answer in order to get reward a a consequence careless answer from sloppy user are discouraged under the wise market framework we define an optimization problem to minimize expected cost of paying out reward while guaranteeing a minimum confidence level called the effective market problem emp we propose exact algorithm for calculating the market confidence and the expected cost with o nlog n time cost in a wise market with n investor to deal with the enormous number of user on social medium we design a central limit theorem based approximation algorithm to compute the market confidence with o n time cost a well a a bounded approximation algorithm to calculate the expected cost with o n time cost finally we have conducted extensive experiment to validate effectiveness of the proposed algorithm on real and synthetic data 
a reasonable definition of intrusion is entering a community to which one doe not belong this suggests that in a network intrusion attempt may be detected by looking for communication that doe not respect community boundary in this paper we examine the utility of this concept for identifying malicious network source in particular our goal is to explore whether this concept allows a core network operator using flow data to augment signature based system located at network edge we show that simple measure of community can be defined for flow data that allow a remarkably effective level of intrusion detection simply by looking for flow that do not respect those community we validate our approach using labeled intrusion attempt data collected at a large number of edge network our result suggest that community based method can offer an important additional dimension for intrusion detection system 
the integrative mining of heterogeneous data and the interpretability of the data mining result are two of the most important challenge of today s data mining it is commonly agreed in the community that particularly in the research area of clustering both challenge have not yet received the due attention only few approach for clustering of object with mixed type attribute exist and those few approach do not consider cluster specific dependency between numerical and categorical attribute likewise only a few clustering paper address the problem of interpretability to explain why a certain set of object have been grouped into a cluster and what a particular cluster distinguishes from another in this paper we approach both challenge by constructing a relationship to the concept of data compression using the minimum description length principle a detected cluster structure is the better the more efficient it can be exploited for data compression following this idea we can learn during the run of a clustering algorithm the optimal trade off for attribute weight and distinguish relevant attribute dependency from coincidental one we extend the efficient cholesky decomposition to model dependency in heterogeneous data and to ensure interpretability our proposed algorithm inconco successfully find cluster in mixed type data set identifies the relevant attribute dependency and explains them using linear model and case by case analysis thereby it outperforms existing approach in effectiveness a our extensive experimental evaluation demonstrates 
genome wide association study gwas have become a popular method for analyzing set of dna sequence in order to discover the genetic basis of disease unfortunately statistic published a the result of gwas can be used to identify individual participating in the study to prevent privacy breach even previously published result have been removed from public database impeding researcher access to the data and hindering collaborative research existing technique for privacy preserving gwas focus on answering specific question such a correlation between a given pair of snp dna sequence variation this doe not fit the typical gwas process where the analyst may not know in advance which snp to consider and which statistical test to use how many snp are significant for a given dataset etc we present a set of practical privacy preserving data mining algorithm for gwas datasets our framework support exploratory data analysis where the analyst doe not know a priori how many and which snp to consider we develop privacy preserving algorithm for computing the number and location of snp that are significantly associated with the disease the significance of any statistical test between a given snp and the disease any measure of correlation between snp and the block structure of correlation we evaluate our algorithm on real world datasets and demonstrate that they produce significantly more accurate result than prior technique while guaranteeing differential privacy 
composed of several hundred of processor the graphic processing unit gpu ha become a very interesting platform for computationally demanding task on massive data a special hierarchy of processor and fast memory unit allow very powerful and efficient parallelization but also demand novel parallel algorithm expectation maximization em is a widely used technique for maximum likelihood estimation in this paper we propose an innovative em clustering algorithm particularly suited for the gpu platform on nvidia s fermi architecture the central idea of our algorithm is to allow the parallel thread exchanging their local information in an asynchronous way and thus updating their cluster representative on demand by a technique called asynchronous model update async em async em enables our algorithm not only to accelerate convergence but also to reduce the overhead induced by memory bandwidth limitation and synchronization requirement we demonstrate how to reformulate the em algorithm to be able to exchange information using async em and how to exploit the special memory and processor architecture of a modern gpu in order to share this information among thread in an optimal way a a perspective async em is not limited to em but can be applied to a variety of algorithm 
in web era we are confronted with a huge amount of raw data and a tremendous change of man machine interaction mode we have to deal with the content semantics of data rather than their form alone traditional information processing approach face a new challenge since they cannot deal with the semantic meaning or content of information but human can handle such a problem easily so it s needed a new information processing strategy that correlated with the content of information by learning some mechanism from human being therefore we need a set of robust detector for detecting semantically meaningful feature such a boundary shape etc in image word sentence etc in text and a set of method that can effectively analyze and exploit the information structure that encode the content of information during the past year the probability theory ha made a great progress it ha provided a set of mathematical tool for representing and analyzing information structure in the talk we will discus what difficulty we face what we can do and how we should do in the content based information processing 
crowd selection is essential to crowd sourcing application since choosing the right worker with particular expertise to carry out crowdsourced task is extremely important the central problem is simple but tricky given a crowdsourced task who are the most knowledgable user to ask in this demo we show our framework that tackle the problem of crowdsourced task assignment on twitter according to the social activity of it user since user profile on twitter do not reveal user interest and skill we transfer the knowledge from categorized yahoo answer datasets for learning user expertise then we select the right crowd for certain task based on user expertise we study the effectiveness of our system using extensive user evaluation we further engage the attendee to participate a game called whom to ask on twitter this help understand our idea in an interactive manner our crowd selection can be accessed by the following url http webproject cse ust hk tcrowd 
nonnegative matrix factorization nmf is an effective dimension reduction method for non negative dyadic data and ha proven to be useful in many area such a text mining bioinformatics and image processing nmf is usually formulated a a constrained non convex optimization problem and many algorithm have been developed for solving it recently a coordinate descent method called fasthals ha been proposed to solve least square nmf and is regarded a one of the state of the art technique for the problem in this paper we first show that fasthals ha an inefficiency in that it us a cyclic coordinate descent scheme and thus performs unneeded descent step on unimportant variable we then present a variable selection scheme that us the gradient of the objective function to arrive at a new coordinate descent method our new method is considerably faster in practice and we show that it ha theoretical convergence guarantee moreover when the solution is sparse a is often the case in real application our new method benefit by selecting important variable to update more often thus resulting in higher speed a an example on a text dataset rcv our method is time faster than fasthals and more than time faster when the sparsity is increased by adding an l penalty we also develop new coordinate descent method when error in nmf is measured by kl divergence by applying the newton method to solve the one variable sub problem experiment indicate that our algorithm for minimizing the kl divergence is faster than the lee seung multiplicative rule by a factor of on the cbcl image dataset 
large amount of data arise in a multitude of situation ranging from bioinformatics to astronomy manufacturing and medical application for concreteness our tutorial focus on data obtained in the context of the internet such a user generated content microblogs e mail message behavioral data location interaction click query and graph due to it magnitude much of the challenge are to extract structure and interpretable model without the need for additional label i e to design effective unsupervised technique we present design pattern for hierarchical nonparametric bayesian model efficient inference algorithm and modeling tool to describe salient aspect of the data 
a social tagging system provides user an effective way to collaboratively annotate and organize item with their own tag a social tagging system contains heterogeneous information like user tagging behavior social network tag semantics and item profile all the heterogeneous information help alleviate the cold start problem due to data sparsity in this paper we model a social tagging system a a multi type graph to learn the weight of different type of node and edge we propose an optimization framework called optrank optrank can be characterized a follows edge and node are represented by feature different type of edge and node have different set of feature optrank learns the best feature weight by maximizing the average auc area under the roc curve of the tag recommender we conducted experiment on two publicly available datasets i e delicious and last fm experimental result show that optrank outperforms the existing graph based method when only user tag item relation is available optrank successfully improves the result by incorporating social network tag semantics and item profile 
in this paper we investigate the highly reliable subgraph problem which arises in the context of uncertain graph this problem attempt to identify all induced subgraphs for which the probability of connectivity being maintained under uncertainty is higher than a given threshold this problem arises in a wide range of network application such a protein complex discovery network routing and social network analysis since exact discovery may be computationally intractable we introduce a novel sampling scheme which enables approximate discovery of highly reliable subgraphs with high probability furthermore we transform the core mining task into a new frequent cohesive set problem in deterministic graph such transformation enables the development of an efficient two stage approach which combine novel peeling technique for maximal set discovery with depth first search for further enumeration we demonstrate the effectiveness and efficiency of the proposed algorithm on real and synthetic data set 
learning from electronic medical record emr is challenging due to their relational nature and the uncertain dependence between a patient s past and future health status statistical relational learning is a natural fit for analyzing emrs but is le adept at handling their inherent latent structure such a connection between related medication or disease one way to capture the latent structure is via a relational clustering of object we propose a novel approach that instead of pre clustering the object performs a demand driven clustering during learning we evaluate our algorithm on three real world task where the goal is to use emrs to predict whether a patient will have an adverse reaction to a medication we find that our approach is more accurate than performing no clustering pre clustering and using expert constructed medical heterarchies 
most time series data mining algorithm use similarity search a a core subroutine and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithm the difficulty of scaling search to large datasets largely explains why most academic work on time series data mining ha plateaued at considering a few million of time series object while much of industry and science sits on billion of time series object waiting to be explored in this work we show that by using a combination of four novel idea we can search and mine truly massive time series for the first time we demonstrate the following extremely unintuitive fact in large datasets we can exactly search under dtw much more quickly than the current state of the art euclidean distance search algorithm we demonstrate our work on the largest set of time series experiment ever attempted in particular the largest dataset we consider is larger than the combined size of all of the time series datasets considered in all data mining paper ever published we show that our idea allow u to solve higher level time series data mining problem such a motif discovery and clustering at scale that would otherwise be untenable in addition to mining massive datasets we will show that our idea also have implication for real time monitoring of data stream allowing u to handle much faster arrival rate and or use cheaper and lower powered device than are currently possible 
revenue authority characteristically have large store of historic audit data with outcome ready for analysis the australian taxation office established one of the largest data mining team in australia in a a foundation to becoming a knowledge based organization today every tax return lodged in australia is risk assessed by one or more model developed through data mining generally based on historic data we observe that any of the traditional modeling approach particularly including random forest generally deliver similar model in term of accuracy we take advantage of combining different model type and modeling approach for risk scoring and in particular report on recent research that increase the diversity of tree that make up a random forest we also review in a practical context how such model are evaluated and delivered 
building an accurate emerging pattern classifier with a high dimensional dataset is a challenging issue the problem becomes even more difficult if the whole feature space is unavailable before learning start this paper present a new technique on mining emerging pattern using streaming feature selection we model high feature dimension with streaming feature that is feature arrive and are processed one at a time a feature flow in one by one we online evaluate each coming feature to determine whether it is useful for mining predictive emerging pattern eps by exploiting the relationship between feature relevance and ep discriminability the predictive ability of an ep we employ this relationship to guide an online ep mining process this new approach can mine eps from a high dimensional dataset even when it entire feature set is unavailable before learning the experiment on a broad range of datasets validate the effectiveness of the proposed approach against other well established method in term of predictive accuracy pattern number and running time 
in this paper we present our approach for geographic personalization of a content recommendation system more specifically our work focus on recommending query topic to user we do this by mining the search query log to detect trending local topic for a set of query we compute their count and what we call buzz score which is a metric for detecting trending behavior we also compute the entropy of the geographic distribution of the query a mean of detecting their location affinity we cluster the query into trending topic and assign the topic to their corresponding location human editor then select a subset of these local topic and enter them into a recommendation system in turn the recommendation system optimizes a pool of trending local and global topic by exploiting user feedback we present some editorial evaluation of the technique and result of a live experiment inclusion of local topic in selected location into the global pool of topic resulted in more than relative increase in user engagement with the recommendation system compared to using the global topic exclusively 
graph mining is a challenging task by itself and even more so when processing data stream which evolve in real time data stream mining face hard constraint regarding time and space for processing and also need to provide for concept drift detection in this paper we present a framework for studying graph pattern mining on time varying stream three new method for mining frequent closed subgraphs are presented all method work on coresets of closed subgraphs compressed representation of graph set and maintain these set in a batch incremental manner but use different approach to address potential concept drift an evaluation study on datasets comprising up to four million graph explores the strength and limitation of the proposed method to the best of our knowledge this is the first work on mining frequent closed subgraphs in non stationary data stream 
much work in optimal control and inverse control ha assumed that the controller ha perfect knowledge of plant dynamic however if the controller is a human or animal subject the subject s internal dynamic model may differ from the true plant dynamic here we consider the problem of learning the subject s internal model from demonstration of control and knowledge of task goal due to sensory feedback delay the subject us an internal model to generate an internal prediction of the current plant state which may differ from the actual plant state we develop a probabilistic framework and exact em algorithm to jointly estimate the internal model internal state trajectory and feedback delay we applied this framework to demonstration by a nonhuman primate of brain machine interface bmi control we discovered that the subject s internal model deviated from the true bmi plant dynamic and provided significantly better explanation of the recorded neural control signal than did the true plant dynamic 
slow convergence and poor initial accuracy are two problem that plague effort to use very large feature set in online learning this is especially true when only a few feature are active in any training example and the frequency of activation of different feature is skewed we show how these problem can be mitigated if a graph of relationship between feature is known we study this problem in a fully bayesian setting focusing on the problem of using facebook user id a feature with the social network giving the relationship structure our analysis uncovers significant problem with the obvious regularization and motivates a two component mixture model social prior that is provably better empirical result on large scale click prediction problem show that our algorithm can learn a well a the baseline with m fewer training example and continuously outperforms it for over m example on a second problem using binned feature our model outperforms the baseline even after the latter see x a much data 
analyzing functional interaction between small compound and protein is indispensable in genomic drug discovery since rich information on various compound protein interaction is available in recent molecular database strong demand for making best use of such database require to invent powerful method to help u find new functional compound protein pair on a large scale we present the succinct interval splitting tree algorithm sita that efficiently performs similarity search in database for compound protein pair with respect to both binary fingerprint and real valued property sita achieves both time and space efficiency by developing the data structure called interval splitting tree which enables to efficiently prune the useless portion of search space and by incorporating the idea behind wavelet tree a succinct data structure to compactly represent tree we experimentally test sita on the ability to retrieve similar compound protein pair substrate product pair for a query from large database with over million compoundprotein pair substrate product pair and show that sita performs better than other possible approach 
we tackle the challenging problem of mining the simplest boolean pattern from categorical datasets instead of complete enumeration which is typically infeasible for this class of pattern we develop effective sampling method to extract a representative subset of the minimal boolean pattern in disjunctive normal form dnf we make both theoretical and practical contribution which allow u to prune the search space based on provable property our approach can provide a near uniform sample of the minimal dnf pattern we also show that the mined minimal dnf pattern are very effective when used a feature for classification 
precision recall pr curve and the area under them are widely used to summarize machine learning result especially for data set exhibiting class skew they are often used analogously to roc curve and the area under roc curve it is known that pr curve vary a class skew change what wa not recognized before this paper is that there is a region of pr space that is completely unachievable and the size of this region depends only on the skew this paper precisely characterizes the size of that region and discus it implication for empirical evaluation methodology in machine learning 
we present a system called risk o meter to predict and analyze clinical risk via data imputation visualization predictive modeling and association rule exploration clinical risk calculator provide information about a person s chance of having a disease or encountering a clinical event such tool could be highly useful to educate patient to understand and monitor their health condition unlike existing risk calculator that are primarily designed for domain expert risko meter is useful to patient who are unfamiliar with medical terminology or provider who have limited information about a patient risk o meter is designed in a way such that it is flexible enough to accept limited or incomplete data input and still manages to predict the clinical risk efficiently and effectively current version of risk o meter evaluates day risk of hospital readmission however the proposed system framework is applicable to general clinical risk prediction in this demonstration paper we describe different component of risk o meter and the intelligent algorithm associated with each of these component to evaluate risk of readmission using incomplete patient data input 
the least square problem is one of the most important regression problem in statistic machine learning and data mining in this paper we present the constrained stochastic gradient descent csgd algorithm to solve the large scale least square problem csgd improves the stochastic gradient descent sgd by imposing a provable constraint that the linear regression line pass through the mean point of all the data point it result in the best regret bound o log t and fastest convergence speed among all first order approach empirical study justify the effectiveness of csgd by comparing it with sgd and other state of the art approach an example is also given to show how to use csgd to optimize sgd based least square problem to achieve a better performance 
learning in non stationary environment is an increasingly important problem in a wide variety of real world application in non stationary environment data arrives incrementally however the underlying generating function may change over time in addition to the environment being non stationary they also often exhibit class imbalance that is one class the majority class vastly outnumbers the other class the minority class this combination of class imbalance with non stationary environment pose significant and interesting practical problem for classification to overcome these issue we introduce a novel instance selection mechanism a well a provide a modification to the heuristic updatable weighted random subspace huwrs method for the class imbalance problem we then compare our modification of huwrs called huwrs ip to other state of the art algorithm concluding that huwrs ip often achieves vastly superior performance 
the problem of point of interest poi recommendation is to provide personalized recommendation of place of interest such a restaurant for mobile user due to it complexity and it connection to location based social network lbsns the decision process of a user choose a poi is complex and can be influenced by various factor such a user preference geographical influence and user mobility behavior while there are some study on poi recommendation it lack of integrated analysis of the joint effect of multiple factor to this end in this paper we propose a novel geographical probabilistic factor analysis framework which strategically take various factor into consideration specifically this framework allows to capture the geographical influence on a user s check in behavior also the user mobility behavior can be effectively exploited in the recommendation model moreover the recommendation model can effectively make use of user check in count data a implicity user feedback for modeling user preference finally experimental result on real world lbsns data show that the proposed recommendation method outperforms state of the art latent factor model with a significant margin 
the voluminous malware variant that appear in the internet have posed severe threat to it security in this work we explore technique that can automatically classify malware variant into their corresponding family we present a generic framework that extract structural information from malware program a attributed function call graph in which rich malware feature are encoded a attribute at the function level our framework further learns discriminant malware distance metric that evaluate the similarity between the attributed function call graph of two malware program to combine various type of malware attribute our method adaptively learns the confidence level associated with the classification capability of each attribute type and then adopts an ensemble of classifier for automated malware classification we evaluate our approach with a number of window based malware instance belonging to family and experimental result show that our automated malware classification method is able to achieve high classification accuracy 
reconstruction based subspace clustering method compute a self reconstruction matrix over the sample and use it for spectral clustering to obtain the final clustering result their success largely relies on the assumption that the underlying subspace are independent which however doe not always hold in the application with increasing number of subspace in this paper we propose a novel reconstruction based subspace clustering model without making the subspace independence assumption in our model certain property of the reconstruction matrix are explicitly characterized using the latent cluster indicator and the affinity matrix used for spectral clustering can be directly built from the posterior of the latent cluster indicator instead of the reconstruction matrix experimental result on both synthetic and real world datasets show that the proposed model can outperform the state of the art method copyright by the author s owner s 
we consider estimation of multiple high dimensional gaussian graphical model corresponding to a single set of node under several distinct condition we assume that most aspect of the network are shared but that there are some structured difference between them specifically the network difference are generated from node perturbation a few node are perturbed across network and most or all edge stemming from such node differ between network this corresponds to a simple model for the mechanism underlying many cancer in which the gene regulatory network is disrupted due to the aberrant activity of a few specific gene we propose to solve this problem using the perturbed node joint graphical lasso a convex optimization problem that is based upon the use of a row column overlap norm penalty we then solve the convex problem using an alternating direction method of multiplier algorithm our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data 
with the development of web application textual document are not only getting richer but also ubiquitously interconnected with user and other object in various way which brings about text rich heterogeneous information network topic model have been proposed and shown to be useful for document analysis and the interaction among multi typed object play a key role at disclosing the rich semantics of the network however most of topic model only consider the textual information while ignore the network structure or can merely integrate with homogeneous network none of them can handle heterogeneous information network well in this paper we propose a novel topic model with biased propagation tmbp algorithm to directly incorporate heterogeneous information network with topic modeling in a unified way the underlying intuition is that multi typed object should be treated differently along with their inherent textual information and the rich semantics of the heterogeneous information network a simple and unbiased topic propagation across such a heterogeneous network doe not make much sense consequently we investigate and develop two biased propagation framework the biased random walk framework and the biased regularization framework for the tmbp algorithm from different perspective which can discover latent topic and identify cluster of multi typed object simultaneously we extensively evaluate the proposed approach and compare to the state of the art technique on several datasets experimental result demonstrate that the improvement in our proposed approach is consistent and promising 
it program in japan to build powerful engine for big data wa launched quite recently the initial version is commercialized this presentation will give a brief overview of the project also some of the potential application will be introduced 
this paper is concerned with the joint allocation of bid price and campaign budget in sponsored search in this application an advertiser can create a number of campaign and set a budget for each of them in a campaign he she can further create several ad group with bid keywords and bid price data analysis show that many advertiser are dealing with a very large number of campaign bid keywords and bid price at the same time which pose a great challenge to the optimality of their campaign management a a result the budget of some campaign might be too low to achieve the desired performance goal while those of some other campaign might be wasted the bid price for some keywords may be too low to win competitive auction while those of some other keywords may be unnecessarily high in this paper we propose a novel algorithm to automatically address this issue in particular we model the problem a a constrained optimization problem which maximizes the expected advertiser revenue subject to the constraint of the total budget of the advertiser and the range of bid price change by solving this optimization problem we can obtain an optimal budget allocation plan a well a an optimal bid price setting our simulation result based on the sponsored search log of a commercial search engine have shown that by employing the proposed method we can effectively improve the performance of the advertiser while at the same time we also see an increase in the revenue of the search engine in addition the result indicate that this method is robust to the second order effect caused by the bid fluctuation from other advertiser 
new challenge have been presented to classical topic model when applied to social medium a user generated content suffers from significant problem of data sparseness a variety of heuristic adjustment to these model have been proposed many of which are based on the use of context information to improve the performance of topic modeling existing contextualized topic model rely on arbitrary manipulation of the model structure by incorporating various context variable into the generative process of classical topic model in an ad hoc manner such manipulation usually result in much more complicated model structure sophisticated inference procedure and low generalizability to accommodate arbitrary type or combination of context in this paper we explore a different direction we propose a general solution that is able to exploit multiple type of context without arbitrary manipulation of the structure of classical topic model we formulate different type of context a multiple view of the partition of the corpus a co regularization framework is proposed to let these view collaborate with each other vote for the consensus topic and distinguish them from view specific topic experiment with real world datasets prove that the proposed method is both effective and flexible to handle arbitrary type of context 
due to the rich information in graph data the technique for privacy protection in published social network is still in it infancy a compared to the protection in relational database in this paper we identify a new type of attack called a friendship attack in a friendship attack an adversary utilizes the degree of two vertex connected by an edge to re identify related victim in a published social network data set to protect against such attack we introduce the concept of k degree anonymity which limit the probability of a vertex being re identified to k for the k degree anonymization problem we propose an integer programming formulation to find optimal solution in small scale network we also present an efficient heuristic approach for anonymizing large scale social network against friendship attack the experimental result demonstrate that the proposed approach can preserve much of the characteristic of social network 
marketer often rely on a set of descriptive segment or qualitative subset of the population to specify the audience of targeted advertising campaign for example the descriptive segment empty nester might describe a desirable target audience for extended vacation package offer while some segment may be easily described and generated using demographic data a ground truth others such a soccer mom or urban hipster reflect a combination of demographic and behavioral attribute ideally these attribute would be available a the basis for ground truth labeling of a classifier training set or even direct member selection from the population unfortunately ground truth attribute are often scarce or unavailable in which case a proxy labeling scheme is needed we devise a method for labeling a population according to criterion based on a postulated set of shopping behavior specific to a descriptive segment we then perform supervised binary classification on this labeled dataset in order to discover additional identifying pattern of behavior typical of labeled positive in the population finally the resulting classifier is used to perform selection from the population into the segment extending reach to cooky who may not have exhibited the postulated behavior but likely belong in the segment we validate our approach by comparing a descriptive segment trained on ground truth to one trained on behavioral attribute only we show that our behavior based approach produce classifier having performance comparable to that of a classifier trained on the ground truth data 
modern computer hardware offer an elaborate hierarchy of storage subsystem with different speed capacity and cost associated with them furthermore processor are now inherently parallel offering the execution of several diverse thread simultaneously this paper proposes streamsvm the first algorithm for training linear support vector machine svms which take advantage of these property by integrating caching with optimization streamsvm work by performing update in the dual thus obviating the need to rebalance frequently visited example furthermore we trade off file i o with data expansion on the fly by generating feature on demand this significantly increase throughput experiment show that streamsvm outperforms other linear svm solver including the award winning work of by order of magnitude and produce more accurate solution within a shorter amount of time 
this paper tackle the efficiency problem of making recommendation in the context of large user and item space in particular we address the problem of learning binary code for collaborative filtering which enables u to efficiently make recommendation with time complexity that is independent of the total number of item we propose to construct binary code for user and item such that the preference of user over item can be accurately preserved by the hamming distance between their respective binary code by using two loss function measuring the degree of divergence between the training and predicted rating we formulate the problem of learning binary code a a discrete optimization problem although this optimization problem is intractable in general we develop effective relaxation that can be efficiently solved by existing method moreover we investigate two method to obtain the binary code from the relaxed solution evaluation are conducted on three public domain data set and the result suggest that our proposed method outperforms several baseline alternative 
why just count crime when you can anticipate prevent and respond more effectively company in the commercial sector have long understood the importance of being able to anticipate or predict future behavior and demand in order to respond efficiently and effectively embracing the promise of predictive analytics the public safety community is moving from a focus on what happened to a system that enables the ability to anticipate future event and effectively deploy resource in front of crime thereby changing outcome while we have become familiar with the use of advanced analytics in support of fraud detection and prevention technique similar to those used to support customer loyalty program and supply chain management have been used to prevent and solve violent crime enhance investigative pace and efficacy support information based risk and threat assessment and deploy public safety resource more efficiently a public safety agency increasingly are asked to do more with le the ability to anticipate crime represents a game changing paradigm shift enabling information based tactic strategy and policy in support of prevention and response reporting collecting and compiling data are necessary but not sufficient to increasing public safety ultimately the ability to anticipate prevent and respond more effectively will enable u to do more with le and change public safety outcome 
in an era of information overload many people struggle to make sense of complex story such a presidential election or economic reform we propose a methodology for creating structured summary of information which we call zoomable metro map just a cartographic map have been relied upon for century to help u understand our surroundings metro map can help u understand the information landscape given large collection of news document our proposed algorithm generates a map of connection that explicitly capture story development a different user might be interested in different level of granularity the map are zoomable with each level of zoom showing finer detail and interaction in this paper we formalize characteristic of good zoomable map and formulate their construction a an optimization problem we provide efficient scalable method with theoretical guarantee for generating map pilot user study over real world datasets demonstrate that our method help user comprehend complex story better than prior work 
over the last decade great stride have been made in developing technique to compute function privately in particular differential privacy give strong promise about conclusion that can be drawn about an individual in contrast various syntactic method for providing privacy criterion such a k anonymity and l diversity have been criticized for still allowing private information of an individual to be inferred in this paper we consider the ability of an attacker to use data meeting privacy definition to build an accurate classifier we demonstrate that even under differential privacy such classifier can be used to infer private attribute accurately in realistic data we compare this to similar approach for inference based attack on other form of anonymized data we show how the efficacy of all these attack can be measured on the same scale based on the probability of successfully inferring a private attribute we observe that the accuracy of inference of private attribute for differentially private data and l diverse data can be quite similar 
in this paper we develop a semantic annotation technique for location based social network to automatically annotate all place with category tag which are a crucial prerequisite for location search recommendation service or data cleaning our annotation algorithm learns a binary support vector machine svm classifier for each tag in the tag space to support multi label classification based on the check in behavior of user we extract feature of place from i explicit pattern ep of individual place and ii implicit relatedness ir among similar place the feature extracted from ep are summarized from all check in at a specific place the feature from ir are derived by building a novel network of related place nrp where similar place are linked by virtual edge upon nrp we determine the probability of a category tag for each place by exploring the relatedness of place finally we conduct a comprehensive experimental study based on a real dataset collected from a location based social network whrrl the result demonstrate the suitability of our approach and show the strength of taking both ep and ir into account in feature extraction 
this paper present latent association analysis laa a generative model that analyzes the topic within two document set simultaneously a well a the correlation between the two topic structure by considering the semantic association among document pair laa defines a correlation factor that represents the connection between two document and considers the topic proportion of paired document based on this factor word in the document are assumed to be randomly generated by particular topic assignment and topic to word probability distribution the paper also present a new ranking algorithm based on laa that can be used to retrieve target document that are potentially associated with a given source document the ranking algorithm us the latent factor in laa to rank target document by the strength of their semantic association with the source document we evaluate the laa algorithm with real datasets specifically the it change and the it solution document set from the ibm it service environment and the symptom treatment document set from google health experimental result demonstrate that the laa algorithm significantly outperforms existing algorithm 
we consider the automated recognition of human action in surveillance video most current method build classifier based on complex handcrafted feature computed from the raw input convolutional neural network cnns are a type of deep model that can act directly on the raw input however such model are currently limited to handling d input in this paper we develop a novel d cnn model for action recognition this model extract feature from both the spatial and the temporal dimension by performing d convolution thereby capturing the motion information encoded in multiple adjacent frame the developed model generates multiple channel of information from the input frame and the final feature representation combine information from all channel to further boost the performance we propose regularizing the output with high level feature and combining the prediction of a variety of different model we apply the developed model to recognize human action in the real world environment of airport surveillance video and they achieve superior performance in comparison to baseline method 
a multilinear subspace regression model based on so called latent variable decomposition is introduced unlike standard regression method which typically employ matrix d data representation followed by vector subspace transformation the proposed approach us tensor subspace transformation to model common latent variable across both the independent and dependent data the proposed approach aim to maximize the correlation between the so derived latent variable and is shown to be suitable for the prediction of multidimensional dependent data from multidimensional independent data where for the estimation of the latent variable we introduce an algorithm based on multilinear singular value decomposition msvd on a specially defined cross covariance tensor it is next shown that in this way we are also able to unify the existing partial least square pls and n way pls regression algorithm within the same framework simulation on benchmark synthetic data confirm the advantage of the proposed approach in term of it predictive ability and robustness especially for small sample size the potential of the proposed technique is further illustrated on a real world task of the decoding of human intracranial electrocorticogram ecog from a simultaneously recorded scalp electroencephalograph eeg 
with the coming of age of web a a mainstream customer service channel b c company have invested substantial resource in enhancing their web presence today customer can interact with a company through channel such a phone chat email social medium or web self service with the availability of web log crm data and text transcript these online channel are rich with data and they track several aspect of customer behavior and intent customer innovation lab ha developed a series of data mining and statistic driven solution to improve customer experience in each of these online channel this talk will focus on solution to enhance performance of web chat a a customer service channel stage of customer life cycle will be considered new customer acquisition or sale and service of existing customer in customer acquisition the key objective is to maximize incremental revenue via chat while in customer service the objective is to drive up the quality of customer experience measured by customer satisfaction survey or mined customer sentiment through chat the solution based on machine learning method involves real time targeting of the right visitor to chat predicting customer need routing customer to the right customer service agent mining chat transcript and social medium portal to identify key customer issue and customer sentiment mining agent response for performance improvement feeding back learning from and to better targeting real life case study will be presented to show how that this closed loop solution can quickly improve key metric 
the netflix competition of ha spurred significant activity in the recommendation field particularly in approach using latent factor model however the near ubiquity of the netflix and the similar movielens datasets may be narrowing the generality of lesson learned in this field at getjar our goal is to make appealing recommendation of mobile application apps for app usage we observe a distribution that ha higher kurtosis heavier head and longer tail than that for the aforementioned movie datasets this happens primarily because of the large disparity in resource available to app developer and the low cost of app publication relative to movie in this paper we compare a latent factor puresvd and a memory based model with our novel pca based model which we call eigenapp we use both accuracy and variety a evaluation metric puresvd did not perform well due to it reliance on explicit feedback such a rating which we do not have memory based approach that perform vector operation in the original high dimensional space over predict popular apps because they fail to capture the neighborhood of le popular apps they have high accuracy due to the concentration of mass in the head but did poorly in term of variety of apps exposed eigenapp which exploit neighborhood information in low dimensional space did well both on precision and variety underscoring the importance of dimensionality reduction to form quality neighborhood in high kurtosis distribution 
we present a method for efficiently training binary and multiclass kernelized svms on a graphic processing unit gpu our method apply to a broad range of kernel including the popular gaussian kernel on datasets a large a the amount of available memory on the graphic card our approach is distinguished from earlier work in that it cleanly and efficiently handle sparse datasets through the use of a novel clustering technique our optimization algorithm is also specifically designed to take advantage of the graphic hardware this lead to different algorithmic choice then those preferred in serial implementation our easy to use library is order of magnitude faster then existing cpu library and several time faster than prior gpu approach 
multi instance multi label learning miml is a framework for supervised classification where the object to be classified are bag of instance associated with multiple label for example an image can be represented a a bag of segment and associated with a list of object it contains prior work on miml ha focused on predicting label set for previously unseen bag we instead consider the problem of predicting instance label while learning from data labeled only at the bag level we propose rank loss support instance machine which optimize a regularized rank loss objective and can be instantiated with different aggregation model connecting instance level prediction with bag level prediction the aggregation model that we consider are equivalent to defining a support instance for each bag which allows efficient optimization of the rank loss objective using primal sub gradient descent experiment on artificial and real world datasets show that the proposed method achieve higher accuracy than other loss function used in prior work e g hamming loss and recent work in ambiguous label classification 
the moneyball revolution coincided with a shift in the way professional sporting organization handle and utilize data in term of decision making process due to the demand for better sport analytics and the improvement in sensor technology there ha been a plethora of ball and player tracking information generated within professional sport for analytical purpose however due to the continuous nature of the data and the lack of associated high level label to describe it this rich set of information ha had very limited use especially in the analysis of a team s tactic and strategy in this paper we give an overview of the type of analysis currently performed mostly with hand labeled event data and highlight the problem associated with the influx of spatiotemporal data by way of example we present an approach which us an entire season of ball tracking data from the english premier league season to reinforce the common held belief that team should aim to win home game and draw away one we do this by i forming a representation of team behavior by chunking the incoming spatiotemporal signal into a series of quantized bin and ii generate an expectation model of team behavior based on a code book of past performance we show that home advantage in soccer is partly due to the conservative strategy of the away team we also show that our approach can flag anomalous team behavior which ha many potential application 
road surface skid resistance ha been shown to have a strong relationship to road crash risk however applying the current method of using investigatory level to identify crash prone road is problematic a they may fail in identifying risky road outside of the norm the proposed method analysis a complex and formerly impenetrable volume of data from road and crash using data mining this method rapidly identifies road with elevated crash rate potentially due to skid resistance deficit for investigation a hypothetical skid resistance crash risk curve is developed for each road segment driven by the model deployed in a novel regression tree extrapolation method the method potentially solves the problem of missing skid resistance value which occurs during network wide crash analysis and allows risk assessment of the major proportion of road without skid resistance value 
this paper address estimating the number of the user of a specific application behind ip address ip this problem is central to combating abusive traffic such a ddos attack ad click fraud and email spam we share our experience building a general framework at google for estimating the number of user behind ip called hereinafter the size of the ip the primary goal of this framework is combating abusive traffic without violating the user privacy the estimation technique produce statistically sound estimate of size relying solely on passively mining aggregated application log data without probing machine or deploying active content like java applet this paper also explores using the estimated size to detect and filter abusive traffic the proposed framework wa used to build and deploy an ad click fraud filter at google the first m click tagged by the filter had a significant recall of all tagged click and their false positive rate wa below for the sake of comparison we simulated a naive ip based filter that doe not consider the size of the ip to reach a comparable recall the naive filter s false positive rate wa due to aggressive tagging 
many online experiment exhibit dependence between user and item for example in online advertising observation that have a user or an ad in common are likely to be associated because of this even in experiment involving million of subject the difference in mean outcome between control and treatment condition can have substantial variance previous theoretical and simulation result demonstrate that not accounting for this kind of dependence structure can result in confidence interval that are too narrow leading to inaccurate hypothesis test we develop a framework for understanding how dependence affect uncertainty in user item experiment and evaluate how bootstrap method that account for differing level of dependence perform in practice we use three real datasets describing user behavior on facebook user response to ad search result and news feed story to generate data for synthetic experiment in which there is no effect of the treatment on average by design we then estimate empirical type i error rate for each bootstrap method accounting for dependence within a single type of unit i e within user dependence is often sufficient to get reasonable error rate but when experiment have effect a one might expect in the field accounting for multiple unit with a multiway bootstrap can be necessary to get close to the advertised type i error rate this work provides guidance to practitioner evaluating large scale experiment and highlight the importance of analysis of inferential method for complex dependence structure common to online experiment 
along with the development of web application social medium service ha attracted many user and become their hand on toolkits for recording life sharing idea and social networking though social medium service are essentially web or mobile application and service they combine user generated content and social network together so that information can be created transmitted transformed and consumed in the cyberspace thus social medium somehow could be regarded a a kind of sensor to the real life of it user in general the data from social medium is of low quality piece of information in social medium are usually short with informal presentation and in some specific context that is highly related to the physical world therefore it is challenging to extract semantics from social medium data however we argue that given sufficient social medium data user collective behavior could be sensed studied and even predicted in a certain circumstance our study is conducted on data from two service i e twitter and sina weibo the most popular microblogging service all over the world and in china respectively collective behavior are action of a large amount of various people which are neither conforming nor deviant various collective behavior are studied in the context of social medium our study show that there are various information flow pattern in social medium some of which are similar to traditional medium such a newspaper while others are embedded deep in the social network structure the evolution of hotspot is highly affected by external stimulation the social network structure and individual user s activity furthermore social medium tends to be immune to some repeated similar external stimulation last but not the least there is considerable difference in user behavior between twitter and sina weibo 
determining anomaly in data stream that are collected and transformed from various type of network ha recently attracted significant research interest principal component analysis pca ha been extensively applied to detecting anomaly in network data stream however none of existing pca based approach address the problem of identifying the source that contribute most to the observed anomaly or anomaly localization in this paper we propose novel sparse pca method to perform anomaly detection and localization for network data stream our key observation is that we can localize anomaly by identifying a sparse low dimensional space that capture the abnormal event in data stream to better capture the source of anomaly we incorporate the structure information of the network stream data in our anomaly localization framework we have performed comprehensive experimental study of the proposed method and have compared our method with the state ofthe art using three real world data set from different application domain our experimental study demonstrate the utility of the proposed method 
with the popularity of online social network service influence maximization on social network ha drawn much attention in recent year most of these study approximate a greedy based sub optimal solution by proving the submodular nature of the utility function instead of using the analytical technique we are interested in solving the diffusion competition and influence maximization problem by a data driven approach we propose information propagation game ipg a framework that can collect a large number of seed picking strategy for analysis through the ipg framework human player are not only having fun but also helping contributing the seed picking strategy preliminary experiment suggests that centrality based heuristic are too simple for seed selection in a multiple player environment 
meaningful work is a deep human need we all yearn to contribute to something greater than ourselves be listened to and work alongside friendly peer data mining consulting is a powerful way to use technical skill and gain these great side benefit the power of analytics and it high return on investment make one s expertise welcome virtually everywhere and the variety of project and domain encountered lead to continual learning a new problem are met and solved teaching and writing are possible and there is great satisfaction in seeing one s work actually implemented and used potentially touching million still in industry one ha the joy and hazard of working closely with other human where final success can depend a much on others a oneself and on social a well a technical issue in my experience business risk strongly outweighs technical risk in whether a solution is used i will share some hard won lesson learned on how to best succeed both technically and socially in the result oriented world of industry 
in this demo we present amethyst a system for exploring and analyzing a topical hierarchy constructed from a heterogeneous information network hin hin composed of multiple type of entity and link are very common in the real world many have a text component and thus can benefit from a high quality hierarchical organization of the topic in the network dataset by organizing the topic into a hierarchy amethyst help understand search result in the context of an ontology and explain entity relatedness at different granularity the automatically constructed topical hierarchy reflects a domain specific ontology interacts with multiple type of linked entity and can be tailored for both free text and olap query 
we present a framework for interactive visual pattern mining our system enables the user to browse through the data and pattern easily and intuitively using a toolbox consisting of interestingness measure mining algorithm and post processing algorithm to assist in identifying interesting pattern by mining interactively we enable the user to combine their subjective interestingness measure and background knowledge with a wide variety of objective measure to easily and quickly mine the most important and interesting pattern basically we enable the user to become an essential part of the mining algorithm our demo currently applies to mining interesting itemsets and association rule and it extension to episode and decision tree is ongoing 
in this paper we consider the problem of extracting structured data from web page taking into account both the content of individual attribute a well a the structure of page and site we use markov logic network mlns to capture both content and structural feature in a single unified framework and this enables u to perform more accurate inference we show that inference in our information extraction scenario reduces to solving an instance of the maximum weight subgraph problem we develop specialized procedure for solving the maximum subgraph variant that are far more efficient than previously proposed inference method for mlns that solve variant of max sat experiment with real life datasets demonstrate the effectiveness of our approach 
improving the performance of classifier using pattern mining technique ha been an active topic of data mining research in this work we introduce the recent temporal pattern mining framework for finding predictive pattern for monitoring and event detection problem in complex multivariate time series data this framework first convert time series into time interval sequence of temporal abstraction it then construct more complex temporal pattern backwards in time using temporal operator we apply our framework to health care data of diabetic patient and show it benefit by efficiently finding useful pattern for detecting and diagnosing adverse medical condition that are associated with diabetes 
topic model have played a pivotal role in analyzing large collection of complex data besides discovering latent semantics supervised topic model stm can make prediction on unseen test data by marrying with advanced learning technique the predictive strength of stm have been dramatically enhanced such a max margin supervised topic model state of the art method that integrate max margin learning with topic model though powerful max margin stm have a hard non smooth learning problem existing algorithm rely on solving multiple latent svm subproblems in an em type procedure which can be too slow to be applicable to large scale categorization task in this paper we present a highly scalable approach to building max margin supervised topic model our approach build on three key innovation a new formulation of gibbs max margin supervised topic model for both multi class and multi label classification a simple augment and collapse gibbs sampling algorithm without making restricting assumption on the posterior distribution an efficient parallel implementation that can easily tackle data set with hundred of category and million of document furthermore our algorithm doe not need to solve svm subproblems though performing the two task of topic discovery and learning predictive model jointly which significantly improves the classification performance our method have comparable scalability a the state of the art parallel algorithm for the standard lda topic model which perform the single task of topic discovery only finally an open source implementation is also provided at http www ml thu net jun medlda 
the detection of outlier in spatio temporal traffic data is an important research problem in the data mining and knowledge discovery community however to the best of our knowledge the discovery of relationship especially causal interaction among detected traffic outlier ha not been investigated before in this paper we propose algorithm which construct outlier causality tree based on temporal and spatial property of detected outlier frequent substructure of these causality tree reveal not only recurring interaction among spatio temporal outlier but potential flaw in the design of existing traffic network the effectiveness and strength of our algorithm are validated by experiment on a very large volume of real taxi trajectory in an urban road network 
in an interconnected and dynamic world the evolution of one entity may cause a series of significant value change for some others for example the currency inflation of thailand caused the currency slump of other asian country which eventually led to the financial crisis of we call such high impact entity shaker to discover shaker we first introduce the concept of a cascading graph to capture the causality relationship among evolving entity over some period of time and then infer shaker from the graph in a cascading graph node represent entity and weighted link represent the causality effect in order to find hidden shaker in such a graph two scoring function are proposed each of which estimate how much the target entity can affect the value of some others the idea is to artificially inject a significant change on the target entity and estimate it direct and indirect influence on the others by following an inference rule under the markovian assumption both scoring function are proven to be only dependent on the structure of a cascading graph and can be calculated in polynomial time experiment included three datasets in social science without directly applicable previous method we modified three graphical model a baseline the two proposed scoring function can effectively capture those high impact entity for example in the experiment to discover stock market shaker the proposed model outperform the three baseline by a much a in accuracy with the ground truth obtained from yahoo finance 
in any competitive business success is based on the ability to make an item more appealing to customer than the competition a number of question arise in the context of this task how do we formalize and quantify the competitiveness relationship between two item who are the true competitor of a given item what are the feature of an item that most affect it competitiveness despite the impact and relevance of this problem to many domain only a limited amount of work ha been devoted toward an effective solution in this paper we present a formal definition of the competitiveness between two item we present efficient method for evaluating competitiveness in large datasets and address the natural problem of finding the top k competitor of a given item our methodology is evaluated against strong baseline via a user study and experiment on multiple datasets from different domain 
historical user activity is key for building user profile to predict the user behavior and affinity in many web application such a targeting of online advertising content personalization and social recommendation user profile are temporal and change in a user s activity pattern are particularly useful for improved prediction and recommendation for instance an increased interest in car related web page may well suggest that the user might be shopping for a new vehicle in this paper we present a comprehensive statistical framework for user profiling based on topic model which is able to capture such effect in a fully emph unsupervised fashion our method model topical interest of a user dynamically where both the user association with the topic and the topic themselves are allowed to vary over time thus ensuring that the profile remain current we describe a streaming distributed inference algorithm which is able to handle ten of million of user our result show that our model contributes towards improved behavioral targeting of display advertising relative to baseline model that do not incorporate topical and or temporal dependency a a side effect our model yield human understandable result which can be used in an intuitive fashion by advertiser 
motor prosthesis aim to restore function to disabled patient despite compelling proof of concept system barrier to clinical translation remain one challenge is to develop a low power fully implantable system that dissipates only minimal power so a not to damage tissue to this end we implemented a kalman filter based decoder via a spiking neural network snn and tested it in brain machine interface bmi experiment with a rhesus monkey the kalman filter wa trained to predict the arm s velocity and mapped on to the snn using the neural engineering framework nef a neuron embedded matlab snn implementation run in real time and it closed loop performance is quite comparable to that of the standard kalman filter the success of this closed loop decoder hold promise for hardware snn implementation of statistical signal processing algorithm on neuromorphic chip which may offer power saving necessary to overcome a major obstacle to the successful clinical translation of neural motor prosthesis 
a large portion of real world data is either text or structured e g relational data moreover such data object are often linked together e g structured specification of product linking with the corresponding product description and customer comment even for text data such a news data typed entity can be extracted with entity extraction tool the eventcube project construct textcube and topiccube from interconnected structured and text data or from text data via entity extraction and dimension building and performs multidimensional search and analysis on such datasets in an informative powerful and user friendly manner this proposed eventcube demo will show the power of the system not only on the originally designed asrs aviation safety report system data set but also on news datasets collected from multiple news agency and academic datasets constructed from the dblp and web data the system ha high potential to be extended in many powerful way and serve a a general platform for search olap online analytical processing and data mining on integrated text and structured data after the system demo in the conference the system will be put on the web for public access and evaluation 
complex model for regression and classification have high accuracy but are unfortunately no longer interpretable by user we study the performance of generalized additive model gam which combine single feature model called shape function through a linear function since the shape function can be arbitrarily complex gam are more accurate than simple linear model but since they do not contain any interaction between feature they can be easily interpreted by user we present the first large scale empirical comparison of existing method for learning gam our study includes existing spline and tree based method for shape function and penalized least square gradient boosting and backfitting for learning gam we also present a new method based on tree ensemble with an adaptive number of leaf that consistently outperforms previous work we complement our experimental result with a bias variance analysis that explains how different shape model influence the additive model our experiment show that shallow bagged tree with gradient boosting distinguish itself a the best method on lowto medium dimensional datasets 
non negative matrix factorization nmf provides a lower rank approximation of a matrix due to nonnegativity imposed on the factor it give a latent structure that is often more physically meaningful than other lower rank approximation such a singular value decomposition svd most of the algorithm proposed in literature for nmf have been based on minimizing the frobenius norm this is partly due to the fact that the minimization problem based on the frobenius norm provides much more flexibility in algebraic manipulation than other divergence in this paper we propose a fast nmf algorithm that is applicable to general bregman divergence through taylor series expansion of the bregman divergence we reveal a relationship between bregman divergence and euclidean distance this key relationship provides a new direction for nmf algorithm with general bregman divergence when combined with the scalar block coordinate descent method the proposed algorithm generalizes several recently proposed method for computation of nmf with bregman divergence and is computationally faster than existing alternative we demonstrate the effectiveness of our approach with experiment conducted on artificial a well a real world data 
we present a bayesian scheme for the approximate diagonalisation of several square matrix which are not necessarily symmetric a gibbs sampler is derived to simulate sample of the common eigenvectors and the eigenvalue for these matrix several synthetic example are used to illustrate the performance of the proposed gibbs sampler and we then provide comparison to several other joint diagonalization algorithm which show that the gibbs sampler achieves the state of theart performance on the example considered a a byproduct the output of the gibbs sampler could be used to estimate the log marginal likelihood however we employ the approximation based on the bayesian information criterion bic which in the synthetic example considered correctly located the number of common eigenvectors we then succesfully applied the sampler to the source separation problem a well a the common principal component analysis and the common spatial pattern analysis problem copyright by the author s owner s 
unlike existing nonparametric bayesian model which rely solely on specially conceived prior to incorporate domain knowledge for discovering improved latent representation we study nonparametric bayesian inference with regularization on the desired posterior distribution while prior can indirectly affect posterior distribution through bayes theorem imposing posterior regularization is arguably more direct and in some case can be much easier we particularly focus on developing infinite latent support vector machine ilsvm and multi task infinite latent support vector machine mt ilsvm which explore the largemargin idea in combination with a nonparametric bayesian model for discovering predictive latent feature for classification and multi task learning respectively we present efficient inference method and report empirical study on several benchmark datasets our result appear to demonstrate the merit inherited from both large margin learning and bayesian nonparametrics 
the development of a city gradually foster different functional region such a educational area and business district in this paper we propose a framework titled drof that discovers region of different function in a city using both human mobility among region and point of interest poi located in a region specifically we segment a city into disjointed region according to major road such a highway and urban express way we infer the function of each region using a topic based inference model which regard a region a a document a function a a topic category of poi e g restaurant and shopping mall a metadata like author affiliation and key word and human mobility pattern when people reach leave a region and where people come from and leave for a word a a result a region is represented by a distribution of function and a function is featured by a distribution of mobility pattern we further identify the intensity of each function in different location the result generated by our framework can benefit a variety of application including urban planning location choosing for a business and social recommendation we evaluated our method using large scale and real world datasets consisting of two poi datasets of beijing in and and two month gps trajectory datasets representing human mobility generated by over taxicab in beijing in and respectively the result justify the advantage of our approach over baseline method solely using poi or human mobility 
opinionated social medium such a product review are now widely used by individual and organization for their decision making however due to the reason of profit or fame people try to game the system by opinion spamming e g writing fake review to promote or to demote some target product in recent year fake review detection ha attracted significant attention from both the business and research community however due to the difficulty of human labeling needed for supervised learning and evaluation the problem remains to be highly challenging this work proposes a novel angle to the problem by modeling spamicity a latent an unsupervised model called author spamicity model asm is proposed it work in the bayesian setting which facilitates modeling spamicity of author a latent and allows u to exploit various observed behavioral footprint of reviewer the intuition is that opinion spammer have different behavioral distribution than non spammer this creates a distributional divergence between the latent population distribution of two cluster spammer and non spammer model inference result in learning the population distribution of the two cluster several extension of asm are also considered leveraging from different prior experiment on a real life amazon review dataset demonstrate the effectiveness of the proposed model which significantly outperform the state of the art competitor 
we present a novel learning algorithm directrank which directly and exactly optimizes ranking measure without resorting to any upper bound or approximation our approach is essentially an iterative coordinate ascent method in each iteration we choose one coordinate and only update the corresponding parameter with all others remaining fixed since the ranking measure is a stepwise function of a single parameter we propose a novel line search algorithm that can locate the interval with the best ranking measure along this coordinate quite efficiently in order to stabilize our system in small datasets we construct a probabilistic framework for document query pair to maximize the likelihood of the objective permutation of top tau document this iterative procedure ensures convergence furthermore we integrate regression tree a our weak learner in order to consider the correlation between the different feature experiment on letor datasets and two large datasets yahoo challenge data and microsoft k web data show an improvement over state of the art system 
in recent year graph kernel have received considerable interest within the machine learning and data mining community here we introduce a novel approach enabling kernel method to utilize additional information hidden in the structural neighborhood of the graph under consideration our novel structural cluster kernel sck incorporates similarity induced by a structural clustering algorithm to improve state of the art graph kernel the approach taken is based on the idea that graph similarity can not only be described by the similarity between the graph themselves but also by the similarity they posse with respect to their structural neighborhood we applied our novel kernel in a supervised and a semi supervised setting to regression and classification problem on a number of real world datasets of molecular graph our result show that the structural cluster similarity information can indeed leverage the prediction performance of the base kernel particularly when the dataset is structurally sparse and consequently structurally diverse by additionally taking into account a large number of unlabeled instance the performance of the structural cluster kernel can further be improved 
the volume of web video have increased sharply through the past several year because of the evolvement of web video site enhanced algorithm on retrieval classification and tdt abbreviation of topic detection and tracking can bring lot of convenience to web user a well a release tedious work from the administrator nevertheless due to the the insufficiency of annotation keywords and the gap between video feature and semantic concept it is still far away from satisfactory to implement them based on initial keywords and visual feature in this paper we utilize a keyword propagation algorithm based on manifold structure to enrich the keyword information and remove the noise for video both text similarity and temporal similarity are employed to explore the relationship between any pair of video and to construct the propagation model we explore three application i e tdt retrieval and classification based on a web news video dataset obtained from a famous online video distributing website youku and evaluate our approach experimental result demonstrate that they achieve satisfactory performance and always outperform the baseline method 
most traditional supervised learning method are developed to learn a model from labeled example and use this model to classify the unlabeled one into the same label space predefined by the model however in many real world application the label space for both the labeled training and unlabeled testing example can be different to solve this problem this paper proposes a novel notion of serendipitous learning sl which is defined to address the learning scenario in which the label space can be enlarged during the testing phase in particular a large margin approach is proposed to solve sl the basic idea is to leverage the knowledge in the labeled example to help identify novel unknown class and the large margin formulation is proposed to incorporate both the classification loss on the example within the known category a well a the clustering loss on the example in unknown category an efficient optimization algorithm based on cccp and the bundle method is proposed to solve the optimization problem of the large margin formulation of sl moreover an efficient online learning method is proposed to address the issue of large scale data in online learning scenario which ha been shown to have a guaranteed learning regret an extensive set of experimental result on two synthetic datasets and two datasets from real world application demonstrate the advantage of the proposed method over several other baseline algorithm one limitation of the proposed method is that the number of unknown class is given in advance it may be possible to remove this constraint if we model it by using a non parametric way we also plan to do experiment on more real world application in the future 
many data are modeled a tensor or multi dimensional array example include the predicate subject verb object in knowledge base hyperlink and anchor text in the web graph sensor stream time location and type social network over time and dblp conference author keyword relation tensor decomposition is an important data mining tool with various application including clustering trend detection and anomaly detection however current tensor decomposition algorithm are not scalable for large tensor with billion of size and hundred million of nonzeros the largest tensor in the literature remains thousand of size and hundred thousand of nonzeros consider a knowledge base tensor consisting of about million noun phrase the intermediate data explosion problem associated with naive implementation of tensor decomposition algorithm would require the materialization and the storage of a matrix whose largest dimension would be x this amount to petabyte or equivalently a few data center worth of storage thereby rendering the tensor analysis of this knowledge base in the naive way practically impossible in this paper we propose gigatensor a scalable distributed algorithm for large scale tensor decomposition gigatensor exploit the sparseness of the real world tensor and avoids the intermediate data explosion problem by carefully redesigning the tensor decomposition algorithm extensive experiment show that our proposed gigatensor solves time bigger problem than existing method furthermore we employ gigatensor in order to analyze a very large real world knowledge base tensor and present our astounding finding which include discovery of potential synonym among million of noun phrase e g the noun pollutant and the noun phrase greenhouse gas 
this paper show how coupling from the past cftp can be used to avoid time and memory bottleneck in direct local pattern sampling procedure such procedure draw controlled amount of suitably biased sample directly from the pattern space of a given dataset in polynomial time previous direct pattern sampling method can produce pattern in rapid succession after some initial preprocessing phase this preprocessing phase however turn out to be prohibitive in term of time and memory for many datasets we show how cftp can be used to avoid any super linear preprocessing and memory requirement this allows to simulate more complex distribution which previously were intractable we show for a large number of public real world datasets that these new algorithm are fast to execute and their pattern collection outperform previous approach both in unsupervised a well a supervised context 
user review is a crucial component of open mobile app market such a the google play store how do we automatically summarize million of user review and make sense out of them unfortunately beyond simple summary such a histogram of user rating there are few analytic tool that can provide insight into user review in this paper we propose wiscom a system that can analyze ten of million user rating and comment in mobile app market at three different level of detail our system is able to a discover inconsistency in review b identify reason why user like or dislike a given app and provide an interactive zoomable view of how user review evolve over time and c provide valuable insight into the entire app market identifying user major concern and preference of different type of apps result using our technique are reported on a gb dataset consisting of over million user review of android apps in the google play store we discus how the technique presented herein can be deployed to help a mobile app market operator such a google a well a individual app developer and end user 
due to increased adoption of digital inclusion in various business location based service are gaining importance to provide value added service for their customer in this work we present a computer vision based system for tracking customer location by recognizing individual shopping cart inside shopping mall in order to facilitate location based service we provide an efficient approach for cart recognition that consists of two stage cart detection and then cart recognition a binary pattern is placed between two pre defined color marker and attached to each cart for recognition the system take live video feed a input from the camera mounted on the aisle of the shopping mall and process frame in real time in the cart detection stage color segmentation feature extraction and classification are used for detection of binary pattern along with color marker in recognition stage segmented binary strip is processed using spatial image processing technique to decode the cart identification number 
the low rank regression model ha been studied and applied to capture the underlying class task correlation pattern such that the regression classification result can be enhanced in this paper we will prove that the low rank regression model is equivalent to doing linear regression in the linear discriminant analysis lda subspace our new theory reveals the learning mechanism of low rank regression and show that the low rank structure exacted from class task are connected to the lda projection result thus the low rank regression efficiently work for the high dimensional data moreover we will propose new discriminant low rank ridge regression and sparse low rank regression method both of them are equivalent to doing regularized regression in the regularized lda subspace these new regularized objective provide better data mining result than existing low rank regression in both theoretical and empirical validation we evaluate our discriminant low rank regression method by six benchmark datasets in all empirical result our discriminant low rank model consistently show better result than the corresponding full rank method 
we improve the theoretical analysis and empirical performance of algorithm for the stochastic multi armed bandit problem and the linear stochastic multi armed bandit problem in particular we show that a simple modification of auer s ucb algorithm auer achieves with high probability constant regret more importantly we modify and consequently improve the analysis of the algorithm for the for linear stochastic bandit problem studied by auer dani et al rusmevichientong and tsitsiklis li et al our modification improves the regret bound by a logarithmic factor though experiment show a vast improvement in both case the improvement stem from the construction of smaller confidence set for their construction we use a novel tail inequality for vector valued martingale 
this paper study the problem of semi supervised learning from the vector field perspective many of the existing work use the graph laplacian to ensure the smoothness of the prediction function on the data manifold however beyond smoothness it is suggested by recent theoretical work that we should ensure second order smoothness for achieving faster rate of convergence for semisupervised regression problem to achieve this goal we show that the second order smoothness measure the linearity of the function and the gradient field of a linear function ha to be a parallel vector field consequently we propose to find a function which minimizes the empirical error and simultaneously requires it gradient field to be a parallel a possible we give a continuous objective function on the manifold and discus how to discretize it by using random point the discretized optimization problem turn out to be a sparse linear system which can be solved very efficiently the experimental result have demonstrated the effectiveness of our proposed approach 
graph appear in numerous application including cyber security the internet social network protein network recommendation system and many more graph with million or even billion of node and edge are common place how to store such large graph efficiently what are the core operation query on those graph how to answer the graph query quickly we propose gbase a scalable and general graph management and mining system the key novelty lie in our storage and compression scheme for a parallel setting and the carefully chosen graph operation and their efficient implementation we designed and implemented an instance of gbase using mapreduce hadoop gbase provides a parallel indexing mechanism for graph mining operation that both save storage space a well a accelerates query we ran numerous experiment on real graph spanning billion of node and edge and we show that our proposed gbase is indeed fast scalable and nimble with significant saving in space and time 
many real world data mining application need varying cost for different type of classification error and thus call for cost sensitive classification algorithm existing algorithm for cost sensitive classification are successful in term of minimizing the cost but can result in a high error rate a the trade off the high error rate hold back the practical use of those algorithm in this paper we propose a novel cost sensitive classification methodology that take both the cost and the error rate into account the methodology called soft cost sensitive classification is established from a multicriteria optimization problem of the cost and the error rate and can be viewed a regularizing cost sensitive classification with the error rate the simple methodology allows immediate improvement of existing cost sensitive classification algorithm experiment on the benchmark and the real world data set show that our proposed methodology indeed achieves lower test error rate and similar sometimes lower test cost than existing cost sensitive classification algorithm 
we apply the recently proposed context dependent deep neural network hmms or cd dnn hmms to speech to text transcription for single pas speaker independent recognition on the rt s fisher portion of phone call transcription benchmark switchboard the word error rate is reduced from obtained by discriminatively trained gaussian mixture hmms to a relative improvement cd dnn hmms combine classic artificial neural network hmms with traditional tied state triphones and deep beliefnetwork pre training they had previously been shown to reduce error by relatively when trained on ten of hour of data using hundred of tied state this paper take cd dnnhmms further and applies them to transcription using over hour of training data over tied state and up to hidden layer and demonstrates how sparseness can be exploited on four le well matched transcription task we observe relative error reduction of copyright isca 
telecom bi business intelligence system consists of a set of application program and technology for gathering storing analyzing and providing access to data which contribute to manage business information and make decision precisely however traditional analysis algorithm meet new challenge a the continued exponential growth in both the volume and the complexity of telecom data with the cloud computing development some parallel data analysis system have been emerging however existing system have rarely comprehensive function either providing data analysis service or providing social network analysis we need a comprehensive tool to store and analysis large scale data efficiently in response to the challenge the saas software a a service bi system bc pdm big cloud parallel data mining are proposed bc pdm support parallel etl process statistical analysis data mining text mining and social network analysis which are based on hadoop this demo introduces three task business recommendation customer community detection and user preference classification by employing a real telecom data set experimental result show bc pdm is very efficient and effective for intelligence data analysis 
many different machine learning algorithm exist taking into account each algorithm s hyperparameters there is a staggeringly large number of possible alternative overall we consider the problem of simultaneously selecting a learning algorithm and setting it hyperparameters going beyond previous work that attack these issue separately we show that this problem can be addressed by a fully automated approach leveraging recent innovation in bayesian optimization specifically we consider a wide range of feature selection technique combining search and evaluator method and all classification approach implemented in weka s standard distribution spanning ensemble method meta method base classifier and hyperparameter setting for each classifier on each of popular datasets from the uci repository the kdd cup variant of the mnist dataset and cifar we show classification performance often much better than using standard selection and hyperparameter optimization method we hope that our approach will help non expert user to more effectively identify machine learning algorithm and hyperparameter setting appropriate to their application and hence to achieve improved performance 
intelligence analyst grapple with many challenge chief among them is the need for software support in storytelling i e automatically connecting the dot between disparate entity e g people organization in an effort to form hypothesis and suggest non obvious relationship we present a system to automatically construct story in entity network that can help form directed chain of relationship with support for co referencing evidence marshaling and imposing syntactic constraint on the story generation process a novel optimization technique based on concept lattice mining enables u to rapidly construct story on massive datasets using several public domain datasets we illustrate how our approach overcomes many limitation of current system and enables the analyst to efficiently narrow down to hypothesis of interest and reason about alternative explanation 
we pose the problem of network discovery which involves simplifying spatio temporal data into cohesive region node and relationship between those region edge such problem naturally exist in fmri scan of human subject these scan consist of activation of thousand of voxels over time with the aim to simplify them into the underlying cognitive network being used we propose supervised and semi supervised variation of this problem and postulate a constrained tensor decomposition formulation and a corresponding alternating least square solver that is easy to implement we show this formulation work well in controlled experiment where supervision is incomplete superfluous and noisy and is able to recover the underlying ground truth network we then show that for real fmri data our approach can reproduce well known result in neurology regarding the default mode network in resting state healthy and alzheimer affected individual finally we show that the reconstruction error of the decomposition provides a useful measure of the network strength and is useful at predicting key cognitive score both by itself and with clinical information 
in this work we demonstrate a web application available at http d index di unito it that permit to analyze the scientific profile of all the researcher indexed by dblp by focusing on the collaboration that contributed to define their curriculum the presented application allows the user to analyze the profile of a researcher her dependence degree on all the co author along her entire scientific publication history and to make comparison among them in term of dependence pattern in particular it is possible to estimate and visualize how much a researcher ha benefited from collaboration with another researcher a well a the community in which she ha been involved moreover the application permit to compare in a single chart each researcher with all the scientist indexed in dblp by focusing on their dependence with respect to many other parameter like the total number of paper the number of collaboration and the length of the scientific career 
the department store retailer john wanamaker famously stated half the money i spend on advertising is wasted i just don t know which half compared with the measurement of advertising effectiveness in traditional medium online advertiser and publisher have considerable data advantage including individual level data on advertising exposure click search and other online user behavior however a i shall discus in this talk the science of advertising effectiveness requires more than just quantity of data even more important is the quality of the data in particular in many case using various statistical technique with observational data lead to incorrect measurement to measure the true causal effect we run controlled experiment that suppress advertising to a control group much like the placebo in a drug trial with experiment to determine the ground truth we can show that in many circumstance observational data technique rely on identifying assumption that prove to be incorrect and they produce estimate differing wildly from the truth despite increase in data availability wanamaker s complaint remains just a true for online advertising a it wa for print advertising a century ago in this talk i will discus recent advance in running randomized experiment online measuring the impact of online display advertising on consumer behavior interesting result include the measurable effect of online advertising on offline transaction the impact on viewer who do not click the ad the surprisingly large effect of frequency of exposure and the heterogeneity of advertising effectiveness across user in different demographic group or geographic location i also show that sample size of a million or more customer may be necessary to get enough precision for statistical significance of economically important effect so we have just reached the cusp of being able to measure effect precisely with present technology by comparison previous controlled experiment using split cable tv system with sample size in the mere thousand have lacked statistical power to measure precise effect for a given campaign a i show with several example that establish the ground truth using controlled experiment the bias in observational study can be extremely large over or underestimating the true causal effect by an order of magnitude i will discus the implicit or explicit modeling assumption made by researcher using observational data and identify several reason why these assumption are violated in practice i will also discus future direction in using experiment to measure advertising effectiveness 
total variation tv regularization ha important application in signal processing including image denoising image deblurring and image reconstruction a significant challenge in the practical use of tv regularization lie in the nondifferentiable convex optimization which is difficult to solve especially for large scale problem in this paper we propose an efficient alternating augmented lagrangian method admm to solve total variation regularization problem the proposed algorithm is applicable for tensor thus it can solve multidimensional total variation regularization problem one appealing feature of the proposed algorithm is that it doe not need to solve a linear system of equation which is often the most expensive part in previous admm based method in addition each step of the proposed algorithm involves a set of independent and smaller problem which can be solved in parallel thus the proposed algorithm scale to large size problem furthermore the global convergence of the proposed algorithm is guaranteed and the time complexity of the proposed algorithm is o dn on a d mode tensor with n entry for achieving an optimal solution extensive experimental result demonstrate the superior performance of the proposed algorithm in comparison with current state of the art method 
the key algorithmic problem in viral marketing is to identify a set of influential user called seed in a social network who when convinced to adopt a product shall influence other user in the network leading to a large number of adoption when two or more player compete with similar product on the same network we talk about competitive viral marketing which so far ha been studied exclusively from the perspective of one of the competing player in this paper we propose and study the novel problem of competitive viral marketing from the perspective of the host i e the owner of the social network platform the host sell viral marketing campaign a a service to it customer keeping control of the selection of seed each company specifies it budget and the host allocates the seed accordingly from the host s perspective it is important not only to choose the seed to maximize the collective expected spread but also to assign seed to company so that it guarantee the bang for the buck for all company is nearly identical which we formalize a the fair seed allocation problem we propose a new propagation model capturing the competitive nature of viral marketing our model is intuitive and retains the desired property of monotonicity and submodularity we show that the fair seed allocation problem is np hard and develop an efficient algorithm called needy greedy we run experiment on three real world social network showing that our algorithm is effective and scalable 
selective sampling is an active variant of online learning in which the learner is allowed to adaptively query the label of an observed example the goal of selective sampling is to achieve a good trade off between prediction performance and the number of queried label existing selective sampling algorithm are designed for vector based data in this paper motivated by the ubiquity of graph representation in real world application we propose to study selective sampling on graph we first present an online version of the well known learning with local and global consistency method ollgc it is essentially a second order online learning algorithm and can be seen a an online ridge regression in the hilbert space of function defined on graph we prove it regret bound in term of the structural property cut size of a graph based on ollgc we present a selective sampling algorithm namely selective sampling with local and global consistency sslgc which query the label of each node based on the confidence of the linear function on graph it bound on the label complexity is also derived we analyze the low rank approximation of graph kernel which enables the online algorithm scale to large graph experiment on benchmark graph datasets show that ollgc outperforms the state of the art first order algorithm significantly and sslgc achieves comparable or even better result than ollgc while querying substantially fewer node moreover sslgc is overwhelmingly better than random sampling 
influence driven diffusion of information is a fundamental process in social network learning the latent variable of such process i e the influence strength along each link is a central question towards understanding the structure and function of complex network modeling information cascade and developing application such a viral marketing motivated by modern microblogging platform such a twitter in this paper we study the problem of learning influence probability in a data stream scenario in which the network topology is relatively stable and the challenge of a learning algorithm is to keep up with a continuous stream of tweet using a small amount of time and memory our contribution is a number of randomized approximation algorithm categorized according to the available space superlinear linear and sublinear in the number of node n and according to different model landmark and sliding window among several result we show that we can learn influence probability with one pas over the data using o nlog n space in both the landmark model and the sliding window model and we further show that our algorithm is within a logarithmic factor of optimal for truly large graph when one need to operate with sublinear space we show that we can still learn influence probability in one pas assuming that we restrict our attention to the most active user our thorough experimental evaluation on large social graph demonstrates that the empirical performance of our algorithm agrees with that predicted by the theory 
due to their damage to internet security malware such a virus worm trojan spyware backdoor and rootkits detection ha caught the attention not only of anti malware industry but also of researcher for decade resting on the analysis of file content extracted from the file sample like application programming interface api call instruction sequence and binary string data mining method such a naive bayes and support vector machine have been used for malware detection however besides file content relation among file sample such a a downloader is always associated with many trojan can provide invaluable information about the property of file sample in this paper we study how file relation can be used to improve malware detection result and develop a file verdict system named valkyrie building on a semi parametric classifier model to combine file content and file relation together for malware detection to the best of our knowledge this is the first work of using both file content and file relation for malware detection a comprehensive experimental study on a large collection of pe file obtained from the client of anti malware product of comodo security solution incorporation is performed to compare various malware detection approach promising experimental result demonstrate that the accuracy and efficiency of our valkyrie system outperform other popular anti malware software tool such a kaspersky antivirus and mcafee virusscan a well a other alternative data mining based detection system 
many business now have almost real time data available about their operation this data can be helpful in contemporaneous prediction nowcasting of various economic indicator we illustrate how one can use google search data to nowcast economic metric of interest and discus some of the ramification for research and policy our approach combine three bayesian technique kalman filtering spike and slab regression and model averaging we use kalman filtering to whiten the time series in question by removing the trend and seasonal behavior spike and slab regression is a bayesian method for variable selection that work even in case where the number of predictor is far larger than the number of observation finally we use markov chain monte carlo method to sample from the posterior distribution for our model the final forecast is an average over thousand of draw from the posterior an advantage of the bayesian approach is that it allows u to specify informative prior that affect the number and type of predictor in a flexible way 
the automated targeting of online display ad at scale requires the simultaneous evaluation of a single prospect against many independent model when deciding which ad to show to a user one must calculate likelihood to convert score for that user across all potential advertiser in the system for modern machine learning based targeting a conducted by medium degree m d this can mean scoring against thousand of model in a large sparse feature space dimensionality reduction within this space is useful a it decrease scoring time and model storage requirement to meet this need we develop a novel algorithm for scalable supervised dimensionality reduction across hundred of simultaneous classification task the algorithm performs hierarchical clustering in the space of model parameter from historical model in order to collapse related feature into a single dimension this allows u to implicitly incorporate feature and label data across all task without operating directly in a massive space we present experimental result showing that for this task our algorithm outperforms other popular dimensionality reduction algorithm across a wide variety of ad campaign a well a production result that showcase it performance in practice 
recently there ha been a lot of interest in graph based analysis one of the most important aspect of graph based analysis is to measure similarity between node and to do similarity search in a graph for example in social network such a facebook system may want to recommend potential friend to a particular user based on connection between user in custom product network such a ebay one may wish to recommend product to others based on purchase history in this talk i will introduce some method on vertex similarity computation and their application on similarity search in real world network 
new challenge have been presented to classical topic model when applied to social medium a user generated content suffers from significant problem of data sparseness a variety of heuristic adjustment to these model have been proposed many of which are based on the use of context information to improve the performance of topic modeling existing con textualized topic model rely on arbitrary manipulation of the model structure by incorporating various context variable into the generative process of classical topic model in an ad hoc manner such manipulation usually result in much more complicated model structure sophisticated inference procedure and low generalizability to accommodate arbitrary type or combination of context in this paper we explore a different direction we propose a general solution that is able to exploit multiple type of context without arbitrary manipulation of the structure of classical topic model we formulate different type of context a multiple view of the partition of the corpus a co regularization framework is proposed to let these view collaborate with each other vote for the consensus topic and distinguish them from view specific topic experiment with real world datasets prove that the proposed method is both effective and flexible to handle arbitrary type of context 
many business now have almost real time data available about their operation this data can be helpful in contem poraneous prediction nowcasting of various economic indicator we illustrate how one can use google search data to nowcast economic metric of interest and discus some of the ramification for research and policy our approach combine three bayesian technique kalman filtering spike and slab regression and model averaging we use kalman filtering to whiten the time series in question by removing the trend and seasonal behavior spike and slab regression is a bayesian method for variable selection that work even in case where the number of predictor is far larger than the number of observation finally we use markov chain monte carlo method to sample from the posterior distribution for our model the final forecast is an average over thousand of draw from the posterior an advantage of the bayesian approach is that it allows u to specify informative prior that affect the number and type of predictor in a flexible way 
