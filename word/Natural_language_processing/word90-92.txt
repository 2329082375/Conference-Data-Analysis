unification based nl parser that copy argument graph to prevent their destruction suffer from inefficiency copying is the most expensive operation in such parser and several method to reduce copying have been devised with varying degree of success lazy unification is presented here a a new conceptually elegant solution that reduces copying by nearly an order of magnitude lazy unification requires no new slot in the structure of node and only nominal revision to the unification algorithm 
two independent mechanism of context change have been discussed separately in the literature context change by that make use of it and discus our initial implementation of these idea 
the paper introduces a dependency based grammar and the associated parser and focus on the problem of determinism in parsing and recovery from error first it is shown how dependency based parsing can be afforded by taking into account the suggestion coming from other approach and the preference criterion for parsing are breifly addressed second the issue of the interconnection between the syntactic analysis and the semantic interpretation in incremental processing are discussed and the adoption of a tm for the recovery of the processing error is suggested 
two new parsing algorithm for context free phrase structure grammar are presented which perform a bounded amount of processing per word per analysis path independently of sentence length they are thus capable of parsing in real time in a parallel implementation which fork processor in response to non deterministic choice point 
consideration of when right association work and when it fails lead to a restatement of this parsing principle in term of the notion of heaviness a computational investigation of a syntactically annotated corpus provides evidence for this proposal and suggest circumstance when ra is likely to make correct attachment prediction 
this paper present the result of converting a standard greham harrison ruzzo ghr parser for a unification grammar into an agenda driven parsing system the agenda is controlled by statistical measure of grammar rule likelihood obtained from a training set the technique in the agenda parser lead to substantial reduction in chart size and parse time and can be applied to any chart based parsing algorithm without hand tuning 
in the right situation a speaker can use an unqualified indefinite description without being misunderstood this use of language is a kind of conversational implicature i e a non truth functional context dependent inference based upon language user awareness of principle of cooperative conversation i present a convention for identifying normal state implicatures which is based upon mutual belief of the speaker and hearer about certain property of the speaker s plan a key property is the precondition that an entity playing a role in the plan must be in a normal state with respect to the plan 
aspect of semantic interpretation such a quantifier scoping and reference resolution are often realised computationally by non monotonic operation involving loss of information and destructive manipulation of semantic representation the paper describes how monotonic reference resolution and scoping can be carried out using a revised quasi logical form qlf representation semantics for qlf are presented in which the denotation of formula are extended monotonically a qlf expression are resolved 
this paper present a plan based model that handle negotiation subdialogues by inferring both the communicative action that people pursue when speaking and the belief underlying these action we contend that recognizing the complex discourse action pursued in negotiation subdialogues e g expressing doubt requires both a multistrength belief model and a process model that combine different knowledge source in a unified framework we show how our model identifies the structure of negotiation subdialogues including recognizing expression of doubt implicit acceptance of communicated proposition and negotiation subdialogues embedded within other negotiation subdialogues 
language differ in the concept and real world entity for which they have word and grammatical construct therefore translation must sometimes be a matter of approximating the meaning of a source language text rather than finding an exact counterpart in the target language we propose a translation framework based on situation theory the basic ingredient are an information lattice a representation scheme for utterance embedded in context and a mismatch resolution scheme defined in term of information flow we motivate our approach with example of translation between english and japanese 
we introduce an algorithm for designing a predictive left to right shift reduce non determinisic push down machine corresponding to an arbitrary unrestricted context free grammar and an algorithm for efficiently driving this machine in pseudo parallel the performance of the resulting parser is formally proven to be superior to earley s parser the technique employed consists in constructing before run time a parsing table that encodes a non deterministic machine in the which the predictive behavior ha been compiled out at run time the machine is driven in pseudo parallel with the help of a chart the recognizer behaves in the worst case in space however in practice it is always superior to earley s parser since the prediction step have been compiled before run time finally we explain how other more efficient variant of the basic parser can be obtained by determinizing portion of the basic non deterministic push down machine while still using the same pseudo parallel driver 
example are often used along with textual description to help convey particular idea especially in instructional or explanatory context these accompanying example reflect information in the surrounding text and in turn also influence the text sometimes example replace possible textual elaboration in the description it is thus clear that if object description are to be generated the system must incorporate strategy to handle example in this work we shall investigate some of these issue in the generation of object description 
some problem are discussed that arise for incremental processing using certain flexible categorial grammar which involve either undesirable parsing property or failure to allow combination useful to incrementality we suggest a new calculus which though designed in relation to categorial interpretation of some notion of dependency grammar seems to provide a degree of flexibility that is highly appropriate for incremental interpretation we demonstrate how this grammar may be used for efficient incremental parsing by employing normalisation technique 
unification based nl parser that copy argument graph to prevent their destruction suffer from inefficiency copying is the most expensive operation in such parser and several method to reduce copying have been devised with varying degree of success lazy unification is presented here a a new conceptually elegant solution that reduces copying by nearly an order of magnitude lazy unification requires no new slot in the structure of node and only nominal revision to the unification algorithm 
two independent mechanism of context change have been discussed separately in the literature context change by that make use of it and discus our initial implementation of these idea 
the paper introduces a dependency based grammar and the associated parser and focus on the problem of determinism in parsing and recovery from error first it is shown how dependency based parsing can be afforded by taking into account the suggestion coming from other approach and the preference criterion for parsing are breifly addressed second the issue of the interconnection between the syntactic analysis and the semantic interpretation in incremental processing are discussed and the adoption of a tm for the recovery of the processing error is suggested 
two new parsing algorithm for context free phrase structure grammar are presented which perform a bounded amount of processing per word per analysis path independently of sentence length they are thus capable of parsing in real time in a parallel implementation which fork processor in response to non deterministic choice point 
consideration of when right association work and when it fails lead to a restatement of this parsing principle in term of the notion of heaviness a computational investigation of a syntactically annotated corpus provides evidence for this proposal and suggest circumstance when ra is likely to make correct attachment prediction 
this paper present the result of converting a standard greham harrison ruzzo ghr parser for a unification grammar into an agenda driven parsing system the agenda is controlled by statistical measure of grammar rule likelihood obtained from a training set the technique in the agenda parser lead to substantial reduction in chart size and parse time and can be applied to any chart based parsing algorithm without hand tuning 
in the right situation a speaker can use an unqualified indefinite description without being misunderstood this use of language is a kind of conversational implicature i e a non truth functional context dependent inference based upon language user awareness of principle of cooperative conversation i present a convention for identifying normal state implicatures which is based upon mutual belief of the speaker and hearer about certain property of the speaker s plan a key property is the precondition that an entity playing a role in the plan must be in a normal state with respect to the plan 
aspect of semantic interpretation such a quantifier scoping and reference resolution are often realised computationally by non monotonic operation involving loss of information and destructive manipulation of semantic representation the paper describes how monotonic reference resolution and scoping can be carried out using a revised quasi logical form qlf representation semantics for qlf are presented in which the denotation of formula are extended monotonically a qlf expression are resolved 
this paper present a plan based model that handle negotiation subdialogues by inferring both the communicative action that people pursue when speaking and the belief underlying these action we contend that recognizing the complex discourse action pursued in negotiation subdialogues e g expressing doubt requires both a multistrength belief model and a process model that combine different knowledge source in a unified framework we show how our model identifies the structure of negotiation subdialogues including recognizing expression of doubt implicit acceptance of communicated proposition and negotiation subdialogues embedded within other negotiation subdialogues 
language differ in the concept and real world entity for which they have word and grammatical construct therefore translation must sometimes be a matter of approximating the meaning of a source language text rather than finding an exact counterpart in the target language we propose a translation framework based on situation theory the basic ingredient are an information lattice a representation scheme for utterance embedded in context and a mismatch resolution scheme defined in term of information flow we motivate our approach with example of translation between english and japanese 
we introduce an algorithm for designing a predictive left to right shift reduce non determinisic push down machine corresponding to an arbitrary unrestricted context free grammar and an algorithm for efficiently driving this machine in pseudo parallel the performance of the resulting parser is formally proven to be superior to earley s parser the technique employed consists in constructing before run time a parsing table that encodes a non deterministic machine in the which the predictive behavior ha been compiled out at run time the machine is driven in pseudo parallel with the help of a chart the recognizer behaves in the worst case in space however in practice it is always superior to earley s parser since the prediction step have been compiled before run time finally we explain how other more efficient variant of the basic parser can be obtained by determinizing portion of the basic non deterministic push down machine while still using the same pseudo parallel driver 
example are often used along with textual description to help convey particular idea especially in instructional or explanatory context these accompanying example reflect information in the surrounding text and in turn also influence the text sometimes example replace possible textual elaboration in the description it is thus clear that if object description are to be generated the system must incorporate strategy to handle example in this work we shall investigate some of these issue in the generation of object description 
some problem are discussed that arise for incremental processing using certain flexible categorial grammar which involve either undesirable parsing property or failure to allow combination useful to incrementality we suggest a new calculus which though designed in relation to categorial interpretation of some notion of dependency grammar seems to provide a degree of flexibility that is highly appropriate for incremental interpretation we demonstrate how this grammar may be used for efficient incremental parsing by employing normalisation technique 
official testing will be done in may and the third message understanding conference muc will be held may at the naval ocean system center a proceeding will be published on the basis of this conference the result of the evaluation will be analyzed to discover whether conclusion can be drawn c o n c e r n i n g the c o r r e l a t i o n among task performance text analysis capability and theoretical approach r e c e n t r e s u l t s 
this paper present a new approach for resolving lexical ambiguity in one language using statistical data on lexical relation in another language this approach exploit the difference between mapping of word to sens in different language we concentrate on the problem of target word selection in machine translation for which the approach is directly applicable and employ a statistical model for the selection mechanism the model wa evaluated using two set of hebrew and german example and wa found to be very useful for disambiguation 
incorporation of default in grammar formalism is important for reason of linguistic adequacy and grammar organization in this paper we present an algorithm for handling default information in unification grammar the algorithm specifies a logical operation on feature structure merging with the non default structure only those part of the default feature structure which are not constrained by the non default structure we present various linguistic application of default unification 
this paper describes a series of experiment aimed at producing a bottom up parser that will produce partial par suitable for use in robust interpretation and still be reasonably efficient in the course of these experiment we improved parse time by a factor of over our first attempt ending with a system that wa twice a fast a our previous parser which relied on strong top down constraint the major algorithmic variation we tried are described along with the corresponding performance result 
natural language processing nlp system the effort focus particularly on automatically inferring the meaning of new word in context and on developing partial interpretation of language that is either fragmentary or beyond the capability of the nlp system to understand the technique are being evaluated in a message processing domain such a automatic data base update based on article from the wall street journal on corporate takeover bid 
whereas in the united state work in machine translation mt ha only recently been reinstated a a respectable natural language processing nlp application it ha long been considered a worthwhile and interesting topic for research and development in both europe and japan in term of number of project in one sub field of computational linguistics mt is currently perhaps the most important application one obvious reason for this is simply the daily awareness that people communicate in language other than english a situation that naturally encourages an interest in translation on a practical level for example every television cable system in europe broadcast station from numerous country and on the political level the european community ec is committed to protecting the language of each of the member state which implies providing numerous translation service from an economic viewpoint every company know that in order to market it product the documentation must be in the language of the target country and a last motivation for interest in mt which wa also the origin of mt activity in the u and an important concern for japan is the desire for better access to information important document often exist in some foreign language 
the esprit project polyglot aim at developing multi lingual speech to text and text to speech conversion and to integrate this technology in a number of commercially viable prototype application speech to text conversion is mainly concerned with very large vocabulary isolated word recognition it us a statistical knowledge based approach that wa pioneered for italian and is now being extended to other language work on continuous speech recognition ha the character of an in depth feasibility study for textto speech conversion a new multi level data structure is developed that facilitates rule writing by offering a semi graphical rule format the recognition and synthesis technology is used to build a number of generic prototype application that mainly address office automation 
current complex feature based grammar use a single procedure unification for a multitude of purpose among them enforcing formal agreement between purely syntactic feature this paper present evidence from several natural language that unification variable matching combined with variable substitution is the wrong mechanism for effecting agreement the view of grammar developed here is one in which unification is used for semantic interpretation while purely formal agreement involves only a check for non distinctness i e variable matching without variable substitution 
we describe the modification of a grammar to take advantage of prosodic information provided by a speech recognition system this initial study is limited to the use of relative duration of phonetic segment in the assignment of syntactic structure specifically in ruling out alternative par in otherwise ambiguous sentence taking advantage of prosodic information in parsing can make a spoken language system more accurate and more efficient if prosodic syntactic mismatch or unlikely match can be pruned we know of no other work that ha succeeded in automatically extracting speech information and using it in a parser to rule out extraneous par 
this paper present a rapid and robust parsing system currently used to learn from large body of unedited text the system contains a multivalued part of speech disambiguator and a novel parser employing bottom up recognition to find the constituent phrase of larger structure that might be too difficult to analyze the result of applying the disambiguator and parser to large section of the lancaster oslo bergen corpus are presented 
this paper present a rapid and robust parsing system currently used to learn from large body of unedited text the system contains a multivalued part of speech disambiguator and a novel parser employing bottom up recognition to find the constituent phrase of larger structure that might be too difficult to analyze the result of applying the disambiguator and parser to large section of the lancaster oslo bergen corpus are presented 
graph unification is the most expensive part of unification based grammar parsing it often take over of the total parsing time of a sentence we focus on two speed up element in the design of unification algorithm elimination of excessive copying by only copying successful unification finding unification failure a soon a possible we have developed a scheme to attain these two element without expensive overhead through temporarily modifying graph during unification to eliminate copying during unification we found that parsing relatively long sentence requiring about top level unification during a parse using our algorithm is approximately twice a fast a parsing the same sentence using wroblewski s algorithm 
we propose a syntactic filter for identifying non coreferential pronoun np pair within a sentence the filter applies to the output of a slot grammar parser and is formulated in term of the head argument structure which the parser generates it handle control and unbounded dependency construction without empty category or binding chain by virtue of the unificational nature of the parser the filter provides constraint for a discourse semantics system reducing the search domain to which the inference rule of the system s anaphora resolution component apply 
it ha been hypothesized that tree adjoining grammar tag is particularly well suited for sentence generation it is unclear however how a sentence generation system based on tag should choose among the syntactic possibility made available in the grammar in this paper we consider the question of what need to be done to generate with tag and explain a generation system that provides the necessary feature this approach is compared with other tag based generation system particular attention is given to mumble which like our system make syntactic choice on sophisticated functional ground 
a crucially important adequacy test of any theory of speech act is it ability to handle performatives this paper provides a theory of performatives a a test case for our rationally based theory of illocutionary act we show why i request you is a request and i lie to you that p is self defeating the analysis support and extends earlier work of theorist such a bach and harnish and take issue with recent claim by searle that such performative a declarative analysis are doomed to failure 
this paper present a connectionist syntactic parser which us structure unification grammar a it grammatical framework the parser is implemented in a connectionist architecture which store and dynamically manipulates symbolic representation but which can t represent arbitrary disjunction and ha bounded memory these problem can be overcome with structure unification grammar s extensive use of partial description 
this paper show that a first order unification based semantic interpretation for various coordinate construct is possible without an explicit use of lambda expression if we slightly modify the standard montagovian semantics of coordination this modification along with partial execution completely eliminates the lambda reduction step during semantic interpretation 
we present a new compositional tense aspect deindexing mechanism that make use of a component of discourse context the mechanism allows reference episode to be correctly identified even for embedded clause and for discourse that involves shift in temporal perspective and permit deindexed logical form to be automatically computed with a small number of deindexing rule 
collocational knowledge is necessary for language generation the problem is that collocation come in a large variety of form they can involve two three or more word these word can be of different syntactic category and they can be involved in more or le rigid way this lead to two main difficulty collocational knowledge ha to be acquired and it must be represented flexibly so that it can be used for language generation we address both problem in this paper focusing on the acquisition problem we describe a program xtract that automatically acquires a range of collocation from large textual corpus and we describe how they can be represented in a flexible lexicon using a unification based formalism 
certain pronoun context are argued to establish a local center lc i e a conventionalized indexical similar to st nd pers pronoun demonstrative pronoun also indexicals are shown to access entity that are not lcs because they lack discourse relevance or because they are not yet in the universe of discourse 
lexical disambiguation can be achieved using different source of information aiming at high performance of automatic disambiguation it is important to know the relative importance and applicability of the various source in this paper we classify several source of information and show how some of them can be achieved using statistical data first evaluation indicate the extreme importance of local information which mainly represents lexical association and selectional restriction for syntactically related word 
dictionary contain a rich set of relationship between their sens but often these relationship are only implicit we report on our experiment to automatically identify link between the sens in a machine readable dictionary in particular we automatically identify instance of zero affix morphology and use that information to find specific linkage between sens this work ha provided insight into the performance of a stochastic tagger 
in this paper we report on our use of zero morpheme in unification based combinatory categorial grammar after illustrating the benefit of this approach with several example we describe the algorithm for compiling zero morpheme into unary rule which allows u to use zero morpheme more efficiently in natural language processing then we discus the question of equivalence of a grammar with these unary rule to the original grammar lastly we compare our approach to zero morpheme with possible alternative 
a compositional semantics for focusing subjuncts word such a only even and also is developed from rooth s theory of association with focus by adapting the theory so that it can be expressed in term of a frame based semantic formalism a semantics that is more computationally practical is arrived at this semantics capture pragmatic subtlety by incorporating a two part representation and recognizes the contribution of intonation to meaning 
a method of determining the similarity of noun on the basis of a metric derived from the distribution of subject verb and object in a large text corpus is described the resulting quasi semantic classification of noun demonstrates the plausibility of the distributional hypothesis and ha potential application to a variety of task including automatic indexing resolving nominal compound and determining the scope of modification 
a new approach to bottom up parsing that extends augmented context free grammar to a process grammar is formally presented a processor grammar pg defines a set of rule suited for bottom up parsing and conceived a process that are applied by a pg processor the matching phase is a crucial step for process application and a parsing structure for efficient matching is also presented the pg processor is composed of a process scheduler that allows immediate constituent analysis of structure and behaves in a non deterministic fashion on the other side the pg offer mean for implementing specific parsing strategy improving the lack of determinism innate in the processor 
we developed a prototype information retrieval system which us advanced natural language processing technique to enhance the effectiveness of traditional key word based document retrieval the backbone of our system is a statistical retrieval engine which performs automated indexing of document then search and ranking in response to user query this core architecture is augmented with advanced natural language processing tool which are both robust and efficient in early experiment the augmented system ha displayed capability that appear to make it superior to the purely statistical base 
this paper present an approach to identifying conjuncts of coordinate conjunction appearing in text which ha been labelled with syntactic and semantic tag the overall project of which this research is a part is also briefly discussed the program wa tested on a word chapter of the merck veterinary manual the algorithm is deterministic and domain independent and it performs relatively well on a large real life domain construct not handled by the simple algorithm are also described in some detail 
the structure imposed upon spoken sentence by intonation seems frequently to be orthogonal to their traditional surface syntactic structure however the notion of intonational structure a formulated by pierrehumbert selkirk and others can be subsumed under a rather different notion of syntactic surface structure that emerges from a theory of grammar based on a combinatory extension to categorial grammar interpretation of constituent at this level are in turn directly related to information structure or discourse related notion of theme rheme focus and presupposition some simplification appear to follow for the problem of integrating syntax and other high level module in spoken language system 
the cmu phoenix system is an experiment in understanding spontaneous speech it ha been implemented for the air travel information service task in this task casual user are asked to obtain information from a database of air travel information user are not given a vocabulary grammar or set of sentence to read they compose query themselves in a spontaneous manner this task present speech recognizers with many new problem compared to the resource management task not only is the speech not fluent but the vocabulary and grammar are open also the task is not just to produce a transcription but to produce an action retrieve data from the database taking such action requires parsing and understanding the utteraoce word error rate is not a important a utterance understanding rate phoenix attempt to deal with phenomenon that occur in spontaneous speech unknown word restarts repeat and poody formed or unusual grammar are common is spontaneous speech and are very disruptive to standard recognizers these event lead to misrecognitions which often cause a total parse failure our strategy is to apply grammatical constraint at the phrase level and to use semantic rather than lexical grammar semantics provide more constraint than part of speech and must ultimately be delt with in order to take action applying constraint at the phrase level is more flexible than recognizing sentence a a whole while providing much more constraint than word spotting restarts and repeat are most often between phase occurences so individual phrase can still be recognized correctly poorly constructed grammar often consists of well formed phrase and is often semantically well formed it is only syntactically incorrect we associate phrase by frame based semantics phrase represent word string that can fill slot in frame the slot represent information which the frame is able to act on the current phoenix system us a bigram language model with the sphinx speech recognition system the top scoring word string is passed to a flexible frame based parser the parser assigns phrase word string from the input to slot in frame the slot represent information content needed for the frame a beam of frame hypothesis is produced and the best scoring one is used to produce an sql query 
in this study we map out a way to build event representation incrementally using information which may be widely distributed across a discourse an enhanced discourse representation kamp provides the vehicle both for carrying open event role through the discourse until they can be instantiated by np and for resolving the reference of these otherwise problematic np by binding them to the event role 
in recent year there is much interest in word cooccurrence relation such a n gram verb object combination or cooccurrence within a limited context this paper discus how to estimate the probability of cooccurrences that do not occur in the training data we present a method that make local analogy between each specific unobserved cooccurrence and other cooccurrences that contain similar word a determined by an appropriate word similarity metric our evaluation suggests that this method performs better than existing smoothing method and may provide an alternative to class based model 
i describe a head driven parser for a class of grammar that handle discontinuous constituency by a richer notion of string combination than ordinary concatenation the parser is a generalization of the left corner parser matsumoto et al and can be used for grammar written in powerful formalism such a non concatenative version of hpsg pollard reape 
a formalism is presented for lexical specification in unification based grammar which exploit defeasible multiple inheritance to express regularity sub regularity and exception in classifying the property of word such system are in the general case intractable the present proposal represents an attempt to reduce complexity while retaining sufficient expressive power for the task at hand illustrative example are given of morphological analysis from english and german 
we describe a statistical technique for assigning sens to word an instance of a word is assigned a sense by asking a question about the context in which the word appears the question is constructed to have high mutual information with the translation of that instance in another language when we incorporated this method of assigning sens into our statistical machine translation system the error rate of the system decreased by thirteen percent 
abstract ebmt example based machine translation is proposed ebmt retrieves similar example pair of source phrase sentence or text and their translation from a d t hase of example adapting the example to translate a new input ebmt ha the following feature it is easily upgraded simply by inputting appropriate example to the database it assigns a reliability factcr to the translation result it is acoelerated effectively by both indexing and parallel computing it is robust because of best match reasoning d it well utilizes translator expertise a prototype system ha been implemented to deal with a difficult translation problem for conventional rule based machine translation rbmt i e translating japanese noun phrase of the form n no n into english the system ha achieved about a success rate on average this paper explains the basic idea of ebmt illustrates the experiment in detail explains the broad applicability of ebmt to several difficult translation problem for rbmt and discus the advantage of integrating ebmt with rbmt 
a stochastic model based on insight of mandelbrot and simon is discussed against the background of new criterion of adequacy that have become available recently a a result of study of the similarity relation between word a found in large computerized text corpus 
speaker independent system is desirable in many application where speaker specific data do not exist however if speakerdependent data are available the system could be adapted to the specific speaker such that the error rate could be significantly reduced in this paper darpa resource management task is used a the domain to investigate the performance of speaker adaptive speech recognition since adaptation is based on speaker independent system with only limited adaptation data a good adaptation algorithm should be consistent with the speaker independent parameter estimation criterion and adapt those parameter that are le sensitive to the limited training data two parameter set the codebook mean vector and the output distribution are regarded to be most important they are modified in the framework of maximum likelihood estimation criterion according to the characteristic of each speaker in order to reliably estimate those parameter output distribution are shared with each other if they exhibit certain acoustic similarity in addition to modify these parameter speaker normalization with neural network is also studied in the hope that acoustic data normalization will not only rapidly adapt the system but also enhance the robustness of speakerindependent speech recognition preliminary result indicate that speaker difference can be well minimized in comparison with speaker independent speech recognition the error rate ha been reduced from to by only using parameter adaptation technique with adaptation sentence for each speaker when the number of speaker adaptation sentence is comparable to that of speaker dependent training speaker adaptive recognition work better than the best speaker dependent recognition result on the same test set which indicates the robustness of speaker adaptive speech recognition 
this paper present an implemented psychologically plausible parsing model for government binding theory grammar i make use of two main idea a generalization of the licensing relation of abney allows for the direct encoding of certain principle of grammar e g theta criterion case filter which drive structure building the working space of the parser is constrained to the domain determined by a tree adjoining grammar elementary tree all dependency and constraint are localized within this bounded structure the resultant parser operates in linear time and allows for incremental semantic interpretation and determination of grammaticality 
this paper describes a method of classifying semantically similar noun the approach is based on the distributional hypothesis our approach is characterized by distinguishing among sens of the same word in order to resolve the polysemy issue the classification result demonstrates that our approach is successful 
in this paper we discus a mechanism for modifying context in a tutorial dialogue the context mechanism imposes a pedagogically motivated misrepresentation pmm on a dialogue to achieve instructional goal in the paper we outline several type of pmms and detail a particular pmm in a sample dialogue situation while the notion of pmms are specifically oriented towards tutorial dialogue misrepresentation ha interesting implication for context in dialogue situation generally and also suggests that grice s maxim of quality need to be modified 
we have recently reported on two new word sense disambiguation system one trained on bilingual material the canadian hansard and the other trained on monolingual material roget s thesaurus and grolier s encyclopedia after using both the monolingual and bilingual classifier for a few month we have convinced ourselves that the performance is remarkably good nevertheless we would really like to be able to make a stronger statement and therefore we decided to try to develop some more objective evaluation measure although there ha been a fair amount of literature on sense disambiguation the literature doe not offer much guidance in how we might establish the success or failure of a proposed solution such a the two system mentioned in the previous paragraph many paper avoid quantitative evaluation altogether because it is so difficult to come up with credible estimate of performance this paper will attempt to establish upper and lower bound on the level of performance that can be expected in an evaluation an estimate of the lower bound of averaged over ambiguous type is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all case an estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgment from human informant not surprisingly the upper bound is very dependent on the instruction given to the judge jorgensen for example suspected that lexicographer tend to depend too much on judgment by a single informant and found considerable variation over judgment only agreement a she had suspected in our own experiment we have set out to find word sense disambiguation task where the judge can agree often enough so that we could show that they were outperforming the baseline system under quite different condition we have found agreement over judge 
a system is described for acquiring a context sensitive phrase structure grammar which is applied by a best path bottom up deterministic parser the grammar wa based on english news story and a high degree of success in parsing in reported overall this research concludes that csg is a computationally and conceptually tractable approach to the construction of phrase structure grammar for news story text 
i present a semantic analysis of collective distributive ambiguity and resolution of such ambiguity by model based reasoning this approach go beyond scha and stallard whose reasoning capability wa limited to checking semantic type my semantic analysis is based on link and robert where distributivity come uniformly from a quantificational operator either explicit e g each or implicit e g the d operator i view the semantics module of the natural language system a a hypothesis generator and the reasoner in the pragmatic module a a hypothesis filter cf simmons and davis the reasoner utilizes a model consisting of domain dependent constraint and domain independent axiom for disambiguation there are two kind of constraint type constraint and numerical constraint and they are associated with predicate in the knowledge base whenever additional information is derived from the model the contradiction checker is invoked to detect any contradiction in a hypothesis using simple mathematical knowledge cdcl collective distributive constraint language is used to represent hypothesis constraint and axiom in a way isomorphic to diagram representation of collective distributive ambiguity 
efficient syntactic and semantic parsing for ambiguous context free language are generally characterized a complex specialized highly formal algorithm in fact they are readily constructed from straightforward recursive transition network rtns in this paper we introduce lr rtns and then computationally motivate a uniform progression from basic lr parsing to earley s chart parsing concluding with tomita s parser these apparently disparate algorithm are unified into a single implementation which wa used to automatically generate all the figure in this paper 
we describe a system of reversible grammar in which given a logic grammar specification of a natural language two efficient prolog program are derived by an off line compilation process a parser and a generator for this language the centerpiece of the system is the inversion algorithm designed to compute the generator code from the parser s prolog code using the collection of minimal set of essential argument msea for predicate the system ha been implemented to work with definite clause grammar dcg and is a part of an english japanese machine translation project currently under development at nyu s courant institute 
we present the mapping unit approach to representing subeategorization information a computational framework for encoding subcategorization information which ha been developed and implemented for bbn s delphi system the nl component of the harc spoken language system the advantage of our approach to subeategorization lie in it flexibility a flexibility which in turn offer greater robustness of coverage with respect to unanticipated variation of a verbal argument pattern and easier extension of coverage to new pattern it handle in a quite natural way argument order variation optionality of guments and metonymy 
linear precedence lp rule are widley used for stating word order principle they have been adopted a constraint by hpsg but no encoding in the formalism ha been provided since they only order sibling they are not quite adequate at least not for german we propose a notion of lp constraint that applies to linguistically motivated branching domain such a head domain we show a type based encoding in an hpsg style formalism that support processing the encoding can be achieved by a compilation step 
in contrast to the designer logic approach this paper show how the attribute value feature structure of unification grammar and constraint on them can be axiomatized in classical first order logic which can express disjunctive and negative constraint because only quantifier free formula are used in the axiomatization the satisfiability problem is np complete 
machine translation of locative preposition is not straightforward even between closely related language this paper discus a system of translation of locative preposition between english and french the system is based on the premise that english and french do not always conceptualize object in the same way and that this account for the major difference in the way that locative preposition are used in these language this paper introduces knowledge representation of conceptualization of object and a method for translating preposition based on these conceptual representation 
this paper describes an operational system which can acquire the core meaning of word without any prior knowledge of either the category or meaning of any word it encounter the system is given a input a description of sequence of scene along with sentence which describe the event taking place a those scene unfold and produce a output a lexicon consisting of the category and meaning of each word in the input that allows the sentence to describe the event it is argued that each of the three main component of the system the parser the linker and the inference component make only linguistically and cognitively plausible assumption about the innate knowledge needed to support tractable learning the paper discus the theory underlying the system the representation and algorithm used in the implementation the semantic constraint which support the heuristic necessary to achieve tractable learning the limitation of the current theory and the implication of this work for language acquisition research 
this paper describes an implemented program that take a raw untagged text corpus a it only input no open class dictionary and generates a partial list of verb occurring in the text and the subcategorization frame sfs in which they occur verb are detected by a novel technique based on the case filter of rouvret and vergnaud the completeness of the output list increase monotonically with the total number of occurrence of each verb in the corpus false positive rate are one to three percent of observation five sfs are currently detected and more are planned ultimately i expect to provide a large sf dictionary to the nlp community and to train dictionary for specific corpus 
recent development in generation algorithm have enabled work in unification based computational linguistics to approach more closely the ideal of grammar a declarative statement of linguistic fact neutral between analysis and synthesis from this perspective however the situation is still far from perfect all known method of generation impose constraint on the grammar they assume we briefly consider a number of proposal for generation outlining their consequence for the form of grammar and then report on experience arising from the addition of a generator to an existing unification environment the algorithm in question based on that of shieber et al though among the most permissive currently available excludes certain class of parsable analysis 
we discus algorithm for generation within the lambek theorem proving framework efficient algorithm for generation in this framework take a semantics driven strategy this strategy can be modeled by mean of rule in the calculus that are geared to generation or by mean of an algorithm for the theorem prover the latter possibility enables processing of a bidirectional calculus therefore lambek theorem proving is a natural candidate for a uniform architecture for natural language parsing and generation 
we investigate various contextual effect on text interpretation and account for them by providing contextual constraint in a logical theory of text interpretation on the basis of the way these constraint interact with the other knowledge source we draw some general conclusion about the role of domain specific information top down and bottom up discourse information flow and the usefulness of formalisation in discourse theory 
a method is presented for acquiring perceptually grounded semantics for spatial term in a simple visual domain a a part of the l miniature language acquisition project two central problem in this learning task are a ensuring that the term learned generalize well so that they can be accurately applied to new scene and b learning in the absence of explicit negative evidence solution to these two problem are presented and the result discussed 
this paper present a corpus based approach for deriving heuristic to locate the antecedent of relative pronoun the technique duplicate the performance of hand coded rule and requires human intervention only during the training phase because the training instance are built on parser output rather than word cooccurrences the technique requires a small number of training example and can be used on small to medium sized corpus our initial result suggest that the approach may provide a general method for the automated acquisition of a variety of disambiguation heuristic for natural language system especially for problem that require the assimilation of syntactic and semantic knowledge 
referring expression and other object description should be maximal under the local brevity no unnecessary component and lexical preference preference rule otherwise they may lead hearer to infer unwanted conversational implicatures these preference rule can be incorporated into a polynomial time generation algorithm while some alternative formalization of conversational implicature make the generation task np hard 
in the literature tree adjoining grammar tag are propagated to be adequate for natural language description analysis a well a generation in this paper we concentrate on the direction of analysis especially important for an implementation of that task is how efficiently this can be done i e how readily the word problem can be solved for tag up to now a parser with o n step in the worst case wa known where n is the length of the input string in this paper the result is improved to o n log n a a new lowest upper bound the paper demonstrates how local interpretion of tag tree allows this reduction 
an algorithm is proposed to determine antecedent for vp ellipsis the algorithm eliminates impossible antecedent and then imposes a preference ordering on possible antecedent the algorithm performs with accuracy on a set of example of vp ellipsis collected from the brown corpus the problem of determining antecedent for vp ellipsis ha received little attention in the literature and it is shown that the current proposal is a significant improvement over alternative approach 
this paper discus how a two level knowledge representation model for machine translation integrates aspectual information with lexical semantic information by mean of parameterization the integration of aspect with lexical semantics is especially critical in machine translation because of the lexical selection and aspectual realization process that operate during the production of the target language sentence there are often a large number of lexical and aspectual possibility to choose from in the production of a sentence from a lexical semantic representation aspectual information from the source language sentence constrains the choice of target language term in turn the target language term limit the possibility for generation of aspect thus there is a two way communication channel between the two process this paper will show that the selection realization process may be parameterized so that they operate uniformly across more than one language and it will describe how the parameter based approach is currently being used a the basis for extraction of aspectual information from corpus 
this paper describes recent work on the unisys atis spoken language system and report benchmark result on natural language spoken language and speech recognition we describe enhancement to the system s semantic processing for handling non transparent argument structure and enhancement to the system s pragmatic processing of material in art swers displayed to the user we found that the system s score on the natural language benchmark test decreased from o to without these enhancement we also report result for three spoken language system unisys natural language coupled with mit summit speech recognition unisys natural language coupled wish mit lincoln lab speech recognition and unisys natural language coupled with bbn speech recognition speech recognition result are reported on the result of the unisys natural language selecting a candidate from the mitsummit n best n 
in this paper we describe a statistical technique for aligning sentence with their translation in two parallel corpus in addition to certain anchor point that are available in our data the only information about the sentence that we use for calculating alignment is the number of token that they contain because we make no use of the lexical detail of the sentence the alignment computation is fast and therefore practical for application to very large collection of text we have used this technique to align several million sentence in the english french hansard corpus and have achieved an accuracy in excess of in a random selected set of sentence pair that we checked by hand we show that even without the benefit of anchor point the correlation between the length of aligned sentence is strong enough that we should expect to achieve an accuracy of between and thus the technique may be applicable to a wider variety of text than we have yet tried 
in this paper we represent singular definite noun phrase a function in logical form this representation is designed to model the behavior of both anaphoric and non anaphoric distributive definites it is also designed to obey the computational constraint suggested in harper har our initial representation of a definite place an upper bound on it behavior given it structure and location in a sentence later when ambiguity is resolved the precise behavior of the definite is pinpointed 
we define a set of deterministic bottom up left to right parser which analyze a subset of tree adjoining language the lr parsing strategy for context free grammar is extended to tree adjoining grammar tag we use a machine called bottom up embedded push down automation bepda that recognizes in a bottom up fashion the set of tree adjoining language and exactly this set each parser consists of a finite state control that drive the move of a bottom up embedded pushdown automaton the parser handle deterministically some context sensitive tree adjoining language in this paper we informally describe the bepda then given a parsing table we explain the lr parsing algorithm we then show how to construct an lr parsing table no lookahead an example of a context sensitive language recognized deterministically is given then we explain informally the construction of slr parsing table for bepda we conclude with a discussion of our parsing method and current work 
this paper present a formal account of the temporal interpretation of text the distinct natural interpretation of text with similar syntax are explained in term of defeasible rule characterising causal law and gricean style pragmatic maxim intuitively compelling pattern of defeasible entailment that are supported by the logic in which the theory is expressed are shown to underly temporal interpretation 
the form of rule in combinatory categorial grammar ccg is constrained by three principle called adjacency consistency and inheritance these principle have been claimed elsewhere to constrain the combinatory rule of composition and type raising in such a way a to make certain linguistic universal concerning word order under coordination follow immediately the present paper show that the three principle have a natural expression in a unification based interpretation of ccg in which directional information is an attribute of the argument of function grounded in string position the universal can thereby be derived a consequence of elementary assumption some desirable result for grammar and parser follow concerning type raising rule 
phrase structure grammar are an effective representation for important syntactic and semantic aspect of natural language but are computationally too demanding for use a language model in real time speech recognition an algorithm is described that computes finite state approximation for context free grammar and equivalent augmented phrase structure grammar formalism the approximation is exact for certain context free grammar generating regular language including all left linear and right linear context free grammar the algorithm ha been used to construct finite state language model for limited domain speech recognition task 
we analyze the computational complexity of phonological model a they have developed over the past twenty year the major result are that generation and recognition are undecidable for segmental model and that recognition is np hard for that portion of segmental phonology subsumed by modern autosegmental model formal restriction are evaluated 
in this paper we present algorithm for the interpretation and generation of a kind of particularized conversational implicature occurring in certain indirect reply our algorithm make use of discourse expectation discourse plan and discourse relation the algorithm calculate implicatures of discourse unit of one or more sentence our approach ha several advantage first by taking discourse relation into account it can capture a variety of implicatures not handled before second by treating implicatures of discourse unit which may consist of more than one sentence it avoids the limitation of a sentence at a time approach third by making use of property of discourse which have been used in model of other discourse phenomenon our approach can be integrated with those model also our model permit the same information to be used both in interpretation and generation 
a formalism will be presented in this paper which make it possible to realise the idea of assigning only one scope ambiguous representation to a sentence that is ambiguous with regard to quantifier scope the scope determination result in extending this representation with additional context and world knowledge condition if there is no scope determining information the formalism can work further with this scope ambiguous representation thus scope information doe not have to be completely determined 
to achieve our goal of building a comprehensive lexical database out of various on line resource it is necessary to interpret and disambiguate the information found in these resource in this paper we describe a disambiguation module which analyzes the content of dictionary definition in particular definition of the form to verb with np we discus the semantic relation holding between the head and the prepositional phrase in such structure a well a our heuristic for identifying these relation and for disambiguating the sens of the word involved we present some result obtained by the disambiguation module and evaluate it rate of success a compared with result obtained from human judgment 
in this paper we introduce a logic for describing tree which allows u to reason about both the parent and domination relationship the use of domination ha found a number of application such a in deterministic parser based on description theory marcus hindle fleck in a compact organization of the basic structure of tree adjoining grammar vijay shanker schabes and in a new characterization of the adjoining operation that allows a clean integration of tag into the unification based framework vijay shanker our logic serf to formalize the reasoning on which these application are based 
in the general framework of a constraint based grammar formalism often some sort of feature logic serf a the constraint language to describe linguistic object we investigate the extension of basic feature logic with subsumption or matching constraint based on a weak notion of subsumption this mechanism of one way information flow is generally deemed to be necessary to give linguistically satisfactory description of coordination phenomenon in such formalism we show that the problem whether a set of constraint is satisfiable in this logic is decidable in polynomial time and give a solution algorithm 
determining the relationship between the intonational characteristic of an utterance and other feature inferable from it text is important both for speech recognition and for speech synthesis this work investigates the use of text analysis in predicting the location of intonational phrase boundary in natural speech through analyzing utterance from the darpa air travel information service database for statistical modeling we employ classification and regression tree cart technique we achieve success rate of just over representing a major improvement over other attempt at boundary prediction from unrestricted text 
this empirical study attempt to find answer to the question of how a natural language henceforth nl system could resolve attachment of prepositional phrase henceforth pps by examining naturally occurring pp attachment in typed dialogue examination includes testing predictive power of existing attachment theory against the data the result of this effort will be an algorithm for interpreting pp attachment 
the limited capacity of working memory is intrinsic to human sentence processing and therefore must be addressed by any theory of human sentence processing this paper give a theory of garden path effect and processing overload that is based on simple assumption about human short term memory capacity 
a language processor is to find out a most promising sentence hypothesis for a given word lattice obtained from acoustic signal recognition in this paper a new language processor is proposed in which unification grammar and markov language model are integrated in a word lattice parsing algorithm based on an augmented chart and the island driven parsing concept is combined with various preference first parsing strategy defined by different construction principle and decision rule test result show that significant improvement in both correct rate of recognition and computation speed can be achieved 
unification of disjunctive feature description is important for efficient unification based parsing this paper present constraint projection a new method for unification of disjunctive feature structure represented by logical constraint constraint projection is a generalization of constraint unification and is more efficient because constraint projection ha a mechanism for abandoning information irrelevant to a goal specified by a list of variable 
this paper present a method for interpreting metaphoric language in the context of a portable natural language inferface the method license metaphoric us via coercion between incompatible ontological sort the machinery allows both previously known and unexpected metaphoric us ot be correctly interpreted and evaluated with respect to the backend expert system 
prosody can be useful in resolving certain lexical and structural ambiguity in spoken english in this paper we present some result of employing two type of prosodic information namely pitch and pause to assist syntactic and semantic analysis during parsing 
prosody can be useful in resolving certain lexical and structural ambiguity in spoken english in this paper we present some result of employing two type of prosodic information namely pitch and pause to assist syntactic and semantic analysis during parsing 
abstract this paper present a new method for producing a dictionary of subcategorization frame from unlabelled text corpus it is shown that statistical filtering of the result of a finite state parser running on the output of a stochastic tagger produce high quality result despite the error rate of the tagger and the parser further it is argued that this method can be used to learn all subcategorization frame whereas previous method are not extensible to a general solution to the problem 
a model of plan recognition in discourse must be based on intended recognition distinguish each agent s belief and intention from the other s and avoid assumption about the correctness or completeness of the agent belief in this paper we present an algorithm for plan recognition that is based on the shared plan model of collaboration grosz and sidner lochbaum et al and that satisfies these constraint 
in this article we outline a basic approach to treating metonymy properly in a multilingual machine translation system this is the first attempt at treating metonymy in an machine translation environment the approach is guided by the difference of acceptability of metonymy which were obtained by our comparative survey among three language english chinese and japanese the characteristic of the approach are a follows influence of the context individual and familiality with metonymy are not used an actual acceptability of each metonymic expression is not realized directly grouping metonymic example into pattern is determined by the acceptability judgement of the speaker surveyed a well a the analyst intuition the analysis and generation component treat metonymy differently using the pattern the analysis component accepts a wider range of metonymy than the actual result of the survey and the generation component treat metonymy more strictly than the actual result we think that the approach is a starting point for more sophisticated approach to translation in a multilingual machine translation environment 
taking example from english and french idiom this paper show that not only constituent structure rule but also most syntactic rule such a topicalization wh question pronominalization are subject to lexical constraint on top of syntactic and possibly semantic one we show that such puzzling phenomenon are naturally handled in a lexicalized formalism such a tree adjoining grammar the extended domain of locality of tag also allows one to jexicalize syntactic rule while defining them at the level of constituent structure 
the paper describes work on applying a general purpose natural language processing system to transfer based interactive translation transfer take place at the level of quasi logical form qlf a contextually sensitive logical form representation which is deep enough for dealing with cross linguistic difference theoretical argument and experimental result are presented to support the claim that this framework ha good property in term of modularity compositionality reversibility and monotonicity 
we have analyzed sentence of spontaneous human computer speech data containing repair drawn from a total corpus of sentence we present here criterion and technique for automatically detecting the presence of a repair it location and making the appropriate correction the criterion involve integration of knowledge from several source pattern matching syntactic and semantic analysis and acoustic 
we present an approach to grammar development where the task is decomposed into two separate subtasks the first task linguistic with the goal of producing a set of rule that have a large coverage in the sense that the correct parse is among the proposed par on a blind test set of sentence the second task is statistical with the goal of developing a model of the grammar which assigns maximum probability for the correct parse we give parsing result on text from computer manual 
conversation between two people is usually of mixed initiative with control over the conversation being transferred from one person to another we apply a set of rule for the transfer of control to set of dialogue consisting of a total of turn the application of the control rule let u derive domain independent discourse structure the derived structure indicate that initiative play a role in the structuring of discourse in order to explore the relationship of control and initiative to discourse process like centering we analyze the distribution of four different class of anaphora for two data set this distribution indicates that some control segment are hierarchically related to others the analysis suggests that discourse participant often mutually agree to a change of topic we also compared initiative in task oriented and advice giving dialogue and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue type these difference can be explained in term of collaborative planning principle 
this paper i will consider parsing a a discretecombinatorial problem which consists in constructing alabeled graph that satisfies a set of linguisticconstraints i will identify some property of linguisticconstraints which allow this problem to be solvedefficiently using constraint satisfaction algorithm ithen describe briefly a modular parsing algorithmwhich construct a syntactic graph using a set ofgenerative operation and applies a filtering algorithmto eliminate inconsistent 
we present a new grammatical formalism called constraint dependency grammar cdg in which every grammatical rule is given a a constraint on word to word modification cdg parsing is formalized a a constraint satisfaction problem over a finite domain so that efficient constraint propagation algorithm can be employed to reduce structural ambiguity without generating individual parse tree the weak generative capacity and the computational complexity of cdg parsing are also discussed 
we propose that ambiguous prepositional phrase attachment can be resolved on the basis of the relative strength of association of the preposition with noun and verb estimated on the basis of word distribution in a large corpus this work suggests that a distributional approach can be effective in resolving parsing problem that apparently call for complex reasoning 
abstract itsdefinition quot syndrome property are not alwaysof the static kind though sometimes dynamicproperties are also used e g an agent is theperceived instigator of the action since one of the desired characteristic of aroles system is the power to discriminate event another quot desired quot property being to offer aneasier selection of grammatical function therecognition of semantic role should be linked tothe interpretation of the event that is to their dymmic 
a user may typically need to combine the strength of more than one system in order to perform a task in this paper we describe a component of the janus natural language interface that translates intensional logic expression representing the meaning of a request into executable code for each application program chooses which combination of application system to use and design the transfer of data among them in order to provide an answer the complete janus natural language system ha been ported to two large command and control decision support aid 
this paper present an enhanced model of plan based dialogue understanding most plan based dialogue understanding model derived from litman and allen assume that the dialogue speaker have access to the same domain plan library and that the active domain plan are shared by the two speaker we call these feature shared domain plan constraint these assumption however ae too strict to account for mixed initiative dialogue where each speaker ha a different set of domain plan that are housed in his or her own plan library and where an individual speaker s domain plan may be activated at any point in the dialogue we propose an extension to the litman and allen model by relaxing the shared domain plan constraint our extension improves the ability to track the currently active plan the ability to explain the planning behind speaker utterance and the ability to track which speaker control the conversational initiative in the dialogue 
functional unification grammar fug are popular for natural language application because the formalism us very few primitive and is uniform and expressive in our work on text generation we have found that it also ha annoying limitation it is not suited for the expression of simple yet very common taxonomic relation and it doe not allow the specification of completeness condition we have implemented an extension of traditional functional unification this extension address these limitation while preserving the desirable property of fug it is based on the notion of typed feature and typed constituent we show the advantage of this extension in the context of a grammar used for text generation 
the ida natural language generation system us a kl one type classifier to perform content determination surface realisation and part of text planning generation by classification allows ida to use a single representation and reasoning component for both domain and linguistic knowledge which is difficult for system based on unification or systemic generation technique 
in previous paper we presented method for retrieving collocation from large sample of text we described a tool xtract that implement these method and able to retrieve a wide range of collocation in a two stage process these method a well a other related method however have some limitation mainly the produced collocation do not include any kind of functional information and many of them are invalid in this paper we introduce method that address these issue these method are implemented in an added third stage to xtract that examines the set of collocation retrieved during the previous two stage to both filter out a number of invalid collocation and add useful syntactic information to the retained one by combining parsing and statistical technique the addition of this third stage ha raised the overall precision level of xtract from to with a precision of in the paper we describe the method and the evaluation experiment 
this paper describes a natural language processing system reinforced by the use of association of word and concept implemented a a neural network combining an associative network with a conventional system contributes to semantic disambiguation in the process of interpretation the model is employed within a and the advantage over conventional one are shown 
this paper present an analysis of purpose clause in the context of instruction understanding such analysis show that goal affect the interpretation and or execution of action lends support to the proposal of using generation and enablement to model relation between action and shed light on some inference process necessary to interpret purpose clause 
this paper present a unification procedure which eliminates the redundant copying of structure by using a lazy incremental copying approach to achieve structure sharing copying of structure account for a considerable amount of the total processing time several method have been proposed to minimize the amount of necessary copying lazy incremental copying lic is presented a a new solution to the copying problem it synthesizes idea of lazy copying with the notion of chronological dereferencing for achieving a high amount of structure sharing 
we investigate the logical structure of concept generated by conjunction and disjunction over a monotonic multiple inheritance network where concept node represent linguistic category and link indicate basic inclusion isa and disjointness isnota relation we model the distinction between primitive and defined concept a well a between closed and open world reasoning we apply our logical analysis to the sort inheritance and unification system of hpsg and also to classification in systemic choice system 
we show that the class of string language generated by linear context free rewriting system is equal to the class of output language of deterministic tree walking transducer from equivalence that have previously been established we know that this class of language is also equal to the string language generated by context free hypergraph grammar multicomponent tree adjoining grammar and multiple context free grammar and to the class of yield of image of the regular tree language under finite copying top down tree transducer 
in natural language processing ambiguity resolution is a central issue and can be regarded a a preference assignment problem in this paper a generalized probabilistic semantic model gpsm is proposed for preference computation an effective semantic tagging procedure is proposed for tagging semantic feature a semantic score function is derived based on a score function which integrates lexical syntactic and semantic preference under a uniform formulation the semantic score measure show substantial improvement in structural disambiguation over a syntax based approach 
this paper describes a new hardware algorithm for morpheme extraction and it implementation on a specific machine mex i a the first step toward achieving natural language parsing accelerator it also show the machine s performance time faster than a personal computer this machine can extract morpheme from character japanese text by searching an morpheme dictionary in second it can treat multiple text stream which are composed of character candidate a well a one text stream the algorithm is implemented on the machine in linear time for the number of candidate while conventional sequential algorithm are implemented in combinational time 
a pattern in the translation of locative prepositional phrase between english and spanish is presented a way of exploiting this pattern is proposed in the context of a multilingual machine translation system under development 
the three tiered discourse representation defined in luperfoy is applied to multimodal human computer interface hci dialogue in the applied system the three tier are a linguistic analysis morphological syntactic sentential semantic of input and output communicative event including keyboard entered command language atom nl string mouse click output text string and output graphical event a discourse model representation containing one discourse object called a peg for each construct each guise of an individual under discussion and the knowledge base kb representation of the computer agent s belief system which is used to support it interpretation procedure i present evidence to justify the added complexity of this three tiered system over standard two tiered representation based on a cognitive process that must be supported for any non idealized dialogue environment e g the agent can discus construct not present in their current belief system including information decay and the need for a distinction between understanding a discourse and believing the information content of a discourse b linguistic phenomenon in particular context dependent np which can be partially or totally anaphoric and c observed requirement of three implemented hci dialogue system that have employed this three tiered discourse representation 
i argue that because of spelling and typing error and other property of typed text the identification of word and word boundary in general requires syntactic and semantic knowledge a lattice representation is therefore appropriate for lexical analysis i show how the use of such a representation in the clare system allows different kind of hypothesis about word identity to be integrated in a uniform framework i then describe a quantitative evaluation of clare s performance on a set of sentence into which typographic error have been introduced the result show that syntax and semantics can be applied a powerful source of constraint on the possible correction for misspelled word 
strategy are proposed for combining different kind of constraint in declarative grammar with a detachable layer of control information the added control information is the basis for parametrized dynamically controlled linguistic deduction a form of linguistic processing that permit the implementation of plausible linguistic performance model without giving up the declarative formulation of linguistic competence the information can be used by the linguistic processor for ordering the sequence in which conjuncts and disjuncts are processed for mixing depth first and breadth first search for cutting off undesired derivation and for constraint relaxation 
the information state of an agent is changed when a text in natural language is processed the meaning of a text can be taken to be this information state change potential the inference of a consequence make explicit something already implicit in the premise i e that no information state change occurs if the assumed consequence text is processed after the given premise text have been processed elementary logic i e first order logic can be used a a logical representation language for text but the notion of a information state a set of possibility namely first order model is not available from the object language belongs to the meta language this mean that text with other text a part e g propositional attitude with embedded sentence cannot be treated directly traditional intensional logic i e modal logic allow via modal operator access to the information state from the object language but the access is limited and interference with extensional notion like standard identity variable etc is introduced this doe not mean that the idea present in intensional logic will not work possibly improved by adding a notion of partiality but rather that often a formalisation in the simple type theory with sort for entity and index making information state first class citizen like individual is more comprehensible flexible and logically well behaved 
this paper describes the initial development of a natural language text processor a the first step in an inr dialogue by voice system the eventual system will accept natural spontaneous speech from user and produce response from the database in the form of synthetic speech this paper report result in processing the textual version of atis air travel information system query the current system programmed in c accepts a input the cleaned up text snor version of the spoken query and produce the desired official airline guide oag information a output it us only the word in the input text and not any punctuation mark on the assumption that such mark are difficult to obtain directly from speech input based on the training text data the system correctly interprets a large majority of the textual query 
this paper present a tripartite model of dialogue in which three different kind of action are modeled domain action problem solving action and discourse or communicative action we contend that our process model provides a more finely differentiated representation of user intention than previous model enables the incremental recognition of communicative action that cannot be recognized from a single utterance alone and account for implicit acceptance of a communicated proposition 
we describe a method for obtaining subject dependent word set relative to some subject domain using the subject classification given in the machine redable version of longman s dictionary of contemporary english we established subject dependent co occurrence link between word of the defining vocabulary to construct these neighborhood here we describe the application of these neighborhood to information retrieval and present a method of word sense disambiguation based on these co occurrence an extension of previous work 
in this paper we compare two grammar based generation algorithm the semantic head driven generation algorithm shdga and the essential argument algorithm eaa both algorithm have successfully addressed several outstanding problem in grammar based generation including dealing with non monotonic compositionality of representation left recursion deadlock prone rule and nondeterminism we concentrate here on the comparison of selected property generality efficiency and determinism we show that eaa s traversal of the analysis tree for a given language construct include also the one taken on by shdga we also demonstrate specific and common situation in which shdga will invariably run into serious inefficiency and nondeterminism and which eaa will handle in an efficient and deterministic manner we also point out that only eaa allows to treat the underlying grammar in a truly multi directional manner 
in modeling the structure of task related discourse using plan it is important to distinguish between plan that the agent ha adopted and is pursuing and those that are only being considered and explored since the kind of utterance arising from a particular domain plan and the pattern of reference to domain plan and movement within the plan tree are quite different in the two case this paper present a three level discourse model that us separate domain and exploration layer in addition to a layer of discourse metaplans allowing these distinct behavior pattern and the plan adoption and reconsideration move they imply to be recognized and modeled 
researcher in both machine translation e g brown et al and bilingual lexicography e g klavans and tzoukermann have recently become interested in studying bilingual corpus body of text such a the canadian hansard parliamentary proceeding which are available in multiple language such a french and english one useful step is to align the sentence that is to identify correspondence between sentence in one language and sentence in the other language this paper will describe a method and a program align for aligning sentence based on a simple statistical model of character length the program us the fact that longer sentence in one language tend to be translated into longer sentence in the other language and that shorter sentence tend to be translated into shorter sentence a probabilistic score is assigned to each proposed correspondence of sentence based on the scaled difference of length of the two sentence in character and the variance of this difference this probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentence it is remarkable that such a simple approach work a well a it doe an evaluation wa performed based on a trilingual corpus of economic report issued by the union bank of switzerland ubs in english french and german the method correctly aligned all but of the sentence moreover it is possible to extract a large subcorpus that ha a much smaller error rate by selecting the best scoring of the alignment the error rate is reduced from to there were more error on the english french subcorpus than on the english german subcorpus showing that error rate will depend on the corpus considered however both were small enough to hope that the method will be useful for many language pair to further research on bilingual corpus a much larger sample of canadian hansard approximately million word half in english and and half in french ha been aligned with the align program and will be available through the data collection initiative of the association for computational linguistics acl dci in addition in order to facilitate replication of the align program an appendix is provided with detailed c code of the more difficult core of the align program 
this paper report a handful of experiment designed to test the feasibility of applying well known partial parsing technique to the problem of automatic data base update from an open ended source of message and the feasiblity of automatically learning semantic knowledge from annotated example the challenge arise from the incompleteness of any lexicon sentence that average over word in length and the lack of a complete semantics 
the class of linear context free rewriting system ha been introduced a a generalization of a class of grammar formalism known a mildly context sensitive the recognition problem for linear context free rewriting language is studied at length here presenting evidence that even in some restricted case it cannot be solved efficiently this entail the existence of a gap between for example tree adjoining language and the subclass of linear context free rewriting language that generalizes the former class such a gap is attributed to crossing configuration a few other interesting consequence of the main result are discussed that concern the recognition problem for linear context free rewriting language 
integration of language constraint into a large vocabulary speech recognition system often lead to prohibitive complexity we propose to factor the constraint into two component the first is characterized by a covering grammar which is small and easily integrated into existing speech recognizers the recognized string is then decoded by mean of an efficient language post processor in which the full set of constraint is imposed to correct possible error introduced by the speech recognizer 
prosodic structure and syntactic structure are not identical neither are they unrelated knowing when and how the two eorrespoud could yield better quality speech synthesis could aid in the disambiguation of com peting syntactic hypothesis in speech understanding and could lead to a more comprehensive view of human speech processing in a set of exper iments involving pair of phonetically similar sentence s representing seven type of structural contrast the perceptual evidence show that some but not all of the pair can be disambiguated on the basis of pro sodie difference the phonological evidence relates the disambiguation primarily to boundary phenomenon although prominence sometimes play a role finally phonetic analysis describing the attribute of these phonological marker indicate the importance of both absolute and rela tive measure 
for a very long time it ha been considered that the only way of automatically extracting similar group of word from a text collection for which no semantic information exists is to use document co occurrence data but with robust syntactic parser that are becoming more frequently available syntactically recognizable phenomenon about word usage can be confidently noted in large collection of text we present here a new system called sextant which us these parser and the finer grained context they produce to judge word similarity 
recent study on the analysis of intonational function examine a range of material from cue phrase in monologue litman and hirschberg and dialogue hirschberg and litman hockey to longer utterance in both monologue and dialogue mclemore result match specific intonational tune to certain discourse function which are more or le well defined although these result make a convincing case that intonation doe signal a change in discourse structure the specification of discourse function remains vague a suitable taxonomy is needed to fine tune the relationship between intonation and discourse function a recent analysis of dialogue kowtko et al provides a framework of conversational game which allows more fine grained examination of prosodic function the current paper introduces an intonational analysis of mono and di syllabic word based upon such a framework and compare result in progress with previous work on intonation 
this paper proposes a set of representation for tense and a set of constraint on how they can be combined in adjunct clause the semantics we propose explains the possible meaning of tense in a variety of sentential context it also support an elegant constraint on tense combination in adjunct clause these semantic representation provide insight into the interpretation of tense and the constraint provide a source of syntactic disambiguation that ha not previously been demonstrated we demonstrate an implemented disambiguator for a certain class of three clause sentence based on our theory 
in this paper we present a polynomial time parsing algorithm for combinatory categorial grammar the recognition phase extends the cky algorithm for cfg the process of generating a representation of the parse tree ha two phase initially a shared forest is build that encodes the set of all derivation tree for the input string this shared forest is then pruned to remove all spurious ambiguity 
though most translation system have some mechanism for translating certain type of divergent predicate argument structure they do not provide a general procedure that take advantage of the relationship between lexical semantic structure and syntactic structure a divergent predicate argument structure is one in which the predicate e g the main verb or it argument e g the subject and object do not have the same syntactic ordering property for both the source and target language to account for such ordering difference a machine translator must consider language specific syntactic idiosyncrasy that distinguish a target language from a source language while making use of lexical semantic uniformity that tie the two language together this paper describes the mechanism used by the unitran machine translation system for mapping an underlying lexical conceptual structure to a syntactic structure and vice versa and it show how these mechanism coupled with a set of general linking routine solve the problem of thematic divergence in machine translation 
in this paper we present a new parsing model of linguistic and computational interest linguistically the relation between the parser and the theory of grammar adopted government and binding gb theory a presented in chomsky a b is clearly specified computationally this model adopts a mixed parsing procedure by using left corner prediction in a modified lr parser 
abstract feature structure are informational element that have been used in several linguistic theory and in computational system for natural language processing a logicaj calculus ha been developed and used a a description language for feature structure in the present work a framework in three valued logic is suggested for defining the semantics of a feature structure description language allowing for a more complete set of logical operator in particular the semantics of the negation and implication operator are examined various proposed interpretation of negation and implication are compared within the suggested framework one particular interpretation of the description language with a negation operator is described and it computational aspect studied 
