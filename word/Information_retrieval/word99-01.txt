in many application large volume of time sensitive textual information require triage rapid approximate prioritization for subsequent action in this paper we explore the use of prospective indication of the importance of a time sensitive document for the purpose of producing better document filtering or ranking by prospective we mean importance that could be assessed by action that occur in the future for example a news story may be assessed retrospectively a being important based on event that occurred after the story appeared such a a stock price plummeting or the issuance of many follow up story if a system could anticipate prospectively such occurrence it could provide a timely indication of importance clearly perfect prescience is impossible however sometimes there is sufficient correlation between the content of an information item and the event that occur subsequently we describe a process for creating and evaluating approximate information triage procedure that are based on prospective indication unlike many information retrieval application for which document labeling is a laborious manual process for many prospective criterion it is possible to build very large labeled training corpus automatically such corpus can be used to train text classification procedure that will predict the prospective importance of each document this paper illustrates the process with two case study demonstrating the ability to predict whether a news story will be followed by many very similar news story and also whether the stock price of one or more company associated with a news story will move significantly following the appearance of that story we conclude by discussing how the comprehensibility of the learned classifier can be critical to success 
we address the problem of integrating document from different source into a master catalog this problem is pervasive in web marketplace and portal current technology for automating this process consists of building a classifier that us the categorization of document in the master catalog to construct a model for predicting the category of unknown document our key insight is that many of the data source have their own categorization and classification accuracy can be improved by factoring in the implicit information in these source categorization we show how a naive bayes classification can be enhanced to incorporate the similarity information present in source catalog our analysis and empirical evaluation show substantial improvement in the accuracy of catalog integration 
given the ranked list of document returned by multiple search engine in response to a given query the problem ofmetasearchis to combine these list in a way which optimizes the performance of the combination this paper make three contribution to the problem of metasearch we describe and investigate a metasearch model based on an optimal democratic voting procedure the borda count we describe and investigate a metasearch model based on bayesian inference and we describe and investigate a model for obtaining upper bound on the performance of metasearch algorithm our experimental result show that metasearch algorithm based on the borda and bayesian model usually outperform the best input system and are competitive with and often outperform existing metasearch strategy finally our initial upper bound demonstrate that there is much to learn about the limit of the performance of metasearch 
the most prevalent experimental methodology for comparing the effectiveness of information retrieval system requires a test collection composed of a set of document a set of query topic and a set of relevance judgment indicating which document are relevant to which topic it is well known that relevance judgment are not infallible but recent retrospective investigation into result from the text retrieval conference trec ha shown that difference in human judgment of relevance do not affect the relative measured performance of retrieval system based on this result we propose and describe the initial result of a new evaluation methodology which replaces human relevance judgment with a randomly selected mapping of document to topic which we refer to aspseudo relevance judgment ranking of system with our methodology correlate positively with official trec ranking although the performance of the top system is not predicted well the correlation are stable over a variety of pool depth and sampling technique with improvement such a methodology could be useful in evaluating system such a world wide web search engine where the set of document change too often to make traditional collection construction technique practical 
we investigate the effect of speech recognition error on a system for the unsupervised nearly synchronous clustering of broadcast news story using the tdt topic detection and tracking corpus two question are addressed are speech recognition error detrimental to the performance of the system can a background collection of contemporaneous clean text improve performance we investigate both the large cluster and small cluster limit 
several machine learning algorithm have recently been used for text categorization and filtering in particular boosting method such a adaboost have shown good performance applied to real text data however most of existing boosting algorithm are based on classifier that use binary valued feature thus they do not fully make use of the weight information provided by standard term weighting method in this paper we present a boosting based learning method for text filtering that us naive bayes classifier a a weak learner the use of naive bayes allows the boosting algorithm to utilize term frequency information while maintaining probabilistically accurate confidence ratio applied to trec and trec filtering track document the proposed method obtained a significant improvement in lf lf f and f measure compared to the best result submitted by other trec entry 
we present a novel algorithm that creates document vector with reduced dimensionality this work wa motivated by an application characterizing relationship among document in a collection our algorithm yielded inter document similarity with an average precision up to higher than that of singular value decomposition svd used for latent semantic indexing the best performance wa achieved with dimensional reduction rate that were higher than svd on average our algorithm creates basis vector for a reduced space by iteratively scaling vector and computing eigenvectors unlike svd it break the symmetry of document and term to capture information more evenly across document we also discus correlation with a probabilistic model and evaluate a method for selecting the dimensionality using log likelihood estimation 
this demonstration will show describe the construction and application of cross domain information server using feature of the standard z information retrieval protocol z the system is currently being used to build and search distributed index for database with disparate structured data sgml and xml we use the z explain database to determine the database and index of a given server then use the z scan facility to extract the content of the index this information is used to build collection document that can be retrieved using probabilistic retrieval algorithm 
many problem are difficult to adequately explore until a prototype exists in order to elicit user feedback one such problem is a system that automatically categorizes and manages email due to a myriad of user interface issue a prototype is necessary to determine what technique and technology are effective in the email domain this paper describes the implementation of an add in for microsoft outlook tm that intends to address two problem with email help manage the inbox by automatically classifying email based on user folder and to aid in search and retrieval by providing a list of email relevant to the selected item this add in represents a first step in an experimental system for the study of other issue related to information management the system ha been set up to allow experimentation with other classification algorithm and the source code is available online in an effort to promote further experimentation 
test collection have traditionally been used by information retrieval researcher to improve their retrieval strategy to be viable a a laboratory tool a collection must reliably rank different retrieval variant according to their true effectiveness in particular the relative effectiveness of two retrieval strategy should be insensitive to modest change in the relevant document set since individual relevance assessment are known to vary widely the test collection developed in the trec workshop have become the collection of choice in the retrieval research community to verify their reliability nist investigated the effect change in the relevance assessment have on the evaluation of retrieval result very high correlation were found among the ranking of system produced using different relevance judgment set the high correlation indicate that the comparative evaluation of retrieval performance is stable despite substantial difference in relevance judgment and thus reaffirm the use of the trec collection a laboratory tool 
this paper investigates whether a machine can automatically learn the task of finding within a large collection of candidate response the answer to question the learning process consists of inspecting a collection of answered question and characterizing the relation between question and answer with a statistical model for the purpose of learning this relation we propose two source of data usenet faq document and customer service call center dialogue from a large retail company we will show that the task of answer finding differs from both document retrieval and tradition question answering presenting challenge different from those found in these problem the central aim of this work is to discover through theoretical and empirical investigation those statistical technique best suited to the answer finding problem 
a method of assisting a user in finding the required document effectively is proposed a user being informed which document are worth examining can browse in a digital library dl in a linear fashion computational evaluation were carried out and a dl and it navigator are designed and constructed 
we present a java based framework swami shared wisdom through the amalgamation of many interpretation for building and studying collaborative ltering system swami consists of three component a prediction engine an evaluation system and a visualization component the prediction engine provides a common interface for implementing dierent prediction algorithm the evaluation system provides a standardized testing methodology and metric for analyzing the accuracy and run time performance of prediction algorithm the visualization component suggests how graphical representation can inform the development and analysis of prediction algorithm we demonstrate swami on the eachmovie data set by comparing three prediction algorithm a traditional pearson correlation based method support vector machine and a new accurate and scalable correlation based method based on clustering technique 
in many application large volume of time sensitive textual information require triage rapid approximate prioritization for subsequent action in this paper we explore the use of prospective indication of the importance of a time sensitive document for the purpose of producing better document filtering or ranking by prospective we mean importance that could be assessed by action that occur in the future for example a news story may be assessed retrospectively a being important based on event that occurred after the story appeared such a a stock price plummeting or the issuance of many follow up story if a system could anticipate prospectively such occurrence it could provide a timely indication of importance clearly perfect prescience is impossible however sometimes there is sufficient correlation between the content of an information item and the event that occur subsequently we describe a process for creating and evaluating approximate information triage procedure that are based on prospective indication unlike many information retrieval application for which document labeling is a laborious manual process for many prospective criterion it is possible to build very large labeled training corpus automatically such corpus can be used to train text classification procedure that will predict the prospective importance of each document this paper illustrates the process with two case study demonstrating the ability to predict whether a news story will be followed by many very similar news story and also whether the stock price of one or more company associated with a news story will move significantly following the appearance of that story we conclude by discussing how the comprehensibility of the learned classifier can be critical to success 
we address the problem of integrating document from different source into a master catalog this problem is pervasive in web marketplace and portal current technology for automating this process consists of building a classifier that us the categorization of document in the master catalog to construct a model for predicting the category of unknown document our key insight is that many of the data source have their own categorization and classification accuracy can be improved by factoring in the implicit information in these source categorization we show how a naive bayes classification can be enhanced to incorporate the similarity information present in source catalog our analysis and empirical evaluation show substantial improvement in the accuracy of catalog integration 
given the ranked list of document returned by multiple search engine in response to a given query the problem ofmetasearchis to combine these list in a way which optimizes the performance of the combination this paper make three contribution to the problem of metasearch we describe and investigate a metasearch model based on an optimal democratic voting procedure the borda count we describe and investigate a metasearch model based on bayesian inference and we describe and investigate a model for obtaining upper bound on the performance of metasearch algorithm our experimental result show that metasearch algorithm based on the borda and bayesian model usually outperform the best input system and are competitive with and often outperform existing metasearch strategy finally our initial upper bound demonstrate that there is much to learn about the limit of the performance of metasearch 
the most prevalent experimental methodology for comparing the effectiveness of information retrieval system requires a test collection composed of a set of document a set of query topic and a set of relevance judgment indicating which document are relevant to which topic it is well known that relevance judgment are not infallible but recent retrospective investigation into result from the text retrieval conference trec ha shown that difference in human judgment of relevance do not affect the relative measured performance of retrieval system based on this result we propose and describe the initial result of a new evaluation methodology which replaces human relevance judgment with a randomly selected mapping of document to topic which we refer to aspseudo relevance judgment ranking of system with our methodology correlate positively with official trec ranking although the performance of the top system is not predicted well the correlation are stable over a variety of pool depth and sampling technique with improvement such a methodology could be useful in evaluating system such a world wide web search engine where the set of document change too often to make traditional collection construction technique practical 
we investigate the effect of speech recognition error on a system for the unsupervised nearly synchronous clustering of broadcast news story using the tdt topic detection and tracking corpus two question are addressed are speech recognition error detrimental to the performance of the system can a background collection of contemporaneous clean text improve performance we investigate both the large cluster and small cluster limit 
several machine learning algorithm have recently been used for text categorization and filtering in particular boosting method such a adaboost have shown good performance applied to real text data however most of existing boosting algorithm are based on classifier that use binary valued feature thus they do not fully make use of the weight information provided by standard term weighting method in this paper we present a boosting based learning method for text filtering that us naive bayes classifier a a weak learner the use of naive bayes allows the boosting algorithm to utilize term frequency information while maintaining probabilistically accurate confidence ratio applied to trec and trec filtering track document the proposed method obtained a significant improvement in lf lf f and f measure compared to the best result submitted by other trec entry 
we present a novel algorithm that creates document vector with reduced dimensionality this work wa motivated by an application characterizing relationship among document in a collection our algorithm yielded inter document similarity with an average precision up to higher than that of singular value decomposition svd used for latent semantic indexing the best performance wa achieved with dimensional reduction rate that were higher than svd on average our algorithm creates basis vector for a reduced space by iteratively scaling vector and computing eigenvectors unlike svd it break the symmetry of document and term to capture information more evenly across document we also discus correlation with a probabilistic model and evaluate a method for selecting the dimensionality using log likelihood estimation 
this demonstration will show describe the construction and application of cross domain information server using feature of the standard z information retrieval protocol z the system is currently being used to build and search distributed index for database with disparate structured data sgml and xml we use the z explain database to determine the database and index of a given server then use the z scan facility to extract the content of the index this information is used to build collection document that can be retrieved using probabilistic retrieval algorithm 
many problem are difficult to adequately explore until a prototype exists in order to elicit user feedback one such problem is a system that automatically categorizes and manages email due to a myriad of user interface issue a prototype is necessary to determine what technique and technology are effective in the email domain this paper describes the implementation of an add in for microsoft outlook tm that intends to address two problem with email help manage the inbox by automatically classifying email based on user folder and to aid in search and retrieval by providing a list of email relevant to the selected item this add in represents a first step in an experimental system for the study of other issue related to information management the system ha been set up to allow experimentation with other classification algorithm and the source code is available online in an effort to promote further experimentation 
test collection have traditionally been used by information retrieval researcher to improve their retrieval strategy to be viable a a laboratory tool a collection must reliably rank different retrieval variant according to their true effectiveness in particular the relative effectiveness of two retrieval strategy should be insensitive to modest change in the relevant document set since individual relevance assessment are known to vary widely the test collection developed in the trec workshop have become the collection of choice in the retrieval research community to verify their reliability nist investigated the effect change in the relevance assessment have on the evaluation of retrieval result very high correlation were found among the ranking of system produced using different relevance judgment set the high correlation indicate that the comparative evaluation of retrieval performance is stable despite substantial difference in relevance judgment and thus reaffirm the use of the trec collection a laboratory tool 
this paper investigates whether a machine can automatically learn the task of finding within a large collection of candidate response the answer to question the learning process consists of inspecting a collection of answered question and characterizing the relation between question and answer with a statistical model for the purpose of learning this relation we propose two source of data usenet faq document and customer service call center dialogue from a large retail company we will show that the task of answer finding differs from both document retrieval and tradition question answering presenting challenge different from those found in these problem the central aim of this work is to discover through theoretical and empirical investigation those statistical technique best suited to the answer finding problem 
a method of assisting a user in finding the required document effectively is proposed a user being informed which document are worth examining can browse in a digital library dl in a linear fashion computational evaluation were carried out and a dl and it navigator are designed and constructed 
we present a java based framework swami shared wisdom through the amalgamation of many interpretation for building and studying collaborative ltering system swami consists of three component a prediction engine an evaluation system and a visualization component the prediction engine provides a common interface for implementing dierent prediction algorithm the evaluation system provides a standardized testing methodology and metric for analyzing the accuracy and run time performance of prediction algorithm the visualization component suggests how graphical representation can inform the development and analysis of prediction algorithm we demonstrate swami on the eachmovie data set by comparing three prediction algorithm a traditional pearson correlation based method support vector machine and a new accurate and scalable correlation based method based on clustering technique 
the goal of this paper is to argue the need to approach the personalization issue in web application from the very beginning in the application s development cycle since personalization is a critical aspect in many popular domain such a e commerce it important enough that it should be dealt with through a design view rather than only an implementation view which discus mechanism rather than design option we present different scenario of personalization covering most existing application since our design approach is based on the object oriented hypermedia design method we briefly introduce it emphasizing the way in which we build web application model a object oriented view of conceptual model we show how we specify personalized web application by refining view according to user profile or preference we show that an object oriented approach allows maximizing reuse in these specification we discus some implementation aspect and compare our work with related approach and present some concluding remark 
this paper present a novel way of examining the accuracy of the evaluation measure commonly used in information retrieval experiment it validates several of the rule of thumb experimenter use such a the number of query needed for a good experiment is at least and is better while challenging other belief such a the common evaluation measure are equally reliable a an example we show that precision at document ha about twice the average error rate a average precision ha these result can help information retrieval researcher design experiment that provide a desired level of confidence in their result in particular we suggest researcher using web measure such a precision at document will need to use many more than query or will have to require two method to have a very large difference in evaluation score before concluding that the two method are actually different 
typically commercial web search engine provide very little feedback to the user concerning how a particular query is processed and interpreted specifically they apply key query transformation without the user knowledge although these transformation have a pronounced effect on query result user have very few resource for recognizing their existence and understanding their practical importance we conducted a user study to gain a better understanding of user knowledge of and reaction to the operation of several query transformation that web search engine automatically employ additionally we developed and evaluated transparent query a software system designed to provide user with lightweight feedback about opaque query transformation the result of the study suggest that user do indeed have difficulty understanding the operation of query transformation without additional assistance finally although transparency is helpful and valuable interface that allow direct control of query transformation might ultimately be more helpful for end user 
mobile internet technology such a wap are important for pervasive anytime anywhere computing although much progress ha been made in term of technological innovation many of mobile internet system are difficult to use lack flexibility and robustness they give a poor user experience evaluation and theoretical analysis of usability combined with innovative design can achieve significant improvement in user performance and satisfaction using such multidisciplinary method explains the negative reaction to wap and more constructively suggest way of developing more effective and efficient device and service 
in this work we examine evidence combination mechanism for classifying multimedia information in particular we examine linear and dempster shafer method of evidence combination in the context of identifying personal image on the world wide web an automatic web search engine named diogenes search the web for personal image and combine different piece of evidence for identification the source of evidence consist of input from face detection recognition and text html analysis module a degree of uncertainty is involved with both of these source diogenes automatically determines the uncertainty locally for each retrieval and us this information to set a relative significance for each evidence to our knowledge diogenes is the first image search engine using dempster shafer evidence combination based on automatic object recognition and dynamic local uncertainty assessment in our experiment diogenes comfortably outperformed some well known commercial and research prototype image search engine for celebrity image query 
in this paper we present the feature of a question answering q a system that had unparalleled performance in the trec evaluation we explain the accuracy of our system through the unique characteristic of it architecture usage of a wide coverage answer type taxonomy repeated passage retrieval lexico semantic feedback loop extraction of the answer based on machine learning technique and answer caching experimental result show the effect of each feature on the overall performance of the q a system and lead to general conclusion about q a from large text collection 
thresholding strategy in automated text categorization are an underexplored area of research this paper present an examination of the effect of thresholding strategy on the performance of a classifier under various condition using k nearest neighbor knn a the classifier and five evaluation benchmark collection a the testbets three common thresholding method were investigated including rank based thresholding rcut proportion based assignment pcut and score based local optimization scut in addition new variant of these method are proposed to overcome significant problem in the existing approach experimental result show that the choice of thresholding strategy can significantly influence the performance of knn and that the optimal strategy may vary by application scut is potentially better for fine tuning but risk overfitting pcut cope better with rare category and exhibit a smoother trade off in recall versus precision but is not suitable for online decision making rcut is most natural for online response but is too coarse grained for global or local optimization rtcut a new method combining the strength of category ranking and scoring outperforms both pcut and rcut significantly 
in the world wide web myriad of hyperlink connect document and page to create an unprecedented highly complex graph structure the web graph this paper present a novel approach to learning probabilistic model of the web which can be used to make reliable prediction about connectivity and information content of web document the proposed method is a probabilistic dimension reduction technique which recasts and unites latent semantic analysis and kleinberg s hub and authori tie algorithm in a statistical setting this is meant to be a first step towards the development of a statistical foundation for web related information technology although this paper doe not focus on a particular application a variety of algorithm operating in the web internet environment can take advantage of the presented technique including search engine web crawler and information agent system 
we explore the relation between classical probabilistic model of information retrieval and the emerging language modeling approach it ha long been recognized that the primary obstacle to effective performance of classical model is the need to estimate arelevance model probability of word in the relevant class we propose a novel technique for estimating these probability using the query alone we demonstrate that our technique can produce highly accurate relevance model addressing important notion of synonymy and polysemy our experiment show relevance model outperforming baseline language modeling system on trec retrieval and tdt tracking task the main contribution of this work is an effective formal method for estimating a relevance model with no training data 
topic segmentation is an important initial step in many text based task a hierarchical representation of a text topic is useful in retrieval and allows judging relevancy at different level of detail this short paper describes research on generic algorithm for topic detection and segmentation that are applicable on text of heterogeneous type and domain 
query expansion is an effective relevance feedback technique for improving performance in information retrieval in general query expansion method select term from the complete content of relevant document one problem with this approach is that expansion term unrelated to document relevance can be introduced into the modified query due to their presence in the relevant document and distribution in the document collection motivated by the hypothesis that query expansion term should only be sought from the most relevant area of a document this investigation explores the use of document summary in query expansion the investigation explores the use of both context independent standard summary and query biased summary experimental result using the okapi bm probabilistic retrieval model with the trec ad hoc retrieval task show that query expansion using document summary can be considerably more effective than using full document expansion the paper also present a novel approach to term selection that separate the choice of relevant document from the selection of a pool of potential expansion term again this technique is shown to be more effective that standard method 
previous work addressing the issue of word distribution in document ha shown the importance of word repetitiveness a an indicator of the word contentbearing characteristic in this paper we propose a simple method using a measure of the tendency of word to repeat within a document to separate the word with similar document frequency but different topic discriminating characteristic 
content based image retrieval cbir present special challenge in term of how image data is indexed accessed and how end system are evaluated this paper discus the design of a cbir system that us global colour a the primary indexing key and a user centered evaluation of the system visual search tool the result indicate that user are able to make use of a range of visual search tool and that different tool are used at different point in the search process the result also show that the provision of a structured navigation and browsing tool can support image retrieval particularly in situation in which the user doe not have a target image in mind the result are discussed in term of their implication for the design of visual search tool and their implication for the use of user centered evaluation for cbir system 
latent semantic indexing lsi dramatically reduces the dimension of the document space by mapping it into a space spanned by conceptual index empirically the number of concept that can represent the document are far fewer than the great variety of word in the textual representation although this almost obviates the problem of lexical matching the mapping incurs a high computational cost compared to document parsing indexing query matching and updating this paper show how lsi is based on a unitary transformation for which there are computationally more attractive alternative this is exemplified by the haar transform which is memory efficient and can be computed in linear to sublinear time the principle advantage of lsi are thus preserved while the computational cost are drastically reduced 
in this paper we report result of an investigation into englishjapanese cross language information retrieval clir comparing a number of query translation method result from experiment using the standard bmir j japanese collection suggest that full machine translation mt can outperform popular dictionary based query translation method and further that in this context mt is largely robust to query with little linguistic structure 
the www is the most important resource for external business information this paper present a tool called insyder an information assistant for finding and analysis business information from the www insyder is a system using different agent for crawling the web evaluating and visualising the result these agent the used visualisation and a first summary of user study are presented 
collaborative filtering is a technique for recommending document to user based on how similar their taste are to other user if two user tend to agree on what they like the system will recommend the same document to them the generalized vector space model of information retrieval represents a document by a vector of it similarity to all other document the process of collaborative filtering is nearly identical to the process of retrieval using gvsm in a matrix of user rating using this observation a model for filtering collaboratively using document content is possible 
many web page have implicit structure in this paper we show the feasibility of automatically extracting data from web page by using approximate matching technique this can be applied to generate automatic wrapper or to notify display web page difference web page change monitoring etc 
in this paper we present a method that can automatically evaluate performance of different term weighting scheme in information retrieval without resorting to precision recall based on human relevance judgment specifically the problem is given two document term matrix generated from two different term weighting scheme can we tell which term weighting scheme will performance better than the other we propose a meta scoring function which take a input the document term matrix generated by some term weighting scheme and computes a goodness score from the document term matrix in our experiment we found out that this score is highly correlated with the precision recall measurement for all the collection and term weighting schema we tried thus we conclude that our meta scoring function can be a substitute for the precision recall measurement that need relevance judgment of human subject furthermore this meta scoring function is not limited only to text information retrieval can be applied to field such a image and dna retrieval 
the retrieval of stored image matching an input configuration is an important form of content based retrieval exhaustive processing i e retrieval of the best solution of configuration similarity query is in general exponential and fast search for sub optimal solution is the only way to deal with the vast and ever increasing amount of multimedia information in several real time application in this paper we discus the utilization of hill climbing heuristic that can provide very good result within limited processing time we propose several heuristic which differ on the way that they search through the solution space and identify the best one depending on the query and image characteristic finally we develop new algorithm that take advantage of the specific structure of the problem to improve performance 
introductionindexing and searching for digital information especially imagesand video are becoming important issue because of theremarkable increase of digital document content based imageretrieval technique is receiving widespread research interest and many image retrieval system such a qbic photobook and visualseek have been developedso far a lot of effort have been put on image feature extractionin term of colour texture shape and spatial relation i e 
this paper proposes a method for event tracking on broadcast news story based on distinction between a topic and an event a topic and an event are identified using a simple criterion called domain dependency of word how greatly a word feature a given set of data the method wa tested on the tdt corpus which ha been developed by the tdt pilot study and the result can be regarded a promising the usefulness of the method 
in this paper we examine the learning behavior of a heuristic threshold setting approach to information filtering in particular we study how different initial threshold setting and different updating parameter setting affect threshold learning the result on one of the trec news database indicate that learning allows recovery from the inevitable nonoptimality of the initial condition and a greater willingness to learn expressed by a deliberate lowering of the score threshold in the learning stage doe eventually lead to a higher performance in spite of the expected initial performance penalty 
we describe a text categorization approach that is based on a combination of feature distributional cluster with a support vector machine svm classifier our feature selection approach employ distributional clustering of word via the recently introducedinformation bottleneck method which generates a more efficientword clusterrepresentation of document combined with the classification power of an svm this method yield high performance text categorization that can outperform other recent method in term of categorization accuracy and representation efficiency comparing the accuracy of our method with other technique we observe significant dependency of the result on the data set we discus the potential reason for this dependency 
the cambridge university multimedia document retrieval demo system is a web based application that allows the user to query a database of automatically generated transcript of radio broadcast that are available on line the paper describes how speech recognition and information retrieval technique are combined in this system and show how the user can interact with it 
an ideal retrieval system should retrieve image that satisfy the user s need and should therefore measure image similarity in a manner consistent with human s perception however existing computational similarity measure are not perceptually consistent this paper proposes an approach of improving retrieval performance by improving the perceptual consistency of computational similarity measure for texture based on relevance feedback judgment 
previous research ha shown that citation and hypertext link can be usefully combined with document content to improve retrieval link can be used in many way e g link topology can be used to identify important page anchor text can be used to augment the text of cited page and activation can be spread to linked page this paper introduces a probabilistic model that integrates content matching and these three us of link information in a single unified framework experiment with a web collection show benefit for link information especially for general query 
algorithm for the segmentation of an audio video source into topically cohesive segment based on automatic speech recognition asr transcription is presented a novel two pas algorithm is described that combine a boundary based method with a content based method in the first pas the temporal proximity and the rate of arrival of ngram feature is analyzed in order to compute an initial segmentation in the contentbased second pas change in content bearing word are detected by using the ngram feature a query in an information retrieval system the second pas validates the initial segment and merges them a needed feasibility of the segmentation task can vary enormously depending on the structure of the audio content and the accuracy of asr for real world corporate training data our method identifies at worst a single salient segment of the audio and at best a high level table of content we illustrate the algorithm in detail with some example and validate the result with segmentation boundary generated manually 
this paper present a layered component model to support web based interactive and collaborative application it is intended to let programmer focus on the particular logic of their application avoiding most of the issue related to collaboration access control and networking management a web based educational application ha been developed over this framework this tele education system which follows the recommendation by the main institution involved in the learning technology 
we introduce a new probabilistic model for combining the output of an arbitrary number of query retrieval system by gathering simple statistic on the average performance of a given set of query retrieval system we construct a bayes optimal mechanism for combining the output of these system our construction yield a metasearch strategy whose empirical performance nearly always exceeds the performance of any of the constituent system our construction is also robust in the sense that if good and bad system are combined the performance of the composite is still on par with or exceeds that of the best constituent system finally our model and theory provide theoretical and empirical avenue for the improvement of this metasearch strategy 
automated tracking of event from chronologically ordered document stream is a new challenge for statistical text classification existing learning technique must be adapted or improved in order to effectively handle difficult situation where the number of positive training instance per event is extremely small the majority of training document are unlabelled and most of the event have a short duration in time we adapted several supervised text categorization method specifically several new variant of the k nearest neighbor knn algorithm and a rocchio approach to track event all of these method showed significant improvement up to reduction in weighted error rate over the performance of the original knn algorithm on tdt benchmark collection making knn among the top performing system in the recent tdt official evaluation furthermore by combining these method we significantly reduced the variance in performance of our event tracking system over different data collection suggesting a robust solution for parameter optimization 
in this paper we propose two generic text summarization method that create text summary by ranking and extracting sentence from the original document the first method us standard ir method to rank sentence relevance while the second method us the latent semantic analysis technique to identify semantically important sentence for summary creation both method strive to select sentence that are highly ranked and different from each other this is an attempt to create a summary with a wider coverage of the document s main content and le redundancy performance evaluation on the two summarization method are conducted by comparing their summarization output with the manual summary generated by three independent human evaluator the evaluation also study the influence of different vsm weighting scheme on the text summarization performance finally the cause of the large disparity in the evaluator manual summarization result are investigated and discussion on human text summarization pattern are presented 
we present a novel probabilistic method for topic segmentation on unstructured text one previous approach to this problem utilizes the hidden markov model hmm method for probabilistically modeling sequence data the hmm treat a document a mutually independent set of word generated by a latent topic variable in a time series we extend this idea by embedding hofmann s aspect model for text into the segmenting hmm to form an aspect hmm ahmm in doing so we provide an intuitive topical dependency between word and a cohesive segmentation model we apply this method to segment unbroken stream of new york time article a well a noisy transcript of radio program on speechbot an online audio archive indexed by an automatic speech recognition engine we provide experimental comparison which show that the ahmm outperforms the hmm for this task 
the paper present a novel approach to unsupervised text summarization the novelty lie in exploiting the diversity of concept in text for summarization which ha not received much attention in the summarization literature a diversity based approach here is a principled generalization of maximal marginal relevance criterion by carbonell and goldstein cite carbonell goldstein we propose in addition aninformation centricapproach to evaluation where the quality of summary is judged not in term of how well they match human created summary but in term of how well they represent their source document in ir task such document retrieval and text categorization to find the effectiveness of our approach under the proposed evaluation scheme we set out to examine how a system with the diversity functionality performs against one without using the bmir j corpus a test data developed by a japanese research consortium the result demonstrate a clear superiority of a diversity based approach to a non diversity based approach 
a sentence extract summary of a document is a subset of the document s sentence that contains the main idea in the document we present an approach to generating such summary a hidden markov model that judge the likelihood that each sentence should be contained in the summary we compare the result of this method with summary generated by human showing that we obtain significantly higher agreement than do earlier method 
the explosion of content in distributed information retrieval ir system requires new mechanism to attain timely and accurate retrieval of unstructured text in this paper we compare two mechanism to improve ir system performance partial collection replication and caching when query have locality both mechanism return result more quickly than sending query to the original collection s cache return result when query exactly match a previous one partial replica are a form of caching that return result when the ir technology determines the query is a good match cache are simpler and faster but replica can increase locality by detecting similarity between query that are not exactly the same we use real trace from thomas and excite to measure query locality and similarity with a very restrictive definition of query similarity similarity improves query locality up to over exact match we use a validated simulator to compare their performance and find that even if the partial replica hit rate increase only to it will outperform simple caching under a variety of configuration a combined approach will probably yield the best performance 
this paper develops a theoretical learning model of text classification for support vector machine svms it connects the statistical property of text classification task with the generalization performance of a svm in a quantitative way unlike conventional approach to learning text classifier which rely primarily on empirical evidence this model explains why and when svms perform well for text classification in particular it address the following question why can support 
we propose a design for displaying and manipulating html form on small pda screen the form input widget are not shown until the user is ready to fill them in at that point only one widget is shown at a time the form is summarized on the screen by displaying just the text label that prompt the user for each widget s information the challenge of this design is to automatically find the match between each text label in a form and the input widget for which it is the prompt we developed eight algorithm for performing such label widget match some of the algorithm are based on n gram comparison while others are based on common form layout convention we applied a combination of these algorithm to simple html form with an average of four input field per form these experiment achieved a matching accuracy we developed a scheme that combine all algorithm into a matching system this system did well even on complex form achieving accuracy in our experiment involving input field spread over complex form 
query clustering is crucial for automatically discovering frequently asked query faq or most popular topic on a question answering search engine due to the short length of query the traditional approach based on keywords are not suitable for query clustering this paper describes our attempt to cluster similar query according to their content a well a the document click information in the user log 
in a directed query distributed search engine wa integrated into a new department of energy virtual library of energy science and technology million of page of government information across multiple agency were made immediately searchable via one query setting the stage for the development of a variety of interagency initiative and application 
most information retrieval system on the internet rely primarily on similarity ranking algorithm based solely on term frequency statistic information quality is usually ignored this lead to the problem that document are retrieved without regard to their quality we present an approach that combine similarity based similarity ranking with quality ranking in centralized and distributed search environment six quality metric including the currency availability information to noise ratio authority popularity and cohesiveness were investigated search effectiveness wa significantly improved when the currency availability information to noise ratio and page cohesiveness metric were incorporated in centralized search the improvement seen when the availability information tonoise ratio popularity and cohesiveness metric were incorporated in site selection wa also significant finally incorporating the popularity metric in information fusion resulted in a significant improvement in summary the result show that incorporating quality metric can generally improve search effectiveness in both centralized and distributed search environment 
this work proposes and evaluates a probabilistic cross lingual retrieval system the system us a generative model to estimate the probability that a document in one language is relevant given a query in another language an important component of the model is translation probability from term in document to term in a query our approach is evaluated when the only resource is a manually generated bilingual word list the only resource is a parallel corpus and both resource are combined in a mixture model the combined resource produce about of monolingual performance in retrieving chinese document for spanish the system achieves of monolingual performance using only a pseudo parallel spanish english corpus retrieval result are comparable with those of the structural query translation technique pirkola when bilingual lexicon are used for query translation when parallel text in addition to conventional lexicon are used it achieves better retrieval result but requires more computation than the structural query translation technique it also produce slightly better result than using a machine translation system for clir but the improvement over the mt system is not significant 
our tdt tracking system wa inuenced by prior work on detection we found that the dual thresholded clustering paradigm of detection system worked well for topic tracking after minimal modification furthermore we found that many of the feature that were beneficial for topic detection continued to be helpful for tracking our basic document document scoring formula wa a symmetrized version of the okapi formula document cluster were represented by the centroid of the cluster and 
we introduce a method for document classification based on using the chi square test to identify characteristic vocabulary of document class 
in presentation recording special effort is usually put into the automation of the production process that is in automatically creating high quality data file without much or any need for manual recording and post editing with the advent of such system and their usage in classroom teaching at conference etc there is an increasing need for technique and ability which enable user to search in those document and to localize some specific information in this paper we describe how we integrated information retrieval technique into the authoring on the fly aof system an approach for automatic presentation recording we have chosen the aof system for two reason on the one hand it is a well established way for presentation recording used by various university and institution on the other hand it is general enough to illustrate typical problem and challenge a developer is facing when designing a system for information retrieval from multimedia data stream which occur in the presentation recording scenario with the authoring on the fly aof approach multimedia presentation are given with an electronic whiteboard program which is used to present material such a slide and annotation a well a external application e g computer animation or mpeg movie during a presentation all data stream that is the audio the slide and whiteboard annotation the video of the lecturer a well a the command controlling the external application are automatically recorded hence the produced file is a multimedia file which consists of several data stream of various medium type aof offer a flexible and convenient way for the synchronized replay of these data stream see for a detailed description of the synchronization model used in the aof system the main feature separating aof from other approach for presentation recording are that the produced multimedia file offer high quality with example of recorded presentation can be found at http www viror de and at http ad informatik uni freiburg de mmgroup aof docdownload the aof software is freely available at http ad informatik uni freiburg de mmgroup tool a reasonable amount of data and that navigation in and access to the recorded data is supported in a convenient way the latter feature is a result of the used synchronization model which realizes random access into the recorded file during replay i e any data stream independent of it medium type can be accessed at any position in real time two other feature which can be realized because of the random accessibility are random visible scrolling and unrestricted cross referencing both are important for information retrieval and will be introduced in section for a detailed description of aof and a comparison with other approach for presentation recording we refer to 
a www grows at an increasing speed a classifier targeted at hypertext ha become in high demand while document categorization is quite a mature the issue of utilizing hypertext structure and hyperlink ha been relatively unexplored in this paper we propose a practical method for enhancing both the speed and the quality of hypertext categorization using hyperlink in comparison against a recently proposed technique that appears to be the only one of the kind we obtained up to of improvement in effectiveness while reducing the processing time dramatically we attempt to explain through experiment what factor contribute to the improvement 
abstract in this paper we show how a simple feedforward neural network can be trained to filter document when only positive information is available and that this method seems to be superior to more standard method such a tf idf retrieval based on an average vector a novel experimental finding that retrieval is enhanced substantially in this context by carrying out a certain kind of uniform transformation hadamard of the information prior to the training of the network 
due to limited bandwidth storage and computational resource and to the dynamic nature of the web search engine cannot index every web page and even the covered portion of the web cannot be monitored continuously for change therefore it is essential to develop effective crawling strategy to prioritize the page to be indexed the issue is even more important for topic specific search engine where crawler must make additional decision based on the relevance of visited page however it is difficult to evaluate alternative crawling strategy because relevant set are unknown and the search space is changing we propose three different method to evaluate crawling strategy we apply the proposed metric to compare three topic driven crawling algorithm based on similarity ranking link analysis and adaptive agent 
our goal is to automatically answer brief factual question of the form when wa the battle of hastings or who wrote the wind in the willow since the answer to nearly any such question can now be found somewhere on the web the problem reduces to finding potential answer in large volume of data and validating their accuracy we apply a method for arbitrary passage retrieval to the first half of the problem and demonstrate that answer redundancy can be used to address the second half the success of our approach depends on the idea that the volume of available web data is large enough to supply the answer to most factual question multiple time and in multiple context a query is generated from a question and this query is used to select short passage that may contain the answer from a large collection of web data these passage are analyzed to identify candidate answer the frequency of these candidate within the passage is used to vote for the most likely answer the approach is experimentally tested on question taken from the trec question answering test collection a an additional demonstration the approach is extended to answer multiple choice trivia question of the form typically asked in trivia quiz and television game show 
the emergence of xml a a standard interchange format for structured document data ha given rise to many xml query language proposal however some of these language do not support information retrieval style ranked query based on textual similarity there have been several extension to these query language to support keyword search but the resulting query language cannot express query such a find book and cd with similar title either these extension use keywords a mere boolean filter or similarity can be calculated only between data value and constant rather than two data value we propose elixir an textbf underline e xpressive and textbf underline e fficient textbf underline l anguage for textbf underline x ml textbf underline i nformation textbf underline r etrieval that extends the query language xml ql cite deutsch www deutsch deb with a textual similarity operator elixir is a general purpose xml information retrieval language sufficiently expressive to handle the above query our algorithm for answering elixir query rewrite the original elixir query into a series of xml ql query that generate intermediate relational data and us relational database technique to efficiently evaluate the similarity operator on this intermediate data yielding an xml document with node ranked by similarity our experiment demonstrate that our prototype scale well with the size of the xml data and complexity of the query 
we propose a new probabilistic approach to information retrieval based upon the idea and method of statistical machine translation the central ingredient in this approach is a statistical model of how a user might distill or quot translate quot a given document into a query to ass the relevance of a document to a user s query we estimate the probability that the query would have been generated a a translation of the document and factor in the user s general preference in the form of a prior 
information filtering system based on statistical retrieval model usually compute a numeric score indicating how well each document match each profile document with score above profile specificdissemination thresholdsare delivered an optimal dissemination threshold is one that maximizes a given utility function based on the distribution of the score of relevant and non relevant document the parameter of the distribution can be estimated using relevance information but relevance information obtained while filtering isbiased this paper present a new method of adjusting dissemination threshold that explicitly model and compensates for this bias the new algorithm which is based on the maximum likelihood principle jointly estimate the parameter of the density distribution for relevant and non relevant document and the ratio of the relevant document in the corpus experiment with trec and trec filtering track data demonstrate the effectiveness of the algorithm 
topic distillation is the analysis of hyperlink graph structure to identify mutually reinforcing authority popular page and hub comprehensive list of link to authority topic distillation is becoming common in web search engine but the best known algorithm model the web graph at a coarse grain with whole page a single node such model may lose vital detail in the markup tag structure of the page and thus lead to a tightly linked irrelevant subgraph winning over a relatively sparse relevant subgraph a phenomenon called topic drift or contamination the problem get especially severe in the face of increasingly complex page with navigation panel and advertisement link we present an enhanced topic distillation algorithm which analyzes text the markup tag tree that constitute html page and hyperlink between page it thereby identifies subtrees which have high textand hyperlink based coherence w r t the query these subtrees get preferential treatment in the mutual reinforcement process using over query from earlier topic distillation work we analyzed over page and obtained quantitative and anecdotal evidence that the new algorithm reduces topic drift 
this paper explores the use of hierarchical structure for classifying a large heterogeneous collection of web content the hierarchical structure is initially used to train different second level classifier in the hierarchical case a model is learned to distinguish a second level category from other category within the same top level in the flat non hierarchical case a model distinguishes a second level category from all other second level category scoring rule can further take advantage of the hierarchy by considering only second level category that exceed a threshold at the top level we use support vector machine svm classifier which have been shown to be efficient and effective for classification but not previously explored in the context of hierarchical classification we found small advantage in accuracy for hierarchical model over flat model for the hierarchical approach we found the same accuracy using a sequential boolean decision rule and a multiplicative decision rule since the sequential approach is much more efficient requiring only of the comparison used in the other approach we find it to be a good choice for classifying text into large hierarchical structure 
this paper present an approach to automatically extracting the bilingual translation of many web query term through mining the web anchor text some preliminary experiment are conducted on using web page containing both chinese and english anchor text in their in link to extract chinese translation of english query selected from popular query term in taiwan it is found that the effective translation of of the popular query term can be extracted in which cannot be obtained in common translation dictionary 
we compare the performance of two database selection algorithm reported in the literature their performance is compared using a common testbed designed specifically for database selection technique the testbed is a decomposition of the trec tipster data into subcollections we present result of a recent investigation of the performance of the cori algorithm and compare the performance with earlier work that examined the performance of ggloss the database from our testbed were ranked using both the ggloss and cori technique and compared to the rbr baseline a baseline derived from trec relevance judgement we examined the degree to which cori and ggloss approximate this baseline our result confirm our earlier observation that the ggloss ideal l rank do not estimate relevance based rank well we also find that cori is a uniformly better estimator of relevance based rank than ggloss for the test environment used in this study part of the advantage of the cori algorithm can be explained by a strong correlation between ggloss and a size based baseline sbr we also find that cori produce consistently accurate ranking on testbeds ranging from site however for a given level of recall search effort appears to scale linearly with the number of database 
we present a statistical model of feature occurrence over time and develop test based on classical hypothesis testing for significance of term appearance on a given date using additional classical hypothesis testing we are able to combine these term to generate topic a defined by the topic detection and tracking study the grouping of term obtained can be used to automatically generate an interactive timeline displaying the major event and topic covered by the corpus to test the validity of our technique we extracted a large number of these topic from a test corpus and had human evaluator judge how well the selected feature captured the gist of the topic and how they overlapped with a set of known topic from the corpus the resulting topic were highly rated by evaluator who compared them to known topic 
this paper proposes evaluation method based on the use of non dichotomous relevance judgement in ir experiment it is argued that evaluation method should credit ir method for their ability to retrieve highly relevant document this is desirable from the user point of view in modern large ir environment the proposed method are a novel application of p r curve and average precision computation based on separate recall base for document of different degree of relevance and two novel measure computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position we then demonstrate the use of these evaluation method in a case study on the effectiveness of query type based on combination of query structure and expansion in retrieving document of various degree of relevance the test wa run with a best match retrieval system in query in a text database consisting of newspaper article the result indicate that the tested strong query structure are most effective in retrieving highly relevant document the difference between the query type are practically essential and statistically significant more generally the novel evaluation method and the case demonstrate that non dichotomous relevance assessment are applicable in ir experiment may reveal interesting phenomenon and allow harder testing of ir method 
in this paper we report on a series of experiment designed to investigate query modification technique motivated by the area of abductive reasoning in particular we use the notion of abductive explanation explanation being a description of data that highlight important feature of the data we describe several method of creating abductive explanation exploring term reweighting and query reformulation technique and demonstrate their suitability for relevance feedback 
given the size of the web the search engine industry ha argued that engine should be evaluated by their ability to retrieve highly relevant page rather than all possible relevant page to explore the role highly relevant document play in retrieval system evaluation assessor for the mbox trec web track used a three point relevance scale and also selected best page for each topic the relative effectiveness of run evaluated by different relevant document set differed confirming the hypothesis that different retrieval technique work better for retrieving highly relevant document yet evaluating by highly relevant document can be unstable since there are relatively few highly relevant document trec assessor frequently disagreed in their selection of the best page and subsequent evaluation by best page across different assessor varied widely the discounted cumulative gain measure introduced by j a rvelin and kek a l a inen increase evaluation stability by incorporating all relevance judgment while still giving precedence to highly relevant document 
this work present an information retrieval model developed to deal with hyperlinked environment the model is based on belief network and provides a framework for combining information extracted from the content of the document with information derived from cross reference among the document the information extracted from the content of the document is based on statistic regarding the keywords in the collection and is one of the basis for traditional information retrieval ir ranking algorithm the information derived from cross reference among the document is based on link reference in a hyperlinked environment and ha received increased attention lately due to the success of the web we discus a set of strategy for combining these two type of source of evidential information and experiment with them using a reference collection extracted from the web the result show that this type of combination can improve the retrieval performance without requiring any extra information from the user at query time in our experiment the improvement reach up to in term of average precision figure 
given explosive data trac in the world wide web www it is crucial to achieve the scalable performance of web server the overall performance and resource utilization can be improvedby spreading document request among a group ofweb server this lead to the design and implementationof distributed cooperative apache webserver in this paper we describe the unique feature ofthe system to migrate and replicate documentsamong cooperating server using 
we introduce five method for summarizing part of web page on handheld device such a personal digital assistant pda or cellular phone each web page is broken into text unit that can each be hidden partially displayed made fully visible or summarized the method accomplish summarization by different mean one method extract significant keywords from the text unit another attempt to find each text unit s most significant sentence to act a a summary for the unit we use information retrieval technique which we adapt to the world wide web context we tested the relative performance of our five method by asking human subject to accomplish single page information search task using each method we found that the combination of keywords and single sentence summary provides significant improvement in access time and number of pen action a compared to other scheme 
based on the document centric view of xml we present the query language xirql current proposal for xml query language lack most ir related feature which are weighting and ranking relevance oriented search datatypes with vague predicate and semantic relativism xirql integrates these feature by using idea from logic based probabilistic ir model in combination with concept from the database area for processing xirql query a path algebra is presented that also serf a a starting point for query optimization 
document filtering is a task to retrieve document relevant to a user s profile from a flow of document generally filtering system calculate the similarity between the profile and each incoming document and retrieve document with similarity higher than a threshold however many system set a relatively high threshold to reduce retrieval of non relevant document which result in the ignorance of many relevant document in this paper we propose the use of a non relevant information profile to reduce the mistaken retrieval of non relevant document result from experiment show that this filter ha successfully rejected a sufficient number of non relevant document resulting in an improvement of filtering performance 
automatic albuming the automatic organization of photograph either a an end in itself or for use in other application is an application that promise to be of great assistance to photographer relatively sophisticated image content analysis technique have been used for image indexing organization and retrieval in this paper we describe a method of organizing photograph into event using spoken photograph caption the result of this process can be used to improve image indexing and retrieval 
we present a new technique for question answering called predictive annotation predictive annotation identifies potential answer to question in text annotates them accordingly and index them this technique along with a complementary analysis of question passage level ranking and answer selection produce a system effective at answering natural language fact seeking question posed against large document collection experimental result show the effect of different parameter setting and lead to a number of general observation about the question answering problem 
we developed and then evaluated a music information retrieval mir system based upon the interval found within the melody of a collection of folksong the song were converted to an interval only representation of monophonic melody and then fragmented t into length n subsection called n gram the length of these n gram and the degree to which we precisely represent the interval are variable analyzed in this paper we constructed a collection of musical word database using the text based smart information retrieval system a group of simulated query some of which contained simulated error wa run against these database the result were evaluated using the normalized precision and normalized recall measure our concept of musical word show great merit thus implying that useful mir system can be constructed simply and efficiently using pre existing text based information retrieval software second this study is a formal and comprehensive evaluation of a mir system using rigorous statistical analysis to determine retrieval effectiveness 
end user base the relevance judgement of the searched document on the expected contribution to their task of the information contained in the document there is a shortage of study analyzing the relationship between the experienced contribution relevance assessment and type of information initially sought this study categorizes the type of information in document being used in writing a research proposal for a master s thesis by eleven student throughout the various stage of the proposal writing process the role of the specificity of the searched information in influencing it contribution is analyzed the result demonstrate that different type of information are sought at different stage of the writing process and thus the contribution of the information also differs at the different stage the category of the contributing information can be understood of topicality 
many researcher have shown that server driven consistencyprotocols can potentially reduce read latency server drivenconsistency protocol are particularly attractive for largescaledynamic web workload because dynamically generateddata can change rapidly and unpredictably however there have been no report on engineering server driven consistencyfor such a workload this paper report our experiencein engineering server driven consistency for a sportingand event web site hosted by 
we present three new visualization front end that aid navigation through the set of document returned by a search engine hit document we cluster the hit document to visually group these document and label the group with related word the dierent front end cater for dierent user need but all can browse cluster information a well a drilling up or down in one or more cluster and refining the search using one or more of the suggested related keywords 
in this paper we present an approach to tackle three important problem of text normalization sentence boundary disambiguation disambiguation of capitalized word when they are used in position where capitalization is expected and identification of abbreviation the main feature of our approach is that it us a minimum of pre built resource instead dynamically inferring disambiguation clue from the entire document itself this make it domain independent closely targeted to each individual document and portable to other language we thoroughly evaluated this approach on several corpus and it showed high accuracy 
the purpose of this paper is to present an on going research that is intended to construct a live thesaurus directly from search term log of real world search engine such a thesaurus designed can contain representative search term their frequency in use the corresponding subject category the associated and relevant term and the hot visiting web site page the search term may reach 
real scale semantic web application such a knowledge portal and e marketplace require the management of large volume of metadata i e information describing the available web content and service better knowledge about their meaning usage accessibility or quality will considerably facilitate an automated processing of web resource the resource description framework rdf enables the creation and exchange of metadata a normal web data although voluminous rdf description are already appearing sufficiently expressive declarative language for querying both rdf description and schema are still missing in this paper we propose a new rdf query language called rql it is a typed functional language a la oql and relies on a formal model for directed labeled graph permitting the interpretation of superimposed resource description by mean of one or more rdf schema rql adapts the functionality of semistructured xml query language to the peculiarity of rdf but foremost it enables to uniformly query both resource description and schema we illustrate the rql syntax semantics and typing system by mean of a set of example query and report on the performance of our persistent rdf store employed by the rql interpreter 
we investigate four hierarchical clustering method single link complete link groupwise average and single pas and two linguistically motivated text feature noun phrase head and proper name in the context of document clustering a statistical model for combining similarity information from multiple source is described and applied to darpa s topic detection and tracking phase tdt data this model based on log linear regression alleviates the need for extensive search in order to determine optimal weight for combining input feature through an extensive series of experiment with more than document from multiple news source and modality we establish that both the choice of clustering algorithm and the introduction of the additional feature have an impact on clustering performance we apply our optimal combination of feature to the tdt test data obtaining partition of the document that compare favorably with the result obtained by participant in the official tdt competition 
traditional method for the system oriented evaluation of boolean ir system suffer from validity and reliability problem laboratory based research neglect the searcher and study suboptimal query research on operational system fails to make a distinction between searcher performance and system performance this approach is neither capable of measuring performance at standard point of operation e g across r r a new laboratory based evaluation method for boolean ir system is proposed it is based on a controlled formulation of inclusive query plan on an automatic conversion of query plan into elementary query and on combining elementary query into optimal query at standard point of operation major result of a large case experiment are reported the validity reliability and efficiency of the method are considered in the light of empirical and analytical test data 
the emergence of networked context aware mobile computing appliance potentially offer opportunity for remote access to huge online information resource information access in context aware information appliance can utilize existing technique developed for effective information retrieval and information filtering however practical physical and operational feature of these device and the availability of context information itself suggest that the document selection process should make use of this contextual data 
considerable research effort ha been invested in improving the effectiveness of information retrieval system technique such a relevance feedback thesaural expansion and pivoting all provide better quality response to query when tested in standard evaluation framework but such enhancement can add to the cost of evaluating query in this paper we consider the pragmatic issue of how to improve the cost effectiveness of searching we describe a new inverted file structure using quantized weight that provides superior retrieval effectiveness compared to conventional inverted file structure when early termination heuristic are employed that is we are able to reach similar effectiveness level with le computational cost and so provide a better cost performance compromise than previous inverted file organisation 
we investigate a meta model approach called meta learning using document feature characteristic mudof for the task of automatic textual document categorization it employ a meta learning phase using document feature characteristic document feature characteristic derived from the training document set capture some inherent category specific property of a particular category different from existing categorization method mudof can automatically recommend a suitable algorithm for each category based on the category specific statistical characteristic hence different algorithm may be employed for different category experiment have been conducted on a real world document collection demonstrating the effectiveness of our approach the result confirm that our meta model approach can exploit the advantage of it component algorithm and demonstrate a better performance than existing algorithm 
hierarchy have long been used for organization summarization and access to information in this paper we define summarization in term of a probabilistic language model and use the definition to explore a new technique for automatically generating topic hierarchy by applying a graph theoretic algorithm which is an approximation of the dominating set problem the algorithm efficiently chooses term according to a language model we compare the new technique to previous method proposed for constructing topic hierarchy including subsumption and lexical hierarchy a well a the top tf idf term our result show that the new technique consistently performs a well a or better than these other technique they also show the usefulness of hierarchy compared with a list of term 
the effect of out of vocabulary oov item in spoken document retrieval sdi are investigated several set of transcription were created for the trec sdr task using a speech recognition system varying the vocabulary size and oov rate and the relative retrieval performance measured the effect of oov term on a simple baseline ir system and on more sophisticated retrieval system are described the use of a parallel corpus for query and document expansion is found to be especially beneficial and with this data set good retrieval performance can be achieved even for fairly high oov rate 
the web is a valuable source of language specific resource but collecting organizing and utilizing this information is difficult we describe corpusbuilder an approach for automatically generating web search query to collect document in a minority language it differs from pseudo relevance feedback in that retrieved document are labeled by an automatic language classifier a relevant or irrelevant and a subset of document is used to generate new query we experiment with various query generation method and query length to find inclusion exclusion term that are helpful for finding document in the target language and find that using odds ratio score calculated over the document acquired so far wa one of the most consistently accurate query generation method we also describe experiment using a handful of word elicited from a user instead of initial document and show that the method perform similarly applying the same approach to multiple language show that our system generalizes to a variety of language 
we present a novel implementation of the recently introduced information bottleneck method for unsupervised document clustering given a joint empirical distribution of word and document p x y we first cluster the word y so that the obtained word cluster ytilde maximally preserve the information on the document the resulting joint distribution p x ytilde contains most of the original information about the document i x ytilde ap i x y but it is much le sparse and noisy using the same procedure we then cluster the document x so that the information about the word cluster is preserved thus we first find word cluster that capture most of the mutual information about to set of document and then find document cluster that preserve the information about the word cluster we tested this procedure over several document collection based on subset taken from the standard newsgroups corpus the result were assessed by calculating the correlation between the document cluster and the correct label for these document finding from our experiment show that this double clustering procedure which us the information bottleneck method yield significantly superior performance compared to other common document distributional clustering algorithm moreover the double clustering procedure improves all the distributional clustering method examined here 
our english chinese cross language ir system is trained from parallel corpus we investigate it performance a a function of training corpus size for three different training corpus we find that the performance of the system a trained on the three parallel corpus can be related by a simple measure namely the out of vocabulary rate of query word 
we introduce an information system for organization and retrieval of news article from web publication incorporating a classification framework based on support vector machine we present the data model for storage and management of news data and the system architecture for news retrieval classification and generation of topical collection we also discus the classification result obtained with a collection of news article gathered from a set of online newspaper 
abstract language modeling approach to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation which ha been studied extensively in other application area such a speech recognition the basic idea of these approach is to estimate a language model for each document and then rank document by the likelihood of the query according to the estimated language model a core problem in language model estimation is smoothing which adjusts the maximum likelihood estimator so a to correct the inaccuracy due to data sparseness in this paper we study the problem of language model smoothing and it influence on retrieval performance we examine the sensitivity of retrieval performance to the smoothing parameter and compare several popular smoothing method on different test collection 
abstract representation of negotiation in electronic market and their support are important issue in today s e commerce research whereas most activity are focused on automation aspect only few effort address the design of electronic negotiation e g the sequence of action or obligation and responsibility of the negotiating party however an explicit negotiation design can also address what is commonly referred to a the ontology problem of electronic negotiation how can one ensure that the negotiating party have the same understanding regarding the issue that are subject to the negotiation the solution this paper proposes is to perform a communication design for electronic negotiation that explicitly specifies the common syntax and semantics of the negotiating party the logical space of the electronic negotiation furthermore xml schema is suggested a the mechanism for the runtime representation of the logical space and the validation of actual negotiation from a syntactical and semantical perspective on the basis of this approach organisation creating an electronic market or seller who intend to offer their buyer the ability to bargain can design and generate support mechanism for electronic negotiation in a flexible and efficient way the communication design actionand 
the paper introduces a query translation model that reflects the structure of the cross language information retrieval task the model is based on a structured bilingual dictionary in which the translation of each term are clustered into group with distinct meaning query translation is modeled a a two stage process with the system first determining the intended meaning of a query term and then selecting translation appropriate to that meaning that might appear in the document collection an implementation of structured translation based on automatic dictionary clustering is described and evaluated by using chinese query to retrieve english document structured translation achieved an average precision that wa statistically indistinguishable from pirkola s technique for very short query but pirkola s technique outperformed structured translation on long query the paper concludes with some observation on future work to improve retrieval effectiveness and on other potential us of structured translation in interactive cross language retrieval application 
this paper present a novel information retrieval system that includes the addition of concept to facilitate the identification of the correct word sense a natural language query interface the inclusion of weight and penalty for proper noun that build upon the okapi weighting scheme and a term clustering technique that exploit the spatial proximity of search term in a document to further improve the performance the effectiveness of the system is validated by experimental result 
the proliferation of online information resource increase the importance of effective and efficient distributed searching distributed searching is cast in three part database selection query processing and result merging in this paper we examine the effect of database selection on retrieval performance we look at retrieval performance in three different distributed retrieval testbeds and distill some general result first we find that good database selection can result in better retrieval effectiveness than can be achieved in a centralized database second we find that good performance can be achieved when only a few site are selected and that the performance generally increase a more site are selected finally we find that when database selection is employed it is not necessary to maintain collection wide information cwi e g global idf local information can be used to achieve superior performance this mean that distributed system can be engineered with more autonomy and le cooperation this work suggests that improvement in database selection can lead to broader improvement in retrieval performance even in centralized i e single database system given a centralized database and a good selection mechanism retrieval performance can be improved by decomposing that database conceptually and employing a selection step 
we present performance measurement result for a parallel sql based information retrieval system implemented on a pc cluster system we used the web trec dataset under a left deep query execution plan we achieved satisfactory speed up 
there are many task that require information finding some can be largely automated and others greatly benefit from successful interaction between system and searcher we are interested in the task of answering question where some synthesis of information is required the answer would not generally be given from a single passage of a single document we investigate whether variation in the way a list of document is delivered affected searcher performance in the question answering task we will show that there is a significant difference in performance using a list customized to the task type compared with a standard web engine list this indicates that paying attention to the task and the searcher interaction may provide substantial improvement in task performance 
do improvement in system performance demonstrated by batch evaluation confer the same benefit for real user we carried out experiment designed to investigate this question after identifying a weighting scheme that gave maximum improvement over the baseline in a non interactive evaluation we used it with real user searching on an instance recall task our result showed the weighting scheme giving beneficial result in batch study did not do so with real user further analysis did identify other factor predictive of instance recall including number of document saved by the user document recall and number of document seen by the user 
most web page are linked to others with related content this idea combined with another that say that text in and possibly around html anchor describe the page to which they point is the foundation for a usable world wide web in this paper we examine to what extent these idea hold by empirically testing whether topical locality mirror spatial locality of page on the web in particular we find that the likelihood of linked page having similar textual content to be high the similarity of sibling page increase when the link from the parent are close together title description and anchor text represent at least part of the target page and that anchor text may be a useful discriminator among unseen child page these result show the foundation necessary for the success of many web system including search engine focused crawler linkage analyzer and intelligent web agent 
we discus technology to help a person monitor change in news coverage over time we define temporal summary of news story a extracting a single sentence from each event within a news topic where the story are presented one at a time and sentence from a story must be ranked before the next story can be considered we explain a method for evaluation and describe an evaluation corpus that we have built we also propose several method for constructing temporal summary and evaluate their effectiveness in comparison to degenerate case we show that simple approach are effective but that the problem is far from solved 
the thresholding of document score ha proved critical for the effectiveness of classification task we review the most important approach to thresholding and introduce thescore distributional s d threshold optimizationmethod the method is based on score distribution and is capable of optimizing any effectiveness measure defined in term of the traditional contingency table a a byproduct we provide a model forscore distribution and demonstrate it high accuracy in describing empirical data the estimation method can be performed incrementally a highly desirable feature for adaptive environment our work in modeling score distribution is useful beyond threshold optimization problem it directly applies to other retrieval environment that make use of score distribution e g distributed retrieval or topic detection and tracking the most accurate version of s d thresholding although incremental can be computationally heavy therefore we also investigate more practical solution we suggest practical approximation and discus adaptivity threshold initialization and incrementality issue the practical version of s d thresholding ha been tested in the context of the trec filtering track and found to be very effective 
summarization research is notorious for it lack of adequat e corpus today there exist only a few small collection of text whose unit have been manually annotated for textual importance given the cost and tediousness of the annotation process it is very unlikely that we will ever manually annotate for textual importance sufficiently large corpus of text to circumvent this problem we have developed an algorithm that construct such corpus automatically our algorithm take a input an abstract text tuple and generates the corresponding extract i e the set of clause sentence in the text that were used to write the abstract the performance of the algorithm is shown to be close to that of human by mean of an empirical experiment the experiment also suggests extraction strategy that could impro ve the performance of automatic summarization system 
much system oriented evaluation of information retrieval system ha used the cranfield approach based upon query run against test collection in a batch mode some researcher have questioned whether this approach can be applied to the real world but little data exists for or against that assertion we have studied this question in the context of the trec interactive track previous result demonstrated that improved performance a measured by relevance based metric in batch study did not correspond with the result of outcome based on real user searching task the experiment in this paper analyzed those result to determine why this occurred our assessment showed that while the query entered by real user into system yielding better result in batch study gave comparable gain in ranking of relevant document for those user they did not translate into better performance on specific task this wa most likely due to user being able to adequately find and utilize relevant document ranked further down the output list 
we present a system that offer a new way of assessing web document relevance and new approach to the web based evaluation of such a system provisionally named webdocsum the system is a query biased web page summariser that aim to provide an alternative to the short irrelevant abstract typical of many web search result list based on an initial evaluation the system appears to be more useful in helping user gauge document relevance than the traditional ranked title abstract approach 
scalable vector graphic svg is a language that describes two dimensional vector graphic for storage and distribution on the web unlike raster image format svg based image scale nicely to arbitrary resolution and size however the current svg standard provides little fle xibility for taking into account varying viewing condition such a different screen format and there is little support for interactive exploration of a diagram we introduce an extension to svg called constraint scalable vector graphic csvg that permit a more fle xible description of figure with csvg an image can contain object whose position and other property are specified in relation to other object using constraint rather than being specified in absolute term for example a box can be specified to remain inside another box without being given an absolute position the precise layout can then be left to the browser which can adapt it dynamically to changing viewing condition on the client side further extension add support for alternate layout interaction and declarative animation leveraging well established method for linear constraint solving we implemented a prototype viewer for csvg by embedding our cassowary constraint solver into an existing svg renderer 
persistent connection address ineciencies associated withmultiple concurrent connection they can improve responsetime when successfully used with pipelining to retrieve a setof object from a web server in practice however there isinconsistent support for persistent connection particularlywith pipelining from web server user agent and intermediary web browser continue to open multiple concurrenttcp connection to the same server this paper proposes a new idea of 
relevance feedback is an appreciated process to produce increasingly better retrieval usually positive feedback play a fundamental role in the feedback process whereas the role of negative feedback is limited we think that negative feedback is a promising precision oriented mechanism and we propose a logical framework in which positive and negative feedback are homogeneously modeled evaluation result against small test collection are provided 
automatic summarization of open domain spoken dialogue is a new research area this paper introduces the task the challenge involved and present an approach to obtain automatic extract summary for multi party dialogue of four different genre without any restriction on domain we address the following issue which are intrinsic to spoken dialogue summarization and typically can be ignored when summarizing written text such a newswire data i detection and removal of speech disfluency ii detection and insertion of sentence boundary iii detection and linking of cross speaker information unit question answer pair a global system evaluation using a corpus of relevance annotated dialogue containing topical segment show that for the two more informal genre our summarization system using dialogue specific component significantly outperforms a baseline using tfidf term weighting with maximum marginal relevance ranking mmr 
link based ranking method have been described in the literature and applied in commercial web search engine however according to recent trec experiment they are no better than traditional content based method we conduct a different type of experiment in which the task is to find the main entry point of a specific web site in our experiment ranking based on link anchor text is twice a effective a ranking based on document content even though both method used the same bm formula we obtained these result using two set of query on a million document set and another set of on a million document set this site finding effectiveness begin to explain why many search engine have adopted link method it also open a rich new area for effectiveness improvement where traditional method fail 
the recognition of proper noun pns is considered an important task in the area of information retrieval and extraction however the high performance of most existing pn classifier heavily depends upon the availability of large dictionary of domain specific proper noun and a certain amount of manual work for rule writing or manual tagging though it is not a heavy requirement to rely on some existing pn dictionary often these resource are available on the web it coverage of a domain corpus may be rather low in absence of manual updating in this paper we propose a technique for the automatic updating of an pn dictionary through the cooperation of an inductive and a probabilistic classifier in our experiment we show that whenever an existing pn dictionary allows the identification of of the proper noun within a corpus our technique allows without additional manual effort the successful recognition of about of the remaining 
combined word based index and phonetic index have been used to improve the performance of spoken document retrieval system primarily by addressing the out of vocabulary retrieval problem however a known problem with phonetic recognition is it limited accuracy in comparison with word level recognition we propose a novel method for phonetic retrieval in the cuevideo system based on the probabilistic formulation of term weighting using phone confusion data in a bayesian framework we evaluate this method of spoken document retrieval against word based retrieval for the search level identified in a realistic video based distributed learning setting using our test data we achieved an average recall of with an average precision of for retrieval of out of vocabulary word on phonetic transcript with word error rate for in vocabulary word we achieved a improvement in recall over word based retrieval with a loss in precision for word error rite ranging from to 
wepresent a new method for information retrieval using hidden markov model hmms wedevelop a general framework for incorporating multiple word generation mechanism within the same model we then demonstrate that an extremely simple realization of this model substantially outperforms standard tf idf ranking on both the trec and trec ad hoc retrieval task we go on to present a novel method for performing blind feedback in the hmm framework a more complex hmm that model bigram 
a three part study wa designed to document internet use in scholarly research using the annual sigir conference proceeding from through the result suggest an increasing trend toward electronic self publishing furthermore while electronic availability did not insure that one would be cited the most highly cited article were available on the free web the study also found that electronic availability ha not in most case decreased the length of time between publication and citation 
in information filtering if system user long term need are expressed a user profile the quality of a user profile ha a major unpact on the performance of if system the focus of the proposed research is on the study of user profile generation and update the paper introduces method for user profile generation and proposes a research agenda for their comparison and evaluation ke ywords information filtering user profile content based filtering rule based filtering 
the trec question answering qa track wa the first large scale evaluation of domain independent question answering system in addition to fostering research on the qa task the track wa used to investigate whether the evaluation methodology used for document retrieval is appropriate for a different natural language processing task a with document relevance judging assessor had legitimate difference of opinion a to whether a response actually answer a question but comparative evaluation of qa system wa stable despite these difference creating a reusable qa test collection is fundamentally more difficult than creating a document retrieval test collection since the qa task ha no equivalent to document identifier 
we introduce ocelot a prototype system for automatically generating the gist of a web page by summarizing it although most text summarization research to date ha focused on the task of news article web page are quite different in both structure and content instead of coherent text with a well defined discourse structure they are more often likely to be a chaotic jumble of phrase link graphic and formatting command such text provides little foothold for extractive summarization technique which attempt to generate a summary of a document by excerpting a contiguous coherent span of text from it this paper build upon recent work in non extractive summarization producing the gist of a web page by translating it into a more concise representation rather than attempting to extract a text span verbatim ocelot us probabilistic model to guide it in selecting and ordering word into a gist this paper describes a technique for learning these model automatically from a collection of human summarized web page 
m read is a prototype application implemented a an extension of the web browser that creates an evolving model of the user topic of interest it us that model to analyze document that are accessed while searching and browsing the web in the presented version of m read the model is used to highlight topic related terminology in the document m read model of the user need is created by applying natural language processing to search query captured within the browser and to topic description explicitly provided by the user while browsing and reading document it is semantically enhanced using linguistic and custom knowledge resource 
with the proliferation of the internet and the huge amount of data it transfer text summarization is becoming more important we present an approach to the design of an automatic text summarizer that generates a summary by extracting sentence segment first sentence are broken into segment by special cue marker each segment is represented by a set of predefined feature e g location of the segment average term frequency of the word occurring in the segment number of title word in the segment and the like then a supervised learning algorithm is used to train the summarizer to extract important sentence segment based on the feature vector result of experiment on u s patent indicate that the performance of the proposed approach compare very favorably with other approach including microsoft word summarizer in term of precision recall and classification accuracy 
we present a framework for information retrieval that combine document model and query model using a probabilistic ranking function based on bayesian decision theory the framework suggests an operational retrieval model that extends recent development in the language modeling approach to information retrieval a language model for each document is estimated a well a a language model for each query and the retrieval problem is cast in term of risk minimization the query language model can be exploited to model user preference the context of a query synonomy and word sens while recent work ha incorporated word translation model for this purpose we introduce a new method using markov chain defined on a set of document to estimate the query model the markov chain method ha connection to algorithm from link analysis and social network the new approach is evaluated on trec collection and compared to the basic language modeling approach and vector space model together with query expansion using rocchio significant improvement are obtained over standard query expansion method for strong baseline tf idf system with the greatest improvement attained for short query on web data 
effort to improve web search facility call for improvedunderstanding of user characteristic we investigated the typesof knowledge that are relevant for web based informationseeking along with the knowledge structure and relatedstrategies in an exploratory field experiment establishedinternet expert were first interviewed about search strategiesand then performed a series of realistic search task on the a href http citeseer ist psu edu rd http aqsqqsqwww onmouseover self status http www return true onmouseout self status return true www a based on this preliminary study a model of informationsearching on 
in this paper the score distribution of a number of text search engine are modeled it is shown empirically that the score distribution on a per query basis may be fitted using an exponential distribution for the set of non relevant document and a normal distribution for the set of relevant document experiment show that this model fit trec and trec data for not only probabilistic search engine like inquery but also vector space search engine like smart for english we have also used this model to fit the output of other search engine like lsi search engine and search engine indexing other language like chinese it is then shown that given a query for which relevance information is not available a mixture model consisting of an exponential and a normal distribution can be fitted to the score distribution these distribution can be used to map the score of a search engine to probability we also discus how the shape of the score distribution arise given certain assumption about word distribution in document we hypothesize that all good text search engine operating on any language have similar characteristic this model ha many possible application for example the output of different search engine can be combined by averaging the probability optimal if the search engine are independent or by using the probability to select the best engine for each query result show that the technique performs a well a the best current combination technique 
web document hierarchical classification approach often rely on textual feature alone even though web page include multimedia data we propose a new hierarchical integrated web classification approach that combine image based and text based approach instead of using a flat classifier to combine text and image classification we perform classification on a hierarchy differently on different level of the tree using text for branch and image only at leaf the result of our experiment show that the use of the hierarchical structure improved web document classification performance significantly 
recently there have been a number of algorithm proposed for analyzing hypertext link structure so a to determine the best authority for a given topic or query while such analysis is usually combined with content analysis there is a sense in which some algorithm are deemed to be more balanced and others more focused we undertake a comparative study of hypertext link analysis algorithm guided by some experimental query we propose some formal criterion for evaluating and comparing link analysis algorithm 
this paper examines the use of generic summary for indexing in information retrieval our main observation are that with or without pseudo relevance feedback a summary index may be a effective a the corresponding fulltext index forprecision oriented search of highly relevant document but a reasonably sophisticated summarizer using a compression ratio of is desirable for this purpose in pseudo relevance feedback using a summary index at initial search and a fulltext index at final search is possibly effective for precision oriented search regardless of relevance level this strategy is significantly more effective than the one using the summary index only and probably more effective than using summary a mere term selection filter the use of summary a mere term selection filter the summary quality is probably not a critical factor for this strategy for this strategy the summary quality is probably not a critical factor and a compression ratio of appears best 
in order to increase retrieval precision some new search engine provide manually verified answer to frequently asked query faq an underlying task is the identification of faq this paper describes our attempt to cluster similar query according to their content a well a user log our preliminary result show that the resulting cluster provide useful information for faq identification 
a new model named boolean latent semantic indexing model based on the singular value decomposition and boolean query formulation is introduced while the singular value decomposition alleviates the problem of lexical matching in the traditional information retrieval model boolean query formulation can help user to make precise representation of their information search need retrieval experiment on a number of test collection seem to show that the proposed model achieves substantial performance gain over the latent semantic indexing model 
we present a method of searching text collection that take advantage of hierarchrical information within document and integrates search of structured and unstructured data we show that multidimensional database mdb designed for accessing data along hierarchical dimension are effective for information retrieval we demonstrate a method of using on line analytic processing olap technique on a text collection this combine traditional information retrieval and the slicing dicing drill down and roll up of olap we demonstrate use of a prototype for searching document from the trec collection 
this article compare search effectiveness when using query based internet search via the google search engine directory based search via yahoo and phrase based query reformulation assisted search via the hyperindex browser by mean of a controlled user based experimental study the focus wa to evaluate aspect of the search process cognitive load wa measured using a secondary digit monitoring task to quantify the effort of the user in various search state independent relevance judgement were employed to gauge the quality of the document accessed during the search process time wa monitored in various search state result indicated the directory based search doe not offer increased relevance over the query based search with or without query formulation assistance and also take longer query reformulation doe significantly improve the relevance of the document through which the user must trawl versus standard query based internet search however the improvement in document relevance come at the cost of increased search time and increased cognitive load 
the feature quantity a quantitative representation of specificity introduced in this paper is based on an information theoretic perspective of co occurrence event between term and document mathematically the feature quantity is defined a a product of probability and information and maintains a good correspondence with the tfidf like measure popularly used in today s ir system in this paper we present a formal description of the feature quantity a well a some illustrative example of applying such a quantity to different type of information retrieval task representative term selection and text categorization 
abstract there ha been much recent interest in retrieval of time series data earlier work ha used a fixed similarity metric e g euclidean distance to determine the similarity between a userspecified query and item in the database here we describe a novel approach to retrieval of time series data by using relevance feedback from the user to adjust the similarity metric this is important because the euclidean distance metric doe not capture many notion of similarity between time series in particular euclidean distance is sensitive to various distortion such a offset translation amplitude scaling etc depending on the domain and the user one may wish a query to be sensitive or insensitive to these distortion to varying degree this paper address this problem by introducing a profile that encodes the user s subjective notion of similarity in a domain these profile can be learned continuously from interaction with the user we further show how the user profile may be embedded in a system that us relevance feedback to modify the query in a manner analogous to the familiar text retrieval algorithm keywords time series multimedia data relevance feedback modeling user subjectivity 
we introduce static index pruning method that significantly reduce the index size in information retrieval system we investigate uniform and term based method that each remove selected entry from the index and yet have only a minor effect on retrieval result in uniform pruning there is a fixed cutoff threshold and all index entry whose contribution to relevance score is bounded above by a given threshold are removed from the index in term based pruning the cutoff threshold is determined for each term and thus may vary from term to term we give experimental evidence that for each level of compression term based pruning outperforms uniform pruning under various measure of precision we present theoretical and experimental evidence that under our term based pruning scheme it is possible to prune the index greatly and still get retrieval result that are almost a good a those based on the full index 
in this paper we describe a type of data fusion involving the combination of evidence derived from multiple document representation our aim is to investigate if a composite representation can improve the online detection of novel event in a stream of broadcast news story this classification process otherwise known a first story detection fsd or in the topic detection and tracking pilot study a online new event detection is one of three main classification task defined by the tdt initiative our composite document representation consists of a semantic representation based on the lexical chain derived from a text and a syntactic representation using proper noun using the tdt evaluation methodology we evaluate a number of document representation combination using these document classifier 
dictionary have often been used for query translation in cross language information retrieval clir however we are faced with the problem of translation ambiguity i e multiple translation are stored in a dictionary for a word in addition a word by word query translation is not precise enough in this paper we explore several method to improve the previous dictionary based query translation first a many a possible noun phrase are recognized and translated a a whole by using statistical model and phrase translation pattern second the best word translation are selected based on the cohesion of the translation word our experimental result on trec english chinese clir collection show that these technique result in significant improvement over the simple dictionary approach and achieve even better performance than a high quality machine translation system 
most approach to cross language information retrieval assume that resource providing a direct translation between the query and document language exist this paper present research examining the situation where such an assumption is false here an intermediate or pivot language provides a mean of transitive translation of the query language to that of the document via the pivot at the cost however of introducing much error the paper report the novel approach of translating in parallel across multiple intermediate language and fusing the result such a technique remove the error raising the effectiveness of the tested retrieval system up to and possibly above the level expected had a direct translation route existed across a number of retrieval situation and combination of language the approach prof to be highly effective 
