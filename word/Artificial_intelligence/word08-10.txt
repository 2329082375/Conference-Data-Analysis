the construction of causal graph from nonexperimental data rest on a set of constraint that the graph structure imposes on all probability distribution compatible with the graph these constraint are of two type conditional independency and algebraic constraint first noted by verma while conditional independency are well studied and frequently used in causal induction algorithm verma constraint are still poorly understood and rarely applied in this paper we examine a special subset of verma constraint which are easy to understand easy to identify and easy to apply they arise from dormant independency namely conditional independency that hold in interventional distribution we give a complete algorithm for determining if a dormant independence between two set of variable is entailed by the causal graph such that this independence is identifiable in other word if it resides in an interventional distribution that can be predicted without resorting to intervention we further show the usefulness of dormant independency in model testing and induction by giving an algorithm that us constraint entailed by dormant independency to prune extraneous edge from a given causal graph 
the surprisingly good performance of modern satisfiability sat solver is usually explained by the existence of a certain hidden structure in real world instance we introduce the notion of backdoor tree a an indicator for the presence of a hidden structure backdoor tree refine the notion of strong backdoor set taking into account the relationship between backdoor variable we present theoretical and empirical result our theoretical result are concerned with the computational complexity of detecting small backdoor tree with our empirical result we compare the size of backdoor tree against the size of backdoor set for real world sat instance and random sat instance of various density the result indicate that backdoor tree amplify the property that have been observed for backdoor set 
we consider the existence and computational complexity of coalitional stability concept based on social network our concept represent a natural and rich combinatorial generalization of a recent approach termed partition equilibrium we assume that player in a strategic game are embedded in a social network and there are coordination constraint that restrict the potential coalition that can jointly deviate in the game to the set of clique in the social network in addition player act in a considerate fashion to ignore potentially profitable group deviation if the change in their strategy may cause a decrease of utility to their neighbor we study the property of such considerate equilibrium in application to the class of resource selection game rsg our main result prof existence of a considerate equilibrium in all symmetric rsg with strictly increasing delay for any social network among the player the existence proof is constructive and yield an efficient algorithm in fact the computed considerate equilibrium is a nash equilibrium for the standard rsg showing that there exists a state that is stable against selfish and considerate behavior simultaneously in addition we show result on convergence of considerate dynamic 
we introduce planning game a study of interaction of self motivated agent in automated planning setting planning game extend strip like model of single agent planning to system of multiple self interested agent providing a rich class of structured game that capture subtle form of local interaction we consider two basic model of planning game and adapt game theoretic solution concept to these model in both model agent may need to cooperate in order to achieve their goal but are assumed to do so only in order to increase their net benefit for each model we study the computational problem of finding a stable solution and provide efficient algorithm for system exhibiting acyclic interaction structure 
landmark for propositional planning task are variable assignment that must occur at some point in every solution plan we propose a novel approach for using landmark in planning by deriving a pseudo heuristic and combining it with other heuristic in a search framework the incorporation of landmark information is shown to improve success rate and solution quality of a heuristic planner we furthermore show how additional landmark and ordering can be found using the information present in multi valued state variable representation of planning task compared to previously published approach our landmark extraction algorithm provides stronger guarantee of correctness for the generated landmark ordering and our novel use of landmark during search solves more planning task and delivers considerably better solution 
imagine a simulated world where the character you interact with are almost human they converse with you in english they understand the world they are in can reason about what to do and they exhibit emotion some of these character may be your friend while others will oppose you unlike current video garnes being successful in this world won t just be a matter of who is quickest on the draw or most adroit at solving puzzle instead it will be the person who understands the social fabric and cultural context and can use interpersonal skill most effectively such a simulation could open up whole new horizon for education entertainment and simulation and given recent advance in ai and graphic it may not be too far off virtual human are computer generated character that can take the part of human in a variety of limited context these can include acting a role player in simulation and training system johnson rickel lester swartout et al traum et al johnson vilhj lmsson marsella where they play a variety of part such a acting a friendly or hostile force or local in the environment other us for virtual human include acting a museum guide gustafson bell marketing assistant cassell bickmore et al or character in entertainment system where the advent of video game such a the sims make clear the growing interest of the computer game industry in virtual human see also mateas stern 
pca can be smarter and make more sensible projection in this paper we propose smart pca an extension to standard pca to regularize and incorporate external knowledge into model estimation based on the probabilistic interpretation of pca the inverse wishart distribution can be used a the informative conjugate prior for the population covariance and useful knowledge is carried by the prior hyperparameters we design the hyperparameters to smoothly combine the information from both the domain knowledge and the data itself the bayesian point estimation of principal component is in closed form in empirical study smart pca show clear improvement on three different criterion image reconstruction error the perceptual quality of the reconstructed image and the pattern recognition performance 
most planning problem have strong structure they can be decomposed into subdomains with causal dependency the idea of exploiting the domain decomposition ha motivated previous work such a hierarchical planning and factored planing however these algorithm require extensive backtracking and lead to few efficient general purpose planner on the other hand heuristic search ha been a successful approach to automated planning the domain decomposition of planning problem unfortunately is not directly and fully exploited by heuristic search we propose a novel and general framework to exploit domain decomposition based on a structure analysis on the sa planning formalism we stratify the sub domain of a planning problem into dependency layer by recognizing the stratification of a planning structure we propose a space reduction method that expands only a subset of executable action at each state this reduction method can be combined with state space search allowing u to simultaneously employ the strength of domain decomposition and high quality heuristic we prove that the reduction preserve completeness and optimality of search and experimentally verify it effectiveness in space reduction 
optimal heuristic search such a a search are widely used for planning but can rarely scale to large complex problem the suboptimal version of heuristic search such a weighted a search can often scale to much larger planning problem by trading off the quality of the solution for efficiency they do so by relying more on the ability of the heuristic function to guide them well towards the goal for complex planning problem however the heuristic function may often guide the search into a large local minimum and make the search examine most of the state in the minimum before proceeding in this paper we propose a novel heuristic search called r search which depends much le on the quality of the heuristic function the search avoids local minimum by solving the whole planning problem with a series of short range and easy to solve search each guided by the heuristic function towards a randomly chosen goal in addition r scale much better in term of memory because it can discard a search state space after each of it search on the theoretical side we derive probabilistic guarantee on the sub optimality of the solution returned by r on the experimental side we show that r can scale to large complex problem 
clustering is an old research topic in data mining and machine learning most of the traditional clustering method can be categorized a local or global one in this paper a novel clustering method that can explore both the local and global information in the data set is proposed the method clustering with local and global regularization clgr aim to minimize a cost function that properly trade off the local and global cost we show that such an optimization problem can be solved by the eigenvalue decomposition of a sparse symmetric matrix which can be done efficiently using iterative method finally the experimental result on several data set are presented to show the effectiveness of our method 
we introduce bayesian coalitional game bcgs a generalization of classical coalitional game to setting with uncertainty we define the semantics of bcg using the partition model and generalize the notion of payoff to contract among agent to analyze these game we extend the solution concept of the core under three natural interpretation ex ante ex interim and ex post which coincide with the classical definition of the core when there is no uncertainty in the special case where agent are risk neutral we show that checking for core emptiness under all three interpretation can be simplified to linear feasibility problem similar to that of their classical counterpart 
bootstrap voting expert bve is an extension to the voting expert algorithm for unsupervised chunking of sequence bve generates a series of segmentation each of which incorporates knowledge gained from the previous segmentation we show that this method of bootstrapping improves the performance of voting expert in a variety of unsupervised word segmentation scenario and generally improves both precision and recall of the algorithm we also show that minimum description length mdl can be used to choose nearly optimal parameter for voting expert in an unsupervised manner 
the next decade will see an abundance of new intelligent system many of which will be market based soon user will indirectly interact with many market without knowing it when driving their car when listening to a song when talking on the phone when backing up their file or even when surfing the web i argue that these new system can only be successful if a new approach is chosen towards designing them in particular the complexity of the market must be hidden and the interaction for the user must be seamless in this paper i introduce the general problem of hidden market design an important goal of this research agenda is to understand the trade off between increasing market efficiency on the one side and decreasing interaction complexity for the user on the other side to illustrate the main paradigm i give a series of example where hidden market could be applied i hope that the problem of hidden market design will inspire other researcher and lead to new research in this direction paving the way for more successful market based system in the future 
grounding is the task of reducing a first order theory to an equivalent propositional one typical grounder work on a sentence by sentence level substituting variable by domain element and simplifying where possible in this work we propose a method for reasoning on the first order theory a a whole to optimize the grounding process concretely we develop an algorithm that computes bound for subformulas such bound indicate for which tuples the subformulas are certainly true and for which they are certainly false these bound can then be used by standard grounding algorithm to substantially reduce grounding size and consequently also grounding time we have implemented the method and demonstrate it practical applicability 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
we consider a very natural problem concerned with game manipulation let g be a directed graph where the node represent player of a game and an edge from u to v mean that u can beat v in the game if an edge u v is not present one cannot match u and v given g and a favorite node a is it possible to set up the bracket of a balanced single elimination tournament so that a is guaranteed to win if match occur a predicted by g we show that the problem is npcomplete for general graph for the case when g is a tournament graph we give several interesting condition on the desired winner a for which there exists a balanced single elimination tournament which a win and it can be found in polynomial time 
imagine a resource allocation scenario in which the interested party can at a cost individually research way of using the resource to be allocated potentially increasing the value they would achieve from obtaining it each agent ha a private model of it research process and obtains a private realization of it improvement in value if any from a social perspective it is optimal to coordinate research in a way that strike the right tradeoff between value and cost ultimately allocating the resource to one partythus this is a problem of multi agent metadeliberation we provide a reduction of computing the optimal deliberation allocation policy to computing gittins index in multi anned bandit world and apply a modification of the dynamic vcg mechanism to yield truthful participation in an ex post equilibrium our mechanism achieves equilibrium implementation ofthe optimal policy even when agent have the capacity to deliberate about other agent valuation and thus address the problem of strategic deliberation 
clustering aggregation ha emerged a an important extension of the classical clustering problem it refers to the situation in which a number of different input clustering have been obtained for a particular data set and it is desired to aggregate those clustering result to get a better clustering solution in this paper we propose a unified framework to solve the clustering aggregation problem where the aggregated clustering result is obtained by minimizing the weighted sum of the bregman divergence between it and all the input clustering moreover under our algorithm framework we also propose a novel cluster aggregation problem where some must link and cannot link constraint are given in addition to the input clustering finally the experimental result on some real world data set are presented to show the effectiveness of our method 
we consider the problem of manipulating election via cloning candidate in our model a manipulator can replace each candidate c by one or more clone i e new candidate that are so similar to c that each voter simply replaces c in his vote with the block of c s clone the outcome of the resulting election may then depend on how each voter order the clone within the block we formalize what it mean for a cloning manipulation to be successful which turn out to be a surprisingly delicate issue and for a number of prominent voting rule characterize the preference profile for which a successful cloning manipulation exists we also consider the model where there is a cost associated with producing each clone and study the complexity of finding a minimum cost cloning manipulation finally we compare cloning with the related problem of control via adding candidate 
in this paper we propose a new spectral clustering method referred to a spectral embedded clustering sec to minimize the normalized cut criterion in spectral clustering a well a control the mismatch between the cluster assignment matrix and the low dimensional embedded representation of the data sec is based on the observation that the cluster assignment matrix of high dimensional data can be represented by a low dimensional linear mapping of data we also discover the connection between sec and other clustering method such a spectral clustering clustering with local and global regularization k mean and discriminative k mean the experiment on many real world data set show that sec significantly out performs the existing spectral clustering method a well a k mean clustering related method 
in several application area for planning in particular helping with the creation of new process in business process management bpm a major obstacle lie in the modeling obtaining a suitable model to plan with is often prohibitively complicated and or costly our core observation in this work is that for software architectural purpose sap is already using a model that is essentially a variant of pddl that model describes the behavior of business object in term of status variable and how they are affected by system transaction we show herein that one can leverage the model to obtain a a promising bpm planning application which incurs hardly any modeling cost and b an interesting planning benchmark we design a suitable planning formalism and an adaptation of ff and we perform large scale experiment our prototype is part of a research extension to the sap netweaver platform 
a recalcitrant problem in approach to iterated belief revision is that after first revising by a formula and then by a formula that is inconsistent with the first formula all information in the original formula is lost a noted by various researcher this phenomenon is made explicit in the second postulate c of the well known darwiche pearl framework and so this postulate ha been a point of criticism of this and related approach in contrast we argue that the true culprit of this problem arises from a basic assumption of the agm framework that new information is represented by a single formula we propose a more general framework for belief revision called parallel belief revision in which individual item of new information are represented by a set of formula in this framework if one revise by a set of formula and then by the negation of some member of this set then other member of the set are still believed after the revision hence the aforecited problem is discharged we present first a basic approach to parallel belief revision and next an approach that combine the basic approach with that of jin and thielscher postulate and semantic condition characterizing these approach are given and representation result provided 
the prometheus design tool pdt support the structured design of intelligent agent system it support the prometheus methodology but can also be used more generally this paper outline the tool and some of it many feature 
machine learning system are deployed in many adversarial condition like intrusion detection where a classifier ha to decide whether a sequence of action come from a legitimate user or not however the attacker being an adversarial agent could reverse engineer the classifier and successfully masquerade a a legitimate user in this paper we propose the notion of a proactive intrusion detection system id that can counter such attack by incorporating feedback into the process a proactive id influence the user s action and observes them in different situation to decide whether the user is an intruder we present a formal analysis of proactive intrusion detection and extend the adversarial relationship between the id and the attacker to present a game theoretic analysis finally we present experimental result on real and synthetic data that confirm the prediction of the analysis 
using efcient method that reduce the search space we design an algorithm strong enough to solve all hex opening 
it ha been recognised that the expressivity of description logic benefit from the introduction of non standard modal operator beyond existential and number restriction such operator support notion such a uncertainty default agency obligation or evidence whose semantics often lie outside the realm of relational structure coalgebraic hybrid logic serf a a unified setting for logic that combine non standard modal operator and nominal which allow reasoning about individual in this framework we prove a generic exptime upper bound for concept satisfiability over general tboxes which instantiates to novel upper bound for many individual logic including probabilistic logic with nominal 
traditional clustering method deal with a single clustering task on a single data set in some newly emerging application multiple similar clustering task are involved simultaneously in this case we not only desire a partition for each task but also want to discover the relationship among cluster of different task it is also expected that utilizing the relationship among task can improve the individual performance of each task in this paper we propose general approach to extend a wide family of traditional clustering model algorithm to multitask setting we first generally formulate the multitask clustering a minimizing a loss function composed of a within task loss and a task regularization then based on the general bregman divergence the within task loss is defined a the average bregman divergence from a data sample to it cluster centroid and two type of task regularization are proposed to encourage coherence among clustering result of task afterwards we further provide a probabilistic interpretation to the proposed formulation from a viewpoint of joint density estimation finally we propose alternate procedure to solve the induced optimization problem in such procedure the clustering model and the relationship among cluster of different task are updated alternately and the two phase boost each other empirical result on several real data set validate the effectiveness of the proposed approach 
skat is germany s national card game played by million of player around the world in this paper we present the world s first computer skat player that play at the level of human expert this performance is achieved by improving state evaluation using game data produced by human player and by using these state evaluation to perform inference on the unobserved hand of opposing player our result demonstrate the gain from adding inference to an imperfect information game player and show that training on data from average human player can result in expert level playing strength 
we review and extend earlier work on the logic cfd a description logic that allows terminological cycle with universal restriction over functional role in particular we consider the problem of reasoning about concept subsumption and the problem of computing certain answer for a family of attribute connected conjunctive query showing that both problem are in ptime we then consider the effect on the complexity of these problem after adding a concept constructor that express concept union or after adding a concept constructor for the bottom class finally we show that adding both constructor make both problem exptime complete 
the problem of revising an ontology consistently is closely related to the problem of belief revision which ha been widely discussed in the literature some syntax based belief revision operator have been adapted to revise ontology in description logic dl however these operator remove the whole axiom to resolve logical contradiction and thus are not fine grained in this paper we propose three model based revision operator to revise terminology in dl we show that one of them is more rational than others by comparing their logical property therefore we focus on this revision operator we also consider the problem of computing the result of revision by our operator with the help of the notion of concept forgetting finally we analyze the computational complexity of our revision operator 
the possible winner problem asks whether some distinguished candidate may become the winner of an election when the given incomplete vote are extended into complete one in a favorable way possible winner is np complete for common voting rule such a borda many other positional scoring rule bucklin copeland etc we investigate how three different parameterizations influence the computational complexity of possible winner for a number of voting rule we show fixed parameter tractability result with respect to the parameter number of candidate but intractability result with respect to the parameter number of vote finally we derive fixed parameter tractability result with respect to the parameter total number of undetermined candidate pair and identify an interesting polynomial time solvable special case for borda 
we present a knowledge rich methodology for disambiguating wikipedia category with wordnet synset and using this semantic information to restructure a taxonomy automatically generated from the wikipedia system of category we evaluate against a manual gold standard and show that both category disambiguation and taxonomy restructuring perform with high accuracy besides we ass these method on automatically generated datasets and show that we are able to effectively enrich wordnet with a large number of instance from wikipedia our approach produce an integrated resource thus bringing together the fine grained classification of instance in wikipedia and a well structured top level taxonomy from wordnet 
the web hold tremendous potential a a source of training data for visual classification however web image must be correctly indexed and labeled before this potential can be realized accordingly there ha been considerable recent interest in collecting imagery from the web using image search engine to build database for object and scene recognition research while search engine can provide rough set of image data result are noisy and this lead to problem when training classifier in this paper we propose a semi supervised model for automatically collecting clean example imagery from the web our approach includes both visual and textual web data in a unified framework minimal supervision is enabled by the selective use of generative and discriminative element in a probabilistic model and a novel learning algorithm we show through experiment that our model discovers good training image from the web with minimal manual work classifier trained using our method significantly outperform analogous baseline approach on the caltech dataset 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
previous study have demonstrated that encoding a bayesian network into a sat cnf formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference in bayesian network in this paper we present technique for improving this approach for bayesian network with noisy or and noisy max relation two relation which are widely used in practice a they can dramatically reduce the number of probability one need to specify in particular we present two space efficient cnf encoding for noisy or max and explore alternative search ordering heuristic we experimentally evaluated our technique on large scale real and randomly generated bayesian network on these benchmark our technique gave speedup of up to two order of magnitude over the best previous approach and scaled up to network with larger number of random variable 
recent advancement in model based reinforcement learning have shown that the dynamic of many structured domain e g dbns can be learned with tractable sample complexity despite their exponentially large state space u nfortunately these algorithm all require access to a plann er that computes a near optimal policy and while many traditional mdp algorithm make this guarantee their computation time grows with the number of state we show how to replace these over matched planner with a class of sample based planner whose computation time is independent of the number of state without sacrificing the sampleefficiency guarantee of the overall learning algorithm t o do so we define sufficient criterion for a sample based planne r to be used in such a learning system and analyze two popular sample based approach from the literature we also introduce our own sample based planner which combine the strategy from these algorithm and still meet the criter ia for integration into our learning system in doing so we define the first complete rl solution for compactly represented ex ponentially sized state space with efficiently learnable dynamic that is both sample efficient and whose computation time doe not grow rapidly with the number of state 
we present a novel reasoning procedure for horn shiq ontology shiq ontology that can be translated to the horn fragment of first order logic in contrast to traditional reasoning procedure for ontology our procedure doe not build model or model representation but work by deriving new consequent axiom the procedure is closely related to the so called completion based procedure for el ontology and can be regarded a an extension thereof in fact our procedure is theoretically optimal for horn shiq ontology a well a for the common fragment of el and shiq a preliminary empirical evaluation of our procedure on large medical ontology demonstrates a dramatic improvement over existing ontology reasoner specifically our implementation allows the classification of the largest available owl version of galen to the best of our knowledge no other reasoner is able to classify this ontology 
emerging service oriented technology allow software agent to automatically procure distributed service to complete complex task however in many application scenario service provider demand financial remuneration execution time are uncertain and consumer have deadline for their task in this paper we address these issue by developing a novel approach that dynamically procures multiple redundant service over time in order to ensure success by the deadline specifically we first present an algorithm for finding optimal procurement solution a well a a heuristic algorithm that achieves over of the optimal and is capable of handling thousand of provider using experiment we show that these algorithm achieve an improvement of up to over current strategy that procure only single service finally we consider setting where service cost are not known to the consumer and introduce several mechanism that incentivise provider to reveal their cost truthfully and that still achieve up to efficiency 
a plan with rich control structure like branch and loop can usually serve a a general solution that solves multiple planning instance in a domain however the correctness of such generalized plan is non trivial to define and verify especially when it come to whether or not a plan work for all of the infinitely many instance of the problem in this paper we give a precise definition of a generalized plan representation called an fsa plan with it semantics defined in the situation calculus based on this we identify a class of infinite planning problem which we call one dimensional d and prove a correctness result that d problem can be verified by finite mean we show that this theoretical result lead to an algorithm that doe this verification practically and a planner based on this verification algorithm efficiently generates provably correct plan for d problem 
distance metric ha an important role in many machine learning algorithm recently metric learning for semi supervised algorithm ha received much attention for semi supervised clustering usually a set of pairwise similarity and dissimilarity constraint is provided a supervisory information until now various metric learning method utilizing pairwise constraint have been proposed the existing method that can consider both positive must link and negative cannot link constraint find linear transformation or equivalently global mahalanobis metric additionally they find metric only according to the data point appearing in constraint without considering other data point in this paper we consider the topological structure of data along with both positive and negative constraint we propose a kernel based metric learning method that provides a non linear transformation experimental result on synthetic and real world data set show the effectiveness of our metric learning method 
interactive clustering refers to situation in which a human labeler is willing to assist a learning algorithm in automatically clustering item we present a related but somewhat different task assisted clustering in which a user creates explicit group of item from a large set and want suggestion on what item to add to each group while the traditional approach to interactive clustering ha been to use metric learning to induce a distance metric our situation seems equally amenable to classification using clustering of document from human subject we found that one or the other method proved to be superior for a given cluster but not uniformly so we thus developed a hybrid mechanism for combining the metric learner and the classifier we present result from a large number of trial based on human clustering in which we show that our combination scheme match and often exceeds the performance of a method which exclusively us either type of learner 
past approach for solving mdps have several weakness decision theoretic computation over the state space can yield optimal result but scale poorly value function approximation typically requires human specified basis function and ha not been shown successful on nominal discrete domain such a those in the icaps planning competition replanning by applying a classical planner to a determinized domain model can generate approximate policy for very large problem but ha trouble handling probabilistic subtlety little and thiebaux this paper present retrase a novel mdp solver which combine decision theory function approximation and classical planning in a new way retrase us classical planning to create basis function for value function approximation and applies expected utility analysis to this compact space our algorithm is memory efficient and fast due to it compact approximate representation return high quality solution due to the decision theoretic framework and doe not require additional knowledge from domain engineer since we apply classical planning to automatically construct the basis function experiment demonstrate that retrase outperforms winner from the past three probabilistic planning competition on many hard problem 
recent research in nonmonotonic logic programming ha focused on certain type of program equivalence which we refer to here a hyperequivalence that are relevant for program optimization and modular programming so far most result concern hyperequivalence relative to the stable model semantics however other semantics for logic program are also of interest especially the semantics of supported model which when properly generalized is closely related to the autoepistemic logic of moore in this paper we consider a family of hyperequivalence relation for program based on the semantics of supported and supported minimal model we characterize these relation in model theoretic term we use the characterization to derive complexity result concerning testing whether two program are hyperequivalent relative to supported and supported minimal model 
many situation arise in which an interested party s utility is dependent on the action of an agent e g a teacher is interested in a student learning effectively and a firm is interested in a consumer s behavior we consider an environment in which the interested party can provide incentive to affect the agent s action but cannot otherwise enforce action in value based policy teaching we situate this within the framework of sequential decision task modeled by markov decision process and seek to associate limited reward with state that induce the agent to follow a policy that maximizes the total expected value of the interested party we show value based policy teaching is np hard and provide a mixed integer program formulation focusing in particular on environment in which the agent s reward is unknown to the interested party we provide a method for active indirect elicitation wherein the agent s reward function is inferred from observation about it response to incentive experimental result suggest that we can generally find the optimal incentive provision in a small number of elicitation round 
in many practical context where a number of agent have to find a common decision the vote do not come all together at the same time in such situation we may want to preprocess the information given by the subelectorate consisting of the voter who have expressed their vote so a to compile the known vote for the time when the latecomer have expressed their vote we study the amount of space necessary for such a compilation a a function of the voting rule the number of candidate and the number of vote already known we relate our result to existing work especially on communication complexity 
the survey propagation sp algorithm ha been shown to work well on large instance of the random sat problem near it phase transition it wa shown that sp estimate marginals over cover using joker state to represent cluster of configuration the sp y algorithm generalizes sp to work on the max sat problem but the cover interpretation of sp doe not generalize to sp y recently a relaxed survey propagation rsp algorithm ha been proposed for inference in markov random field mrf rsp for mrfs assigns zero probability to joker state and hence the cover interpretation is also inapplicable we adapt rsp to solve max sat problem and show that it ha an interpretation of estimating marginals over cover violating a minimum number of clause this naturally generalizes the cover interpretation of sp empirically we show that rsp out performs sp y and other state of the art solver on random a well a benchmark instance of max sat 
in most sentiment analysis application the sentiment lexicon play a key role however it is hard if not impossible to collect and maintain a universal sentiment lexicon for all application domain because different word may be used in different domain the main existing technique extract such sentiment word from a large domain corpus based on different conjunction and the idea of sentiment coherency in a sentence in this paper we propose a novel propagation approach that exploit the relation between sentiment word and topic or product feature that the sentiment word modify and also sentiment word and product feature themselves to extract new sentiment word a the method propagates information through both sentiment word and feature we call it double propagation the extraction rule are designed based on relation described in dependency tree a new method is also proposed to assign polarity to newly discovered sentiment word in a domain experimental result show that our approach is able to extract a large number of new sentiment word the polarity assignment method is also effective 
abductive inference is an important ai reasoning technique to find explanation of observation and ha recently been applied to scientific discovery to find best hypothesis among many logically possible hypothesis we need to evaluate hypothesis obtained from the process of hypothesis generation we propose an abductive inference architecture combined with an em algorithm working on binary decision diagram bdds this work open a way of applying bdds to compress multiple hypothesis and to select most probable one from them an implemented system ha been applied to inference of inhibition in metabolic pathway in the domain of system biology 
activity recognition based on data from mobile wearable device is becoming an important application area for machine learning we propose a novel approach based on a combination of feature extraction using time delay embedding and supervised learning the computational requirement are considerably lower than existing approach so the processing can be done in real time on a low powered portable device such a a mobile phone we evaluate the performance of our algorithm on a large noisy data set comprising over hour of data from six different subject including activity such a running and walking up or down stair we also demonstrate the ability of the system to accurately classify an individual from a set of people based only on the characteristic of their walking gait the system requires very little parameter tuning and can be trained with small amount of data 
the canadian traveler problem ctp is a navigation problem where a graph is initially known but some edge may be blocked with a known probability the task is to minimize travel effort of reaching the goal we generalize ctp to allow for remote sensing action now requiring minimization of the sum of the travel cost and the remote sensing cost finding optimal policy for both version is intractable we provide optimal solution for special case graph we then develop a framework that utilizes heuristic to determine when and where to sense the environment in order to minimize total cost several such heuristic based on the expected total cost are introduced empirical evaluation show the benefit of our heuristic and support some of the theoretical result 
the construction of causal graph from nonexperimental data rest on a set of constraint that the graph structure imposes on all probability distribution compatible with the graph these constraint are of two type conditional independency and algebraic constraint first noted by verma while conditional independency are well studied and frequently used in causal induction algorithm verma constraint are still poorly understood and rarely applied in this paper we examine a special subset of verma constraint which are easy to understand easy to identify and easy to apply they arise from dormant independency namely conditional independency that hold in interventional distribution we give a complete algorithm for determining if a dormant independence between two set of variable is entailed by the causal graph such that this independence is identifiable in other word if it resides in an interventional distribution that can be predicted without resorting to intervention we further show the usefulness of dormant independency in model testing and induction by giving an algorithm that us constraint entailed by dormant independency to prune extraneous edge from a given causal graph 
the surprisingly good performance of modern satisfiability sat solver is usually explained by the existence of a certain hidden structure in real world instance we introduce the notion of backdoor tree a an indicator for the presence of a hidden structure backdoor tree refine the notion of strong backdoor set taking into account the relationship between backdoor variable we present theoretical and empirical result our theoretical result are concerned with the computational complexity of detecting small backdoor tree with our empirical result we compare the size of backdoor tree against the size of backdoor set for real world sat instance and random sat instance of various density the result indicate that backdoor tree amplify the property that have been observed for backdoor set 
we consider the existence and computational complexity of coalitional stability concept based on social network our concept represent a natural and rich combinatorial generalization of a recent approach termed partition equilibrium we assume that player in a strategic game are embedded in a social network and there are coordination constraint that restrict the potential coalition that can jointly deviate in the game to the set of clique in the social network in addition player act in a considerate fashion to ignore potentially profitable group deviation if the change in their strategy may cause a decrease of utility to their neighbor we study the property of such considerate equilibrium in application to the class of resource selection game rsg our main result prof existence of a considerate equilibrium in all symmetric rsg with strictly increasing delay for any social network among the player the existence proof is constructive and yield an efficient algorithm in fact the computed considerate equilibrium is a nash equilibrium for the standard rsg showing that there exists a state that is stable against selfish and considerate behavior simultaneously in addition we show result on convergence of considerate dynamic 
we introduce planning game a study of interaction of self motivated agent in automated planning setting planning game extend strip like model of single agent planning to system of multiple self interested agent providing a rich class of structured game that capture subtle form of local interaction we consider two basic model of planning game and adapt game theoretic solution concept to these model in both model agent may need to cooperate in order to achieve their goal but are assumed to do so only in order to increase their net benefit for each model we study the computational problem of finding a stable solution and provide efficient algorithm for system exhibiting acyclic interaction structure 
landmark for propositional planning task are variable assignment that must occur at some point in every solution plan we propose a novel approach for using landmark in planning by deriving a pseudo heuristic and combining it with other heuristic in a search framework the incorporation of landmark information is shown to improve success rate and solution quality of a heuristic planner we furthermore show how additional landmark and ordering can be found using the information present in multi valued state variable representation of planning task compared to previously published approach our landmark extraction algorithm provides stronger guarantee of correctness for the generated landmark ordering and our novel use of landmark during search solves more planning task and delivers considerably better solution 
imagine a simulated world where the character you interact with are almost human they converse with you in english they understand the world they are in can reason about what to do and they exhibit emotion some of these character may be your friend while others will oppose you unlike current video garnes being successful in this world won t just be a matter of who is quickest on the draw or most adroit at solving puzzle instead it will be the person who understands the social fabric and cultural context and can use interpersonal skill most effectively such a simulation could open up whole new horizon for education entertainment and simulation and given recent advance in ai and graphic it may not be too far off virtual human are computer generated character that can take the part of human in a variety of limited context these can include acting a role player in simulation and training system johnson rickel lester swartout et al traum et al johnson vilhj lmsson marsella where they play a variety of part such a acting a friendly or hostile force or local in the environment other us for virtual human include acting a museum guide gustafson bell marketing assistant cassell bickmore et al or character in entertainment system where the advent of video game such a the sims make clear the growing interest of the computer game industry in virtual human see also mateas stern 
pca can be smarter and make more sensible projection in this paper we propose smart pca an extension to standard pca to regularize and incorporate external knowledge into model estimation based on the probabilistic interpretation of pca the inverse wishart distribution can be used a the informative conjugate prior for the population covariance and useful knowledge is carried by the prior hyperparameters we design the hyperparameters to smoothly combine the information from both the domain knowledge and the data itself the bayesian point estimation of principal component is in closed form in empirical study smart pca show clear improvement on three different criterion image reconstruction error the perceptual quality of the reconstructed image and the pattern recognition performance 
most planning problem have strong structure they can be decomposed into subdomains with causal dependency the idea of exploiting the domain decomposition ha motivated previous work such a hierarchical planning and factored planing however these algorithm require extensive backtracking and lead to few efficient general purpose planner on the other hand heuristic search ha been a successful approach to automated planning the domain decomposition of planning problem unfortunately is not directly and fully exploited by heuristic search we propose a novel and general framework to exploit domain decomposition based on a structure analysis on the sa planning formalism we stratify the sub domain of a planning problem into dependency layer by recognizing the stratification of a planning structure we propose a space reduction method that expands only a subset of executable action at each state this reduction method can be combined with state space search allowing u to simultaneously employ the strength of domain decomposition and high quality heuristic we prove that the reduction preserve completeness and optimality of search and experimentally verify it effectiveness in space reduction 
optimal heuristic search such a a search are widely used for planning but can rarely scale to large complex problem the suboptimal version of heuristic search such a weighted a search can often scale to much larger planning problem by trading off the quality of the solution for efficiency they do so by relying more on the ability of the heuristic function to guide them well towards the goal for complex planning problem however the heuristic function may often guide the search into a large local minimum and make the search examine most of the state in the minimum before proceeding in this paper we propose a novel heuristic search called r search which depends much le on the quality of the heuristic function the search avoids local minimum by solving the whole planning problem with a series of short range and easy to solve search each guided by the heuristic function towards a randomly chosen goal in addition r scale much better in term of memory because it can discard a search state space after each of it search on the theoretical side we derive probabilistic guarantee on the sub optimality of the solution returned by r on the experimental side we show that r can scale to large complex problem 
clustering is an old research topic in data mining and machine learning most of the traditional clustering method can be categorized a local or global one in this paper a novel clustering method that can explore both the local and global information in the data set is proposed the method clustering with local and global regularization clgr aim to minimize a cost function that properly trade off the local and global cost we show that such an optimization problem can be solved by the eigenvalue decomposition of a sparse symmetric matrix which can be done efficiently using iterative method finally the experimental result on several data set are presented to show the effectiveness of our method 
we introduce bayesian coalitional game bcgs a generalization of classical coalitional game to setting with uncertainty we define the semantics of bcg using the partition model and generalize the notion of payoff to contract among agent to analyze these game we extend the solution concept of the core under three natural interpretation ex ante ex interim and ex post which coincide with the classical definition of the core when there is no uncertainty in the special case where agent are risk neutral we show that checking for core emptiness under all three interpretation can be simplified to linear feasibility problem similar to that of their classical counterpart 
bootstrap voting expert bve is an extension to the voting expert algorithm for unsupervised chunking of sequence bve generates a series of segmentation each of which incorporates knowledge gained from the previous segmentation we show that this method of bootstrapping improves the performance of voting expert in a variety of unsupervised word segmentation scenario and generally improves both precision and recall of the algorithm we also show that minimum description length mdl can be used to choose nearly optimal parameter for voting expert in an unsupervised manner 
the next decade will see an abundance of new intelligent system many of which will be market based soon user will indirectly interact with many market without knowing it when driving their car when listening to a song when talking on the phone when backing up their file or even when surfing the web i argue that these new system can only be successful if a new approach is chosen towards designing them in particular the complexity of the market must be hidden and the interaction for the user must be seamless in this paper i introduce the general problem of hidden market design an important goal of this research agenda is to understand the trade off between increasing market efficiency on the one side and decreasing interaction complexity for the user on the other side to illustrate the main paradigm i give a series of example where hidden market could be applied i hope that the problem of hidden market design will inspire other researcher and lead to new research in this direction paving the way for more successful market based system in the future 
grounding is the task of reducing a first order theory to an equivalent propositional one typical grounder work on a sentence by sentence level substituting variable by domain element and simplifying where possible in this work we propose a method for reasoning on the first order theory a a whole to optimize the grounding process concretely we develop an algorithm that computes bound for subformulas such bound indicate for which tuples the subformulas are certainly true and for which they are certainly false these bound can then be used by standard grounding algorithm to substantially reduce grounding size and consequently also grounding time we have implemented the method and demonstrate it practical applicability 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
we consider a very natural problem concerned with game manipulation let g be a directed graph where the node represent player of a game and an edge from u to v mean that u can beat v in the game if an edge u v is not present one cannot match u and v given g and a favorite node a is it possible to set up the bracket of a balanced single elimination tournament so that a is guaranteed to win if match occur a predicted by g we show that the problem is npcomplete for general graph for the case when g is a tournament graph we give several interesting condition on the desired winner a for which there exists a balanced single elimination tournament which a win and it can be found in polynomial time 
imagine a resource allocation scenario in which the interested party can at a cost individually research way of using the resource to be allocated potentially increasing the value they would achieve from obtaining it each agent ha a private model of it research process and obtains a private realization of it improvement in value if any from a social perspective it is optimal to coordinate research in a way that strike the right tradeoff between value and cost ultimately allocating the resource to one partythus this is a problem of multi agent metadeliberation we provide a reduction of computing the optimal deliberation allocation policy to computing gittins index in multi anned bandit world and apply a modification of the dynamic vcg mechanism to yield truthful participation in an ex post equilibrium our mechanism achieves equilibrium implementation ofthe optimal policy even when agent have the capacity to deliberate about other agent valuation and thus address the problem of strategic deliberation 
clustering aggregation ha emerged a an important extension of the classical clustering problem it refers to the situation in which a number of different input clustering have been obtained for a particular data set and it is desired to aggregate those clustering result to get a better clustering solution in this paper we propose a unified framework to solve the clustering aggregation problem where the aggregated clustering result is obtained by minimizing the weighted sum of the bregman divergence between it and all the input clustering moreover under our algorithm framework we also propose a novel cluster aggregation problem where some must link and cannot link constraint are given in addition to the input clustering finally the experimental result on some real world data set are presented to show the effectiveness of our method 
we consider the problem of manipulating election via cloning candidate in our model a manipulator can replace each candidate c by one or more clone i e new candidate that are so similar to c that each voter simply replaces c in his vote with the block of c s clone the outcome of the resulting election may then depend on how each voter order the clone within the block we formalize what it mean for a cloning manipulation to be successful which turn out to be a surprisingly delicate issue and for a number of prominent voting rule characterize the preference profile for which a successful cloning manipulation exists we also consider the model where there is a cost associated with producing each clone and study the complexity of finding a minimum cost cloning manipulation finally we compare cloning with the related problem of control via adding candidate 
in this paper we propose a new spectral clustering method referred to a spectral embedded clustering sec to minimize the normalized cut criterion in spectral clustering a well a control the mismatch between the cluster assignment matrix and the low dimensional embedded representation of the data sec is based on the observation that the cluster assignment matrix of high dimensional data can be represented by a low dimensional linear mapping of data we also discover the connection between sec and other clustering method such a spectral clustering clustering with local and global regularization k mean and discriminative k mean the experiment on many real world data set show that sec significantly out performs the existing spectral clustering method a well a k mean clustering related method 
in several application area for planning in particular helping with the creation of new process in business process management bpm a major obstacle lie in the modeling obtaining a suitable model to plan with is often prohibitively complicated and or costly our core observation in this work is that for software architectural purpose sap is already using a model that is essentially a variant of pddl that model describes the behavior of business object in term of status variable and how they are affected by system transaction we show herein that one can leverage the model to obtain a a promising bpm planning application which incurs hardly any modeling cost and b an interesting planning benchmark we design a suitable planning formalism and an adaptation of ff and we perform large scale experiment our prototype is part of a research extension to the sap netweaver platform 
a recalcitrant problem in approach to iterated belief revision is that after first revising by a formula and then by a formula that is inconsistent with the first formula all information in the original formula is lost a noted by various researcher this phenomenon is made explicit in the second postulate c of the well known darwiche pearl framework and so this postulate ha been a point of criticism of this and related approach in contrast we argue that the true culprit of this problem arises from a basic assumption of the agm framework that new information is represented by a single formula we propose a more general framework for belief revision called parallel belief revision in which individual item of new information are represented by a set of formula in this framework if one revise by a set of formula and then by the negation of some member of this set then other member of the set are still believed after the revision hence the aforecited problem is discharged we present first a basic approach to parallel belief revision and next an approach that combine the basic approach with that of jin and thielscher postulate and semantic condition characterizing these approach are given and representation result provided 
the prometheus design tool pdt support the structured design of intelligent agent system it support the prometheus methodology but can also be used more generally this paper outline the tool and some of it many feature 
machine learning system are deployed in many adversarial condition like intrusion detection where a classifier ha to decide whether a sequence of action come from a legitimate user or not however the attacker being an adversarial agent could reverse engineer the classifier and successfully masquerade a a legitimate user in this paper we propose the notion of a proactive intrusion detection system id that can counter such attack by incorporating feedback into the process a proactive id influence the user s action and observes them in different situation to decide whether the user is an intruder we present a formal analysis of proactive intrusion detection and extend the adversarial relationship between the id and the attacker to present a game theoretic analysis finally we present experimental result on real and synthetic data that confirm the prediction of the analysis 
using efcient method that reduce the search space we design an algorithm strong enough to solve all hex opening 
it ha been recognised that the expressivity of description logic benefit from the introduction of non standard modal operator beyond existential and number restriction such operator support notion such a uncertainty default agency obligation or evidence whose semantics often lie outside the realm of relational structure coalgebraic hybrid logic serf a a unified setting for logic that combine non standard modal operator and nominal which allow reasoning about individual in this framework we prove a generic exptime upper bound for concept satisfiability over general tboxes which instantiates to novel upper bound for many individual logic including probabilistic logic with nominal 
traditional clustering method deal with a single clustering task on a single data set in some newly emerging application multiple similar clustering task are involved simultaneously in this case we not only desire a partition for each task but also want to discover the relationship among cluster of different task it is also expected that utilizing the relationship among task can improve the individual performance of each task in this paper we propose general approach to extend a wide family of traditional clustering model algorithm to multitask setting we first generally formulate the multitask clustering a minimizing a loss function composed of a within task loss and a task regularization then based on the general bregman divergence the within task loss is defined a the average bregman divergence from a data sample to it cluster centroid and two type of task regularization are proposed to encourage coherence among clustering result of task afterwards we further provide a probabilistic interpretation to the proposed formulation from a viewpoint of joint density estimation finally we propose alternate procedure to solve the induced optimization problem in such procedure the clustering model and the relationship among cluster of different task are updated alternately and the two phase boost each other empirical result on several real data set validate the effectiveness of the proposed approach 
skat is germany s national card game played by million of player around the world in this paper we present the world s first computer skat player that play at the level of human expert this performance is achieved by improving state evaluation using game data produced by human player and by using these state evaluation to perform inference on the unobserved hand of opposing player our result demonstrate the gain from adding inference to an imperfect information game player and show that training on data from average human player can result in expert level playing strength 
we review and extend earlier work on the logic cfd a description logic that allows terminological cycle with universal restriction over functional role in particular we consider the problem of reasoning about concept subsumption and the problem of computing certain answer for a family of attribute connected conjunctive query showing that both problem are in ptime we then consider the effect on the complexity of these problem after adding a concept constructor that express concept union or after adding a concept constructor for the bottom class finally we show that adding both constructor make both problem exptime complete 
the problem of revising an ontology consistently is closely related to the problem of belief revision which ha been widely discussed in the literature some syntax based belief revision operator have been adapted to revise ontology in description logic dl however these operator remove the whole axiom to resolve logical contradiction and thus are not fine grained in this paper we propose three model based revision operator to revise terminology in dl we show that one of them is more rational than others by comparing their logical property therefore we focus on this revision operator we also consider the problem of computing the result of revision by our operator with the help of the notion of concept forgetting finally we analyze the computational complexity of our revision operator 
the possible winner problem asks whether some distinguished candidate may become the winner of an election when the given incomplete vote are extended into complete one in a favorable way possible winner is np complete for common voting rule such a borda many other positional scoring rule bucklin copeland etc we investigate how three different parameterizations influence the computational complexity of possible winner for a number of voting rule we show fixed parameter tractability result with respect to the parameter number of candidate but intractability result with respect to the parameter number of vote finally we derive fixed parameter tractability result with respect to the parameter total number of undetermined candidate pair and identify an interesting polynomial time solvable special case for borda 
we present a knowledge rich methodology for disambiguating wikipedia category with wordnet synset and using this semantic information to restructure a taxonomy automatically generated from the wikipedia system of category we evaluate against a manual gold standard and show that both category disambiguation and taxonomy restructuring perform with high accuracy besides we ass these method on automatically generated datasets and show that we are able to effectively enrich wordnet with a large number of instance from wikipedia our approach produce an integrated resource thus bringing together the fine grained classification of instance in wikipedia and a well structured top level taxonomy from wordnet 
the web hold tremendous potential a a source of training data for visual classification however web image must be correctly indexed and labeled before this potential can be realized accordingly there ha been considerable recent interest in collecting imagery from the web using image search engine to build database for object and scene recognition research while search engine can provide rough set of image data result are noisy and this lead to problem when training classifier in this paper we propose a semi supervised model for automatically collecting clean example imagery from the web our approach includes both visual and textual web data in a unified framework minimal supervision is enabled by the selective use of generative and discriminative element in a probabilistic model and a novel learning algorithm we show through experiment that our model discovers good training image from the web with minimal manual work classifier trained using our method significantly outperform analogous baseline approach on the caltech dataset 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
previous study have demonstrated that encoding a bayesian network into a sat cnf formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference in bayesian network in this paper we present technique for improving this approach for bayesian network with noisy or and noisy max relation two relation which are widely used in practice a they can dramatically reduce the number of probability one need to specify in particular we present two space efficient cnf encoding for noisy or max and explore alternative search ordering heuristic we experimentally evaluated our technique on large scale real and randomly generated bayesian network on these benchmark our technique gave speedup of up to two order of magnitude over the best previous approach and scaled up to network with larger number of random variable 
recent advancement in model based reinforcement learning have shown that the dynamic of many structured domain e g dbns can be learned with tractable sample complexity despite their exponentially large state space u nfortunately these algorithm all require access to a plann er that computes a near optimal policy and while many traditional mdp algorithm make this guarantee their computation time grows with the number of state we show how to replace these over matched planner with a class of sample based planner whose computation time is independent of the number of state without sacrificing the sampleefficiency guarantee of the overall learning algorithm t o do so we define sufficient criterion for a sample based planne r to be used in such a learning system and analyze two popular sample based approach from the literature we also introduce our own sample based planner which combine the strategy from these algorithm and still meet the criter ia for integration into our learning system in doing so we define the first complete rl solution for compactly represented ex ponentially sized state space with efficiently learnable dynamic that is both sample efficient and whose computation time doe not grow rapidly with the number of state 
we present a novel reasoning procedure for horn shiq ontology shiq ontology that can be translated to the horn fragment of first order logic in contrast to traditional reasoning procedure for ontology our procedure doe not build model or model representation but work by deriving new consequent axiom the procedure is closely related to the so called completion based procedure for el ontology and can be regarded a an extension thereof in fact our procedure is theoretically optimal for horn shiq ontology a well a for the common fragment of el and shiq a preliminary empirical evaluation of our procedure on large medical ontology demonstrates a dramatic improvement over existing ontology reasoner specifically our implementation allows the classification of the largest available owl version of galen to the best of our knowledge no other reasoner is able to classify this ontology 
emerging service oriented technology allow software agent to automatically procure distributed service to complete complex task however in many application scenario service provider demand financial remuneration execution time are uncertain and consumer have deadline for their task in this paper we address these issue by developing a novel approach that dynamically procures multiple redundant service over time in order to ensure success by the deadline specifically we first present an algorithm for finding optimal procurement solution a well a a heuristic algorithm that achieves over of the optimal and is capable of handling thousand of provider using experiment we show that these algorithm achieve an improvement of up to over current strategy that procure only single service finally we consider setting where service cost are not known to the consumer and introduce several mechanism that incentivise provider to reveal their cost truthfully and that still achieve up to efficiency 
a plan with rich control structure like branch and loop can usually serve a a general solution that solves multiple planning instance in a domain however the correctness of such generalized plan is non trivial to define and verify especially when it come to whether or not a plan work for all of the infinitely many instance of the problem in this paper we give a precise definition of a generalized plan representation called an fsa plan with it semantics defined in the situation calculus based on this we identify a class of infinite planning problem which we call one dimensional d and prove a correctness result that d problem can be verified by finite mean we show that this theoretical result lead to an algorithm that doe this verification practically and a planner based on this verification algorithm efficiently generates provably correct plan for d problem 
distance metric ha an important role in many machine learning algorithm recently metric learning for semi supervised algorithm ha received much attention for semi supervised clustering usually a set of pairwise similarity and dissimilarity constraint is provided a supervisory information until now various metric learning method utilizing pairwise constraint have been proposed the existing method that can consider both positive must link and negative cannot link constraint find linear transformation or equivalently global mahalanobis metric additionally they find metric only according to the data point appearing in constraint without considering other data point in this paper we consider the topological structure of data along with both positive and negative constraint we propose a kernel based metric learning method that provides a non linear transformation experimental result on synthetic and real world data set show the effectiveness of our metric learning method 
interactive clustering refers to situation in which a human labeler is willing to assist a learning algorithm in automatically clustering item we present a related but somewhat different task assisted clustering in which a user creates explicit group of item from a large set and want suggestion on what item to add to each group while the traditional approach to interactive clustering ha been to use metric learning to induce a distance metric our situation seems equally amenable to classification using clustering of document from human subject we found that one or the other method proved to be superior for a given cluster but not uniformly so we thus developed a hybrid mechanism for combining the metric learner and the classifier we present result from a large number of trial based on human clustering in which we show that our combination scheme match and often exceeds the performance of a method which exclusively us either type of learner 
past approach for solving mdps have several weakness decision theoretic computation over the state space can yield optimal result but scale poorly value function approximation typically requires human specified basis function and ha not been shown successful on nominal discrete domain such a those in the icaps planning competition replanning by applying a classical planner to a determinized domain model can generate approximate policy for very large problem but ha trouble handling probabilistic subtlety little and thiebaux this paper present retrase a novel mdp solver which combine decision theory function approximation and classical planning in a new way retrase us classical planning to create basis function for value function approximation and applies expected utility analysis to this compact space our algorithm is memory efficient and fast due to it compact approximate representation return high quality solution due to the decision theoretic framework and doe not require additional knowledge from domain engineer since we apply classical planning to automatically construct the basis function experiment demonstrate that retrase outperforms winner from the past three probabilistic planning competition on many hard problem 
recent research in nonmonotonic logic programming ha focused on certain type of program equivalence which we refer to here a hyperequivalence that are relevant for program optimization and modular programming so far most result concern hyperequivalence relative to the stable model semantics however other semantics for logic program are also of interest especially the semantics of supported model which when properly generalized is closely related to the autoepistemic logic of moore in this paper we consider a family of hyperequivalence relation for program based on the semantics of supported and supported minimal model we characterize these relation in model theoretic term we use the characterization to derive complexity result concerning testing whether two program are hyperequivalent relative to supported and supported minimal model 
many situation arise in which an interested party s utility is dependent on the action of an agent e g a teacher is interested in a student learning effectively and a firm is interested in a consumer s behavior we consider an environment in which the interested party can provide incentive to affect the agent s action but cannot otherwise enforce action in value based policy teaching we situate this within the framework of sequential decision task modeled by markov decision process and seek to associate limited reward with state that induce the agent to follow a policy that maximizes the total expected value of the interested party we show value based policy teaching is np hard and provide a mixed integer program formulation focusing in particular on environment in which the agent s reward is unknown to the interested party we provide a method for active indirect elicitation wherein the agent s reward function is inferred from observation about it response to incentive experimental result suggest that we can generally find the optimal incentive provision in a small number of elicitation round 
in many practical context where a number of agent have to find a common decision the vote do not come all together at the same time in such situation we may want to preprocess the information given by the subelectorate consisting of the voter who have expressed their vote so a to compile the known vote for the time when the latecomer have expressed their vote we study the amount of space necessary for such a compilation a a function of the voting rule the number of candidate and the number of vote already known we relate our result to existing work especially on communication complexity 
the survey propagation sp algorithm ha been shown to work well on large instance of the random sat problem near it phase transition it wa shown that sp estimate marginals over cover using joker state to represent cluster of configuration the sp y algorithm generalizes sp to work on the max sat problem but the cover interpretation of sp doe not generalize to sp y recently a relaxed survey propagation rsp algorithm ha been proposed for inference in markov random field mrf rsp for mrfs assigns zero probability to joker state and hence the cover interpretation is also inapplicable we adapt rsp to solve max sat problem and show that it ha an interpretation of estimating marginals over cover violating a minimum number of clause this naturally generalizes the cover interpretation of sp empirically we show that rsp out performs sp y and other state of the art solver on random a well a benchmark instance of max sat 
in most sentiment analysis application the sentiment lexicon play a key role however it is hard if not impossible to collect and maintain a universal sentiment lexicon for all application domain because different word may be used in different domain the main existing technique extract such sentiment word from a large domain corpus based on different conjunction and the idea of sentiment coherency in a sentence in this paper we propose a novel propagation approach that exploit the relation between sentiment word and topic or product feature that the sentiment word modify and also sentiment word and product feature themselves to extract new sentiment word a the method propagates information through both sentiment word and feature we call it double propagation the extraction rule are designed based on relation described in dependency tree a new method is also proposed to assign polarity to newly discovered sentiment word in a domain experimental result show that our approach is able to extract a large number of new sentiment word the polarity assignment method is also effective 
abductive inference is an important ai reasoning technique to find explanation of observation and ha recently been applied to scientific discovery to find best hypothesis among many logically possible hypothesis we need to evaluate hypothesis obtained from the process of hypothesis generation we propose an abductive inference architecture combined with an em algorithm working on binary decision diagram bdds this work open a way of applying bdds to compress multiple hypothesis and to select most probable one from them an implemented system ha been applied to inference of inhibition in metabolic pathway in the domain of system biology 
activity recognition based on data from mobile wearable device is becoming an important application area for machine learning we propose a novel approach based on a combination of feature extraction using time delay embedding and supervised learning the computational requirement are considerably lower than existing approach so the processing can be done in real time on a low powered portable device such a a mobile phone we evaluate the performance of our algorithm on a large noisy data set comprising over hour of data from six different subject including activity such a running and walking up or down stair we also demonstrate the ability of the system to accurately classify an individual from a set of people based only on the characteristic of their walking gait the system requires very little parameter tuning and can be trained with small amount of data 
the canadian traveler problem ctp is a navigation problem where a graph is initially known but some edge may be blocked with a known probability the task is to minimize travel effort of reaching the goal we generalize ctp to allow for remote sensing action now requiring minimization of the sum of the travel cost and the remote sensing cost finding optimal policy for both version is intractable we provide optimal solution for special case graph we then develop a framework that utilizes heuristic to determine when and where to sense the environment in order to minimize total cost several such heuristic based on the expected total cost are introduced empirical evaluation show the benefit of our heuristic and support some of the theoretical result 
the process of extracting useful knowledge from large datasets ha become one of the most pressing problem in today s society the problem span entire sector from scientist to intelligence analyst and web user all of whom are constantly struggling to keep up with the larger and larger amount of content published every day with this much data it is often easy to miss the big picture in this paper we investigate method for automatically connecting the dot providing a structured easy way to navigate within a new topic and discover hidden connection we focus on the news domain given two news article our system automatically find a coherent chain linking them together for example it can recover the chain of event starting with the decline of home price january and ending with the ongoing health care debate we formalize the characteristic of a good chain and provide an efficient algorithm with theoretical guarantee to connect two fixed endpoint we incorporate user feedback into our framework allowing the story to be refined and personalized finally we evaluate our algorithm over real news data our user study demonstrate the algorithm s effectiveness in helping user understanding the news 
complex graph in which multi type node are linked to each other frequently arise in many important application such a web mining information retrieval bioinformatics and epidemiology in this study we propose a general framework for clustering on complex graph under this framework we derive a family of clustering algorithm including both hard and soft version which are capable of learning cluster pattern from complex graph with various structure and statistical property we also establish the connection between the proposed framework and the traditional graph partitioning approach the experimental evaluation provides encouraging result to validate the proposed framework and algorithm 
we look at composition of possibly nonterminating high level program over situation calculus action theory specifically the problem we look at is a follows given a library of available congolog program and a target program not in the library verify whether the target program execution be realized by composing fragment of the execution of the available program and if so synthesize a controller that doe the composition automatically this kind of composition problem have been investigated in the c and ai literature but always assuming finite state setting here instead we investigate the issue in the context of infinite domain that may go through an infinite number of state a a result of action obviously in this context the problem is undecidable nonetheless by exploiting recent result in the ai literature we devise a sound and well characterized technique to actually solve the problem 
we consider the task of assigning expert from a portfolio of specialist in order to solve a set of task we apply a bayesian model which combine collaborative filtering with a feature based description of task and expert to yield a general framework for managing a portfolio of expert the model learns an embedding of task and problem into a latent space in which affinity is measured by the inner product the model can be trained incrementally and can track non stationary data tracking potentially changing expert and task characteristic the approach allows u to use a principled decision theoretic framework for expert selection allowing the user to choose a utility function that best suit their objective the model component for taking into account the performance feedback data is pluggable allowing flexibility we apply the model to manage a portfolio of algorithm to solve hard combinatorial problem this is a well studied area and we demonstrate a large improvement on the state of the art in one domain constraint solving and in a second domain combinatorial auction created a portfolio that performed significantly better than any single algorithm 
what local action can agent take without the benefit of global knowledge to produce the best global solution many dynamic distributed system can be modeled using technique from distributed constraint reasoning however existing work in the distributed constraint reasoning community doe not address the true dynamism inherent in many realworld system 
extreme component analysis xca is a statistical method based on a single eigenvalue decomposition to recover the optimal combination of principal and minor component in the data unfortunately minor component are notoriously sensitive to overfitting when the number of data item is small relative to the number of attribute we present a bayesian extension of xca by introducing a conjugate prior for the parameter of the xca model this bayesian xca is shown to outperform plain vanilla xca a well a bayesian pca and xca based on a frequentist correction to the sample spectrum moreover we show that minor component are only picked when they represent genuine constraint in the data even for very small sample size an extension to mixture of bayesian xca model is also explored 
multi agent decision problem in which independent agent have to agree on a joint plan of action or allocation of resource are central to ai in such situation agent individual preference over available alternative may vary and they may try to reconcile these difference by voting based on the fact that agent may have incentive to vote strategically and misreport their real preference a number of recent paper have explored different possibility for avoiding or eliminating such manipulation in contrast to most prior work this paper focus on convergence of strategic behavior to a decision from which no voter will want to deviate we consider scenario where voter cannot coordinate their action but are allowed to change their vote after observing the current outcome we focus on the plurality voting rule and study the condition under which this iterative game is guaranteed to converge to a nash equilibrium i e to a decision that is stable against further unilater al manipulation we show for the first time how convergence depends on the exact attribute of the game such a the tie breaking scheme and on assumption regarding agent weight and strategy 
classication is one of the most fundamental problem in machine learning which aim to separate the data from different class a far away a possible a common way to get a good classication function is to minimize it empirical prediction loss or structural loss in this paper we point out that we can also enhance the discriminality of those classiers by further incorporating the discriminative information contained in the data set a a prior into the classier construction process in such a way we will show that the constructed classiers will be more powerful and this will also be validated by the nal empirical study on several benchmark data set 
a wide range of constraint can be compactly specified using automaton or formal language in a sequence of recent paper we have shown that an effective mean to reason with such specification is to decompose them into primitive constraint quimper walsh we can then for instance use state of the art sat solver and profit from their advanced feature like fast unit propagation clause learning and conflict based search heuristic this approach hold promise for solving combinatorial problem in scheduling rostering and configuration a well a problem in more diverse area like bioinformatics software testing and natural language processing in addition decomposition may be an effective method to propagate other global constraint 
hierarchical state decomposition address the curse of dimensionality in q learning method for reinforcement learning rl but can suffer from suboptimality in addressing this we introduce the economic hierarchical q learning ehq algorithm for hierarchical rl the ehq algorithm us subsidy to align interest such that agent that would otherwise converge to a recursively optimal policy will instead be motivated to act hierarchically optimally the essential idea is that a parent will pay a child for the relative value to the rest of the system for returning the world in one state over another state the resulting learning framework is simple compared to other algorithm that obtain hierarchical optimality additionally ehq encapsulates relevant information about value tradeoff faced across the hierarchy at each node and requires minimal data exchange between node we provide no theoretical proof of hierarchical optimality but are able demonstrate success with ehq in empirical result 
in many real world planning domain some observation information is optional and useless to the execution of a plan on the other hand information acquisition may require some kind of cost the problem of observation reduction for strong plan ha been addressed in the literature however observation reduction for plan with context which are more general and useful than strong plan in robotics is still a open problem in this paper we present an attempt to solve the problem our first contribution is the definition of structured plan which can encode sequential conditional and iterative behavior and is expressive enough for dealing with incomplete observation information and internal state of the agent a second contribution is an observation reduction algorithm for plan with context which can transform a plan with context into a structured plan that only branch on necessary observation information 
cluster ensemble generate a large number of different clustering solution and combine them into a more robust and accurate consensus clustering on forming the ensemble the literature ha suggested that higher diversity among ensemble member produce higher performance gain in contrast some study also indicated that medium diversity lead to the best performing ensemble such contradicting observation suggest that different data with varying characteristic may require different treatment we empirically investigate this issue by examining the behavior of cluster ensemble on benchmark data set this lead to a novel framework that selects ensemble member for each data set based on it own characteristic our framework first generates a diverse set of solution and combine them into a consensus partition p based on the diversity between the ensemble member and p a subset of ensemble member is selected and combined to obtain the final output we evaluate the proposed method on benchmark data set and the result show that the proposed method can significantly improve the clustering performance often by a substantial margin in some case we were able to produce final solution that significantly outperform even the best ensemble member 
in this paper we set up a framework to study approximation of manipulation control and bribery in election we show existence of approximation algorithm even fully polynomial time approximation scheme a well a obtain inapproximability result in particular we show that a large subclass of scoring protocol admits fully polynomial time approximation scheme for the coalitional weighted manipulation problem and that if certain family of scoring protocol e g veto admitted such approximation scheme then p np we also show that bribery for borda count is np complete and that there is no approximation algorithm that achieves even a polynomial approximation ratio for bribery in borda count for the case where voter have price 
individual robot or agent will often need to form coalition to accomplish shared task e g in sensor network or market furthermore in most real system it is infeasible for entity to interact with all peer the presence of a social network can alleviate this problem by providing a neighborhood system within which entity interact with a reduced number of peer previous research ha shown that the topology of the underlying social network ha a dramatic effect on the quality of coalition formed and consequently on system performance gaston desjardins a it ha also been shown that it is feasible to develop agent which dynamically alter connection to improve an organization s a bility to form coalition on the network however those study have not analysed the network topology that result from connectivity adaptation strategy in this paper the resu lting network topology were analysed and it wa found that high performance and rapid convergence were attained because scale free networkswere being formed however it wa observed that organizational performance is not impacted by limiting the number of link per agent to the total number of skill available within the population implying that ba ndwidth wa wasted by previous approach we used these observation to inform the design of a token based algorithm that attains higher performance using an order of magnitude le message for both uniform and non uniform distribution of skill 
the idea of local learning i e classifying a particular example based on it neighbor ha been successfully applied to many semi supervised and clustering problem recently however the local learning method developed so far are all devised for single view problem in fact in many real world application example are represented by multiple set of feature in this paper we extend the idea of local learning to multi view problem design a multi view local model for each example and propose a multi view local learning regularization mvll reg matrix both it linear and kernel version are given experiment are conducted to demonstrate the superiority of the proposed method over several state of the art one 
the number partitioning problem is to divide a given set of integer into a collection of subset so that the sum of the number in each subset are a nearly equal a possible while a very efficient algorithm exists for optimal two way partitioning it is not nearly a effective for multi way partitioning we develop two new linear space algorithm for multi way partitioning and demonstrate their performance on three four and five way partitioning in each case our algorithm outperform the previous state of the art by order of magnitude in one case by over six order of magnitude empirical analysis of the running time of our algorithm strongly suggest that their asymptotic growth is le than that of previous algorithm the key insight behind both our new algorithm is that if an optimal k way partition includes a particular subset then optimally partitioning the number not in that set k way result in an optimal k way partition 
manifold alignment ha been found to be useful in many area of machine learning and data mining in this paper we introduce a novel manifold alignment approach which differs from semisupervised alignment and procrustes alignment in that it doe not require predetermining correspondence our approach learns a projection that map data instance from two different space to a lower dimensional space simultaneously matching the local geometry and preserving the neighborhood relationship within each set this approach also build connection between space dened by different feature and make direct knowledge transfer possible the performance of our algorithm is demonstrated and validated in a series of carefully designed experiment in information retrieval and bioinformatics 
open form of global constraint allow the addition of new variable to an argument during the execution of a constraint program such form are needed for difficult constraint programming problem where problem construction and problem solving are interleaved however in general filtering that is sound for a global constraint can be unsound when the constraint is open this paper provides a simple characterization called contractibility of the constraint where filtering remains sound when the constraint is open with this characterization we can easily determine whether a constraint is contractible or not in the latter case we can use it to derive the strongest contractible approximation to the constraint we demonstrate how specific algorithm for some closed contractible constraint are easily adapted to open constraint 
many problem have a huge state space and no good heuristic to order move so a to guide the search toward the best position random game can be used to score position and evaluate their interest random game can also be improved using random game to choose a move to try at each step of a game nested monte carlo search address the problem of guiding the search toward better state when there is no available heuristic it us nested level of random game in order to guide the search the algorithm is studied theoretically on simple abstract problem and applied successfully to three different game morpion solitaire samegame and x sudoku 
in many application non metric distance are better than metric distance in reflecting the perceptual distance of human being previous study on non metric distance mainly focused on supervised setting and did not consider the usefulness of unlabeled data in this paper we present probably the first study of label propagation on graph induced from non metric distance the challenge here lie in the fact that the triangular inequality doe not hold for non metric distance and therefore a direct application of existing label propagation method will lead to inconsistency and conflict we show that by applying spectrum transformation non metric distance can be converted into metric one and thus label propagation can be executed such method however suffer from the change of original semantic relation a a main result of this paper we prove that any non metric distance matrix can be decomposed into two metric distance matrix containing different information of the data based on this recognition our proposed nmlp method derives two graph from the original non metric distance and performs a joint label propagation on the joint graph experiment validate the effectiveness of the proposed nmlp method 
in this paper we investigate nonmonotonic mode of inference our approach us modal conditional logic to establish a uniform framework in which to study nonmonotonic consequence we consider a particular mode of inference which employ a majority based account of default reasoning one which differs from the more familiar preferential account and show how modal logic supply a framework which facilitates analysis of and comparison with more traditional formulation of nonmonotonic consequence 
loop calculus introduced by chertkov and chernyak is a new technique to incrementally improve approximation computed by loopy belief propagation lbp with the ability to eventually make them exact in this extended abstract we give a brief overview of this technique and show it relevance to the ai community we consider the problem of boolean satisfiability sat and use lbp with loop calculus correction to perform probabilistic inference about the problem in this preliminary work we focus on identifying the main issue encountered when applying loop calculus and include initial empirical result in the sat domain 
child are facile at both discovering word boundary and using those word to build higher level structure in tandem current research treat lexical acquisition and grammar induction a two distinct task doing so ha led to unreasonable assumption state of the art unsupervised result presuppose a perfectly segmented noise free lexicon while largely ignoring how the lexicon is used this paper combine both task in a novel framework for bootstrapping lexical acquisition and grammar induction 
heuristic function for single agent search application estimate the cost of the optimal solution when multiple heuristic exist taking their maximum is an effective way to combine them a new technique is introduced for combining multiple heuristic value inspired by the evaluation function used in two player game the different heuristic in a single agent application are treated a feature of the problem domain an ann is used to combine these feature into a single heuristic value this idea ha been implemented for the sliding tile puzzle and the peg tower of hanoi two classic single agent search domain experimental result show that this technique can lead to a large reduction in the search effort at a small cost in the quality of the solution obtained 
graphical game provide compact representation of a multiagent interaction when agent payoff depend only on action of agent in their local neighborhood we formally describe the problem of learning a graphical game model from limited observation of the payoff function define three performance metric for evaluating learned game and investigate several learning algorithm based on minimizing empirical loss our first algorithm is a branch and bound search which take advantage of the structure of the empirical loss function to derive upper and lower bound on loss at every node of the search tree we also examine a greedy heuristic and local search algorithm our experiment with directed graphical game show that i when only a small sample of profile payoff is available branch and bound significantly outperforms other method and ha competitive running time but ii when many profile are observed greedy is nearly optimal and considerably better than other method at a fraction of branch andbound s running time the result are comparable for undirected graphical game and when payoff are sampled with noise 
boltzmann machine are a powerful class of undirected graphical model originally proposed a artificial neural network they can be regarded a a type of markov random field in which the connection weight between node are symmetric and learned from data they are also closely related to recent model such a markov logic network and conditional random field a major challenge for boltzmann machine a well a other graphical model is speeding up learning for large scale problem the heart of the problem lie in efficiently and effectively approximating the partition function in this paper we propose a new efficient learning algorithm for boltzmann machine that allows them to be applied to problem with large number of random variable we introduce a new large margin variational approximation to the partition function that allows boltzmann machine to be trained using a support vector machine svm style learning algorithm for discriminative learning task these large margin boltzmann machine provide an alternative approach to structural svms we show that these machine have low sample complexity and derive a generalization bound our result demonstrate that on multilabel classification problem large margin boltzmann machine achieve order of magnitude faster performance than structural svms and also outperform structural svms on problem with large number of label 
autonomous agent that sense reason and act in real world environment for extended period often need to solve stream of incoming problem traditionally effort is applied only to problem that have already arrived and have been noted we examine continual computation method that allow agent to ideally allocate time to solving current a well a potential future problem under uncertainty we first review prior work on continual computation then we present new direction and result including the consideration of shared subtasks and multiple task we present result on the computational complexity of the continual computation problem and provide approximation for arbitrary model of computational performance finally we review special formulation for addressing uncertainty about the best algorithm to apply learning about performance and considering cost associated with delayed use of result 
recently graph based dimensionality reduction ha received a lot of interest in many field of information processing central to it is a graph structure which model the geometrical and discriminant structure of the data manifold when label information is available it is usually incorporated into the graph structure by modifying the weight between data point in this paper we propose a novel dimensionality reduction algorithm called constrained graph embedding which considers the label information a additional constraint specifically we constrain the space of the solution that we explore only to contain embedding result that are consistent with the label experimental result on two real life data set illustrate the effectiveness of our proposed method 
markov logic network mlns combine first order logic and markov network allowing u to handle the complexity and uncertainty of real world problem in a single consistent framework however in mlns all variable and feature are discrete while most real world application also contain continuous one in this paper we introduce hybrid mlns in which continuous property e g the distance between two object and function over them can appear a feature hybrid mlns have all distribution in the exponential family a special case e g multivariate gaussians and allow much more compact modeling of non i i d data than propositional representation like hybrid bayesian network we also introduce inference algorithm for hybrid mlns by extending the maxwalksat and mc sat algorithm to continuous domain experiment in a mobile robot mapping domain involving joint classification clustering and regression illustrate the power of hybrid mlns a a modeling language and the accuracy and efficiency of the inference algorithm 
in this paper we address the problem of generating preferred plan by combining the procedural control knowledge specified by hierarchical task network htns with rich user preference to this end we extend the popular planning domain definition language pddl to support specification of simple and temporally extended preference over htn construct to compute preferred htn plan we propose a branch and bound algorithm together with a set of heuristic that leveraging htn structure measure progress towards satisfaction of preference our preference based planner htnplan p is implemented a an extension of the shop planner we compared our planner with sgplan and hplan pthe top performer in the international planning competition preference track htnplan p generated plan that in all but a few case equalled or exceeded the quality of plan returned by hplan p and sgplan while our implementation build on shop the language and technique proposed here are relevant to a broad range of htn planner 
ranking play a central role in many web search and information retrieval application ensemble ranking sometimes called meta search aim to improve the retrieval performance by combining the output from multiple ranking algorithm many ensemble ranking approach employ supervised learning technique to learn appropriate weight for combining multiple ranker the main shortcoming with these approach is that the learned weight for ranking algorithm are query independent this is suboptimal since a ranking algorithm could perform well for certain query but poorly for others in this paper we propose a novel semi supervised ensemble ranking sser algorithm that learns query dependent weight when combining multiple ranker in document retrieval the proposed sseralgorithm isformulated asan svm likequadratic program qp and therefore can be solved efficiently by taking advantage of optimization technique that were widely used in existing svm solver we evaluated the proposed technique on a standard document retrieval testbed and observed encouraging result by comparing to a number of state of the art technique 
in this paper we propose a machine learning based nlp system for automatically creating animated storyboards using the action description of movie script we focus particularly on the importance of verb semantics when generating graphic command and find that semantic role labelling boost performance and is relatively robust to the effect of unseen verb 
we consider the problem of testing whether two variable should be adjacent either due to a direct effect between them or due to a hidden common cause given an observational distribution and a set of causal assumption encoded a a causal diagram in other word given a set of edge in the diagram known to be true we are interested in testing whether another edge ought to be in the diagram in fully observable faithful model this problem can be easily solved with conditional independence test latent variable make the problem significantly harder since they can imply certain non adjacent variable pair namely those connected by so called inducing path are not independent conditioned on any set of variable we characterize which variable pair can be determined to be non adjacent by a class of constraint due to dormant independence that is conditional independence in identifiable interventional distribution furthermore we show that particular operation on joint distribution which we call truncation are sufficient for exhibiting these non adjacency this suggests a causal discovery procedure taking advantage of these constraint in the latent variable case can restrict itself to truncation 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
real time heuristic search algorithm are used for planning by agent in situation where a constant bounded amount of deliberation time is required for each action regardless of the problem size such algorithm interleave their planning and execution to ensure real time response furthermore to guarantee completeness they typically store improved heuristic estimate for previously expanded state although subsequent planning step can benefit from updated heuristic estimate many of the same state are expanded over and over again here we propose a variant of the a algorithm time bounded a tba that guarantee real time response in the domain of path finding on videogame map tba expands an order of magnitude fewer state than traditional real time search algorithm while finding path of comparable quality it reach the same level of performance a recent state of the art real time search algorithm but unlike these requires neither state space abstraction nor pre computed pattern database 
the swarm application framework saf is a tool that make the development of swarm application more intuitive traditionally swarm application are created by programming several low level rule this approach lead to several problem in designing and testing swarm which serve a inspiration for the feature of saf saf encourages a new paradigm for designing swarm application engineer can interact with a swarm at the abstract swarm level instead of the individual agent level in this paper we discus the design of the framework how agent and rule in saf operate and a planned rule abstraction feature 
incorporating new information into a knowledge base is an important problem which ha been widely investigated in this paper we study this problem in a formal framework for reasoning about action and change in this framework action domain are described in an action language whose semantics is based on the notion of causality unlike the formalism considered in the related work this language allows straightforward representation of non deterministic effect and indirect effect of possibly concurrent action a well a state constraint therefore the update can be more general than elementary statement the expressivity of this formalism allows u to study the update of an action domain description with a more general approach compared to related work first of all we consider the update of an action description with respect to further criterion for instance by ensuring that the updated description entail some observation assertion or general domain property that constitute further constraint that are not expressible in an action description in general moreover our framework allows u to discriminate amongst alternative update of action domain description and to single out a most preferable one based on a given preference relation possibly dependent on the specified criterion we study semantic and computational aspect of the update problem and establish basic property of update a well a a decomposition theorem that give rise to a divide and conquer approach to updating action description under certain condition furthermore we study the computational complexity of decision problem around computing solution both for the generic setting and for two particular preference relation viz set inclusion and weight based preference while deciding the existence of solution and recognizing solution are pspace complete problem in general the problem fall back into the polynomial hierarchy under restriction on the additional constraint we finally discus method to compute solution and approximate solution which disregard preference our result provide a semantic and computational basis for developing system that incorporate new information into action domain description in an action language in the presence of additional constraint 
incremental search algorithm reuse information from previous search to speed up the current search and are thus often able to find shortest path for series of similar search problem faster than by solving each search problem independently from scratch however they do poorly on moving target search problem where both the start and goal cell change over time in this paper we thus develop fringe retrieving a fra an incremental version of a that repeatedly find shortest path for moving target search in known gridworlds we demonstrate experimentally that it run up to one order of magnitude faster than a variety of state of the art incremental search algorithm applied to moving target search in known gridworlds 
in many application the data such a web page and research paper contain relation link structure among entity in addition to textual content information matrix factorization mf method such a latent semantic indexing lsi have been successfully used to map either content information or relation information into a lower dimensional latent space for subsequent processing however how to simultaneously model both the relation information and the content information effectively with an mf framework is still an open research problem in this paper we propose a novel mf method called relation regularized matrix factorization rrmf for relational data analysis by using relation information to regularize the content mf procedure rrmf seamlessly integrates both the relation information and the content information into a principled framework we propose a linear time learning algorithm with convergence guarantee to learn the parameter of rrmf extensive experiment on real data set show that rrmf can achieve state of the art performance 
recent study ha shown that canonical algorithm such a principal component analysis pca and linear discriminant analysis lda can be obtained from graph based dimensionality reduction framework however these algorithm yield projective map which are linear combination of all the original feature the result are difficult to be interpreted psychologically and physiologically this paper present a novel technique for learning a sparse projection over graph the data in the reduced subspace is represented a a linear combination of a subset of the most relevant feature comparing to pca and lda the result obtained by sparse projection are often easier to be interpreted our algorithm is based on a graph embedding model which encodes the discriminating and geometrical structure in term of the data affinity once the embedding result are obtained we then apply regularized regression for learning a set of sparse basis function specifically by using a l norm regularizer e g lasso the sparse projection can be efficiently computed experimental result on two document database demonstrate the effectiveness of our method 
we introduce novel algorithm for generating random solution from a uniform distribution over the solution of a boolean satisfiability problem our algorithm operate in two phase in the first phase we use a recently introduced samplesearch scheme to generate biased sample while in the second phase we correct the bias by using either sampling importance resampling or the metropolishastings method unlike state of the art algorithm our algorithm guarantee convergence in the limit our empirical result demonstrate the superior performance of our new algorithm over several competing scheme 
at aaai zinkevich bowling and burch introduced the range of skill measure of a two player game and used it a a parameter in the analysis of the running time of an algorithm for finding approximate solution to such game they suggested that the range of skill of a typical natural game is a small number but only gave heuristic argument for this in this paper we provide the first method for rigorously estimating the range of skill of a given game we provide some general asymptotic bound that imply that the range of skill of a perfectly balanced game tree is almost exponential in it size and doubly exponential in it depth we also provide technique that yield concrete bound for unbalanced game tree and apply these to estimate the range of skill of tic tac toe and head up limit texas hold em poker in particular we show that the range of skill of tic tac toe is more than 
we examine online learning in the context of the wisconsin card sorting task wcst a task for which the concept acquisition strategy for human and other primate are well documented we describe a new wcst experiment in rhesus monkey comparing the monkey behavior to that of online learning algorithm our expectation is that insight gained from this work and future research can lead to improved artificial learning system 
in this work we aim to narrow the gap between plan recognition and planning by exploiting the power and generality of recent planning algorithm for recognizing the set g of goal g that explain a sequence of observation given a domain theory after providing a crisp definition of this set we show by mean of a suitable problem transformation that a goalgbelongs tog if there is an action sequence that is an optimal plan for both the goal g and the goal g extended with extra goal representing the observation exploiting this result we show how the set g can be computed exactly and approximately by minor modification of existing optimal and suboptimal planning algorithm and existing polynomial heuristic experiment over several domain show that the suboptimal planning algorithm and the polynomial heuristic provide good approximation of the optimal goal set g while scaling up a well a state of the art planning algorithm and heuristic 
we study the computational complexity of the counting version of the possible winner problem for election in the possible winner problem we are given a profile of voter each with a partial preference order and ask if there are lin ear extension of the vote such that a designated candidate win we also analyze a special case of possible winner the manipulation problem we provide polynomial time algorithm for counting manipulation in a class of scoring protocol and in several other voting rule we show phardness of the counting variant of p ossible winner for plurality and veto and give a simple yet general and practically useful randomized algorithm for a variant of p ossiblewinner for all voting rule for which a winner can be computed in polynomial time 
in this poster we describe the tabling technique for sequential and concurrent horn transaction logic horn transaction logic is an extension of classical logic programming with state update and it ha a sld style evaluation algorithm this sld style algorithm enters into infinite loop when computing answer to many recursive program when they change the underlying state of the knowledge base we solve this problem by tabling caching the call call state and answer unification and return state in a searchable structure for the sequential transaction logic or building a graph for the query and memoize the hot vertex vertex currently possible to execute for the propositional concurrent transaction logic so that the same call is not re executed ad infinum with these technique we can efficiently compute query to transaction logic program and when the underlying program have the bounded term depth property transaction datalog the technique are guaranteed to terminate the application of these technique promise termination and great improvement in the us of transaction logic state changing system artificial intelligence planning dynamic constraint on transaction execution workflow modeling and verification and system involving financial transaction 
checking csp consistency is shown in theory to be an np complete problem there is two family of method for csp consistency checking the first family hold the complete method which make an exhaustive search on the solution space these method have the advantage to prove csp inconsistency but their complexity grows exponentially when the problem size increase the second family includes the incomplete method that make a local search on the solution space these method have been efficiently used to find solution for large size consistent csps that complete method can not solve one major drawback of the incomplete method is their inability to prove csp inconsistency one of the challenge that have been put forward by the cp community selman et al is to provide incomplete method that can deal with csp inconsistency efficiently the work that we present here is a contribution towards an answer to this hard challenge we introduce a new incomplete method for csp inconsistency checking that is based on both a new notion of dominance between csps and a coloration of the csp micro structure we experimented the method on randomly generated csp instance and the result obtained are very promising 
we present the first planner capable of reasoning with both the full semantics of pddl level temporal planning and with numeric resource our planner crikey employ heuristic forward search using the start and end semantics of pddl to manage temporal action the planning phase is interleaved with a scheduling phase using a simple temporal network in order to ensure that temporal constraint are met to guide search we introduce a new temporal variant of the relaxed planning graph heuristic that is capable of reasoning with the feature of this class of domain along with the timed initial literal of pddl crikey extends the state of the art in handling the full temporal expressive power of pddl including numeric temporal domain 
the universum sample which is defined a the sample that doesn t belong to any of the class the learning task concern ha been proved to be helpful in both supervised and semi supervised setting the former work treat the universum sample equally our research found that not all the universum sample are helpful and we propose a method to pick the informative one i e in between universum sample we also set up a new semi supervised framework to incorporate the in between universum sample empirical experiment show that our method outperforms the former one 
model based diagnosis mbd approach often yield a large number of diagnosis severely limiting their practical utility this paper present a novel active testing approach based on mbd technique called fractal framework for active testing algorithm which given a system description computes a sequence of control setting for reducing the number of diagnosis the approach complement probing sequential diagnosis and atpg and applies to system where additional test are restricted to setting a subset of the existing system input while observing the existing output this paper evaluates the optimality of fractal both theoretically and empirically fractal generates test vector using a greedy next best strategy and a low cost approximation of diagnostic information entropy further the approximate sequence computed by fractal s greedy approach is optimal over all poly time approximation algorithm a fact which we confirm empirically extensive experimentation with iscas combinational circuit show that fractal reduces the number of remaining diagnosis according to a steep geometric decay function even when only a fraction of input are available for active testing 
lifted inference algorithm exploit repeated structure in probabilistic model to answer query efficiently previous work such a de salvo braz et al s first order variable elimination fove ha focused on the sharing of potential across interchangeable random variable in this paper we also exploit interchangeability within individual potential by introducing counting formula which indicate how many of the random variable in a set have each possible value we present a new lifted inference algorithm c fove that not only handle counting formula in it input but also creates counting formula for use in intermediate potential c fove can be described succinctly in term of six operator along with heuristic for when to apply them because counting formula capture dependency among large number of variable compactly c fove achieves asymptotic speed improvement compared to fove 
the rectangle packing problem consists of finding an enclosing rectangle of smallest area that can contain a given set of rectangle without overlap our algorithm pick the x coordinate of all the rectangle before picking any of the y coordinate for the x coordinate we present a dynamic variable ordering heuristic and an adaptation of a pruning algorithm used in previous solver we then transform the rectangle packing problem into a perfect packing problem that ha no empty space and present inference rule to reduce the instance size for the y coordinate we search a space that model empty position a variable and rectangle a value our solver is over time faster than the previous state of the art on the largest problem solved to date allowing u to extend the known solution for a consecutive square packing benchmark from n to n 
consistency property and algorithm for achieving them are at the heart of the success of constraint programming in this paper we study the relational consistency property r m c which is equivalent to m wise consistency proposed in relational database we also define wr m c a weaker variant of this property we propose an algorithm for enforcing these property on a constraint satisfaction problem by tightening the existing relation and without introducing new one we empirically show that wr m c solves in a backtrackfree manner all the instance of some csp benchmark class thus hinting at the tractability of those class 
many problem in information extraction text mining natural language processing and other field exhibit the same property multiple prediction task are related in the sense that their output label satisfy certain constraint in this paper we propose an active learning framework exploiting such relation among task intuitively with task output coupled by constraint active learning can utilize not only the uncertainty of the prediction in a single task but also the inconsistency of prediction across task we formalize this idea a a crosstask value of information criterion in which the reward of a labeling assignment is propagated and measured over all relevant task reachable through constraint a specific example of our framework lead to the cross entropy measure on the prediction of coupled task which generalizes the entropy in the classical singletask uncertain sampling we conduct experiment on two real world problem web information extraction and document classification empirical result demonstrate the effectiveness of our framework in actively collecting labeled example for multiple related task 
an important feature of many problem domain in machine learning is their geometry for example adjacency relationship symmetry and cartesian coordinate are essential to any complete description of board game visual recognition or vehicle control yet many approach to learning ignore such information in their representation instead inputting flat parameter vector with no indication of how those parameter are situated geometrically this paper argues that such geometric information is critical to the ability of any machine learning approach to effectively generalize even a small shift in the configuration of the task in space from what wa experienced in training can go wholly unrecognized unless the algorithm is able to learn the regularity in decision making across the problem geometry to demonstrate the importance of learning from geometry three variant of the same evolutionary learning algorithm neuroevolution of augmenting topology whose representation vary in their capacity to encode geometry are compared in checker the result is that the variant that can learn geometric regularity produce a significantly more general solution the conclusion is that it is important to enable machine learning to detect and thereby learn from the geometry of it problem 
transfer is the ability to employ knowledge acquired in one task to improve performance in another we study transfer in the context of the icarus cognitive architecture which supply diverse capability for execution inference planning and learning we report on an extension to icarus called representation mapping that transfer structured skill and concept between disparate task that may not even be expressed with the same symbol set we show that representation mapping is naturally integrated into icarus cognitive processing loop resulting in a system that address a qualitatively new class of problem by considering the relevance of past experience to current goal 
in this paper we consider a high fidelity convoy movement problem motivated by the coordination and routing of convoy within a road transportation network in an urban city it encompasses two classical combinatorial optimization problem vehicle routing and resource constrained scheduling we present an effective hybrid algorithm to dynamically manage the movement of convoy where we combine the standard dijkstra s shortest path algorithm with constraint programming technique the effectiveness of the algorithm is illustrated with testing on varying problem size and complexity 
we consider the problem of automatic vocal melody transcription translating an audio recording of a sung melody into a musical score while previous work ha focused on finding the closest note to the singer s tracked pitch we instead seek to recover the melody the singer intended to sing often the melody a singer intended to sing differs from what they actually sang our hypothesis is that this occurs in a singer specific way for example a given singer may often be flat in certain part of her range or another may have difficulty with certain interval we thus pursue method for singer specific training which use learning to combine different method for pitch prediction in our experiment with human subject we show that via a short training procedure we can learn a singer specific pitch predictor and significantly improve transcription of intended pitch over other method for an average user our method give a to percent reduction in pitch classification error with respect to a baseline method which is comparable to commercial voice transcription tool for some user we achieve even more dramatic reduction our best result come from a combination of singer specific learning with non singer specific feature selection we also discus the implication of our work for training more general control signal we make our experimental data available to allow others to replicate or extend our result 
we present a novel application of structured classification identifying function entry point feps the starting byte of each function in program binary such identification is the crucial first step in analyzing many malicious commercial and legacy software which lack full symbol information that specifies feps existing pattern matching fep detection technique are insufficient due to variable instruction sequence introduced by compiler and link time optimization we formulate the fep identification problem a structured classification using conditional random field our conditional random field incorporate both idiom feature to represent the sequence of instruction surrounding feps and control flow structure feature to represent the interaction among feps these feature allow u to jointly label all feps in the binary we perform feature selection and present an approximate inference method for massive program binary we evaluate our model on a large set of real world test binary showing that our model dramatically outperform two existing standard disassemblers 
matrix factorization is a fundamental technique in machine learning that is applicable to collaborative filtering information retrieval and many other area in collaborative filtering and many other task the objective is to fill in missing element of a sparse data matrix one of the biggest challenge in this case is filling in a column or row of the matrix with very few observation in this paper we introduce a bayesian matrix factorization model that performs regression against side information known about the data in addition to the observation the side information help by adding observed entry to the factored matrix we also introduce a nonparametric mixture model for the prior of the row and column of the factored matrix that give a different regularization for each latent class besides providing a richer prior the posterior distribution of mixture assignment reveals the latent class using gibbs sampling for inference we apply our model to the netflix prize problem of predicting movie rating given an incomplete user movie rating matrix incorporating rating information with gathered metadata information our bayesian approach outperforms other matrix factorization technique even when using fewer dimension 
for classification with multiple label a common approach is to learn a classifier for each label with a kernel based classifier there are two option to set up kernel select a specific kernel for each label or the same kernel for all label in this work we present a unified framework for multi label multiple kernel learning in which the above two approach can be considered a two extreme case moreover our framework allows the kernel shared partially among multiple label enabling flexible degree of label commonality we systematically study how the sharing of kernel among multiple label affect the performance based on extensive experiment on various benchmark data including image and microarray data interesting finding concerning efficacy and efficiency are reported 
this paper investigates a generative history based parsing model that synchronises the derivation of non planar graph representing semantic dependency with the derivation of dependency tree representing syntactic structure to process non planarity online the semantic transition based parser us a new technique to dynamically reorder node during the derivation while the synchronised derivation allow different structure to be built for the semantic non planar graph and syntactic dependency tree useful statistical dependency between these structure are modeled using latent variable the resulting synchronous parser achieves competitive performance on the conll shared task achieving relative error reduction of in semantic f score over previously proposed synchronous model that cannot process non planarity online 
we have developed a simulation model that accepts instruction in unconstrained natural language and then guide a robot to the correct destination the instruction are segmented on the basis of the action to be taken and each segment is labeled with the required action this flat formulation reduces the problem to a sequential labeling task to which machine learning method are applied we propose an innovative machine learning method for explicitly modeling the action described in instruction and integrating learning and inference about the physical environment we obtained a corpus of route instruction that experimenter verified a follow able given by people in building navigation situation using the four fold cross validation our experiment showed that the simulated robot reached the correct destination of the time 
traditional information retrieval system use query word to identify relevant document in difficult retrieval task however one need access to a wealth of background knowledge we present a method that us wikipedia based feature generation to improve retrieval performance intuitively we expect that using extensive world knowledge is likely to improve recall but may adversely affect precision high quality feature selection is necessary to maintain high precision but here we do not have the labeled training data for evaluating feature that we have in supervised learning we present a new feature selection method that is inspired by pseudorelevance feedback we use the top ranked and bottom ranked document retrieved by the bag of word method a representative set of relevant and non relevant document the generated feature are then evaluated and filtered on the basis of these set experiment on trec data confirm the superior performance of our method compared to the previous state of the art 
to truly understand language an intelligent system must be able to connect word phrase and sentence to it perception of object and event in the world current natural language processing and computer vision system make extensive use of machine learning to acquire the probabilistic knowledge needed to comprehend linguistic and visual input however to date there ha been relatively little work on learning the relationship between the two modality in this talk i will review some of the existing work on learning to connect language and perception discus important direction for future research in this area and argue that the time is now ripe to make a concerted effort to address this important integrative ai problem 
building architecture for autonomous rational behavior requires the integration of several ai component such a planning learning and execution monitoring in most case the technique used for planning and learning are tailored to the specific integrated architecture so they could not be replaced by other equivalent technique also in order to solve task that require lookahead reasoning under uncertainty these architecture need an accurate domain model to feed the planning component but the manual definition of these model is a difficult task in this paper we propose an architecture that us off the shelf interchangeable planning and learning component to solve task that require flexible planning under uncertainty we show how a relational learning component can be applied to automatically obtain accurate probabilistic action model from execution of plan these model can be used by any classical planner that handle metric function or alternatively by any decision theoretic planner we also show how these component can be integrated to solve task continuously under an online relational learning scheme 
many local search algorithm are based on searching in the k exchange neighborhood this is the set of solution that can be obtained from the current solution by exchanging at most k element a a rule of thumb the larger k is the better are the chance of finding an improved solution however for input of size n a na ve brute force search of the k exchange neighborhood requires no k time which is not practical even for very small value of k we show that for several class of sparse graph like planar graph graph of bounded vertex degree and graph excluding some fixed graph a a minor an improved solution in the k exchange neighborhood for many problem can be found much more efficiently our algorithm run in time o k nc where is a function depending on k only and c is a constant independent of k we demonstrate the applicability of this approach on different problem like r center vertex cover odd cycle transversal max cut and min bisection in particular on planar graph all our algorithm searching for a klocal improvement run in time o o k n which is polynomial for k o log n we also complement the algorithm with complexity result indicating that brute force search is unavoidable in more general class of sparse graph 
this paper describes our experience in using modern web architecture lightweight python framework and rapid prototyping to create an ai rostering and workforce management system to help prepare for the beijing olympic equestrian event which 
recent progress on external memory mdp solver in particular pemvi dai et al ha enabled optimal solution to large probabilistic planning problem however pemvi requires a human to manually partition the mdp before the planning algorithm can be applied putting an added burden on the domain designer and detracting from the vision of automated planning this paper present a novel partitioning scheme which automatically subdivides the state space into block that respect the memory constraint our algorithm first applies static domain analysis to identify candidate for partitioning and then us heuristic search to generate a good partition we evaluate the usefulness of our method in the context of pemvi across many benchmark domain showing that it can successfully solve extremely large problem in each domain we also compare the performance of automatic partitioning with previously reported result using human designed partition experiment show that our algorithm generates significantly superior partition which speed mdp solving and also yield vast memory saving 
in this paper we present coach a cumulative online algorithm for classification of handwriting deficiency a description of our algorithm along with a performance evaluation of coach on real data is provided coach is an innovative algorithm designed for building an online handwriting evaluation tool to be used for classifying and remediating handwriting deficiency coach adapts learning and data mining technique from ai to handwriting deficiency classification in an innovative fashion until now handwriting classification ha been performed manually by trained therapist causing expensive and subjective evaluation this application lower the cost of evaluation increase objectiveness and enables repeated testing that can accompany therapy coach is evaluated on real data obtained from child with poor handwriting using a digitizer tablet result show that coach manages to successfully differentiate between poor to proficient handwriting differentiation is obtained even after using data from only a few word these result prove that coach is a promising emerging application for online evaluation 
the aim of this work is to propose a logical framework for the specification of cognitive emotion that are based on counterfactual reasoning about agent choice an example of this kind of emotion is regret in order to meet this objective we exploit the well known stit logic belnap et al horty stit logic ha been proposed in the domain of formal philosophy in the ninety and more recently it ha been imported into the field of theoretical computer science where it formal relationship with other logic for multi agent system such a atl and coalition logic cl have been studied stit is a very suitable formalism to reason about choice and capability of agent and group of agent unfortunately the version of stit with agent and group ha been recently proved to be undecidable in this work we study a decidable fragment of stit with agent and group which is sufficiently expressive for our purpose of formalizing counterfactual emotion 
qualitative spatial and temporal reasoning qsr is concerned with constraint based formalism for representing and reasoning with spatial and temporal information over infinite domain within the qsr community it ha been a widely accepted assumption that genuine qualitative reasoning method outperform other reasoning method that are applicable to encoding of qualitative csp instance recently this assumption ha been tackled by several author who proposed to encode qualitative csp instance a finite csp or sat instance in this paper we report on the result of a broad empirical study in which we compared the performance of several reasoner on instance from different qualitative formalism our result show that for small sized qualitative calculus e g allen s interval algebra and rcc a state of theart implementation of qsr method currently give the most efficient performance however on recently suggested large size calculus e g opra finite csp encoding provide a considerable performance gain these result confirm a conjecture by bessi re stating that support based constraint propagation algorithm provide better performance for large sized qualitative calculus 
a significant challenge in distributed work system is to address the moral hazard problem wherein user may seek to free ride on the work contributed by others we formalize the problem of designing incentive compatible accounting mechanism that measure the net contribution of user despite relying on voluntary report of work performed and work received a new mechanism is introduced that remove any incentive for a user to manipulate via misreports about either work contributed or work consumed the mechanism is demonstrated in simulation to provide good system efficiency compared to an existing manipulable mechanism in closing we illustrate that sybil attack are however powerful in accounting mechanism which leaf an open research challenge 
temporal logic are widely used in specifying goal of agent we noticed that when directing agent human often revise their requirement for the agent especially a they gather more knowledge about the domain however all existing temporal logic except one do not focus on the revision of goal in an elaboration tolerant manner thus formal temporal logic that can allow elaboration tolerant revision of goal are needed a non monotonic language are often used for elaboration tolerant specification we propose to explore non monotonic temporal logic for goal specification recently a non monotonic temporal logic n ltl wa proposed with similar aim in n ltl goal specification could be changed via strong and weak exception however in nltl one had to a priori declare whether exception will be weak or strong exception we propose a new nonmonotonic temporal logic that not only overcomes this but is also able to express exception to exception strengthen and weaken precondition and revise and replace consequents all in an elaboration tolerant manner 
decomposition is an effective technique for solving discrete constraint optimization problem cop with low tree width on problem with high tree width however existing decomposition algorithm offer little advantage over branch and bound search b b in this paper we propose a method for exploiting decomposition on problem with high treewidth our technique involves modifying b b to detect and exploit decomposition on a selected subset of the problem s objective decomposition over this subset generated during search are exploited to compute tighter bound allowing b b to prune more of it search space we present a heuristic for selecting an appropriate subset of objective one that readily decomposes during search and yet can still provide good bound we demonstrate empirically that our approach can significantly improve b b s performance and outperform standard decomposition algorithm on a variety of high tree width problem 
recent work on alternating time temporal logic and coalition logic ha allowed the expression of many interesting property of coalition and strategy however there is no natural way of expressing resource requirement in these logic this paper present a resource bounded coalition logic rbcl which ha explicit representation of resource bound in the language and give a complete and sound axiomatisation of rbcl 
in this paper we propose a general formulation for kernel nonnegative matrix factorization with flexible kernel specifically we propose the gaussian nonnegative matrix factorization gnmf algorithm by using the gaussian kernel in the framework different from a recently developed polynomial nmf pnmf gnmf find basis vector in the kernel induced feature space and the computational cost is independent of input dimension furthermore we prove the convergence and nonnegativity of decomposition of our method extensive experiment compared with pnmf and other nmf algorithm on several face database validate the effectiveness of the proposed method 
this paper present an argumentation based interpreter for golog program traditional golog interpreter are not designed to find the most preferred execution of a program from the perspective of an agent existing technique developed to discover these execution are limited in term of how the preference of an agent can be expressed and the variety of preference type that can be used to guide search for a solution the presented work combine the use of argumentation to compare execution relative to a set of general comparison principle and the theory behind best first search to reduce the cost of the search process to the best of our knowledge this is the first work to integrate argumentation and the interpretation of golog program and to use argumentation a a tool for best first search 
inverse reinforcement learning irl is the problem of recovering the underlying reward function from the behaviour of an expert most of the existing algorithm for irl assume that the expert s environment is modeled a a markov decision process mdp although they should be able to handle partially observable setting in order to widen the applicability to more realistic scenario in this paper we present an extension of the classical irl algorithm by ng and russell to partially observable environment we discus technical issue and challenge and present the experimental result on some of the benchmark partially observable domain 
object recognition using image set or video sequence a input tends to be more robust since image set or video sequence provides much more information than single snap shot about the variability in the appearance of the target subject constrained mutual subspace method cmsm is one of the state of the art algorithm for imageset based object recognition by first projecting the image set pattern onto the so called generalized difference subspace then classifying based on the principal angle based mutual subspace distance by treating the subspace base for each image set pattern a basic element in the grassmann manifold this paper present a framework for robust image set based recognition by cmsm based ensemble learning in a boosting way the proposed boosting constrained mutual subspace method bcmsm improves the original cmsm in the following way a the proposed bcmsm algorithm is insensitive to the dimension of the generalized differnce subspace while the performance of the original cmsm algorithm is quite dependent on the dimension and the selecting of optimum choice is quite empirical and case dependent b by taking advantage of both boosting and cmsm technique the generalization ability is improved and much higher classification performance can be achieved extensive experiment on real life data set two face recognition task and one d object category classification task show that the proposed method outperforms the previous state of the art algorithm greatly in term of classification accuracy 
we consider the problem of computing mutual information between many pair of variable in a bayesian network this task is relevant to a new class of generalized belief propagation gbp algorithm that characterizes iterative belief propagation ibp a a polytree approximation found by deleting edge in a bayesian network by computing in the simplified network the mutual information between variable across a deleted edge we can estimate the impact that recovering the edge might have on the approximation unfortunately it is computationally impractical to compute such score for network over many variable having large state space so that edge recovery can scale to such network we propose in this paper an approximation of mutual information which is based on a soft extension of d separation a graphical test of independence in bayesian network we focus primarily on polytree network which are sufficient for the application we consider although we discus potential extension of the approximation to general network a well empirically we show that our proposal is often a effective a mutual information for the task of edge recovery with order of magnitude saving in computation time in larger network our result lead to a concrete realization of gbp admitting improvement to ibp approximation with only a modest amount of computational effort 
we propose symbolic heuristic search value iteration symbolic hsvi algorithm which extends the heuristic search value iteration hsvi algorithm in order to handle factored partially observable markov decision process factored pomdps the idea is to use algebraic decision diagram add for compactly representing the problem itself and all the relevant intermediate computation result in the algorithm we leverage symbolic perseus for computing the lower bound of the optimal value function using add operator and provide a novel add based procedure for computing the upper bound experiment on a number of standard factored pomdp problem show that we can achieve an order of magnitude improvement in performance over previously proposed algorithm 
this paper investigates hindsight optimization a an approach for leveraging the significant advance in deterministic planning for action selection in probabilistic domain hindsight optimization is an online technique that evaluates the one step reachable state by sampling future outcome to generate multiple non stationary deterministic planning problem which can then be solved using search hindsight optimization ha been successfully used in a number of online scheduling application however it ha not yet been considered in the substantially different context of goal based probabilistic planning we describe an implementation of hindsight optimization for probabilistic planning based on deterministic forward heuristic search and evaluate it performance on planning competition benchmark and other probabilistically interesting problem the planner is able to outperform a number of probabilistic planner including ff replan on many problem finally we investigate condition under which hindsight optimization is guaranteed to be effective with respect to goal achievement and also illustrate example where the approach can go wrong 
answer set programming asp is widely recognised a a viable tool for declarative problem solving however there is currently a lack of tool for developing answer set program in particular providing tool for debugging answer set program ha recently been identified a a crucial prerequisite for a wider acceptance of asp in this paper we introduce a meta programming technique for debugging in asp the basic question we address is why interpretation expected to be answer set are not answer set of the program to debug we thus deal with finding semantical error of program the explanation provided by our method are based on an intuitive scheme of error that relies on a recent characterisation of the answer set semantics furthermore a we are using a meta programming technique debugging query are expressed in term of answer set program themselves which ha several benefit for one we can directly use asp solver for processing debugging query indeed our technique can easily be implemented and we devised a corresponding prototype debugging system also our approach respect the declarative nature of asp and the capability of the system can easily be extended to incorporate differing debugging feature 
for some well known game such a the traveler s dilemma or the centipede game traditional game theoretic solution concept most notably nash equilibrium predict outcome that are not consistent with empirical observation we introduce a new solution concept iterated regret minimization which exhibit the same qualitative behavior a that observed in experiment in many game of interest including traveler s dilemma the centipede game nash bargaining and bertrand competition a the name suggests iterated regret minimization involves the iterated deletion of strategy that do not minimize regret 
it is often desirable to extract structured information from raw web page for better information browsing query answering and pattern mining many such information extraction ie technology are costly and applying them at the web scale is impractical in this paper we propose a novel prioritization approach where candidate page from the corpus are ordered according to their expected contribution to the extraction result and those with higher estimated potential are extracted earlier system employing this approach can stop the extraction process at any time when the resource get scarce i e not all page in the corpus can be processed without worrying about wasting extraction effort on unimportant page more specifically we define a novel notion to measure the value of extraction result and design various mechanism for estimating a candidate page s contribution to this value we further design and build the extraction prioritization ep system with efficient scoring and scheduling algorithm and experimentally demonstrate that ep significantly outperforms the naive approach and is more flexible than the classifier approach 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
there ha been much recent attention to the problem of learning an appropriate distance metric using class label or other side information some proposed algorithm are iterative and computationally expensive in this paper we show how to solve one of these method with a closed form solution rather than using semidefinite programming we provide a new problem setup in which the algorithm performs better or a well a some standard method but without the computational complexity furthermore we show a strong relationship between these method and the fisher discriminant analysis 
we describe a method to improve detection of disease outbreak in pre diagnostic time series data the method us multiple forecaster and learns the linear combination to minimize the expected squared error of the next day s forecast this combination adaptively change over time this adaptive ensemble combination is used to generate a disease alert score for each day using a separate multiday combination method learned from example of different disease outbreak pattern these score are used to generate an alert for the epidemiologist practitioner several variant are also proposed and compared result from the international society for disease surveillance isds technical contest are given evaluating this method on three syndromic series with representative outbreak 
this paper considers the problem of an agent searching for a resource or a tangible good in a physical environment where at each stage of it search it observes one source where this good can be found the cost of acquiring the resource or good at a given source is uncertain a priori and the agent can observe it true value only when physically arriving at the source sample application involving this type of search include agent in exploration and patrol mission e g an agent seeking to find the best location to deploy sensing equipment along it path the uniqueness of these setting is that the expense of observing the source on each step of the process derives from the last source the agent explored we analyze three variant of the problem differing in their objective minimizing the total expected cost maximizing the success probability given an initial budget and minimizing the budget necessary to obtain a given success probability for each variant we first introduce and analyze the problem with a single agent either providing a polynomial solution to the problem or proving it is np complete we also introduce an innovative fully polynomial time approximation scheme algorithm for the minimum budget variant finally the result for the single agent case are generalized to multi agent setting 
this paper proposes a novel framework for image retrieval the retrieval is treated a searching for an ordered cycle in an image database the optimal cycle can be found by minimizing the geometric manifold entropy of image the minimization is solved by the proposed method fast active tabu search experimental result demonstrate the framework for image retrieval is feasible and quite promising 
in this paper we approach the problem of reasoning with quantified boolean formula qbfs by combining search and resolution and by switching between them according to structural property of qbfs we provide empirical evidence that qbfs which cannot be solved by search or resolution alone can be solved by combining them and that our approach make a proof of concept implementation competitive with current qbf solver 
we introduce a nonparametric approach to multiscale analysis of document corpus using a hierarchical matrix analysis framework called diffusion wavelet in contrast to eigenvector method diffusion wavelet construct multiscale basis function in this framework a hierarchy is automatically constructed by an iterative series of dilation and orthogonalization step beginning with an initial set of orthogonal basis function such a the unitvector base each set of basis function at a given level is constructed from the base at the lower level by dilation using the dyadic power of a diffusion operator a novel aspect of our work is that the diffusion analysis is conducted on the space of variable word instead of instance document this approach can automatically and efficiently determine the number of level of the topical hierarchy a well a the topic at each level multiscale analysis of document corpus is achieved by using the projection of the document onto the space spanned by basis function at different level further when the input term term matrix is a local diffusion operator the algorithm run in time approximately linear in the number of non zero element of the matrix the approach is illustrated on various data set including nip conference paper newsgroups and tdt data 
the semantic web language rdf wa designed to unambiguously define and use ontology to encode data and knowledge on the web many people find it difficult however to write complex rdf statement and query because doing so requires familiarity with the appropriate ontology and the term they define we describe a system that suggests appropriate rdf term given semantically related english word and general domain and context information we use the swoogle semantic web search engine to provide rdf term and namespace statistic the worldnet lexical ontology to find semantically related word and a na ve bayes classifier to suggest term a customized graph data structure of related namespaces is constructed from swoogle s database to speed up the classifier model learning and prediction time 
this paper present a new analysis of dynamic constraint satisfaction problem dcsps with finite domans and a new approach to solving them we first show that even very small change in a csp in the form of addition of constraint or change in constraint relation can have profound effect on search performance these effect are reflected in the amenability of the problem to different form of heuristic action a well a overall quality of search in addition classical dcsp method perform poorly on these problem because there are sometimes no solution similar to the original one found we then show that the same change do not markedly affect the location of the major source of contention in the problem a technique for iterated sampling that performs a careful assessment of this property and us the information during subsequent search performs well even when it only us information based on the original problem in the dcsp sequence the result is a new approach to solving dcsps that is based on a robust strategy for ordering variable rather than on robust solution 
we study the problem of learning an optimal subset from a larger ground set of item where the optimality criterion is defined by an unknown preference function we model the problem a a discriminative structural learning problem and solve it using a structural support vector machine ssvm that optimizes a set accuracy performance measure representing set similarity our approach departs from previous approach since we do not explicitly learn a pre defined preference function experimental result on both a synthetic problem domain and a real world face image subset selection problem show that our method significantly outperforms previous learning approach for such problem 
the problem of market clearing in an economy is that of finding price such that supply meet demand in this work we propose a kernel method to compute nonlinear clearing price for instance where linear price do not suffice we first present a procedure that given a sample of value and cost for a set of bundle implicitly computes nonlinear clearing price by solving an appropriately formulated quadratic program we then use this a a subroutine in an elicitation procedure that query demand and supply incrementally over round only a much a needed to reach clearing price an empirical evaluation demonstrates that with a proper choice of kernel function the method is able to find sparse nonlinear clearing price with much le than full revelation of value and cost when the kernel function is not suitable to clear the market the method can be tuned to achieve approximate clearing 
in many scenario quickly solving a relatively small search problem with an arbitrary start and arbitrary goal state is important e g gps navigation in order to speed this process we introduce a new class of memory based heuristic called true distance heuristic that store true distance between some pair of state in the original state space can be used for a heuristic between any pair of state we provide a number of technique for using and improving true distance heuristic such that most of the benefit of the all pair shortest path computation can be gained with le than of the memory experimental result on a number of domain show a fold improvement in search speed compared to traditional heuristic 
modal correspondence theory is a powerful and effective way to guarantee that adding specific syntactic axiom to a modal logic is mirrored by requiring corresponding property of the underlying kripke model however such axiom not only quantify over all formula but they are also global in the sense that the corresponding semantic property is assumed to hold for all state however in for instance epistemic logic one would like to have the flexibility to say that certain property like agent b know at least what agent a know are true locally in a specific state but not necessarily globally in all state this would enable one to say currently b know at least what a know but this is not common knowledge or but this is not always true or but this could be changed by action we offer a logic for knowing at least a where the global axiom scheme ka kb is replaced by a local inference rule we give a complete modal system and discus some consequence of the axiom in an epistemic setting our completeness proof also suggests how achieving such local property can be generalized to other axiom scheme and modal logic 
recently efficient approximation algorithm for finding nash equilibrium have been developed for the interesting class of anonymous game where a player s utility doe not depend on the identity of it opponent in this paper we tackle the problem of computing equilibrium in such game with continuous player type extending the framework to encompass setting with imperfect information in particular given the existence result for pure bayes nash equilibiria in these game we generalise the fictitious play algorithm by developing a novel procedure for finding a best response strategy which is specifically designed to deal with continuous and therefore infinite type space we then combine the best response computation with the general fictitious play structure to obtain an equilibrium to illustrate the power of this approach we apply our algorithm to the domain of simultaneous auction with continuous private value and discrete bid in which the algorithm show quick convergence 
we are interested in the problem of reasoning over very large common sense knowledge base when such a knowledge base contains noisy and subjective data it is important to have a method for making rough conclusion based on similarity and tendency rather than absolute truth we present analogy space which accomplishes this by forming the analogical closure of a semantic network through dimensionality reduction it self organizes concept around dimension that can be seen a making distinction such a good v bad or easy v hard and generalizes it knowledge by judging where concept lie along these dimension an evaluation demonstrates that user often agree with the predicted knowledge and that it accuracy is an improvement over previous technique 
many semantic web application require support for mapping between role or property defined in multiple independently developed ontology module distributed description logic ddl and package based description logic p dl offer two alternative logical formalism that support such mapping we prove that a variant of ddl that allow negated role or cardinality restriction in bridge rule or inverse bridge rule that connect alc ontology are undecidable b a variant of p dl alchio p that support role mapping between ontology module expressed in alchio is decidable 
ecosystem informatics brings together mathematical and computational tool to address scientific and policy challenge in the ecosystem science these challenge include novel sensor for collecting data algorithm for automated data cleaning learning method for building statistical model from data and for fitting mechanistic model to data and algorithm for designing optimal policy for biosphere management this presentation discus these challenge and then describes recent work on the first two of these new method for automated arthropod population counting and linear gaussian dbns for automated cleaning of sensor network data 
we consider a multiagent extension of single agent graph coloring multiple agent hold disjoint autonomous subgraphs of a global graph and every color used by the agent in coloring the graph ha associated cost in this multi agent graph coloring scenario we seek a minimum legal coloring of the global graph s vertex such that the coloring is also pareto efficient socially fair and individual rational we analyze complexity of individual rational solution in special graph class where classical coloring algorithm are known multiagent graph coloring ha application to a wide variety of multi agent coordination problem including multiagent scheduling 
to apply hierarchical task network htn planning to real world planning problem one need to encode the htn schema and action model beforehand however acquiring such domain knowledge is difficult and time consuming because the htn domain definition involves a significant knowledge engineering effort a system that can learn the htn planning domain knowledge automatically would save time and allow htn planning to be used in domain where such knowledgeengineering effort is not feasible in this paper we present a formal framework and algorithm to acquire htn planning domain knowledge by learning the precondition and effect of action and precondition of method our algorithm htn learner first build constraint from given observed decomposition tree to build action model and method precondition it then solves these constraint using a weighted max sat solver the solution can be converted to action model and method precondition unlike prior work on htn learning we do not depend on complete action model or state information we test the algorithm on several domain and show that our htn learner algorithm is both effective and efficient 
correcting recognition error is often necessary in a speech interface these error not only reduce user overall entry rate but can also lead to frustration while making fewer recognition error is undoubtedly helpful facility for supporting user guided correction are also critical we explore how to better support user correction using parakeet a continuous speech recognition system for mobile touch screen device parakeet s interface is designed for easy error correction on a handheld device user correct error by selecting alternative word from a word confusion network and by typing on a predictive software keyboard our interface design wa guided by computational experiment and used a variety of information source to aid the correction process in user study participant were able to write text effectively despite sometimes high initial recognition error rate using parakeet a an example we discus principle we think are important for building effective speech correction interface 
currently the most effective complete sat solver are based on the dpll algorithm augmented by clause learning these solver can handle many real world problem from application area like verification diagnosis planning and design without clause learning however dpll loses most of it effectiveness on real world problem recently there ha been some work on obtaining a deeper understanding of the technique of clause learning in this paper we utilize the idea of effective p simulation which is a new way of comparing clause learning with general resolution and other proof system we then show that pool proof a previously used characterization of clause learning can effectively p simulate general resolution furthermore this result hold even for the more restrictive class of greedy unit propagating pool proof which more accurately characterize clause learning a it is used in practice this result is surprising and indicates that clause learning is significantly more powerful than wa previously known 
kernel method have been applied successfully in many application the kernel matrix play an important role in kernel based learning method but the ideal kernel matrix is usually unknown in practice and need to be estimated in this paper we propose to directly learn the ideal kernel matrix called the optimal neighborhood kernel matrix from a pre specified kernel matrix for improved classification performance we assume that the prespecified kernel matrix generated from the specific application is a noisy observation of the ideal one the resulting optimal neighborhood kernel matrix is shown to be the summation of the pre specified kernel matrix and a rank one matrix we formulate the problem of learning the optimal neighborhood kernel a a constrained quartic problem and propose to solve it using two method level method and constrained gradient descent empirical result on several benchmark data set demonstrate the efficiency and effectiveness of the proposed algorithm 
we propose a novel approach to context sensitive semantic smoothing by making use of an intermediate semantically light representation for sentence called semantically relatable sequence sr sr of a sentence are tuples of word appearing in the semantic graph of the sentence a linked node depicting dependency relation in contrast to pattern based on consecutive word sr make use of grouping of nonconsecutive but semantically related word our experiment on trec ap collection show that the mixture model of sr translation model and two stage language model tslm of lafferty and zhai achieves map score better than the mixture model of multiword expression mwe translation model and tslm furthermore a system which for each test query selects either the sr or the mwe mixture model based on better query map score show significant improvement over the individual mixture model 
computing a nash equilibrium in multiplayer stochastic game is a notoriously difficult problem prior algorithm have been proven to converge in extremely limited setting and have only been tested on small problem in contrast we recently presented an algorithm for computing approximate jam fold equilibrium strategy in a three player nolimit texas hold em tournament a very large real world stochastic game of imperfect information in this paper we show that it is possible for that algorithm to converge to a non equilibrium strategy profile however we develop an ex post procedure that determines exactly how much each player can gain by deviating from his strategy and confirm that the strategy computed in that paper actually do constitute an equilibrium for a very small of the tournament entry fee next we develop several new algorithm for computing a nash equilibrium in multiplayer stochastic game with perfect or imperfect information which can provably never converge to a non equilibrium experiment show that one of these algorithm outperforms the original algorithm on the same poker tournament in short we present the first algorithm for provably computing an equilibrium of a large stochastic game for small finally we present an efficient algorithm that minimizes external regret in both the perfect and imperfect information case 
search engine present fix length passage from document ranked by relevance against the query in this paper we present and compare novel language model based method for extracting variable length document snippet by real time processing of document using the query issued by the user with this extra level of information the returned snippet are considerably more informative unlike previous work on passage retrieval which relies on searching relevant segment for filtering of preoccupied passage we focus on query informed segmentation to extract context aware relevant snippet with variable length in particular we show that when informed through an appropriate relevance language model curvature analysis and hidden markov model hmm based content segmentation technique can facilitate to extract relevant document snippet 
one way for agent to reach a joint decision is to vote over the alternative in open anonymous setting such a the internet an agent can vote more than once without being detected a voting rule is false name proof if no agent ever benefit from casting additional vote previous work ha shown that all false name proof voting rule are unresponsive to agent preference however that work implicitly assumes that casting additional vote is costless in this paper we consider what happens if there is a cost to casting additional vote we characterize the optimal most responsive false name proofwith cost voting rule for alternative in sharp contrast to the costless setting we prove that a the voting population grows larger the probability that this rule selects the majority winner converges to we also characterize the optimal group false name proof rule for alternative which is robust to coalition of agent sharing the cost of additional vote unfortunately the probability that this rule chooses the majority winner a the voting population grows larger is relatively low we derive an analogous rule in a setting with alternative and provide bounding result and computational approach for setting with or more alternative 
we propose a new framework for reasoning about knowledge action and time for domain that include action with non deterministic and context dependent effect the axiomatization is based on the event calculus and combine the expressiveness of possible world semantics with the efficiency of approach that dispense the use of the accessibility relation the framework is proved logically sound and when restricted to deterministic domain is also logically complete to prove correctness of the approach we construct a knowledge theory based on a branching version of the event calculus and study their correlation 
we present an algorithm for selecting an appropriate abstraction when learning a new skill we show empirically that it can consistently select an appropriate abstraction using very little sample data and that it significantly improves skill learning performance in a reasonably large real valued reinforcement learning domain 
with the increasing popularity of location tracking service such a gps more and more mobile data are being accumulated based on such data a potentially useful service is to make timely and targeted recommendation for user on place where they might be interested to go and activity that they are likely to conduct for example a user arriving in beijing might wonder where to visit and what she can do around the forbidden city a key challenge for such recommendation problem is that the data we have on each individual user might be very limited while to make useful and accurate recommendation we need extensive annotated location and activity information from user trace data in this paper we present a new approach known a user centered collaborative location and activity filtering uclaf to pull many user data together and apply collaborative filtering to find like minded user and like patterned activity at different location we model the userlocation activity relation with a tensor representation and propose a regularized tensor and matrix decomposition solution which can better address the sparse data problem in mobile information retrieval we empirically evaluate uclaf using a real world gps dataset collected from user over year and showed that our system can outperform several state of the art solution to the problem 
off policy reinforcement learning is aimed at efficiently reusing data sample gathered in the past which is an essential problem for physically grounded ai a experiment are usually prohibitively expensive a common approach is to use importance sampling technique for compensating for the bias caused by the difference between data sampling policy and the target policy however existing off policy method do not often take the variance of value function estimator explicitly into account and therefore their performance tends to be unstable to cope with this problem we propose using an adaptive importance sampling technique which allows u to actively control the trade off between bias and variance we further provide a method for optimally determining the trade off parameter based on a variant of cross validation we demonstrate the usefulness of the proposed approach through simulation 
we propose a novel method for helping human make good decision in complex game for which common equilibrium solution may be too difficult to compute or not relevant our method leverage and augments human natural use of argument in the decision making process we believe that if computer were capable of generating similar argument from the mathematical description of a game and presented those to a human decision maker the synergy would result in better performance overall the theory of reasoning pattern naturally lends itself to such a use we use reasoning pattern to derive localized evaluation function for each decision in a game then present their output to human we have implemented this approach in a repeated principal agent game and used it to generate advice given to subject experimental result show that human who received advice performed better than those who did not 
in this paper we propose a novel approach to expand query by exploring both location information and topic information of the query user at different location tend to have different vocabulary while the different expression coming from different vocabulary may relate to the same topic thus these expression are identified a location sensitive and can be used for query expansion we propose a hierarchical query expansion model which employ a two level svm classification model to classify query a location sensitive or location non sensitive where the former are further classified into same location sensitive and different location sensitive for the location sensitive query we propose an lda based topic level query similarity measure to rank the list of similar query experiment with g raw log data from citeseer and excite show that our hierarchical classification model predicts the query location sensitivity with more than precision and that the final search result is significantly better than existing query expansion method 
the map maximum a posteriori assignment problem in bayesian network is the problem of finding the most probable instantiation of a set of variable given partial evidence for the remaining variable the state of the art exact solution method is depth first branch and bound search using dynamic variable ordering and a jointree upper bound proposed by park and darwiche since almost all search time is spent computing the jointree bound we introduce an efficient method for computing these bound incrementally we point out that using a static variable ordering it is only necessary to compute relevant upper bound at each search step and it is also possible to cache potential of the jointree for efficient backtracking since the jointree computation typically produce bound for joint configuration of group of variable our method also instantiates multiple variable at each search step instead of a single variable in order to reduce the number of time that upper bound need to be computed experiment show that this approach lead to order of magnitude reduction in search time 
this paper describes our experience in using modern web architecture lightweight python framework and rapid prototyping to create an ai rostering and workforce management system to help prepare for the beijing olympic equestrian event which will be held in hong kong a part of the olympics preparation a scaled down test event the good luck beijing hksar th anniversary cup wa held in august a a dressed rehearsal for the olympics equestrian event to schedule and manage all the volunteer and staff of this event a web based ai workforce management system wms wa created by leveraging on our existing web based application framework and ai platform we were able to complete the entire ai project under very tight time constraint and helped hong kong successfully pas the readiness requirement of the international olympic committee this paper focus on our experience with using ai to support the good luck beijing equestrian game it describes the ai development approach we took a well a the ai architecture for the hr xml compliant rostering system 
in this paper we proposed a novel method to detect new word in domain specific field based on user behavior first we select the most representative word from domain specific lexicon then combining with user behavior we try to discover the potential expert in this field who use those terminology frequently finally we make further effort to identify new word from behavior of those expert word used much more frequently in this community than others are most probably new word in brief our method follows a collaborative filtering way first from word to find professional expert then from expert to discover new word which is different from the traditional new word detection method our method achieves up to in accuracy on a computer science related data set moreover the proposed method can be easily extended to related word retrieval task we compare our method with google set and bayesian set experiment show that our method and bayesian set give better result than google set 
graph based manifold ranking method have been successfully applied to topic focused multi document summarization this paper further proposes to use the multi modality manifold ranking algorithm for extracting topic focused summary from multiple document by considering the within document sentence relationship and the cross document sentence relationship a two separate modality graph three different fusion scheme namely linear form sequential form and score combination form are exploited in the algorithm experimental result on the duc benchmark datasets demonstrate the effectiveness of the proposed multi modality learning algorithm with all the three fusion scheme 
we have spelling and grammar checking tool available on today s word processor but what they are missing is a tool that can recommend several possibility of a given written sentence to assist a user to write better sentence therefore we aim to develop a linguistic tool to beautify text by applying our developed lexical resource regarding textual affect sensing the developed tool will allow a user to beautify an input sentence in term of tuning it on different scale like valence affect prospect and praise for example using such a tool one may get the recommendation like your lovely email make me very glad or i become glad to read your email or i am very happy to obtain your nice email for the input sentence i am happy to receive your email after scaling up the input sentence on affective or prospective or valence scale respectively such tool will be especially helpful to the non native english speaker to write better english 
multilingual parallel text corpus provide a powerful mean for propagating linguistic knowledge across language we present a model which jointly learns linguistic structure for each language while inducing link between them our model support fully symmetrical knowledge transfer utilizing any combination of supervised and unsupervised data across language barrier the proposed non parametric bayesian model effectively combine cross lingual alignment with target language prediction this architecture is a potent alternative to projection method which decompose these decision into two separate stage we apply this approach to the task of morphological segmentation where the goal is to separate a word into it individual morpheme when tested on a parallel corpus of hebrew and arabic our joint bilingual model effectively incorporates all available evidence from both language yielding significant performance gain 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
voting on multiple related issue is an important and difficult problem the key difficulty is that the number of alternative is exponential in the number of issue and hence it is infeasible for the agent to rank all the alternative a simple approach is to vote on the issue one at a time in sequence however a drawback is that the outcome may depend on the order in which the issue are voted upon and decided which give the chairperson some control over the outcome of the election because she can strategically determine the order while this is undeniably a negative feature of sequential voting in this paper we temper this judgment by showing that the chairperson s control problem is in most case computationally hard 
this paper deal with the relationship between intelligent behaviour on the one hand and the mental quality needed to produce it on the other we consider two well known opposing position on this issue one due to alan turing and one due to john searle via the chinese room in particular we argue against searle showing that his answer to the so called system reply doe not work the argument take a novel form we shift the debate to a different and more plausible room where the required conversational behaviour is much easier to characterize and to analyze despite being much simpler than the chinese room we show that the behaviour there is still complex enough that it cannot be produced without appropriate mental quality 
given the complexity of planning it is often beneficial to create plan that work for a wide class of problem this facilitates reuse of existing plan for different instance drawn from the same problem or from an infinite family of similar problem we define a class of such planning problem called generalized planning problem and present a novel approach for transforming classical plan into generalized plan these algorithm like plan include loop and work for problem instance having varying number of object that must be manipulated to reach the goal our approach take a input a classical plan for a certain problem instance it output a generalized plan along with a classification of the problem instance where it is guaranteed to work we illustrate the utility of our approach through result of a working implementation on various practical example 
practical data analysis relies on the ability to count observation of object succinctly and efficiently unfortunately the space usage of an exact estimator grows with the size of the a priori set from which object are drawn while the time required to maintain such an estimator grows with the size of the data set we present static and on line approximation scheme that avoid these limitation when approximate frequency estimate are acceptable our log frequency sketch extends the approximate counting algorithm of morris to estimate frequency with bounded relative error via a single pas over a data set it us constant space per object when the frequency follow a power law and can be maintained in constant time per observation we give an approximation scheme which we verify empirically on a large natural language data set where for instance percent of frequency are estimated with relative error le than using fewer than bit per object in the static case and bit per object on line 
we introduce a symmetry reduction technique for model checking temporal epistemic property of multi agent system defined in the mainstream interpreted system framework the technique based on counterpart semantics aim to reduce the set of initial state that need to be considered in a model we present theoretical result establishing that there are neither false positive nor false negative in the reduced model we evaluate the technique by presenting the result of an implementation tested against two well known application of epistemic logic the muddy child and the dining cryptographer the experimental result obtained confirm that the reduction in model checking time can be dramatic thereby allowing for the verification of hitherto intractable system 
planning under uncertainty for multiple agent ha grown rapidly with the development of formal model such a multi agent mdps and decentralized mdps but despite their richness the applicability of these model remains limited due to their computational complexity we present the class of event detecting multi agent mdps emmdps designed to detect multiple mobile target by a team of sensor agent we show that emmdps are np hard and present a scalable approximation algorithm for solving them using matroid theory and constraint optimization the complexity of the algorithm is linear in the state space and number of agent quadratic in the horizon and exponential only in a small parameter that depends on the interaction among the agent despite the worst case approximation ratio of experimental result show that the algorithm produce near optimal policy for a range of test problem 
gaussian process gps are promising bayesian method for classification and regression problem they have also been used for semi supervised learning task in this paper we propose a new algorithm for solving semi supervised binary classification problem using sparse gp regression gpr model it is closely related to semi supervised learning based on support vector regression svr and maximum margin clustering the proposed algorithm is simple and easy to implement it give a sparse solution directly unlike the svr based algorithm also the hyperparameters are estimated easily without resorting to expensive cross validation technique use of sparse gpr model help in making the proposed algorithm scalable preliminary result on synthetic and real world data set demonstrate the efficacy of the new algorithm 
this paper present an axiomatic analysis of negotiation problem within task oriented domain tod we start by applying three classical bargaining solution of nash kalai smorodinsky and egalitarian to the domain of problem with a preprocess of randomization on possible agreement we find out that these three solution coincide within any tod and can be characterized by the same set of axiom which specify a solution of task oriented negotiation a an outcome of dual process of maximizing cost reduction and minimizing workload imbalance this axiomatic characterization is then used to produce an approximate solution to the domain of problem without randomization on possible agreement 
the paper present a calculus based on resolution for credulous reasoning in answer set programming the new approach allows a top down and goal directed resolution in the same spirit a traditional sld resolution the proposed credulous resolution can be used in query answering with nonground query and with non ground and possibly infinite program soundness and completeness result for the resolution procedure are proved for large class of logic program the resolution procedure is also extended to handle some traditional syntactic extension used in answer set programming such a choice rule and constraint the paper also describes an initial implementation of a system for credulous reasoning in answer set programming 
multi label learning deal with data associated with multiple label simultaneously like other machine learning and data mining task multi label learning also suffers from the curse of dimensionality although dimensionality reduction ha been studied for many year multi label dimensionality reduction remains almost untouched in this paper we propose a multi label dimensionality reduction method mddm which attempt to project the original data into a lower dimensional feature space maximizing the dependence between the original feature description and the associated class label based on the hilbert schmidt independence criterion we derive a closed form solution which enables the dimensionality reduction process to be efficient experiment validate the performance of mddm 
modern bayesian network learning algorithm are time efficient scalable and produce high quality model these algorithm feature prominently in decision support model development variable selection and causal discovery the quality of the model however ha often only been empirically evaluated the available theoretical result typically guarantee asymptotic correctness consistency of the algorithm this paper describes theoretical bound on the quality of a fundamental bayesian network local learning task in the finite sample using theory for controlling the false discovery rate the behavior of the derived bound is investigated across various problem and algorithm parameter empirical result support the theory which ha immediate ramification in the design of new algorithm for bayesian network learning variable selection and causal discovery 
we describe a novel integration of planning with probabilistic state estimation and execution the resulting system is a unified representational and computational framework based on declarative model and constraint based temporal plan the work is motivated by the need to explore the ocean more cost effectively through the use of autonomous underwater vehicle auv requiring them to be goal directed perceptive adaptive and robust in the context of dynamic and uncertain condition the novelty of our approach is in integrating deliberation and reaction over different temporal and functional scope within a single model and in breaking new ground in oceanography by allowing for precise sampling within a feature of interest using an autonomous robot the system is general purpose and adaptable to other ocean going and terrestrial platform 
the use of auction mechanism like the gsp in online advertising can lead to loss of both efficiency and revenue when advertise r have rich preference even simple form of expressiveness like budget constraint can lead to suboptimal outcome this ha led to the recognition of the value of sequential and or stocha stic optimization in ad allocation unfortunately natural formu lations of such optimization problem fall prey to channel explosion specifically available ad inventory must be partitioned into sub set or channel of indistinguishable supply each channel containing inventory that is interchangeable from the perspective of each active advertiser the number of such channel grows exponentially in the number of feature of interest we propose a mean for automatically abstracting these channel grouping together channel so that irrelevant distinction are ignored our approach based on lp mip column and constraint generation dramatically reduces the number of distinct channel over which ad are allocated thus rendering optimization computationally feasible at pract ical scale our algorithm also allow revenue efficiency to be sacrifice d in a principled fashion by ignoring potentially relevant disti nctions but retaining the most important distinction ignoring only t hose that have low impact on solution quality this allows tradeoff to be made between tractability and solution quality numerical experiment demonstrate the computational practicality of our approach a well a the quality of the abstraction generated 
we formalize in this paper a key property of asserting clause the most common type of clause learned by sat solver we show that the formalized property which is called empowerment is not exclusive to asserting clause and introduce a new class of learned clause which can also be empowering we show empirically that the new class of clause tends to be much shorter and induce further backtracks than asserting clause and an empowering subset of this new class of clause significantly improves the performance of the rsat solver on unsatisfiable problem 
in this paper we address the problem of merging multiple imprecise probabilistic belief represented a probabilistic logic program plps obtained from multiple source belief in each plp are modeled a conditional event attached with probability bound the major task of syntax based merging is to obtain the most rational probability bound for each conditional event from the original plps to form a new plp we require the minimal change principle to be followed so that each source give up it belief a little a possible some instantiated merging operator are derived from our merging framework furthermore we propose a set of postulate for merging plps some of which extend the postulate for merging classical knowledge base whilst others are specific to the merging of probabilistic belief 
when autonomous agent decide on their bidding strategy in real world auction they have a number of concern that go beyond the model that are normally analyzed in traditional auction theory oftentimes the agent have budget constraint and the auction have a reserve price both of which restrict the bid the agent can place in addition their attitude need not be risk neutral and they may have uncertainty about the value of the good they are buying some of these issue have been examined individually for single unit sealed bid auction in this paper we extend this analysis to the multi unit case and also analyze the multi unit sealed bid auction in which a combination of these issue are present for unit demand bidder this analysis constitutes the main contribution of this paper we then demonstrate the usefulness in practice of this analysis we show in simulation that taking into account all these issue allows the bidder to maximize their utility furthermore using this analysis allows a seller to improve her revenue le by selecting the optimal reserve price 
research ha shown promise in the design of large scale common sense probabilistic model to infer human state from environmental sensor data these model have made use of mined and preexisting common sense data and traditional probabilistic machine learning technique to improve recognition of the state of everyday human life in this paper we demonstrate effective technique for structure learning on graphical model designed for this domain improving the srcs system of pentney et al by learning additional dependency between variable because the model used for common sense reasoning typically involve a large number of variable issue of scale arise in searching for additional dependency we discus how we use data mining technique to address this problem we show experimentally that these technique improve the accuracy of state prediction and that with a good prior model the use of a common sense model with structure learning provides better prediction of unlabeled variable a well a labeled variable the result also demonstrate that it is possible to collect new common sense information about daily life using such a statistical model and labeled data 
in this paper we propose a semi supervised learning ssl algorithm based on local and global regularization in the local regularization part our algorithm construct a regularized classifier for each data point using it neighborhood while the global regularization part adopts a laplacian regularizer to smooth the data label predicted by those local classifier we show that some existing ssl algorithm can be derived from our framework finally we present some experimental result to show the effectiveness of our method 
in artificial intelligence and pervasive computing research inferring user high level goal from activity sequence is an important task a major challenge in goal recognition is that user often pursue several high level goal in a concurrent and interleaving manner where the pursuit of goal may spread over different part of an activity sequence and may be pursued in parallel existing approach to recognizing multiple goal often formulate this problem either a a single goal recognition problem or in a deterministic way ignoring uncertainty in this paper we propose cigar concurrent and interleaving goal and activity recognition a novel and simple two level probabilistic framework for multiple goal recognition where we can recognize both concurrent and interleaving goal we use skip chain conditional random field sccrf for modeling interleaving goal and we model concurrent goal by adjusting inferred probability through a correlation graph which is a major advantage in that we are able to reason about goal interaction explicitly through the correlation graph the two level framework also avoids the high training complexity when modeling concurrency and interleaving together in a unified crf model experimental result show that our method can effectively improve recognition accuracy on several real world datasets collected from various wireless and sensor network 
we consider approval voting election in which each voter vote for a possibly empty set of candidate and the outcome consists of a set of k candidate for some parameter k e g committee election we are interested in the minimax approval voting rule in which the outcome represents a compromise among the voter in the sense that the maximum distance between the preference of any voter and the outcome is a small a possible this voting rule ha two main drawback first computing an outcome that minimizes the maximum distance is computationally hard furthermore any algorithm that always return such an outcome provides incentive to voter to misreport their true preference in order to circumvent these drawback we consider approximation algorithm i e algorithm that produce an outcome that approximates the minimax distance for any given instance such algorithm can be considered a alternative voting rule we present a polynomial time approximation algorithm that us a natural linear programming relaxation for the underlying optimization problem and deterministically round the fractional solution in order to compute the outcome this result improves upon the previously best known algorithm that ha an approximation ratio of we are furthermore interested in approximation algorithm that are resistant to manipulation by coalition of voter i e algorithm that do not motivate voter to misreport their true preference in order to improve their distance from the outcome we complement previous result in the literature with new upper and lower bound on strategyproof and group strategyproof algorithm 
machine learning and data mining have become aware that using constraint when learning pattern and rule can be very useful to this end a large number of special purpose system and technique have been developed for solving such constraint based mining and learning problem these technique have so far been developed independently of the general purpose tool and principle of constraint programming known within the field of artificial intelligence this paper show that off the shelf constraint programming technique can be applied to various pattern mining and rule learning problem cf also de raedt gun and nijssen nijssen gun and de raedt this doe not only lead to methodology that are more general and flexible but also provides new insight into the underlying mining problem that allow u to improve the state ofthe art in data mining such a combination of constraint programming and data mining raise a number of interesting new question and challenge 
we present a human robot dialogue system that enables a robot to work together with a human user to build wooden construction toy we then describe a study which assessed the response of na ve user to output that varied along two dimension the method of describing an assembly plan preorder or post order and the method of referring to object in the world basic and full varying both of these factor produced significant result subject using the system that employed a preorder description strategy asked for instruction to be repeated significantly le often than those who experienced the post order strategy while the subject who heard reference generated by the full reference strategy judged the robot s instruction to be significantly more understandable than did those who heard the output of the basic strategy 
the most difficult and often most essential aspect of many interception and tracking task is constructing motion model of the target expert rarely can provide complete information about a target s expected motion pattern and fitting parameter for complex motion pattern can require large amount of training data specifying how to parameterize complex motion pattern is in itself a difficult task in contrast bayesian nonparametric model of target motion are very flexible and generalize well with relatively little training data we propose modeling target motion pattern a a mixture of gaussian process gp with a dirichlet process dp prior over mixture weight the gp provides an adaptive representation for each individual motion pattern while the dp prior allows u to represent an unknown number of motion pattern both automatically adjust the complexity of the motion model based on the available data our approach outperforms several parametric model on a helicopter based car tracking task on data collected from the greater boston area 
the recognition of the goal a user is pursing when interacting with a software application is a crucial task for an interface agent a it serf a a context for making opportune intervention to provide assistance to the user the prediction of the user goal must be fast and a goal recognizer must be able to make early prediction with few observation of the user action in this work we propose an approach to automatically build an intention model from a plan corpus using variable order markov model we claim that following our approach an interface agent will be capable of accurately ranking the most probable user goal in a time linear to the number of goal modeled 
despite the success of gaussian process gps in modelling spatial stochastic process dealing with large datasets is still challenging the problem arises by the need to invert a potentially large covariance matrix during inference in this paper we address the complexity problem by constructing a new stationary covariance function mercer kernel that naturally provides a sparse covariance matrix the sparseness of the matrix is defined by hyperparameters optimised during learning the new covariance function enables exact gp inference and performs comparatively to the squared exponential one at a lower computational cost this allows the application of gps to large scale problem such a ore grade prediction in mining or d surface modelling experiment show that using the proposed covariance function very sparse covariance matrix are normally obtained which can be effectively used for faster inference and le memory usage 
most of the existing metric learning method are accomplished by exploiting pairwise constraint over the labeled data and frequently suffer from the insufficiency of training example to learn a robust distance metric from few labeled example prior knowledge from unlabeled example a well a the metric previously derived from auxiliary data set can be useful in this paper we propose to leverage such auxiliary knowledge to assist distance metric learning which is formulated following the regularized loss minimization principle two algorithm are derived on the basis of manifold regularization and log determinant divergence regularization technique respectively which can simultaneously exploit label information i e the pairwise constraint over labeled data unlabeled example and the metric derived from auxiliary data set the proposed method directly manipulate the auxiliary metric and require no raw example from the auxiliary data set which make them efficient and flexible we conduct extensive evaluation to compare our approach with a number of competing approach on face recognition task the experimental result show that our approach can derive reliable distance metric from limited training example and thus are superior in term of accuracy and labeling effort 
we consider optimizing the coalition structure in coalitional skill game csgs a succinct representation of coalitional game bachrach and rosenschein in csgs the value of a coalition depends on the task it member can achieve the task require various skill to complete them and agent may have different skill set the optimal coalition structure is a partition of the agent to coalition that maximizes the sum of utility obtained by the coalition we show that csgs can represent any characteristic function and consider optimal coalition structure generation in this representation we provide hardness result showing that in general csgs a well a in very restricted version of them computing the optimal coalition structure is hard on the positive side we show that the problem can be reformulated a constraint satisfaction on a hyper graph and present an algorithm that find the optimal coalition structure in polynomial time for instance with bounded tree width and number of task 
the problem of environment design considers a setting in which an interested party aim to influence an agent s decision by making limited change to the agent s environment zhang and parkes first introduced the environment design concept for a specific problem in the markov decision process setting in this paper we present a general framework for the formulation and solution of environment design problem with one agent we consider both the case in which the agent s local decision model is known and partially unknown to the interested party and illustrate the framework and result on a linear programming setting for the latter problem we formulate an active indirect elicitation method and provide condition for convergence and logarithmic convergence we relate to the problem of inverse optimization and also offer a game theoretic interpretation of our method 
we describe a cognitive architecture for creating more robust intelligent system our approach is to enable hybrid of algorithm based on different computational formalism to be executed the architecture is motivated by some feature of human cognitive architecture and the following belief most existing computational method often exhibit some of the characteristic desired of intelligent system at the cost of other desired characteristic and a system exhibiting robust intelligence can be designed by implementing hybrid of these computational method the main obstacle to this approach is that the various relevant computational method are based on data structure and algorithm that are difficult to integrate into one system we describe a new method of executing hybrid of algorithm using the focus of attention of multiple module the key to this approach is the following two principle algorithm based on very different computational framework e g logical reasoning probabilistic inference and case based reasoning can be implemented using the same set of five common function and each of these common function can be executed using multiple data structure and algorithm this approach ha been embodied in the polyscheme cognitive architecture system based on polyscheme in planning spatial reasoning robotics and information retrieval illustrate that this approach to hybridizing algorithm enables qualitative and measurable quantitative advance in the ability of intelligent system 
adtrees a data structure useful for caching sufficient statistic have been successfully adapted to grow lazily when memory is limited and to update sequentially with an incrementally updated dataset for low arity symbolic feature adtrees trade a slight increase in query time for a reduction in overall tree size unfortunately for high arity feature the same technique can often result in a very large increase in query time and a nearly negligible tree size reduction in the dynamic lazy version of the tree both query time and tree size can increase for some application here we present two modification to the adtree which can be used separately or in combination to achieve the originally intended space time tradeoff in the adtree when applied to datasets containing very high arity feature 
we present a new comer finding algorithm based on merging like stroke segmentation together in order to eliminate false positive comer we compare our system to two benchmark corner finder with substantial improvement in both polyline and complex fit 
this paper describes a method and system for integrating machine learning with planning and data visualization for the management of mobile sensor for earth science investigation data mining identifies discrepancy between previo u observation and prediction made by earth science model location of these discrepancy become interesting target for future observation such target become goal used by a flight planner to generate the observation activity the c ycle of observation data analysis and planning is repeated continuously throughout a multi week earth science investigation 
logic programming with negation offer a compelling approach to abductive reasoning this paper show a simple view of abduction in this context for the completion semantics under which the problem of abduction becomes one of solving quantified equation and disequations by this way of treating abduction the problem with nonground negative query in the previous approach no longer exist we show the soundness and completeness result for our approach 
nonnegative matrix factorization nmf ha been widely used in machine learning and data mining it aim to find two nonnegative matrix whose product can well approximate the nonnegative data matrix which naturally lead to part based representation in this paper we present a local learning regularized nonnegative matrix factorization llnmf for clustering it imposes an additional constraint on nmf that the cluster label of each point can be predicted by the point in it neighborhood this constraint encodes both the discriminative information and the geometric structure and is good at clustering data on manifold an iterative multiplicative updating algorithm is proposed to optimize the objective and it convergence is guaranteed theoretically experiment on many benchmark data set demonstrate that the proposed method outperforms nmf a well a many state of the art clustering method 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
finite state controller represent an effective action selection mechanism widely used in domain such a video game and mobile robotics in contrast to the policy obtained from mdps and pomdps finite state controller have two advantage they are often extremely compact and they are general applying to many problem and not just one a limitation of finite state controller on the other hand is that they are written by hand in this paper we address this limitation presenting a method for deriving controller automatically from model the model represent a class of contingent problem where action are deterministic and some fluents are observable the problem of deriving a controller is converted into a conformant problem that is solved using classical planner taking advantage of a complete translation into classical planning introduced recently the controller derived are general in the sense that they do not solve the original problem only but many variation a well including change in the size of the problem or in the uncertainty of the initial situation and action effect several experiment illustrating the automatic derivation of controller are presented 
to facilitate ordinary people to search medical information we have built an intelligent medical web search engine called imed imed us medical knowledge and an interactive questionnaire to find multiple disease serving a query the search result of these query are combined together and returned to the searcher in a traditional sequential order nevertheless searcher still frequently miss desired information because the traditional search result output interface cannot capture the internal structure of medical search result this paper present a new intelligent search result output interface devoted to intelligent medical search the new output interface automatically offer searcher what they want instead of waiting until they ask explicitly it structure all the search result into a multi level hierarchy with explicitly marked medical meaning in this way searcher can efficiently navigate among all the search result and quickly obtain desired information we demonstrate the effectiveness of our technique through an evaluation using usmle medical exam case 
we propose an iterated version of nesterov s first order smoothing method for the two person zero sum game equilibrium problem min x q max y q x tay max y q min x q x tay this formulation applies to matrix game a well a sequential game our new algorithmic scheme computes an equilibrium to this min max problem in o a ln first order iteration where a is a certain condition measure of the matrix a this improves upon the previous first order method which required o iteration and it match the iteration complexity bound of interior point method in term of the algorithm s dependence on unlike the interior point method that are inapplicable to large game due to their memory requirement our algorithm retains the small memory requirement of prior first order method our scheme supplement nesterov s algorithm with an outer loop that lower the target between iteration this target affect the amount of smoothing in the inner loop we find it surprising that such a simple modification yield an exponential speed improvement finally computational experiment both in matrix game and sequential game show that a significant speed improvement is obtained in practice a well and the relative speed improvement increase with the desired accuracy a suggested by the complexity bound 
we describe yoopick a combinatorial sport prediction market that implement a flexible betting language and in tum facilitates fine grained probabilistic estimation of outcome 
multi category classification of short dialogue is a common task performed by human when assigning a question to an expert a customer service operator try to classify the customer query into one of n different class for which expert are available similarly question on the web for example question at yahoo answer can be automatically forwarded to a restricted group of people with a specific expertise typical question are short and assume background world knowledge for correct classification with exponentially increasing amount of knowledge available with distinct property labeled v unlabeled structured v unstructured no single knowledge transfer algorithm such a transfer learning multi task learning or selftaught learning can be applied universally in this work we show that bag of word classifier performs poorly on noisy short conversational text snippet we present an algorithm for leveraging heterogeneous data source and algorithm with significant improvement over any single algorithm rivaling human performance using different algorithm for each knowledge source we use mutual information to aggressively prune feature with heterogeneous data source including wikipedia open directory project odp and yahoo answer we show and correct classification on google answer corpus and switchboard corpus using only feature class this reflects a huge improvement over bag of word approach and error reduction over previously published state of art gabrilovich et al 
model based diagnosis mbd typically focus on diagnosis minimal under some minimality criterion e g the minimal cardinality set of faulty component that explain an observation however for different there may be minimal cardinality diagnosis of differing cardinality and several application such a test pattern generation and benchmark model analysis need to identify the leading to the max cardinality diagnosis amongst them we denote this problem a a max fault min cardinality mfmc problem this paper considers the generation of observation that lead to mfmc diagnosis we present a near optimal stochastic algorithm called miranda max fault min cardinality observation deduction algorithm that computes mfmc observation compared to optimal deterministic approach such a atpg the algorithm ha very low cost allowing u to generate observation corresponding to high cardinality fault experiment show that miranda delivers optimal result on the xxx circuit a well a good mfmc cardinality estimate on the larger iscas circuit 
this work address the problem of in the dark traffic classification for tcp session an important problem in network management an innovative use of support vector machine svms with a spectrum representation of packet flow is demonstrated to provide a highly accurate fast and robust method for classifying common application protocol the use of a linear kernel allows for an analysis of svm feature weight to gain insight into the underlying protocol mechanism 
recently many web service such a social networking service blog and collaborative tagging have become widely popular many attempt are being made to investigate user interaction by analyzing social network among user however analyzing a social network with attributional data is often not an easy task because numerous way exist to define feature through aggregation of different table in this study we propose an algorithm to identify important network based feature systematically from a given social network to analyze user behavior efficiently and to expand the service we apply our method for link based classification and link prediction task with two different datasets i e an cosme an online viral marketing site dataset and a hatena bookmark collaborative tagging service dataset to demonstrate the usefulness of our algorithm our algorithm is general and can provide useful network based feature for social network analysis 
depth first search is effective at solving hard combinatorial problem but if the problem space ha a graph structure the same node may be searched many time this can increase the size of the search exponentially we explore two technique that prevent this duplicate detection and duplicate avoidance we illustrate these technique on the treewidth problem a combinatorial optimization problem with application to a variety of research area the bottleneck for previous treewidth algorithm is a large memory requirement we develop a duplicate avoidance technique for treewidth and demonstrate that it significantly outperforms other algorithm when memory is limited additionally we are able to find for the first time the treewidth of several hard benchmark graph 
mixed multi unit combinatorial auction mmucas are extension of classical combinatorial auction ca where bidder trade transformation of good rather than just set of good solving mmucas i e determining the sequence of bid to be accepted by the auctioneer is computationally intractable in general however differently from ca little wa known about whether polynomial time solvable class of mmucas can be singled out based on constraining their characteristic the paper precisely fill this gap by depicting a clear picture of the tractability frontier for mmuca instance under both structural and qualitative restriction which characterize interaction among bidder and type of bid involved in the various transformation respectively by analyzing these restriction a sharp frontier is charted based on various dichotomy result in particular tractability island resulting from the investigation generalize on mmucas the largest class of tractable ca emerging from the literature 
we develop a method for detecting symmetry in arbitrary game and exploiting these symmetry when using tree search to play the game game in the general game playing domain are given a a set of logic based rule defining legal move their effect and goal of the player the presented method transforms the rule of a game into a vertex labeled graph such that automorphisms of the graph correspond with symmetry of the game the algorithm detects many kind of symmetry that often occur in game e g rotation and reflection symmetry of board interchangeable object and symmetric role a transposition table is used to efficiently exploit the symmetry in many game 
in social choice a preference function pf take a set of vote linear order over a set of alternative a input and produce one or more ranking also linear order over the alternative a output such function have many application for example aggregating the preference of multiple agent or merging ranking of say webpage into a single ranking the key issue is choosing a pf to use one natural and previously studied approach is to assume that there is an unobserved correct ranking and the vote are noisy estimate of this then we can use the pf that always chooses the maximum likelihood estimate mle of the correct ranking in this paper we define simple ranking scoring function srsfs and show that the class of neutral srsfs is exactly the class of neutral pfs that are mles for some noise model we also define composite ranking scoring function crsfs and show a condition under which these coincide with srsfs we study key property such a consistency and continuity and consider some example pfs in particular we study single transferable vote stv a commonly used pf showing that it is a crsf but not an srsf thereby clarifying the extent to which it is an mle function this also give a new perspective on how tie should be broken under stv we leave some open question 
this paper address the problem of plan recognition for multi agent team complex multi agent task typically require dynamic team where the team membership change over time team split into subteams to work in parallel merge with other team to tackle more demanding task and disband when plan are completed we introduce a new multi agent plan representation that explicitly encodes dynamic team membership and demonstrate the suitability of this formalism for plan recognition from our multi agent plan representation we extract local temporal dependency that dramatically prune the hypothesis set of potentially valid team plan the reduced plan library can be efficiently processed to obtain the team state history naive pruning can be inadvisable when low level observation are unreliable due to sensor noise and classification error in such condition we eschew pruning in favor of prioritization and show how our scheme can be extended to rank order the hypothesis experiment show that this robust pre processing approach rank the correct plan within the top even under condition of severe noise 
road network information simplifies autonomous driving by providing strong prior about environment it informs a robotic vehicle with where it can drive model of what can be expected and contextual cue that influence driving behavior currently however road network information is manually generated using a combination of gps survey and aerial imagery these manual technique are labor intensive and error prone to fully exploit the benefit of digital imagery these process should be automated a a step toward this goal we present an algorithm that extract the structure of a parking lot visible from a given aerial image to minimize human intervention in the use of aerial imagery we devise a self supervised learning algorithm that automatically generates a set of parking spot template to learn the appearance of a parking lot and estimate the structure of the parking lot from the learned model the data set extracted from a single image alone is too small to sufficiently learn an accurate parking spot model however strong prior trained using large data set collected across multiple image dramatically improve performance our self supervised approach outperforms the prior alone by adapting the distribution of example toward that found in the current image a thorough empirical analysis compare leading state of the art learning technique on this problem 
robot operating in home environment must be able to interact with articulated object such a door or drawer ideally robot are able to autonomously infer articulation model by observation in this paper we present an approach to learn kinematic model by inferring the connectivity of rigid part and the articulation model for the corresponding link our method us a mixture of parameterized and parameter free gaussian process representation and find low dimensional manifold that provide the best explanation of the given observation our approach ha been implemented and evaluated using real data obtained in various realistic home environment setting 
online service such a web search news portal and e commerce application face the challenge of providing high quality experience to a large heterogeneous user base recent effort have highlighted the potential to improve performance by personalizing service based on special knowledge about user for example a user s location demographic and search and browsing history may be useful in enhancing the result offered in response to web search query however reasonable concern about privacy by both user provider and government agency acting on behalf of citizen may limit access to such information we introduce and explore an economics of privacy in personalization where people can opt to share personal information in return for enhancement in the quality of an online service we focus on the example of web search and formulate realistic objective function for search efficacy and privacy we demonstrate how we can identify a near optimal solution to the utility privacy tradeoff we evaluate the methodology on data drawn from a log of the search activity of volunteer participant we separately ass user preference about privacy and utility via a large scale survey aimed at eliciting preference about people willingness to trade the sharing of personal data in return for gain in search efficiency we show that a significant level of personalization can be achieved using only a small amount of information about user 
diffusion process taking place in social network are used to model a number of phenomenon such a the spread of human or computer virus and the adoption of product in viral marketing campaign it is generally difficult to obtain accurate information about how such spread actually occur so a variety of stochastic diffusion model are used to simulate spreading process in network instead we show that a canonical genetic algorithm with a spatially distributed population when paired with specific form of holland s synthetic hyperplane defined objective function can simulate a large and rich class of diffusion model for social network these include standard diffusion model such a the independent cascade and competing process model in addition our genetic algorithm diffusion model gadm can also model complex phenomenon such a information diffusion we demonstrate an application of the gadm to modeling information flow in a large dynamic social network derived from e mail header 
we describe a simple environment to study cooperation between two agent and a method of achieving cooperation in that environment the environment consists of randomly generated normal form game with uniformly distributed pay offs agent play multiple game against each other each game drawn independently from the random distribution in this environment cooperation is difficult tit for tat cannot be used because move are not labeled a cooperate or defect fictitious play cannot be used because the agent never see the same game twice and approach suitable for stochastic game cannot be used because the set of state is not finite our agent identifies cooperative move by assigning an attitude to it opponent and to itself the attitude determines how much a player value it opponent payoff i e how much the player is willing to deviate from strictly selfinterested behavior to cooperate our agent estimate the attitude of it opponent by observing it move and reciprocates by setting it own attitude accordingly we show how the opponent s attitude can be estimated using a particle filter even when the opponent is changing it attitude 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
in a seminal paper lin and reiter introduced a model theoretic definition for the progression of the initial knowledge base of a basic action theory this definition come with a strong negative result namely that for certain kind of action theory first order logic is not expressive enough to correctly characterize this form of progression and second order axiom are necessary however lin and reiter also considered an alternative definition for progression which is always first order definable they conjectured that this alternative definition is incorrect in the sense that the progressed theory is too weak and may sometimes lose information this conjecture and the status of first order definable progression ha remained open since then in this paper we present two significant result about this alternative definition of progression first we prove the lin and reiter conjecture by presenting a case where the progressed theory indeed doe lose information second we prove that the alternative definition is nonetheless correct for reasoning about a large class of sentence including some that quantify over situation in this case the alternative definition is a preferred option due to it simplicity and the fact that it is always first order 
breadth first and depth first search are basic search strategy upon which many other search algorithm are built in this paper we describe an approach to integrating these two strategy in a single algorithm that combine the complementary strength of both we show the benefit of this approach using the treewidth problem a an example 
effectively modeling an agent s cognitive model is an important problem in many domain in this paper we explore the agent people wrote to operate within optimization problem we claim that the overwhelming majority of these agent used strategy based on bounded rationality even when optimal solution could have been implemented particularly we believe that many element from aspiration adaptation theory aat are useful in quantifying these strategy to support these claim we present extensive empirical result from over a hundred agent programmed to perform in optimization problem involving solving for one and two variable 
our research is motivated by the observation that nlp system frequently mislabel passive voice verb phrase a being in the active voice when there is no auxiliary verb e g the man arrested had a long record these error directly impact thematic role recognition and nlp application that depend on it we present a learned classifier that can accurately identify reduced passive voice construction in shallow parsing environment 
recently argumentation mechanism design argmd wa introduced a a new paradigm for studying argumentation among self interested agent using game theoretic technique preliminary result showed a condition under which a direct mechanism based on dung s grounded semantics is strategy proof i e truth enforcing but these early result dealt with a highly restricted form of agent preference and assumed agent can only hide but not lie about argument in this paper we characterise strategy proofness under grounded semantics for a more realistic preference class namely focal argument we also provide the first analysis of the case where agent can lie 
we consider query answering on description logic dl ontology with dboxes where a dbox is a set of assertion on individual involving atomic concept and role called dbox predicate the extension of a dbox predicate is exactly defined in every interpretation by the content of the dbox i e a dbox faithfully represents a database whose table name are the dbox predicate and the tuples are the dbox assertion our goal are i to find out whether the answer to a given query are solely determined by the dbox predicate and if so ii to find a rewriting of the query in term of them the resulting query can then be efficiently evaluated using standard database technology we have that i can be reduced to entailment checking and ii can be reduced to finding an interpolant we present a procedure for computing interpolants in the dl alc with general tboxes we extend the procedure with standard tableau optimisation and we discus abduction a a technique for amending ontology to gain definability of query of interest 
particle physic experiment like the large hadron collider in geneva can generate thousand of data point listing detected particle reaction an important learning task is to analyze the reaction data for evidence of conserved quantity and hidden particle this task involves latent structure in two way first hypothesizing hidden quantity whose conservation determines which reaction occur and second hypothesizing the presence of hidden particle we model this problem in the classic linear algebra framework of automated scientific discovery due to vald s p rez zytkow and simon where both reaction data and conservation law are represented a matrix we introduce a new criterion for selecting a matrix model for reaction data find hidden particle and conserved quantity that rule out a many interaction among the nonhidden particle a possible a polynomial time algorithm for optimizing this criterion is based on the new theorem that hidden particle are required if and only if the smith normal form of the reaction matrix r contains entry other than or to our knowledge this is the first application of smith matrix decomposition to a problem in ai using data from particle accelerator we compare our algorithm to the main model of particle in physic known a the standard model our algorithm discovers conservation law that are equivalent to those in the standard model and indicates the presence of a hidden particle the electron antineutrino in accordance with the standard model 
due to the non stationary environment learning in multi agent system is a challenging problem this paper first introduces a new gradient based learning algorithm augmenting the basic gradient ascent approach with policy prediction we prove that this augmentation result in a stronger notion of convergence than the basic gradient ascent that is strategy converge to a nash equilibrium within a restricted class of iterated game motivated by this augmentation we then propose a new practical multi agent reinforcement learning marl algorithm exploiting approximate policy prediction empirical result show that it converges faster and in a wider variety of situation than state of the art marl algorithm an agent can only observe the immediate reward after selecting and performing an action in this paper we first propose a new gradient based algorithm that augments a basic gradient ascent algorithm with policy prediction the key idea behind this algorithm is that a player adjusts it strategy in response to forecasted stra tegies of the other player instead of their current one we analyze this algorithm in two person two action generalsum iterated game and prove that if at least one player us this algorithm if not both assume the other player us the standard gradient ascent algorithm then player strate gy will converge to a nash equilibrium like other marl algorithm besides the common assumption this algorithm also ha additional requirement that a player know the other player s strategy and current strategy gradient or p ayoff matrix so that it can forecast the other player s strate gy motivated by our theoretical convergence analysis we then propose a new practical marl algorithm exploiting the idea of policy prediction our practical algorithm only requires an agent to observe it reward when choosing a given action we show that our practical algorithm can learn an optimal policy when other player use stationary policy empirical result show that it converges in more situation than that covered by our formal analysis compared to stateof the art marl algorithm wpl abdallah and lesser wolf phc bowling and veloso and gigawolf bowling it empirically converges faster and in a wider variety of situation in the remainder of this paper we first review the basic gradient ascent algorithm and then introduce our gradientbased algorithm with policy prediction followed by it theoretical analysis we then describe a new practical marl algorithm and evaluate it in benchmark game distributed task allocation problem and network routing notation 
a autonomous agent proliferate in the real world both in software and robotic setting they will increasingly need to band together for cooperative activity with previously unfamiliar teammate in such ad hoc team setting team strategy cannot be developed a priori rather an agent must be prepared to cooperate with many type of teammate it must collaborate without pre coordination this paper challenge the ai community to develop theory and to implement prototype of ad hoc team agent it defines the concept of ad hoc team agent specifies an evaluation paradigm and provides example of possible theoretical and empirical approach to challenge the goal is to encourage progress towards this ambitious newly realistic and increasingly important research goal 
a method for expressive melody synthesis is presented seeking to capture the prosodic stress and directional element of musical interpretation an expressive performance is represented a a notelevel annotation classifying each note according to a small alphabet of symbol describing the role of the note within a larger context an audio performance of the melody is represented in term of two time varying function describing the evolving frequency and intensity a method is presented that transforms the expressive annotation into the frequency and intensity function thus giving the audio performance the problem of expressive rendering is then cast a estimation of the most likely sequence of hidden variable corresponding to the prosodic annotation example are presented on a dataset of around folk like melody realized both from hand marked and estimated annotation 
detecting abnormal activity from sensor reading is an important research problem in activity recognition a number of different algorithm have been proposed in the past to tackle this problem many of the previous state based approach suffer from the problem of failing to decide the appropriate number of state which are difficult to find through a trial and error approach in real world application in this paper we propose an accurate and flexible framework for abnormal activity recognition from sensor reading that involves le human tuning of model parameter our approach first applies a hierarchical dirichlet process hidden markov model hdp hmm which support an infinite number of state to automatically find an appropriate number of state we incorporate a fisher kernel into the one class support vector machine ocsvm model to filter out the activity that are likely to be normal finally we derive an abnormal activity model from the normal activity model to reduce false positive rate in an unsupervised manner our main contribution is that our proposed hdp hmm model can decide the appropriate number of state automatically and that by incorporating a fisher kernel into the ocsvm model we can combine the advantage from generative model and discriminative model we demonstrate the effectiveness of our approach by using several real world datasets to test our algorithm s performance 
ab initio protein structure prediction is an important problem for which several algorithm have been developed algorithm differ by how they represent d protein conformation on lattice off lattice coarse grain or fine grain model by the energy model they consider and whether they are heuristic or exact algorithm this paper present a local search algorithm to find the native state for the hydrophobic polar hp model on the face centered cubic fcc lattice i e a self avoiding walk on the fcc lattice with maximum number of h h contact the algorithm relies on a randomized structured initialization a novel fitness function to guide the search and efficient data structure to obtain self avoiding walk experimental result on benchmark instance show the efficiency and excellent performance of our algorithm and illustrate the biological pertinence of the fcc lattice 
we show that tool from circuit complexity can be used to study decomposition of global constraint in particular we study decomposition of global constraint into conjunctive normal form with the property that unit propagation on the decomposition enforces the same level of consistency a a specialized propagation algorithm we prove that a constraint propagator ha a a polynomial size decomposition if and only if it can be computed by a polynomial size monotone boolean circuit lower bound on the size of monotone boolean circuit thus translate to lower bound on the size of decomposition of global constraint for instance we prove that there is no polynomial sized decomposition of the domain consistency propagator for the alldifferent constraint 
we design a representation based on the situation calculus to facilitate development maintenance and elaboration of very large taxonomy of action this representation lead to more compact and modular basic action theory bat for reasoning about action than currently possible we compare our representation with reiter s bat and prove that our representation inherits all useful property of his bat moreover we show that our axiom can be more succinct but extended reiter s regression can still be used to solve the projection problem this is the problem of whether a given logical expression will hold after executing a sequence of action we also show that our representation ha significant computational advantage for taxonomy of action that can be represented a finitely branching tree the regression operator can work exponentially faster with our theory than it work with reiter s bat finally we propose general guideline on how a taxonomy of action can be constructed from the given set of effect axiom in a domain 
usually a voting rule or correspondence requires agent to give their preference a linear order however in some case it is impractical for an agent to give a linear order over all the alternative it ha been suggested to let agent submit partial order instead then given a profile of partial order and a candidate c two important question arise first is c guaranteed to win and second is it still possible for c to win these are the necessary winner and possible winner problem respectively we consider the setting where the number of alternative is unbounded and the vote are unweighted we prove that for copeland maximin bucklin and ranked pair the possible winner problem is np complete also we give a sufficient condition on scoring rule for the possible winner problem to be np complete borda satisfies this condition we also prove that for copeland and ranked pair the necessary winner problem is conp complete all the hardness result hold even when the number of undetermined pair in each vote is no more than a constant we also present polynomial time algorithm for the necessary winner problem for scoring rule maximin and bucklin 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
growing email volume cause flooded inboxes and swelled email archive making search and new email processing difficult while email have rich metadata such a recipient and folder suitable for creating filtered view it is often difficult to choose appropriate filter for new inbox message without first examining message in this work we consider a system that automatically suggests relevant view filter to the user for the currently viewed message we propose several ranking algorithm for suggesting useful filter our work suggests that such system quickly filter group of inbox message and find message more easily during search 
the objective of this paper is to study the existing method for unsupervised object recognition and image categorization and propose a model that can learn directly from the output of image search engine e g google image bypassing the need to manually collect large quantity of training data this model can then be used to refine the quality of the image search or to search through other source of image this integrated scheme ha been implemented and optimized to be used in the semantic robot vision challenge a a new test bed for research in the area of image understanding and knowledge retrieval in large unstructured image database 
dialogue based question answering qa is a highly complex task that brings together a qa system including various natural language processing component i e component for question classification information extraction and retrieval with dialogue system for effective and natural communication the dialogue based access is difficult to establish when the qa system in use is complex and combine many different answer service with different quality and access characteristic for example some question are processed by opendomain qa service with a broad coverage others should be processed by using a domain specific instance ontology for more reliable answer different answer service may change their characteristic over time and the dialogue reaction model have to be updated according to that to solve this problem we developed introspective method to integrate adaptable model of the answer service we evaluated the impact of the learned model on the dialogue performance i e whether the adaptable model can be used for a more convenient dialogue formulation process we show significant effectiveness improvement in the resulting dialogue when using the machine learning ml model example are provided in the context of the generation of system initiative feedback to user question and answer a provided by heterogeneous information service 
the search of a precise measure of what hardness of sat instance mean for state of the art solver is a relevant research question among others the space complexity of treelike resolution also called hardness the minimal size of strong backdoor and of cycle cutsets and the treewidth can be used for this purpose we propose the use of the tree like space complexity a a solid candidate to be the best measure for solver based on dpll to support this thesis we provide a comparison with the other mentioned measure we also conduct an experimental investigation to show how the proposed measure characterizes the hardness of random and industrial instance 
we introduce the problem of zero data learning where a model must generalize to class or task for which no training data are available and only a description of the class or task are provided zero data learning is useful for problem where the set of class to distinguish or task to solve is very large and is not entirely covered by the training data the main contribution of this work lie in the presentation of a general formalization of zero data learning in an experimental analysis of it property and in empirical evidence showing that generalization is possible and significant in this context the experimental work of this paper address two classification problem of character recognition and a multitask ranking problem in the context of drug discovery finally we conclude by discussing how this new framework could lead to a novel perspective on how to extend machine learning towards ai where an agent can be given a specification for a learning problem before attempting to solve it with very few or even zero example 
an important problem in computational social choice concern whether it is possible to prevent manipulation of voting rule by making it computationally intractable to answer this a key question is how frequently voting rule are manipulable we xia and conitzer recently defined the class of generalized scoring rule gsr and characterized the frequency of manipulability for such rule we showed by example that most common rule seem to fall into this class however no natural axiomatic characterization of the class wa given leaving the possibility that there are natural rule to which these result do not apply in this paper we characterize the class of gsr based on two natural property it is equal to the class of rule that are anonymous and finitely locally consistent generalized scoring rule also have other us in computational social choice for these us the order of the gsr the dimension of it score vector is important our characterization result implies that the order of a gsr is related to the minimum number of locally consistent component of the rule we proceed to bound the minimum number of locally consistent component for some common rule 
many path planning algorithm leverage a to determine optimal path however when an actor deviate from the optimal path a typical application of a executes a new search from the deviation point to the goal this approach redundantly calculates path that may have been examined during the initial search rather than leveraging previous information we introduce plan b a pba which us a for the initial search and substantially reduces the number of searched state during all subsequent search while incurring minimal space overhead pba not only remembers certain state it ha examined it proactively creates solution path for the most likely deviation in our experiment pba search only of the a search space when recovering from execution error by storing a limited amount of search history 
optimization in graphical model is an important problem which ha been studied in many ai framework such a weighted csp maximum satisfiability or probabilistic network by identifying conditionally independent subproblems which are solved independently and whose optimum is cached recent branch and bound algorithm offer better asymptotic time complexity but the locality of bound induced by decomposition often hamper the practical effect of this result because subproblems are often uselessly solved to optimality following the russian doll search rds algorithm a possible approach to overcome this weakness is to inductively solve a relaxation of each subproblem to strengthen bound the algorithm obtained generalizes both rds and tree decomposition based algorithm such a btd or and or branch and bound we study it efficiency on different problem closing a very hard frequency assignment instance which ha been open for more than year 
recently algorithm for computing game theoretic solution have been deployed in real world security application such a the placement of checkpoint and canine unit at los angeles international airport these algorithm assume that the defender security personnel can commit to a mixed strategy a so called stackelberg model a pointed out by kiekintveld et al kiekintveld et al in these application generally multiple resource need to be assigned to multiple target resulting in an exponential number of pure strategy for the defender in this paper we study how to compute optimal stackelberg strategy in such game showing that this can be done in polynomial time in some case and is np hard in others 
a contract algorithm is an algorithm which is given a part of it input a specified amount of allowable computation time in contrast interruptible algorithm may be interrupted throughout their execution at which point they must report their current solution simulating interruptible algorithm by mean of schedule of execution of contract algorithm in parallel processor is a well studied problem with significant application in ai in the classical case the interruption are hard deadline in which a solution must be reported immediately at the time the interruption occurs in this paper we study the more general setting of scheduling contract algorithm at the presence of soft deadline this is motivated by the observation of practitioner that soft deadline are a common an occurrence a hard deadline if not more common in our setting at the time t of interruption the algorithm is given an additional window of time w t c t to continue the contract or indeed start a new contract for some fixed constant c we explore this variation using the acceleration ratio which is the canonical measure of performance for these schedule and derive schedule of optimal acceleration ratio for all function w 
many real world problem are characterized by complex relational structure which can be succinctly represented in first order logic however many relational inference algorithm proceed by first fully instantiating the first order theory and then working at the propositional level the applicability of such approach is severely limited by the exponential time and memory cost of propositionalization singla and domingo addressed this by developing a lazy version of the walksat algorithm which ground atom and clause only a needed in this paper we generalize their idea to a much broader class of algorithm including other type of sat solver and probabilistic inference method like mcmc lazy inference is potentially applicable whenever variable and function have default value i e a value that is much more frequent than the others in relational domain the default is false for atom and true for clause we illustrate our framework by applying it to mc sat a state of the art mcmc algorithm experiment on a number of real world domain show that lazy inference reduces both space and time by several order of magnitude making probabilistic relational inference applicable in previously infeasible domain 
in this paper we present a method that us web photo for measuring frame interestingness of a travel video web photo collection such a those on flickr tend to contain interesting image because their image are more carefully taken composed and selected because these photo have already been chosen a subjectively interesting they serve a evidence that similar image are also interesting our idea is to leverage these web photo to measure the interestingness of video frame specifically we measure the interestingness of each video frame according to it similarity to web photo the similarity is defined based on the scene content and composition we characterize the scene content using scale invariant local feature specifically sift keypoints we characterize composition by feature distribution accordingly we measure the similarity between a web photo and a video frame based on the co occurrence of the sift feature and the similarity between their spatial distribution interestingness of a video frame is measured by considering how many photo it is similar to and how similar it is to them our experiment on measuring frame interestingness of video from youtube using photo from flickr show the initial success of our method 
query recommendation is considered an effective assistant in enhancing keyword based query in search engine and web search software conventional approach to query recommendation ha been focused on query term based analysis over the user access log in this paper we argue that utilizing the connectivity of a query url bipartite graph to recommend relevant query can significantly improve the accuracy and effectiveness of the conventional query term based query recommendation system we refer to the query url bipartite based query recommendation approach a qubic the qubic approach ha two unique characteristic first instead of operating on the original bipartite graph directly using biclique based approach or graph clustering we extract an affinity graph of query from the initial query url bipartite graph the affinity graph consists of only query a it vertex and it edge are weighted according to a query url vector based similarity distance measure by utilizing the query affinity graph we are able to capture the propagation of similarity from query to query by inducing an implicit topical relatedness between query we devise a novel rank mechanism for ordering the related query based on the merging distance of a hierarchical agglomerative clustering we compare our proposed ranking algorithm with both na ve ranking that us the query url similarity measure directly and the single linkage based ranking method in addition we make it possible for user to interactively participate in the query recommendation process to bridge the gap between the determinacy of actual similarity value and the indeterminacy of user information need allowing the list of related query to be changed from user to user and query to query thus personalizing the query recommendation on demand the experimental result from two query collection demonstrate the effectiveness and feasibility of our approach 
the goal of testing is to discriminate between multiple hypothesis about a system for example different fault diagnosis by applying input pattern and verifying or falsifying the hypothesis from the observed output definitely discriminating test ddt are those input pattern that are guaranteed to discriminate between different hypothesis of non deterministic system finding ddt is important in practice but can be very expensive p p complete even more challenging is the problem of finding a ddt that minimizes the cost of the testing process i e an input pattern that can be most cheaply enforced and that is a ddt this paper address both problem we show how we can transform a given problem into a boolean structure in decomposable negation normal form dnnf and extract from it a boolean formula whose model correspond to ddt this allows u to harness recent advance in both knowledge compilation and satisfiability for efficient and scalable ddt computation in practice furthermore we show how we can generate a dnnf structure compactly encoding all ddt of the problem and use it to obtain a cost optimal ddt in time linear in the size of the structure experimental result from a realworld application show that our method can compute ddt in le than second for instance that were previously intractable and cost optimal ddt in le than second where previous approach could not even compute an arbitrary ddt 
the study of random instance of np complete and conp complete problem ha had much impact on our understanding of the nature of hard problem in this work we initiate an effort to extend this line of research to random instance of intractable parameterized problem we propose random model for a representative intractable parameterized problem the weighted d cnf satisfiability and it generalization to the constraint satisfaction problem the exact threshold for the phase transition of the proposed model is determined lower bound on the time complexity of variant of the dpll algorithm for these parameterized problem are also established in particularly we show that random instance of the weighted cnf satisfiability already an intractable parameterized problem are typically easy in both of the satisfiable and unsatisfiable region by exploiting an interesting connection between the unsatisfiability of a weighted cnf formula and the existence of a hamiltonian cycle like global structure 
recently online reputation mechanism have been proposed that reward agent for honest feedback about product and service with fixed quality many real world setting however are inherently dynamic a an example consider a web service that wish to publish the expected download speed of a file mirrored on different server site in contrast to the model of miller resnick and zeckhauser and of jurca and faltings the quality of the service e g a server s available bandwidth change over time and future agent are solely interested in the present quality level we show that hidden markov model hmm provide natural generalization of these static model and design a payment scheme that elicits honest report from the agent after they have experienced the quality of the service 
cooperative problem solving with resource constraint are important in practical multi agent system resource constraint are necessary to handle practical problem including distributed task scheduling with limited resource availability a dedicated framework called resource constrained dcop rcdcop ha recently been proposed rcdcop model objective function and resource constraint separately a resource constraint is an n ary constraint that represents the limit on the number of resource of a given type available to agent previous research addressing rcdcops employ the adopt algorithm which is an efficient solver for dcops an important graph structure for adopt is the pseudo tree for constraint network a pseudo tree implies a partial ordering of variable in this variable ordering n ary constrained variable are placed on a single path of the tree therefore resource constraint that have large arity augment the depth of the pseudo tree this also reduces the parallelism and therefore the efficiency of adopt in this paper we propose another version of the adopt algorithm for rcdcop using a pseudo tree that is generated ignoring resource constraint the proposed method reduces the previous limitation in the construction of rcdcop pseudo tree the key idea of our work are a follows i the pseudo tree is generated ignoring resource constraint ii virtual variable are introduced representing the usage of resource these virtual variable are used to share resource among sub tree however the addition of virtual variable increase the search space to handle this problem influence of placement of virtual variable resource constraint in the pseudo tree is considered moreover the search is pruned using the bound defined by the resource constraint if possible these idea are used to extend adopt the efficiency of our technique depends on the class of problem being considered and we describe the obtained experimental result 
runtime commitment verification is an important open issue in multiagent research to address it we build on yolum and singh s formalization of commitment operation on chittaro and montanari s cached event calculus and on the sciff abductive logic programming proof procedure we propose a framework consisting of a declarative and compact language to express the domain knowledge and a reactive and complete procedure to track the status of commitment effectively producing provably sound and irrevocable answer 
search method based on monte carlo simulation have recently led to breakthrough performance improvement in difficult game playing domain such a go and general game playing monte carlo random walk mrw planning applies monte carlo idea to deterministic classical planning in the forward chaining planner arvand monte carlo random walk are used to explore the local neighborhood of a search state for action selection in contrast to the stochastic local search approach used in the recent planner identidem random walk yield a larger and unbiased sample of the search neighborhood and require state evaluation only at the endpoint of each walk on ipc competition problem the performance of arvand is competitive with state of the art system 
we consider the problem of sequential prediction and change detection that arise often in interactive application a semi automatic predictor is applied to a time series and is expected to make proper prediction and request new human input when change point are detected motivated by the transductive support vector machine vapnik we propose an online framework that naturally address these problem in a unified manner our empirical study with a synthetic dataset and a road tracking dataset demonstrates the efficacy of the proposed approach 
in this paper we propose a system to solve a language game called guillotine which requires a player with a strong cultural and linguistic background knowledge the player observes a set of five word generally unrelated to each other and in one minute she ha to provide a sixth word semantically connected to the others several knowledge source such a a dictionary and a set of proverb have been modeled and integrated in order to realize a knowledge infusion process into the system the main motivation for designing an artificial player for guillotine is the challenge of providing the machine with the cultural and linguistic background knowledge which make it similar to a human being with the ability of interpreting natural language document and reasoning on their content experiment carried out showed promising result and both the knowledge source modeling and the reasoning mechanism implementing a spreading activation algorithm to find out the solution seem to be appropriate we are convinced that the approach ha a great potential for other more practical application besides solving a language game such a semantic search 
interoperability requires the resolution of syntactic and semantic variation among system data model to address this problem we developed the intelligent mapping toolkit imt which employ a distributed multi agent architecture to enable the mixed initiative mapping of metadata and instance this architecture includes a novel federation of service encapsulated matching agent that leverage case based reasoning method we recently used the imt matching service to develop several domain specific search application in addition to the imt mapping application 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
typical graph theoretic approach for semi supervised classification infer label of unlabeled instance with the help of graph laplacians founded on the spectral decomposition of the graph laplacian this paper learns a kernel matrix via minimizing the leave one out classification error on the labeled instance to this end an efficient algorithm is presented based on linear programming resulting in a transductive spectral kernel the idea of our algorithm stem from regularization methodology and also ha a nice interpretation in term of spectral clustering a simple classifier can be readily built upon the learned kernel which suffices to give prediction for any data point aside from those in the available dataset besides this usage the spectral kernel can be effectively used in tandem with conventional kernel machine such a svms we demonstrate the efficacy of the proposed algorithm through experiment carried out on challenging classification task 
this paper examines by argument the dynamic of sequence of behavioural choice made when non cooperative restricted memory agent learn in partially observable stochastic game these sequence of combined agent strategy joint policy can be thought of a a walk through the space of all possible joint policy we argue that this walk while containing random element is also driven by each agent s drive to improve their current situation at each point and posit a learning pressure field across policy space to represent this drive different learning choice may skew this learning pressure and affect the simultaneous joint learning of multiple agent 
based on information theory optimal feature selection should be carried out by searching markov blanket in this paper we formally analyze the current markov blanket discovery approach for support vector machine and propose to discover markov blanket by performing a fast heuristic bayesian network structure learning we give a sufficient condition that our approach will improve the performance two major factor that make it prohibitive for learning bayesian network from high dimensional data set are the large search space and the expensive cycle detection operation we propose to restrict the search space by only considering the promising candidate and detect cycle using an online topological sorting method experimental result show that we can efficiently reduce the feature dimensionality while preserving a high degree of classification accuracy 
beside impressive progress made by sat solver over the last ten year only few work tried to understand why conflict directed clause learning algorithm cdcl are so strong and efficient on most industrial application we report in this work a key observation of cdcl solver behavior on this family of benchmark and explain it by an unsuspected side effect of their particular clause learning scheme this new paradigm allows u to solve an important still open question how to designing a fast static accurate and predictive measure of new learnt clause pertinence our paper is followed by empirical evidence that show how our new learning scheme improves state of the art result by an order of magnitude on both sat and unsat industrial problem 
random forest have been shown to perform very well in propositional learning forf is an upgrade of random forest for relational data in this paper we investigate shortcoming of forf and propose an alternative algorithm r f for generating random forest over relational data r f employ randomly generated relational rule a fully self contained boolean test inside each node in a tree and thus can be viewed a an instance of dynamic propositionalization the implementation of r f allows for the simultaneous or parallel growth of all the branch of all the tree in the ensemble in an efficient shared but still single threaded way experiment favorably compare r f to both forf and the combination of static propositionalization together with standard random forest various strategy for tree initialization and splitting of node a well a resulting ensemble size diversity and computational complexity of r f are also investigated 
fisher score and laplacian score are two popular feature selection algorithm both of which belong to the general graph based feature selection framework in this framework a feature subset is selected based on the corresponding score subset level score which is calculated in a trace ratio form since the number of all possible feature subset is very huge it is often prohibitively expensive in computational cost to search in a brute force manner for the feature subset with the maximum subset level score instead of calculating the score of all the feature subset traditional method calculate the score for each feature and then select the leading feature based on the rank of these feature level score however selecting the feature subset based on the feature level score cannot guarantee the optimum of the subset level score in this paper we directly optimize the subset level score and propose a novel algorithm to efficiently find the global optimal feature subset such that the subset level score is maximized extensive experiment demonstrate the effectiveness of our proposed algorithm in comparison with the traditional method for feature selection 
this paper highlight the crucial role that modern machine learning technique can play in the optimization of treatment strategy for patient with chronic disorder in particular we focus on the task of optimizing a deep brain stimulation strategy for the treatment of epilepsy the challenge is to choose which stimulation action to apply a a function of the observed eeg signal so a to minimize the frequency and duration of seizure we apply recent technique from the reinforcement learning literature namely fitted q iteration and extremely randomized tree to learn an optimal stimulation policy using labeled training data from animal brain tissue our result show that these method are an effective mean of reducing tile incidence of seizure while also minimizing the amount ot stimulation applied if these result carry over to the human model of epilepsy the impact for patient will be substantial 
many robotic path planning application such a search and rescue involve uncertain environment with complex dynamic that can be only partially observed when selecting the best subset of observation location subject to constrained resource such a limited time or battery capacity it is an important problem to trade off exploration gathering information about the environment and exploitation using the current knowledge about the environment most effectively for efficiently observing these environment even the nonadaptive setting where path are planned before observation are made is np hard and ha been subject to much research in this paper we present a novel approach to adaptive informative path planning that address this exploration exploitation tradeoff our approach is nonmyopic i e it plan ahead for possible observation that can be made in the future we quantify the benefit of exploration through the adaptivity gap between an adaptive and a nonadaptive algorithm in term of the uncertainty in the environment exploiting the submodularity a diminishing return property and locality property of the objective function we develop an algorithm that performs provably near optimally in setting where the adaptivity gap is small in case of large gap we use an objective function that simultaneously optimizes path for exploration and exploitation we also provide an algorithm to extend any single robot algorithm for adaptive informative path planning to the multi robot setting while approximately preserving the theoretical guarantee of the single robot algorithm we extensively evaluate our approach on a search and rescue domain and a scientific monitoring problem using a real robotic system 
recent advance in classical planning have used the sa formalism and several effective heuristic have been developed based on the sa formalism comparing to the traditional strip adl formalism sa is capable of capturing vital information such a domain transition structure and causal dependency in this paper we propose a new sa based incomplete planning approach instead of using sa to derive heuristic within a heuristic search planner we directly search in domain transition graph dtgs and causal graph cgs derived from the sa formalism the new method is efficient because the sa representation is often much more compact than strip the cgs and dtgs provide rich information of domain structure that can effectively guide the search towards solution experimental result show strong performance of the proposed planner on recent international planning competition domain 
recently ferraris lee and lifschitz presented a general definition of a stable model that is similar to the definition of circumscription and can even be characterized in term of circumscription in this paper we show the opposite direction which is how to turn circumscription into the general stable model semantics and based on this how to turn circumscriptive event calculus into answer set program the reformulation of the event calculus in answer set programming allows answer set solver to be applied to event calculus reasoning handling more expressive reasoning task than the current sat based approach our experiment also show clear computational advantage of the answer set programming approach 
formal approach to modelling argumentation provide way to present argument and counterargument and to evaluate which argument are in a formal sense warranted while these proposal allow for evaluating object level argument and counterargument they do not give sufficient consideration to evaluating the proponent of the argument yet in everyday life we consider both the content of an argument and it proponent so if we do not trust a proponent we may choose to not trust their argument or if we are faced with an argument that we do not have the expertise to ass for example when deciding whether to agree to having a particular surgical operation we tend to agree to an argument by someone who is an expert in general we see that for each argument we need to determine the appropriateness of the proponent for it so for an argument about our health our doctor is normally an appropriate proponent but for an argument about our investment our doctor is normally not an appropriate proponent in this way a celebrity is rarely an appropriate proponent for an argument and a liar is not necessarily an inappropriate proponent for an argument in this paper we provide a logic based framework for evaluating argument in term of the appropriateness of the proponent 
this paper describes how to learn hierarchical task network htns in nondeterministic planning domain where action may have multiple possible outcome we discus several desired property that guarantee that the resulting htns will correctly handle the nondeterminism in the domain we developed a new learning algorithm called htn makernd that exploit these property we implemented htn makernd in the recently proposed htn maker system a goal regression based htn learning approach in our theoretical study we show that htn makernd soundly produce htn planning knowledge in low order polynomial time despite the nondeterminism in our experiment with two nondeterministic planning domain nd shop a well known htn planning algorithm for nondeterministic domain significantly outperformed in some case by about order of magnitude the well known planner mbp using the learned htns 
electrical power system play a critical role in spacecraft and aircraft this paper discus our development of a diagnostic capability for an electrical power system testbed adapt using prohabilistic technique in the context of adapt we present two challenge regarding modelling and real time performance often encountered in real world diagnostic application to meet the modelling challenge we discus our novel high level specification language which support autogeneration of bayesian network to meet the real time challenge we compile bayesian network into arithmetic circuit arithmetic circuit typically have small footprint and are optimized for the real time avionics system found in spacecraft and aircraft using our approach we present how bayesian network with over node are auto generated and then compiled into arithmetic circuit using real world data from adapt a well a simulated data we obtain average inference time smaller than one millisecond when computing diagnostic query using arithmetic circuit that model our real world electrical power system 
this paper address the question of how statistical learning algorithm can be integrated into a larger ai system both from a practical engineering perspective and from the perspective of correct representation learning and reasoning our goal is to create an integrated intelligent system that can combine observed fact hand written rule learned rule and learned classifier to perform joint learning and reasoning our solution which ha been implemented in the calo system integrates multiple learning component with a markov logic inference engine so that the component can benefit from each other s prediction we introduce two design of the learning and reasoning layer in calo the mpe architecture and the marginal probability architecture the architecture interface and algorithm employed in our two design are described followed by experimental evaluation of the performance of the two design we show that by integrating multiple learning component through markov logic the performance of the system can be improved and that the marginal probability architecture performs better than the mpe architecture 
the computing industry is currently facing a major architectural shift extra computing power is not coming anymore from higher processor frequency but from a growing number of computing core and processor for ai and constraint solving in particular this raise the question of how to scale current solving technique to massively parallel architecture while prior work focus mostly on small scale parallel constraint solving we conduct the first study on scalability of constraint solving on processor and beyond in this paper we propose technique that are simple to apply and show empirically that they scale surprisingly well these technique establish a performance baseline for parallel constraint solving technology against which more sophisticated parallel algorithm need to compete in the future 
two dimensional plot d in digital document on the web are an important source of information that is largely under utilized in this paper we outline how data and text can be extracted automatically from these d plot thus eliminating a time consuming manual process our information extraction algorithm identifies the ax of the figure extract text block like ax label and legend and identifies data point in the figure it also extract the unit appearing in the ax label and segment the legend to identify the different line in the legend the different symbol and their associated text explanation our algorithm also performs the challenging task of separating out overlapping text and data point effectively our experiment indicate that these technique are computationally efficient and provide acceptable accuracy 
recognizing activity in real world video is a difficult problem exacerbated by background clutter change in camera angle zoom and rapid camera movement large corpus of labeled video can be used to train automated activity recognition system but this requires expensive human labor and time this paper explores how closed caption that naturally accompany many video can act a weak supervision that allows automatically collecting labeled data for activity recognition we show that such an approach can improve activity retrieval in soccer video our system requires no manual labeling of video clip and need minimal human supervision we also present a novel caption classifier that us additional linguistic information to determine whether a specific comment refers to an ongoing activity we demonstrate that combining linguistic analysis and automatically trained activity recognizers can significantly improve the precision of video retrieval 
we propose a new webpage ranking algorithm which is personalized our idea is to rely on the attention time spent on a document by the user a the essential clue for producing the user oriented webpage ranking the prediction of the attention time of a new webpage is based on the attention time of other previously browsed page by this user to acquire the attention time of the latter webpage we developed a browser plugin which is able to record the time a user spends reading a certain webpage and then automatically send that data to a server once the user attention time is acquired we calibrate it to account for potential repetitive occurrence of the webpage before using it in the prediction process after the user s attention time of a collection of document are known our algorithm can predict the user s attention time of a new document through document content similarity analysis which is applied to both text and image we evaluate the webpage ranking result from our algorithm by comparing them with the one produced by google s pagerank algorithm 
while the most accurate word sense disambiguation system are built using supervised learning from sense tagged data scaling them up to all word of a language ha proved elusive since preparing a sense tagged corpus for all word of a language is time consuming and human labor intensive in this paper we propose and implement a completely automatic approach to scale up word sense disambiguation to all word of english our approach relies on english chinese parallel corpus english chinese bilingual dictionary and automatic method of finding synonym of chinese word no additional human sense annotation or word translation are needed we conducted a large scale empirical evaluation on more than noun token in english text annotated in ontonotes based on it coarsegrained sense inventory the evaluation result show that our approach is able to achieve high accuracy outperforming the first sense baseline and coming close to a prior reported approach that requires manual human effort to provide chinese translation of english sens 
we describe an exact dynamic programming update for constrained partially observable markov decision process cpomdps state of the art exact solution of unconstrained pomdps relies on implicit enumeration of the vector in the piecewise linear value function and pruning operation to obtain a minimal representation of the updated value function in dynamic programming for cpomdps each vector take two valuation one with respect to the objective function and another with respect to the constraint function the dynamic programming update consists of finding for each belief state the vector that ha the best objective function valuation while still satisfying the constraint function whereas the pruning operation in an unconstrained pomdp requires solution of a linear program the pruning operation for cpomdps requires solution of a mixed integer linear program 
the efficiency of optimal planning algorithm based on heuristic search crucially depends on the accuracy of the heuristic function used to guide the search often we are interested in domain independent heuristic for planning in order to ass the limitation of domain independent heuristic planning we analyze the in accuracy of common domain independent planning heuristic in the ipc benchmark domain for a selection of these domain we analytically investigate the accuracy of the h heuristic the hm family of heuristic and certain additive pattern database heuristic compared to the perfect heuristic h whereas h and additive pattern database heuristic usually return cost estimate proportional to the true cost non additive hm and nonadditive pattern database heuristic can yield result underestimating the true cost by arbitrarily large factor 
particle filtering algorithm can be used for the monitoring of dynamic system with continuous state variable and without any constraint on the form of the probability distribution the dimensionality of the problem remains a limitation of these approach due to the growing number of particle required for the exploration of the state space computer vision problem such a d motion tracking are an example of complex monitoring problem which have a high dimensional state space and observation function with high computational cost in this article we focus on reducing the required number of particle in the case of monitoring task where the state vector and the observation function can be factored we introduce a particle filtering algorithm based on the dynamic bayesian network dbn formalism which take advantage of a factored representation of the state space for efficiently weighting and selecting the particle we illustrate the approach on a simulated and a realworld d motion tracking task 
cp network have been proposed a a simple and intuitive graphical tool for representing conditional ceteris paribus preference statement over the value of a set of variable while the problem of reasoning with cp network ha been receiving some attention there are very few work that address the problem of learning cp network in this work we investigate the task of learning cp network given access to a set of pairwise comparison we first prove that the learning problem is intractable even under several simplifying assumption we then present an algorithm that under certain assumption about the observed pairwise comparison identifies a cp network that entail these comparison we finally show that the proposed algorithm is a pac learner and thus that the cp network it induces accurately predict the user s preference on previously unseen situation 
there are two basic approach to generalize the propagation mechanism of the two player minimax search algorithm to multi player or more game the maxn algorithm and the paranoid algorithm the main shortcoming of these approach is that their strategy is fixed in this paper we suggest a new approach called mp mix that dynamically change the propagation strategy based on the player relative strength between maxn paranoid and a newly presented offensive strategy in addition we introduce the opponent impact factor for multi player game which measure the player ability to impact their opponent score and show it relation to the relative performance of our new mp mix strategy experimental result show that mp mix outperforms all other approach under most circumstance 
this paper deal with preference representation and aggregation in the context of multiattribute utility theory we consider a set of alternative having a combinatorial structure we assume that preference are compactly represented by graphical utility model derived from generalized additive decomposable gai utility function such function enable to model interaction between attribute while preserving some decomposability property we address the problem of finding a compromise solution from several gai utility representing different point of view on the alternative this scheme can be applied both to multicriteria decision problem and to collective decision making problem over combinatorial domain we propose a procedure using graphical model for the fast determination of a pareto optimal solution achieving a good compromise between the conflicting utility the procedure relies on a ranking algorithm enumerating solution according to the sum of all the gai utility until a boundary condition is reached numerical experiment are provided to highlight the practical efficiency of our procedure 
we present a real time strategy rts game ai agent that integrates multiple specialist component to play a complete game based on an analysis of how skilled human player conceptualize rts gameplay we partition the problem space into domain of competence seen in expert human play this partitioning help u to manage and take advantage of the large amount of sophisticated domain knowledge developed by human player we present result showing that incorporating expert high level strategic knowledge allows our agent to consistently defeat established scripted ai player in addition this work lay the foundation to incorporate tactic and unit micromanagement technique developed by both man and machine 
the ultimate goal of human robot interaction is to enable the robot to seamlessly communicate with a human in a natural human like fashion most work in this field concentrate on the speech interpretation and gesture recognition side assuming that a propositional scene representation is available le work wa dedicated to the extraction of relevant scene structure that underlies these proposition a a consequence most approach are restricted to place recognition or simple table top setting and do not generalize to more complex room setup in this paper we propose a hierarchical spatial model that is empirically motivated from psycholinguistic study using this model the robot is able to extract scene structure from a time of flight depth sensor and adjust it spatial scene representation by taking verbal statement about partial scene aspect into account without assuming any pre known model of the specific room we show that the system aligns it sensor based room representation to a semantically meaningful representation typically used by the human descriptor 
in group decision making often the agent need to decide on multiple attribute at the same time so that there are exponentially many alternative in this case it is unrealistic to ask agent to communicate a full ranking of all the alternative to address this earlier work ha proposed decomposing such voting process by using local voting rule on the individual attribute unfortunately the existing method work only with rather severe domain restriction a they require the voter preference to extend acyclic cp net compatible with a common order on the attribute we first show that this requirement is very restrictive by proving that the number of linear order extending an acyclic cp net is exponentially smaller than the number of all linear order then we introduce a very general methodology that allows u to aggregate preference when voter express cp net that can be cyclic there doe not need to be any common structure among the submitted cp net our methodology generalizes the earlier more restrictive methodology we study whether property of the local rule transfer to the global rule and vice versa we also address how to compute the winning alternative 
in this paper we combine for the first time the method of dynamic mechanism design with technique from decentralized decision making under uncertainty consider a multi agent system with self interested agent acting in an uncertain environment each with private action state and reward there is also a social planner with it own action reward and state acting a a coordinator and able to influence the agent via action e g resource allocation agent can only communicate with the center but may become inaccessible e g when their communication device fails when accessible to the center agent can report their local state and model and receive recommendation from the center about local policy to follow for the present period and also should they become inaccessible until becoming accessible again without self interest this pose a new problem class which we call partially synchronized dec mdps and for which we establish some positive complexity result under reasonable assumption allowing for self interested agent we are able to bridge to method of dynamic mechanism design aligning incentive so that agent truthfully report local state when accessible and choose to follow the prescribed emergency policy of the center 
monte carlo tree search ha brought significant improvement to the level of computer player in game such a go but so far it ha not been used very extensively in game of strongly imperfect information with a dynamic board and an emphasis on risk management and decision making under uncertainty in this paper we explore it application to the game of kriegspiel invisible chess providing three monte carlo method of increasing strength for playing the game with little specific knowledge we compare these monte carlo agent to the strongest known minimax based kriegspiel player obtaining significantly better result with a considerably simpler logic and le domain specific knowledge 
in rule the conclusion may contain existentially quantified variable which make reasoning task a deduction non decidable these rule have the same logical form a tgd tuple generating dependency in database and a conceptual graph rule we extend known decidable case by combining backward and forward chaining scheme in association with a graph that capture exactly the notion of dependency between rule finally we draw a map of known decidable case including an extension obtained by combining our approach with very recent result on tgd 
we present an unified methodology for representation and development of dialectical proof procedure in abstract argumentation based on the notion of legal environment and dispute derivation a legal environment specifies the legal move of the dispute party while a dispute derivation describes the procedure structure a key insight of this paper is that the opponent move determine the soundness of a dispute while the completeness of a dispute procedure depends on the proponent move 
this paper study commitment in multi agent system a dialectical commitment corresponds to an agent taking a position about a putative fact including for the sake of argument a practical commitment corresponds to an agent being obliged to another to bring about a condition although commitment have been used in many work an adequate formal semantics and axiomatization for them doe not yet exist this paper present a logic of commitment that illustrates the commonality and difference of the two kind of commitment in this manner it generalizes the development of previous paper precisely delineates the meaning of commitment and identifies important postulate used informally or semiformally in previous work 
many problem require repeated inference on probabilistic graphical model with different value for evidence variable or other change example of such problem include utility maximization map inference online and interactive inference parameter and structure learning and dynamic inference since small change to the evidence typically only affect a small region of the network repeatedly performing inference from scratch can be massively redundant in this paper we propose expanding frontier belief propagation efbp an efficient approximate algorithm for probabilistic inference with incremental change to the evidence or model efbp is an extension of loopy belief propagation bp where each run of inference reuses result from the previous one instead of starting from scratch with the new evidence message are only propagated in region of the network affected by the change we provide theoretical guarantee bounding the difference in belief generated by efbp and standard bp and apply efbp to the problem of expected utility maximization in influence diagram experiment on viral marketing and combinatorial auction problem show that efbp can converge much faster than bp without significantly affecting the quality of the solution 
a recent formalization of iterative belief propagation ibp ha shown that it can be understood a an exact inference algorithm on an approximate model that result from deleting every model edge this formalization ha led to new realization of generalized belief propagation gbp in which edge are recovered incrementally to improve approximation quality and edge recovery heuristic that are motivated by improving the approximation quality of all node marginals in a graphical model in this paper we propose new edge recovery heuristic which are focused on improving the approximation of targeted node marginals the new heuristic are based on newly identified property of edge deletion and in turn ibp which guarantee the exactness of edge deletion in simple and idealized case these property also suggest new improvement to ibp approximation which are based on performing edge by edge correction on targeted marginals which are le costly than improvement based on edge recovery 
application of semantic web technology often require the management of metalevel information that is information that provides additional detail about domain level information such a provenance or access right policy existing owl based tool provide little or no support for the representation and management of metalevel information to fill this gap we propose a framework based on metaviews ontology that describe fact in the application domain we have implemented our framework in the kaon reasoner and have successfully applied it in a nontrivial scenario 
most manifold learning method consider only one similarity matrix to induce a low dimensional manifold embedded in data space in practice however we often use multiple sensor at a time so that each sensory information yield different similarity matrix derived from the same object in such a case manifold integration is a desirable task combining these similarity matrix into a compromise matrix that faithfully reflects multiple sensory information a small number of method exists for manifold integration including a method based on reproducing kernel krein space rkks or distatis where the former is restricted to the case of only two manifold and the latter considers a linear combination of normalized similarity matrix a a compromise matrix in this paper we present a new manifold integration method markov random walk on multiple manifold ram which integrates transition probability defined on each manifold to compute a compromise matrix numerical experiment confirm that ram find more informative manifold with a desirable projection property 
traditional artificial intelligence technique do not perform well in application such a real time strategy game because of the extensive search space which need to be explored in addition this exploration must be carried out on line during performance time it cannot be precomputed we have developed on line case based planning technique that are effective in such domain in this paper we extend our earlier work using idea from traditional planning to inform the real time adaptation of plan in our framework when a plan is retrieved a plan dependency graph is inferred to capture the relation between action in the plan the plan is then adapted in real time using it plan dependency graph this allows the system to create and adapt plan in an efficient and effective manner while performing the task the approach is evaluated using wargus a well known real time strategy game 
it is standard in multi agent setting to assume that agent will adopt nash equilibrium strategy however study in experimental economics demonstrate that nash equilibrium is a poor description of human player actual behavior in this study we consider a wide range of widely studied model from behavioral game theory for what we believe is the first time we evaluate each of these model in a meta analysis taking a our data set large scale and publicly available experimental data from the literature we then propose a modified model that we believe is more suitable for practical prediction of human behavior 
in distributed constraint optimization problem dynamic programming method have been recently proposed e g dpop in dynamic programming many valuation are grouped together in fewer message which produce much le networking overhead than search nevertheless these message are exponential in size the basic dpop always communicates all possible assignment even when some of them may be inconsistent due to hard constraint many real problem contain hard constraint that significantly reduce the space of feasible assignment this paper introduces h dpop a hybrid algorithm that is based on dpop which us constraint decision diagram cdd to rule out infeasible assignment and thus compactly represent util message experimental result show that h dpop requires several order of magnitude le memory than dpop especially for dense and tightly constrained problem 
in this paper we study distributed algorithm for cooperative agent that allow them to exchange their assigned task in order to reduce their team cost we define a new type of contract called k swap that describes multiple task exchange among multiple agent at a time which generalizes the concept of single task exchange we design a distributed algorithm that construct all possible k swap that reduce the team cost of a given task allocation and show that each agent typically only need to communicate a small part of it local computation result to the other agent we then demonstrate empirically that k swap can reduce the team cost of several existing task allocation algorithm significantly even if k is small 
we study the relative best case performance of dpll based structure aware sat solver in term of the power of the underlying proof system the system result from i varying the style of branching and ii enforcing dynamic restriction on the decision heuristic considering dpll both with and without clause learning we present a relative efficiency hierarchy for refinement of dpll resulting from combination of decision heuristic top down restricted justification restricted and unrestricted heuristic and branching style typical dpll style and atpg style branching an an example for dpll without clause learning we establish a strict hierarchy with the atpg style justification restricted branching variant a the weakest system 
voting theory can provide useful insight for multiagent preference aggregation however the standard setting assumes voter with preference that are total order a well a a ballot language that coincides with the preference language in typical ai scenario these assumption do not hold certain alternative may be incomparable for some agent and others may have their preference encoded in a format that is different from how the preference aggregation mechanism want them we study the consequence of dropping these assumption in particular we investigate the consequence for the important notion of strategy proofness while strategy proofness cannot be guaranteed in the classical setting we are able to show that there are situation in our more general framework where this is possible we also consider computational aspect of the problem 
when agent have conflicting preference over a set of alternative and they want to make a joint decision a natural way to do so is by voting how to design and analyze desirable voting rule ha been studied by economist for century in recent decade technological advance especially those in internet economy have introduced many new application for voting theory for example we can rate movie based on people s preference a done on many movie recommendation site however in such new application we always encounter a large number of alternative or an overwhelming amount of information which make computation in voting process a big challenge such challenge have led to a burgeoning area computational social choice aiming to address problem in computational aspect of preference representation and aggregation in a multi agent scenario the high level goal of my research is to better understand and prevent the agent strategic behavior in voting system a well a to design computationally efficient way for agent to present their preferencesand make a joint decision 
existing controller based approach for centralized and decentralized pomdps are based on automaton with output known a moore machine in this paper we show that several advantage can be gained by utilizing another type of automaton the mealy machine mealy machine are more powerful than moore machine provide a richer structure that can be exploited by solution method and can be easily incorporated into current controller based approach to demonstrate this we adapted some existing controller based algorithm to use mealy machine and obtained result on a set of benchmark domain the mealy based approach always outperformed the moore based approach and often outperformed the state of the art algorithm for both centralized and decentralized pomdps these finding provide fresh and general insight for the improvement of existing algorithm and the development of new one 
this paper deal with decision making in the context of multiattribute utility theory and more precisely with the problem of efficiently determining the best alternative w r t an agent s preference choice problem we assume that alternative are element of a product set of attribute and that the agent s preference are represented by a generalized additive decomposable gai utility on this set such a function allows an efficient representation of interaction between attribute while preserving some decomposability of the model gai utility can be compiled into graphical structure called gai network that can be exploited to solve choice problem using collect distribute scheme essentially similar to those used in bayesian network in this paper rather than directly using this scheme on the gai network for determining the most preferred alternative we propose to work with another gai function acting a an upper bound on utility value and enhancing the model s decomposability this method still provides the exact optimal solution but speed up significantly the search it prof to be particularly useful when dealing with choice and ranking under constraint and within collective decision making where gai net tend to have a large size we present an efficient algorithm for determining this new gai function and provide experimental result highlighting the practical efficiency of our procedure 
the aim of general game playing ggp is to create intelligent agent that can automatically learn how to play many different game at an expert level without any human intervention one of the main challenge such agent face is to automatically learn knowledge based heuristic in real time whether for evaluating game position or for search guidance in recent year ggp agent that use monte carlo simulation to reason about their action have become increasingly more popular for competitive play such an approach requires an effective search control mechanism for guiding the simulation playouts in here we introduce several scheme for automatically learning search guidance based on both statistical and reinforcement learning technique we compare the different scheme empirically on a variety of game and show that they improve significantly upon the current state of theart in simulation control in ggp for example in the chesslike game skirmish which ha proved a particularly challenging game for simulation based ggp agent an agent employing one of the proposed scheme achieves winning rate against an unmodified agent 
a first order conditional logic is considered with semantics given by a variant of semantics adam goldszmidt pearl where mean that pr approach super polynomially faster than any inverse polynomial this type of convergence is needed for reasoning about security protocol a complete axiomatization is provided for this semantics and it is shown how a qualitative proof of the correctness of a security protocol can be automatically converted to a quantitative proof appropriate for reasoning about concrete security 
one way to solve the knowledge acquisition bottleneck is to have way to translate natural language sentence and discourse to a formal knowledge representation language especially one that are appropriate to express domain knowledge in science such a biology while there have been several proposal including by montague to give model theoretic semantics for natural language and to translate natural language sentence and discourse to classical logic none of these approach use knowledge representation language that can express domain knowledge involving normative statement and exception in this paper we take a first step to illustrate how one can automatically translate natural language sentence about normative statement and exception to representation in the knowledge representation language answer set programming asp to do this we use calculus representation of word and their composition a dictated by a ccg grammar 
we address the problem of minimizing the propagation of undesirable thing such a computer virus or malicious rumor by blocking a limited number of link in a network a dual problem to the influence maximization problem of finding the most influential node in a social network for information diffusion this minimization problem is another approach to the problem of preventing the spread of contamination by removing node in a network we propose a method for efficiently finding a good approximate solution to this problem based on a naturally greedy strategy using large real network we demonstrate experimentally that the proposed method significantly outperforms conventional link removal method we also show that unlike the strategy of removing node blocking link between node with high out degree is not necessarily effective 
many real life datasets such a those produced by gene expression study exhibit complex substructure at various level of granularity and thus do not have unique well defined number of cluster in such case it is important to be able to trace the evolution of the individual cluster a the number of dimension of the clustering is varied while the dendrograms produced by bottom up clustering method such a hierarchical clustering are very useful for this purpose the approach is known to produce unreliable cluster due to it instability w r t resampling moreover hierarchical clustering doe not apply to overlapping bi cluster such a those obtained in gene expression study on the other hand the instability w r t the initialization of top down method such a k mean prevents the comparison between cluster obtained at different dimensionality in this paper we present a method for constructing generalized dendrograms for overlapping biclusters which depict the evolution of the biclusters a their number is varied an essential ingredient is a stable biclustering method based on positive tensor factorization of a number of nonnegative matrix factorization run we apply our approach to a large colon cancer dataset which show several distinct subclass whose dimensional evolution must be carefully analyzed to enable a more meaningful biological interpretation and sub classification 
we present a new algorithm for reasoning in the description logic shiq which is the most prominent fragment of the web ontology language owl the algorithm is based on ordered binary decision diagram obdds a a datastructure for storing and operating on large model representation we thus draw on the success and the proven scalability of obdd based system to the best of our knowledge we present the very first agorithm for using obdds for reasoning with general tboxes 
game theory ha been playing an increasingly visible role in computer science in general and ai in particular most notably in the area of multi agent system i briefly list the area where most of the action ha been in the past decade or so i then suggest that going forward the most dramatic interaction between computer science and game theory with a special role for ai could be around what might be called game theory pragmatic 
in this paper we present a novel probabilistic framework for recovering global latent social network structure from local noisy observation we extend curved exponential random graph model to include two type of variable hidden variable that capture the structure of the network and observational variable that capture the behavior between actor in the network we develop a novel combination of informative and intuitive conversational local and structural global feature to specify our model the model learns in an unsupervised manner the relationship between observable behavior and hidden social structure while simultaneously learning property of the latent structure itself we present empirical result on both synthetic data and a real world dataset of face to face conversation collected from individual using wearable sensor over the course of month 
local pattern mining is concerned with finding the set of pattern that satisfy a constraint in a database we study local pattern mining in the context of problog a probabilistic prolog system and introduce an approach for finding correlated pattern in the form of query in such a prolog system the approach combine principle of inductive logic programming data mining and statistical relational learning experiment on a challenging biological network mining task provide evidence for the interestingness of the approach 
we consider risk sensitive generalization of nash and correlated equilibrium in noncooperative game we prove that except for a class of degenerate game unless a two player game ha a pure nash equilibrium it doe not have a risksensitive nash equilibrium we also show that every game ha a risk sensitive correlated equilibrium the striking contrast between these existence result is due to the different source of randomization in nash private randomization and correlated equilibrium third party randomization 
an important challenge in understanding climate change is to uncover the dependency relationship between various climate observation and forcing factor graphical lasso a recently proposed penalty based structure learning algorithm ha been proven successful for learning underlying dependency structure for the data drawn from a multivariate gaussian distribution however climatological data often turn out to be non gaussian e g cloud cover precipitation etc in this paper we examine nonparametric learning method to address this challenge in particular we develop a methodology to learn dynamic graph structure from spatial temporal data so that the graph structure at adjacent time or location are similar experimental result demonstrate that our method not only recovers the underlying graph well but also capture the smooth variation property on both synthetic data and climate data 
parsing play an important role in semantic role labeling srl because most srl system infer semantic relation from best par therefore parsing error inevitably lead to labeling mistake to alleviate this problem we propose to use packed forest which compactly encodes all par for a sentence we design an algorithm to exploit exponentially many par to learn semantic relation efficiently experimental result on the conll shared task show that using forest achieves an absolute improvement of in term of f score over using best par and over using best par 
most framework for utility elicitation assume a predefined set of feature over which user preference are expressed we consider utility elicitation in the presence of subjective or user defined feature whose definition are not known in advance we treat the problem of learning a user s feature definition a one of concept learning but whose goal is to learn only enough about the concept definition to enable a good decision to be made this is complicated by the fact that user utility is unknown we describe computational procedure for identifying optimal alternative w r t minimax regret in the presence of both utility and concept uncertainty and develop several heuristic query strategy that focus simultaneously on reduction of relevant concept and utility uncertainty 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
negotiation scenario involving nonlinear utility function are specially challenging because traditional negotiation mechanism cannot be applied even mechanism designed and proven useful for nonlinear utility space may fail if the utility space is highly nonlinear for example although both contract sampling and constraint sampling have been successfully used in auction based negotiation with constraint based utility space they tend to fail in highly nonlinear utility scenario in this paper we will show that the performance of these approach decrease drastically in highly nonlinear utility scenario and propose a mechanism which balance utility and deal probability for the bidding and deal identification process the experiment show that the proposed mechanism yield better result than the previous approach in highly nonlinear negotiation scenario 
appropriately designing sampling policy is highly important for obtaining better control policy in reinforcement learning in this paper we first show that the least square policy iteration lspi framework allows u to employ statistical active learning method for linear regression then we propose a design method of good sampling policy for efficient exploration which is particularly useful when the sampling cost of immediate reward is high we demonstrate the usefulness of the proposed method named active policy iteration api through simulation with a batting robot 
unsupervised learning of morphology is an important task for human learner and in natural language processing system previous system focus on segmenting word into substring taking tak ing but sometimes a segmentation only analysis is insufficient e g taking may be more appropriately analyzed a take ing with a spelling rule accounting for the deletion of the stem final e in this paper we develop a bayesian model for simultaneously inducing both morphology and spelling rule we show that the addition of spelling rule improves performance over the baseline morphology only model 
compilation is an important approach to a range of inference problem since it enables linear time inference in the size s of the compiled representation however the main drawback is that s can be exponentially larger than the size of the original function to address this issue we propose an incremental approximate compilation technique that guarantee a sound and space bounded compilation for weighted boolean function at the expense of query completeness in particular our approach selectively compiles all solution exceeding a particular threshold given a range of weighting function without having to perform inference over the full solution space we describe incremental approximate algorithm for the prime implicant and dnnf compilation language and provide empirical evidence that these algorithm enable space reduction of several order of magnitude over the full compilation while losing relatively little query completeness 
we consider how to learn hierarchical task network htns for planning problem in which both the quality of solution plan generated by the htns and the speed at which those plan are found is important we describe an integration of htn learning with reinforcement learning to both learn method by analyzing semantic annotation on task and to produce estimate of the expected value of the learned method by performing monte carlo update we performed an experiment in which plan quality wa inversely related to plan length in two planning domain we evaluated the planning performance of the learned method in comparison to two state of the art satisficing classical planner fastforward and sgplan and one optimal planner hsp f the result demonstrate that a greedy htn planner using the learned method wa able to generate higher quality solution than sgplan in both domain and fastforward in one our planner fastforward and sgplan ran in similar time while hsp f wa exponentially slower 
a major challenge in the field of ai is combining s ymbolic and statistical technique my dissertation work ai m to bridge this gap in the domain of real time strategy game 
we present a breadth first search algorithm two bit breadth first search tbbfs which requires only two bit for each state in the problem space tbbfs can be parallelized in several way and can store it data on magnetic disk using tbbfs we perform complete breadth first search of the original pancake problem with and pancake and the burned pancake problem with and pancake determining the diameter of these problem space for the first time we also performed a complete breadth first search of the subspace of rubik s cube determined by the edge cubies 
a significantly complete account of the complexity underlying the computation of relevant solution concept in compact coalitional game is provided the starting investigation point is the setting of graph game about which various long standing open problem were stated in the literature the paper give an answer to most of them and in addition provides new insight on this setting by stating a number of complexity result about some relevant generalization and specialization the presented result also pave the way towards precisely carving the tractability frontier characterizing computation problem on compact coalitional game 
heuristic search using heuristic extracted from the delete relaxation is one of the most effective method in planning since finding the optimal solution of the delete relaxation is intractable various heuristic introduce independence assumption the implication of which are not yet fully understood here we use concept from graph theory to show that in problem with unary action precondition the delete relaxation is closely related to the steiner tree problem and that the independence assumption for the set of goal result in a tree of shortest path approximation we analyze the limitation of this approximation and develop an alternative method for computing relaxed plan that address them the method is used to guide a greedy best first search where it is shown to improve plan quality and coverage over several benchmark domain 
in the context of dung s theory of abstract argumentation framework the recently introduced resolution based grounded semantics feature the unique property of fully complying with a set of general requirement only partially satisfied by previous literature proposal this paper contributes to the investigation of resolution based grounded semantics by analyzing it computational property with reference to a standard set of decision problem for abstract argumentation semantics a checking the property of being an extension for a set of argument b checking agreement with traditional grounded semantics c checking the existence of a non empty extension d checking credulous acceptance of an argument e checking skeptical acceptance of an argument it is shown that problem a c admit polynomial time decision process while d is np complete and e conp complete 
written instruction are a common way of teaching people how to accomplish task on the web however study have shown that written instruction are difficult to follow even for experienced user a system that understands human written instruction could guide user through the process of following the direction improving completion rate and enhancing the user experience while general natural language understanding is extremely difficult we believe that in the limited domain of howto instruction it should be possible to understand enough to provide guided help in a mixed initiative environment based on a qualitative analysis of instruction gathered for web based task we have formalized the problem of understanding and interpreting how to instruction we compare three different approach to interpreting instruction a keyword based interpreter a grammar based interpreter and an interpreter based on machine learning and information extraction our empirical result demonstrate the feasibility of automated how to instruction understanding 
multi label learning deal with data associated with multiple label simultaneously previous work on multi label learning assumes that for each instance the full label set associated with each training instance is given by user in many application however to get the full label set for each instance is difficult and only a partial set of label is available in such case the appearance of a label mean that the instance is associated with this label while the absence of a label doe not imply that this label is not proper for the instance we call this kind of problem weak label problem in this paper we propose the well weak label learning method to solve the weak label problem we consider that the classification boundary for each label should go across low density region and that each label generally ha much smaller number of positive example than negative example the objective is formulated a a convex optimization problem which can be solved efficiently moreover we exploit the correlation between label by assuming that there is a group of low rank base similarity and the appropriate similarity between instance for different label can be derived from these base similarity experiment validate the performance of well 
existing ie system tend to focus on a tight window of context surrounding the desired information to be extracted this research address shortcoming of these system by introducing a two phase approach to ie that incorporates global relevance information with local contextual evidence to effectively extract information from free text 
conjunctive query answering is a key reasoning service for many ontology based application in order to improve scalability many semantic web query answering system give up completeness i e they do not guarantee to return all query answer it may be useful or even critical to the designer and user of such system to understand how much and what kind of information is potentially being lost we present a method for generating test data that can be used to provide at least partial answer to these question a purpose for which existing benchmark are not well suited in addition to developing a general framework that formalises the problem we describe practical data generation algorithm for some popular ontology language and present some very encouraging result from our preliminary evaluation 
we address the problem of effective reuse of subproblem solution in dynamic programming in dynamic programming a memoed solution of a subproblem can be reused for another if the latter s context is a special case of the former our objective is to generalize the context of the memoed subproblem such that more subproblems can be considered subcases and hence enhance reuse toward this goal we propose a generalization of context that doe not add better solution than the subproblem s optimal yet requires that subsumed sub problem preserve the optimal solution in addition we also present a general technique to search for at most k optimal solution we provide experimental result on resource constrained shortest path rcsp benchmark and program s exact worst case execution time wcet analysis 
the popularity of social network have burgeoned in recent year user share and access large volume of information on social networking site like facebook flickr del icio u etc whereas a few of these site have generic impersonal searching mechanism we have developed an agent based framework that mine the social network of a user to improve search result our social network based item search snis system us agent that utilize the connection of a user in the social network to facilitate the search for item of interest our approach generates targeted search result that can improve the precision of the result returned from a user s query we have implemented the snis agent based framework in flickr a photo sharing social network for searching for photo by using tag list a search query we discus the architecture of snis motivate the searching scheme used and demonstrate the effectiveness of the snis approach by presenting result we also show how snis can be utilized for expertise location 
evolutionary tree of specie can be reconstructed by pairwise comparison of their entire genome such a comparison can be quantified by determining the number of event that change the order of gene in a genome earlier erdem and tillier formulated the pairwise comparison of entire genome a the problem of planning rearrangement event that transform one genome to the other we reformulate this problem a a planning problem to extend it applicability to genome with multiple copy of gene and with unequal gene content and illustrate it applicability and effectiveness on three real datasets mitochondrial genome of metazoa chloroplast genome of campanulaceae chloroplast genome of various land plant and green algae 
agent often have to construct plan that obey resource limit for continuous resource whose consumption can only be characterized by probability distribution while markov decision process mdps with a state space of continuous and discrete variable are popular for modeling these domain current algorithm for such mdps can exhibit poor performance with a scale up in their state space to remedy that we propose an algorithm called dpfp dpfp s key contribution is it exploitation of the dual space cumulative distribution function this dual formulation is key to dpfp s novel combination of three feature first it enables dpfp s membership in a class of algorithm that perform forward search in a large possibly infinite policy space second it provides a new and efficient approach for varying the policy generation effort based on the likelihood of reaching different region of the mdp state space third it yield a bound on the error produced by such approximation these three feature conspire to allow dpfp s superior performance and systematic trade off of optimality for speed our experimental evaluation show that when run stand alone dpfp outperforms other algorithm in term of it any time performance whereas when run a a hybrid it allows for a significant speedup of a leading continuous resource mdp solver 
csiec computer simulation in educational communication is not only an intelligent web based human computer dialogue system with natural language for english instruction but also a learning assessment system for learner and teacher it multiple function including grammar gap filling exercise talk show free chatting and chatting on a given topic can satisfy the various need from the student with different background and learning ability after the brief explanation of the motivation and the survey of the related work we illustrate the system structure and describe it pedagogical function with the underlying ai technique in detail such a nlp and human computer interaction we summarize the free internet usage from a six month period and it integration into english class in university and middle school the evaluation finding show that the chatting function ha been improved and frequently utilized by the user and the application of the csiec system on english instruction can motivate the learner to practice english and enhance their learning process finally we draw some conclusion for the future improvement 
conjunctive query cq are fundamental for accessing description logic dl knowledge base we study cq answering in extension of the dl el which is popular for large scale ontology and underlies the designated owl el profile of owl our main contribution is a novel approach to cq answering that enables the use of standard relational database system a the basis for query execution we evaluate our approach using the ibm db system with encouraging result 
security at major location of economic or political importance is a key concern around the world particularly given the threat of terrorism limited security resource prevent full security coverage at all time which allows adversary to observe and exploit pattern in selective patrolling or monitoring e g they can plan an attack avoiding existing patrol hence randomized patrolling or monitoring is important but randomization must provide distinct weight to different action based on their complex cost and benefit to this end this demonstration showcase a promising transition of the latest in multi agent algorithm into a deployed application in particular it exhibit a software assistant agent called armor assistant for randomized monitoring over route that cast this patrolling monitoring problem a a bayesian stackelberg game allowing the agent to appropriately weight the different action in randomization a well a uncertainty over adversary type armor combine two key feature i it us the fastest known solver for bayesian stackelberg game called dobss where the dominant mixed strategy enable randomization ii it mixed initiative based interface allows user to occasionally adjust or override the automated schedule based on their local constraint armor ha been successfully deployed since august at the los angeles international airport lax to randomize checkpoint on the roadway entering the airport and canine patrol route within the airport terminal 
we analyze the complexity of reasoning with circumscribed low complexity dl such a dl lite and the el family under suitable restriction on the use of abnormality predicate we prove that in circumscribed dl liter complexity drop from nexpnp to the second level of the polynomial hierarchy in el reasoning remains exptime hard in general however by restricting the possible occurrence of existential restriction we obtain membership in p and p for an extension of el 
we describe htn maker an algorithm for learning hierarchical planning knowledge in the form of decomposition method for hierarchical task network htns htn maker take a input the initial state from a set of classical planning problem in a planning domain and solution to those problem a well a a set of semantically annotated task to be accomplished the algorithm analyzes this semantic information in order to determine which portion of the input plan accomplish a particular task and construct htn method based on those analysis our theoretical result show that htn maker is sound and complete we also present a formalism for a class of planning problem that are more expressive than classical planning these planning problem can be represented a htn planning problem we show that the method learned by htn maker enable an htn planner to solve those problem our experiment confirm the theoretical result and demonstrate convergence in three well known planning domain toward a set of htn method that can be used to solve nearly any problem expressible a a classical planning problem in that domain relative to a set of goal 
heuristic search using algorithm such a a and ida is the prevalent method for obtaining optimal sequential solution for classical planning task theoretical analysis of these classical search algorithm such a the well known result of pohl gaschnig and pearl suggest that such heuristic search algorithm can obtain better than exponential scaling behaviour provided that the heuristic are accurate enough here we show that for a number of common planning benchmark domain including one that admit optimal solution in polynomial time general search algorithm such a a must necessarily explore an exponential number of search node even under the optimistic assumption of almost perfect heuristic estimator whose heuristic error is bounded by a small additive constant our result shed some light on the comparatively bad performance of optimal heuristic search approach in simple planning domain such a gripper they suggest that in many application further improvement in run time require change to other part of the search algorithm than the heuristic estimator 
clustering classification and regression are three major research topic in machine learning so far much work ha been conducted in solving multiple instance classification and multiple instance regression problem where supervised training pattern are given a bag and each bag consists of some instance but the research on unsupervised multiple instance clustering is still limited this paper formulates a novel maximum margin multiple instance clustering m ic problem for the multiple instance clustering task to avoid solving a nonconvex optimization problem directly m ic is further relaxed which enables an efficient optimization solution with a combination of constrained concave convex procedure cccp and the cutting plane method furthermore this paper analyzes some important property of the proposed method and the relationship between the proposed method and some other related one an extensive set of empirical result demonstrate the advantage of the proposed method against existing research for both effectiveness and efficiency 
this paper proposes an optimal approach to infinite state action planning exploiting automaton theory state set and action are characterized by presburger formula and represented using minimized finite state machine the exploration that contributes to the planning via model checking paradigm applies symbolic image in order to compute the deterministic finite automaton for the set of successor a large fraction of metric planning problem can be translated into presburger arithmetic while derived predicate are simply compiled away we further propose three algorithm for computing optimal plan one for uniform action cost one for the additive cost model and one for linear plan metric furthermore an extension for infinite state set is discussed 
to enable ontology reuse the web ontology language owl allows an ontology v to import an ontology h to reason with such a v a reasoner need physical access to the axiom of h for copyright and or privacy reason however the author of h might not want to publish the axiom of h instead they might prefer to provide an oracle that can answer a limited set of query over h thus allowing v to import h by query in this paper we study import by query algorithm which can answer question about v h by accessing only v and the oracle we show that no such algorithm exists in general and present restriction under which importing by query becomes feasible 
interactive dynamic influence diagram i dids are graphical model for sequential decision making in partially observable setting shared by other agent algorithm for solving i dids face the challenge of an exponentially growing space of candidate model ascribed to other agent over time previous approach for exactly solving i dids group together model having similar solution into behaviorally equivalent class and update these class we present a new method that in addition to aggregating behaviorally equivalent model further group model that prescribe identical action at a single time step we show how to update these augmented class and prove that our method is exact the new approach enables u to bound the aggregated model space by the cardinality of other agent action we evaluate it performance and provide empirical result in support 
real time dynamic programming rtdp solves markov decision process mdps when the initial state is restricted by focusing dynamic programming on the envelope of state reachable from an initial state set rtdp often provides performance guarantee without visiting the entire state space building on rtdp recent work ha sought to improve it efficiency through various optimization including maintaining upper and lower bound to both govern trial termination and prioritize state exploration in this work we take a bayesian perspective on these upper and lower bound and use a value of perfect information vpi analysis to govern trial termination and exploration in a novel algorithm we call vpi rtdp vpi rtdp lead to an improvement over state of the art rtdp method empirically yielding up to a three fold reduction in the amount of time and number of visited state required to achieve comparable policy performance 
we introduce a multi model variant of the emt based control algorithm the new algorithm mm emt is capable of balancing several control task expressed using separate dynamic model with a common action space such multiple model are common in both single agent environment when the agent ha multiple task to achieve and in team activity when agent action affect both the local agent s task a well a the overall team s coordination to demonstrate the behaviour that mm emt engenders several experimental setup were devised simulation result support the effectiveness of the approach which in the multi agent scenario is expressed in the mm emt algorithm s ability to balance local and team coordinated motion requirement 
we describe the degree of grounding model which track the extent to which material ha reached mutual belief in a dialogue and conduct experiment in which the model is used to manage grounding behavior in spoken dialogue with a virtual human we show that the model produce improvement in virtual human performance a measured by post session questionnaire 
we address the problem of efficiently estimating the influence function of initially activated node in a social network under the susceptible infected susceptible si model a diffusion model where node are allowed to be activated multiple time the computational complexity drastically increase because of this multiple activation property we solve this problem by constructing a layered graph from the original social network with each layer added on top a the time proceeds and applying the bond percolation with a pruning strategy we show that the computational complexity of the proposed method is much smaller than the conventional naive probabilistic simulation method by a theoretical analysis and confirm this by applying the proposed method to two real world network 
the field of information retrieval and text manipulation classification clustering still strives for model allowing semantic information to be folded in to improve performance with respect to standard bag of word based model many approach aim at a concept based retrieval but differ in the nature of the concept which range from linguistic concept a defined in lexical resource such a wordnet latent topic derived from the data itself a in latent semantic indexing lsi or latent dirichlet allocation lda to wikipedia article a proxy for concept a in the recently proposed explicit semantic analysis esa model a crucial question which ha not been answered so far is whether model based on explicitly given concept a in the esa model for instance perform inherently better than retrieval model based on latent concept a in lsi and or lda in this paper we investigate this question closer in the context of a cross language setting which inherently requires concept based retrieval bridging between different language in particular we compare the recently proposed esa model with two latent model lsi and lda showing that the former is clearly superior to the both from a general perspective our result contribute to clarifying the role of explicit v implicitly derived or latent concept in cross language information retrieval research 
there are common intuition about how social graph are generated for example it is common to talk informally about nearby node sharing a link there are also common heuristic for predicting whether two currently unlinked node in a graph should be linked e g for suggesting friend in an online social network or movie to customer in a recommendation network this paper provides what we believe to be the first formal connection between these intuition and these heuristic we look at a familiar class of graph generation model in which node are associated with location in a latent metric space and connection are more likely between closer node we also look at popular link prediction heuristic such a number of common neighbor and it weighted variant adamic adar which have proved successful in predicting missing link but are not direct derivative of latent space graph model we provide theoretical justification for the success of some measure a compared to others a reported in previous empirical study in particular we present a sequence of formal result that show bound related to the role that a node s degree play in it usefulness for link prediction the relative importance of short path versus long path and the effect of increasing non determinism in the link generation process on link prediction quality our result can be generalized to any model a long a the latent space assumption hold 
finding the longest common subsequence of multiple string is a classical computer science problem and ha many application in the area of bioinformatics and computational genomics in this paper we present a new sequential algorithm for the general case of mlcs problem and it parallel realization the algorithm is based on the dominant point approach and employ a fast divide and conquer technique to compute the dominant point when applied to find a mlcs of string our general algorithm is shown to exhibit the same performance a the best existing mlcs algorithm by hakata and imai designed specifically for the case of string moreover we show that for a general case of more than string the algorithm is significantly faster than the best existing sequential approach reaching up to order of magnitude faster on the large size problem finally we propose a parallel implementation of the algorithm evaluating the parallel algorithm on a benchmark set of both random and biological sequence reveals a near linear speed up with respect to the sequential algorithm 
increasing the expressiveness of qualitative spatial calculus is an essential step towards meeting the requirement of application this can be achieved by combining existing calculus in a way that we can express spatial information using relation from both calculus the great challenge is to develop reasoning algorithm that are correct and complete when reasoning over the combined information previous work ha mainly studied case where the interaction between the combined calculus wa small or where one of the two calculus wa very simple in this paper we tackle the important combination of topological and directional information for extended spatial object we combine some of the best known calculus in qualitative spatial reasoning qsr the rcc algebra for representing topological information and the rectangle algebra ra and the cardinal direction calculus cdc for directional information although cdc is more expressive than ra reasoning with cdc is of the same order a reasoning with ra we show that reasoning with basic rcc and basic ra relation is in p but reasoning with basic rcc and basic cdc relation is np complete 
deciding consistency of constraint network is a fundamental problem in qualitative spatial and temporal reasoning in this paper we introduce a divide and conquer method that recursively partition a given problem into smaller sub problem in deciding consistency we identify a key theoretical property of a qualitative calculus that ensures the soundness and completeness of this method and show that it is satisfied by the interval algebra ia and the point algebra pa we develop a new encoding scheme for ia network based on a combination of our divide and conquer method with an existing encoding of ia network into sat we empirically show that our new encoding scheme scale to much larger problem and exhibit a consistent and significant improvement in efficiency over state of the art solver on the most difficult instance 
sequential single item auction can be used for the distributed allocation of task to cooperating agent we study how to improve the team performance of sequential single item auction while still controlling the agent in real time our idea is to assign that task to agent during the current round whose regret is large where the regret of a task is defined a the difference of the second smallest and smallest team cost resulting from assigning the task to the second best and best agent respectively our experimental result show that sequential single item auction with regret clearing indeed result in smaller team cost than standard sequential single item auction for three out of four combination of two different team objective and two different capacity constraint including no capacity constraint 
we study a multi step hider seeker game where the hider is moving on a graph and in each step the seeker is able to search c subset of the graph node we model this game a a zero sum bayesian game which can be solved in weakly polynomial time in the player action space the seeker s action space is exponential in c and both player action space are exponential in the game horizon to manage this intractability we use a column constraint generation approach for both player this approach requires an oracle to determine best response for each player however we show that computing a best response for the seeker is np hard even for a single step game when c is part of the input and that computing a best response is np hard for both player for the multi step game even if c an integer programming formulation of the best response for the hider is practical for moderate horizon but computing an exact seeker best response is impractical due to the exponential dependence on both c and the horizon we therefore develop an approximate best response oracle with bounded suboptimality for the seeker we prove performance bound on the strategy that result when column constraint generation with approximate best response converges and we measure the performance of our algorithm in simulation in our experimental result column constraint generation converges to near minimax strategy for both player fairly quickly 
this paper provides a decentralized data model and associated algorithm for peer data management system pdms based on the dl liter description logic our approach relies on reducing query reformulation and consistency checking for dl liter into reasoning in propositional logic this enables a straightforward deployment of dl liter pdmss on top of some where a scalable propositional peer to peer inference system we also show how to use the state of the art minicon algorithm for rewriting query using view in dl liter in the centralized and decentralized case 
for many machine learning solution to complex application there are significant performance advantage to decomposing the overall task into several simpler sequential stage commonly referred to a a pipeline model typically such scenario are also characterized by high sample complexity motivating the study of active learning for these situation while most active learning research examines single prediction we extend such work to application which utilize pipelined prediction specifically we present an adaptive strategy for combining local active learning strategy into one that minimizes the annotation requirement for the overall task empirical result for a three stage entity and relation extraction system demonstrate a significant reduction in supervised data requirement when using the proposed method 
tagging ha become a primary tool for user to organize and share digital content on many social medium site in addition tag information ha been shown to enhance capability of existing search engine however many resource on the web still lack tag information this paper proposes a content based approach to tag recommendation which can be applied to webpage with or without prior tag information while social bookmarking service such a delicious enables user to share annotated bookmark tag recommendation is available only for page with tag specified by other user our proposed approach is motivated by the observation that similar webpage tend to have the same tag each webpage can therefore share the tag they own with similar webpage the propagation of a tag depends on it weight in the originating webpage and the similarity between the sending and receiving webpage the similarity metric between two webpage is defined a a linear combination of four cosine similarity taking into account both tag information and page content experiment using data crawled from delicious show that the proposed method is effective in populating untagged webpage with the correct tag 
labeling node in a network is an important problem that ha seen a growing interest a number of method that exploit both local and relational information have been developed for this task acquiring the label for a few node at inference time can greatly improve the accuracy however the question of figuring out which node label to acquire is challenging previous approach have been based on simple structural property here we present a novel technique which we refer to a reflect and correct that can learn and predict when the underlying classification system is likely to make mistake and it suggests acquisition to correct those mistake 
we introduce the budget limited multi armed bandit mab which capture situation where a learner s action are costly and constrained by a fixed budget that is incommensurable with the reward earned from the bandit machine and then describe a first algorithm for solving it since the learner ha a budget the problem s duration is finite consequently an optimal exploitation policy is not to pull the optimal arm repeatedly but to pull the combination of arm that maximises the agent s total reward within the budget a such the reward for all arm must be estimated because any of them may appear in the optimal combination this difference from existing mabs mean that new approach to maximising the total reward are required to this end we propose an epsilon first algorithm in which the first epsilon of the budget is used solely to learn the arm reward exploration while the remaining epsilon is used to maximise the received reward based on those estimate exploitation we derive bound on the algorithm s loss for generic and uniform exploration method and compare it performance with traditional mab algorithm under various distribution of reward and cost showing that it outperforms the others by up to 
there is a growing need for efficient and scalable semantic web query that handle inference there is also a growing interest in representing uncertainty in semantic web knowledge base in this paper we present a bit vector schema specifically designed for rdf resource description framework datasets we propose a system for materializing and storing inferred knowledge using this schema we show experimental result that demonstrate that our solution drastically improves the performance of inference query we also propose a solution for materializing uncertain information and probability using multiple bit vector and threshold 
we propose a new discriminative framework namely hidden dynamic conditional random field hdcrfs for building probabilistic model which can capture both internal and external class dynamic to label sequence data we introduce a small number of hidden state variable to model the sub structure of a observation sequence and learn dynamic between different class label an hdcrf offer several advantage over previous discriminative model and is attractive both conceptually and computationally we performed experiment on three well established sequence labeling task in natural language including part of speech tagging noun phrase chunking and named entity recognition the result demonstrate the validity and competitiveness of our model in addition our model compare favorably with current state of the art sequence labeling approach conditional random field crfs which can only model the external dynamic 
open ended language communication remains an enormous challenge for autonomous robot this paper argues that the notion of a language strategy is the appropriate vehicle for addressing this challenge a language strategy package all the procedure that are necessary for playing a language game we present a specific example of a language strategy for playing an action game in which one robot asks another robot to take on a body posture such a stand or sit and show how it effectively allows a population of agent to self organise a perceptually grounded ontology and a lexicon from scratch without any human intervention next we show how a new language strategy can arise by exaptation from an existing one concretely how the body posture strategy can be exapted to a strategy for playing language game about the spatial position of object a in the bottle stand on the table 
the following activity recognition problem is considered a description of the action capability of an agent being observed is given this includes the precondition and effect of atomic action and of the activity sequence of action the agent may execute given this description and a set of proposition called history about action occurrence intended action and property of the world all at various point in time the problem is to complete the picture a much a possible and determine what ha already happened what the intention of the agent are and what may happen a a result of the agent acting on those intention we present a framework to solve these activity recognition problem based on a formal language for reasoning about action that includes a notion of intended action and a corresponding formalization in answer set programming 
hard and soft precedence constraint play a key role in many application domain in telecommunication one application is the configuration of callcontrol feature subscription where the task is to sequence a set of user selected feature subject to a set of hard catalogue precedence constraint and a set of soft user selected precedence constraint when no such consistent sequence exists the task is to find an optimal relaxation by discarding some feature or user precedence for this purpose we present the global constraint softprec enforcing generalized arc consistency gac on softprec is np complete therefore we approximate gac based on domain pruning rule that follow from the semantics of softprec this pruning is polynomial empirical result demonstrate that the search effort required by softprec is up to one order of magnitude le than the previously known best cp approach for the feature subscription problem softprec is also applicable to other problem domain including minimum cutset problem for which initial experiment confirm the interest 
we introduce nl a learning algorithm for inferring non deterministic finite state automaton using membership and equivalence query more specifically residual finite state automaton rfsa are learned similarly a in angluin s popular l algorithm which however learns deterministic finite state automaton dfa like in a dfa the state of an rfsa represent residual language unlike a dfa an rfsa restricts to prime residual language which cannot be described a the union of other residual language in doing so rfsa can be exponentially more succinct than dfa they are therefore the preferable choice for many learning application the implementation of our algorithm is applied to a collection of example and confirms the expected advantage of nl over l 
early research in heuristic search discovered that using inconsistent heuristic with a could result in an exponential increase in the number of node expansion a a result the use of inconsistent heuristic ha largely disappeared from practice recently inconsistent heuristic have been shown to be effective in ida especially when applying the bidirectional pathmax bpmx enhancement this paper present new worst case complexity analysis of a s behavior with inconsistent heuristic discus how bpmx can be used with a and give experimental result justifying the use of inconsistent heuristic in a search 
surgery room are complex environment where many interaction take place between staff member and the electronic and mechanical system in spite of their inherent complexity surgery of the same kind bear numerous similarity and are usually performed with similar workftows this give the possibility to design support system in the operating room or whose applicability range from easy task such a the activation of or light and calling the next patient to more complex one such a context sensitive user interface or automatic reporting an essential feature when designing such system is the ability for on line recognition of what is happening inside the or based on recorded signal in this paper we present an approach using signal from the or and hidden markov model to recognize on line the surgical step performed by the surgeon during a laparoscopic surgery we also explain how the system can be deployed in the or experiment are presented using real surgery performed by different surgeon in several or recorded at our partner hospital we believe that similar system will quickly develop in the near future in order to efficiently support surgeon trainee and the medical staff in general a well a to improve administrative task like scheduling within hospital 
we develop an efficient algorithm for computing pure strategy nash equilibrium that satisfy various criterion such a the utilitarian or nash bernoulli social welfare function in game with sparse interaction structure our algorithm called valued nash propagation vnp integrates the optimisation problem of maximising a criterion with the constraint satisfaction problem of finding a game s equilibrium to construct a criterion that defines a c semiring given a suitably compact game structure this criterion can be efficiently optimised using message passing to this end we first show that vnp is complete in game whose interaction structure form a hypertree then we go on to provide theoretic and empirical result justifying it use on game with arbitrary structure in particular we show that it computes the optimum ensuremath of the time and otherwise selects an equilibrium that is always within of the optimum on average 
we present preliminary result of using physical feature of a user s sketching style such a pen tilt and pressure to identify a user from their sketched stroke 
the kernel fisher s discriminant kfd ha proven to be competitive to several state of the art classifier however it is assuming equal covariance structure for all transformed class which is not true in many application in this paper we propose a novel bayesian kernel logistic discriminant model bkld which go one step further by representing each transformed class by it own covariance matrix this can perform better than the kfd an extensive comparison of the bkld to the kfd and to other state of the art non linear classifier is performed 
coverbal gesture provides a channel for the visual expression of idea while some gestural emblem have culturally predefined form e g thumb up the relationship between gesture and meaning is in general not conventionalized it is natural to ask whether such gesture can be interpreted in a speaker independent way or whether gestural form is determined by the speaker s idiosyncratic view of the discourse topic we address this question using an audiovisual dataset across multiple speaker and topic our analysis employ a hierarchical bayesian author topic model in which gestural pattern are stochastically generated by a mixture of speaker specific and topic specific prior these gestural pattern are characterized using automatically extracted visual feature based on spatio temporal interest point this framework detects significant cross speaker pattern in gesture that are governed by the discourse topic suggesting that even unstructured gesticulation can be interpreted across speaker in addition the success of this approach show that the semantic characteristic of gesture can be detected via a low level interest point representation 
this paper present a fast simulated annealing framework for combining multiple clustering i e clustering ensemble based on some measure of agreement between partition which are originally used to compare two clustering the obtained clustering v a ground truth clustering for the evaluation of a clustering algorithm though we can follow a greedy strategy to optimize these measure a objective function of clustering ensemble some local optimum may be obtained and simultaneously the computational cost is too large to avoid the local optimum we then consider a simulated annealing optimization scheme that operates through single label change moreover for these measure between partition based on the relationship joined or separated of pair of object such a rand index we can update them incrementally for each label change which make sure the simulated annealing optimization scheme is computationally feasible the simulation and real life experiment then demonstrate that the proposed framework can achieve superior result 
advertising in the case of textual web page ha been studied extensively by many researcher however with the increasing amount of multimedia data such a image audio and video on the web the need for recommending advertisement for the multimedia data is becoming a reality in this paper we address the novel problem of visual contextual advertising which is to directly advertise when user are viewing image which do not have any surrounding text a key challenging issue of visual contextual advertising is that image and advertisement are usually represented in image space and word space respectively which are quite different with each other inherently a a result existing method for web page advertising are inapplicable since they represent both web page and advertisement in the same word space in order to solve the problem we propose to exploit the social web to link these two feature space together in particular we present a unified generative model to integrate advertisement word and image specifically our solution combine two part in a principled approach first we transform image from a image feature space to a word space utilizing the knowledge from image with annotation from social web then a language model based approach is applied to estimate the relevance between transformed image and advertisement moreover in this model the probability of recommending an advertisement can be inferred efficiently given an image which enables potential application to online advertising 
multi task learning aim at combining information across task to boost prediction performance especially when the number of training sample is small and the number of predictor is very large in this paper we first extend the sparse discriminate analysis sda of clemmensen et al we call this multi task sparse discriminate analysis mtsda mtsda formulates multi label prediction a a quadratic optimization problem whereas sda obtains single label via a nearest class mean rule second we propose a class of equicorrelation matrix to use in mtsda which includes the identity matrix mtsda with both matrix are compared with singletask learning svm and lda svm and multi task learning hsml the comparison are made on real data set in term of auc and f measure the data result show that mtsda outperforms other method substantially almost all the time and in some case mtsda with the equicorrelation matrix substantially outperforms mtsda with identity matrix 
with the introduction of constraint based on finite automaton a new line of research ha opened where constraint are based on formal language recently constraint based on grammar higher up in the chomsky hierarchy were introduced we devise a timeand space efficient incremental arc consistency algorithm for context free grammar particularly we show how to filter a sequence of monotonically tightening problem in cubic time and quadratic space experiment on a scheduling problem show order of magnitude improvement in time and space consumption 
sequential voting rule and correspondence provide a way for agent to make group decision when the set of available option ha a multiissue structure one important question about sequential voting rule correspondence is whether they satisfy two crucial criterion namely neutrality and efficiency recently benoit and kornhauser established an important result about seat by seat voting rule which are a special case of sequential voting rule they proved that if the multi issue domain satisfies some property then the only seat by seat rule being either efficient or neutral are dictatorship however there are still some case not covered by their result including a very important and interesting case voting correspondence in this paper we extend the impossibility theorem by benoit and kornhauser to voting correspondence and obtain a dichotomy theorem on the existence of efficient or neutral sequential seat by seat voting rule and correspondence therefore the question of whether sequential seat by seat voting rule correspondence can be efficient or neutral is now completely answered 
this paper considers the problem of composing or scheduling several non deterministic behavior so a to conform to a specified target behavior a well a satisfying constraint imposed by the environment in which the behavior are to be performed this problem ha already been considered by several work in the literature and applied to area such a web service composition the composition of robot behavior and co ordination of distributed device we develop a sound and complete algorithm for determining such a composition which ha a number of significant advantage over previous proposal a our algorithm is different from previous proposal which resort to dynamic logic or simulation relation b we realized an implementation in java a opposed to other approach for which there are no known implementation c our algorithm determines all possible scheduler at once and d we can use our framework to define a notion of approximation when the target behavior cannot be realized 
most standard learning algorithm such a logistic regression lr and the support vector machine svm are designed to deal with i i d independent and identically distributed data they therefore do not work effectively for task that involve non i i d data such a region segmentation eg the tumor v non tumor label in a medical image are correlated in that adjacent pixel typically have the same label this ha motivated the work in random field which ha produced classifier for such noni i d data that are significantly better than standard i i d based classifier however these random field method are often too slow to be trained for the task they were designed to solve this paper present a novel variant pseudo conditional random field pcrfs that is also based on i i d learner to allow efficient training but also incorporates correlation like random field we demonstrate that this system is a accurate a other random field variant but significantly faster to train 
the gibbard satterthwaite theorem asserts that any reasonable voting rule cannot be strategyproof a large body of research in ai deal with circumventing this theorem via computational consideration the goal is to design voting rule that are computationally hard in the worst case to manipulate however recent work indicates that the prominent voting rule are usually easy to manipulate in this paper we suggest a new c oriented approach to circumventing gibbard satterthwaite using randomization and approximation specifically we wish to design strategyproof randomized voting rule that are close in a standard approximation sense to prominent score based deterministic voting rule we give tight lower and upper bound on the approximation ratio achievable via strategyproof randomized rule with respect to positional scoring rule copeland and maximin 
due to their flexible nonparametric nature gaussian process model are very effective at solving hard machine learning problem while existing gaussian process model focus on modeling one single relation we present a generalized gp model named multi relational gaussian process model that is able to deal with an arbitrary number of relation in a domain of interest the proposed model is analyzed in the context of bipartite directed and undirected univariate relation experimental result on real world datasets show that exploiting the correlation among different entity type and relation can indeed improve prediction performance 
we present a new algorithm for computing upper bound for an optimization version of the emajsat problem called functional e majsat the algorithm utilizes the compilation language d dnnf which underlies several state of the art algorithm for solving related problem this bound computation can be used in a branch and bound solver for solving functional e majsat we then present a technique for pruning value from the branch and bound search tree based on the information available after each bound computation we evaluated the proposed technique in a map solver and a probabilistic conformant planner in both case our experiment showed that the new technique improved the efficiency of state of the art solver by order of magnitude 
planning in large partially observable domain is challenging especially when a long horizon lookahead is necessary to obtain a good policy traditional pomdp planner that plan a different potential action for each future observation can be prohibitively expensive when planning many step ahead an efficient solution for planning far into the future in fully observable domain is to use temporallyextended sequence of action or macro action in this paper we present a pomdp algorithm for planning under uncertainty with macro action puma that automatically construct and evaluates open loop macro action within forward search planning where the planner branch on observation only at the end of each macro action additionally we show how to incrementally refine the plan over time resulting in an anytime algorithm that provably converges to an optimal policy in experiment on several large pomdp problem which require a long horizon lookahead puma outperforms existing state of the art solver most partially observable markov decision process pomdp planner select action conditioned on the prior observation at each timestep we refer to such planner a fully conditional when good performance relies on considering different possible observation far into the future both online and offline fully conditional planner typically struggle an extreme alternative is unconditional or openloop planning where a sequence of action is fixed and doe not depend on the observation that will be received during execution while open loop planning can be extremely fast and perform surprisingly well in certain domain acting well in most real world domain requires plan where at least some action choice are conditional on the obtained observation this paper focus on the significant subset of pomdp domain including scientific exploration target surveillance and chronic care management where it is possible to act well by planning using conditional sequence of openloop fixed length action chain or macro action we call this approach semi conditional planning in that action are chosen based on the received observation only at the end of each macro action 
planning landmark are fact that must be true at some point in every solution plan previous work ha very successfully exploited planning landmark in satisficing non optimal planning we propose a methodology for deriving admissible heuristic estimate for cost optimal planning from a set of planning landmark the resulting heuristic fall into a novel class of multi path dependent heuristic and we present a simple best first search procedure exploiting such heuristic our empirical evaluation show that this framework favorably competes with the state of the art of cost optimal heuristic search 
it is well known that diversity among base classifier is crucial for constructing a strong ensemble most existing ensemble method obtain diverse individual learner through resampling the instance or feature in this paper we propose an alternative way for ensemble construction by resampling pairwise constraint that specify whether a pair of instance belongs to the same class or not using pairwise constraint for ensemble construction is challenging because it remains unknown how to influence the base classifier with the sampled pairwise constraint we solve this problem with a two step process first we transform the original instance into a new data representation using projection learnt from pairwise constraint then we build the base classifier with the new data representation we propose two method for resampling pairwise constraint following the standard bagging and boosting algorithm respectively extensive experiment validate the effectiveness of our method 
the demonstration present an application of multiagent system and wireless networking to remote robot based surveillance 
we propose an expressive auction design that allows advertiser to specify the kind of demographic and website they wish to target within an advertising network the design allows the network to differentiate impression according to relevant attribute e g geographic location of the user topic of the webpage advertiser can place bid for different kind of impression according to their attribute and can also specify volume constraint to control exposure the novelty of the design is a bidding language that admits scalable allocation and pricing algorithm we discus the incentive property of different pricing approach we also propose a bidder feedback mechanism to mitigate the complexity of expressive bidding 
statistical relational reasoning ha received much attention due to it ability to robustly model complex relationship a key challenge is tractable inference especially in domain involving many object due to the combinatorics involved one can accelerate inference by using approximation technique lazy algorithm etc we consider markov logic network mlns which involve counting how often logical formula are satisfied we propose a preprocessing algorithm that can substantially reduce the effective size of mlns by rapidly counting how often the evidence satisfies each formula regardless of the truth value of the query literal this is a general preprocessing method that loses no information and can be used for any mln inference algorithm we evaluate our algorithm empirically in three real world domain greatly reducing the work needed during subsequent inference such reduction might even allow exact inference to be performed when sampling method would be otherwise necessary 
intelligent tutoring system it is the interdisciplinary field that investigates how to devise educational system that provide instruction tailored to the need of individual learner a many good teacher do research in this field ha successfully delivered technique and system that provide adaptive support for student problem solving in a variety of domain there are however other educational activity that can benefit from individualized computer based support such a studying example exploring interactive simulation and playing educational game providing individualized support for these activity pose unique challenge because it requires an it that can model and adapt to student behavior skill and mental state often not a structured and welldefined a those involved in traditional problem solving this paper present a variety of project that illustrate some of these challenge our proposed solution and future opportunity 
the web service composition wsc problem with respect to behavioral description deal with the automatic synthesis of a coordinator web service c that control a set of web service to reach a goal state despite it importance however solving the wsc problem for a general case when c ha only partial observation remains to be doubly exponential in the number of variable in web service description rendering any attempt to compute an exact solution for modest size impractical toward this challenge in this paper we propose two novel signature preserving and subsuming approximation based approach using abstraction and refinement we empirically validate that our proposal can solve realistic problem efficiently 
this paper considers the execution of a multi agent plan in a partially observable environment and face the problem of recovering from action failure the paper formalizes a local plan repair strategy where each agent in the system is responsible for controlling monitoring and diagnosing the action it executes and for autonomously repairing it own plan when an action failure is detected the paper describes also how to mitigate the impact of an action failure on the plan of other agent when the local recovery strategy fails 
sketch recognition system usually recognize stroke either a stylistic gesture or geometric shape both technique have their advantage this paper present a method for integrating gesture based and geometric recognition technique significantly outperforming either technique on it own 
it is useful to impose organizational structure over multiagent coalition hierarchy for instance allow for compartmentalization of task if organized correctly task in disjoint subtrees of the hierarchy may be performed in parallel given a notion of the way in which a group of agent need to interact the dynamic distributed multiagent hierarchy generation dyndismhg problem is to determine the best hierarchy that might expedite the process of coordination this paper introduces a distributed algorithm called mobed for both constructing and maintaining organizational agent hierarchy enabling exploitation of parallelism in distributed problem solving the algorithm is proved correct and it is shown that individual addition of agent to the hierarchy will run in an amortized linear number of round the hierarchy resulting after perturbation to the agent coalition have constant bounded edit distance making mobed very well suited to highly dynamic problem 
with the success of local feature in object recognition feature set representation are widely used in computer vision and related domain pyramid match kernel pmk is an efficient approach to quantifying the similarity between two unordered feature set which allows well established kernel machine to learn with such representation however the approximation of pmk to the optimal feature match deteriorates linearly with the dimension of local feature which prohibits the direct use of high dimensional feature in this paper we propose a general data independent kernel to quantify the feature set similarity which give an upper bound of approximation error independent of the dimension of local feature the key idea is to employ the technique of normal random projection to construct a number of low dimensional subspace and perform the original pmk algorithm therein by leveraging on the invariance property of p stable distribution our approach achieves the desirable dimension free property extensive experiment on the eth image database solidly demonstrate the advantage of our approach to high dimensional feature 
this paper summarizes recent work reported at icaps on applying artificial intelligence technique to the control of production printing equipment like many other real world application such a mobile robotics this complex domain requires real time autonomous decision making and robust continual operation to our knowledge this work represents the first successful industrial application of embedd ed domain independent temporal planning at the heart of our system is an on line algorithm that combine technique from state space planning and partial order scheduling for example our planning graph based planning heuristic take resource contention into account when estimating makespan remaining we suggest that this general architecture may prove useful a more intelligent system operate in continual online setting our system ha enabled a new product architecture for our industrial partner and ha been used to drive several commercial prototype when compared with stateof the art off line planner our system is hundred of tim e faster and often find better plan 
to harness modern multi core processor it is imperative to develop parallel version of fundamental algorithm in this paper we present a general approach to best first heuristic search in a shared memory setting each thread attempt to expand the most promising open node by using abstraction to partition the state space we detect duplicate state without requiring frequent locking we allow speculative expansion when necessary to keep thread busy we identify and fix potential livelock condition in our approach verifying it correctness using temporal logic in an empirical comparison on strip planning grid pathfinding and sliding tile puzzle problem using an core machine we show that a implemented in our framework yield faster search than improved version of previous parallel search proposal our approach extends easily to other best first search such a anytime weighted a 
automated abstraction algorithm for sequential imperfect information game have recently emerged a a key component in developing competitive game theory based agent the existing literature ha not investigated the relative performance of different abstraction algorithm instead agent whose construction ha used automated abstraction have only been compared under confounding effect different granularity of abstraction and equilibrium finding algorithm that yield different accuracy when solving the abstracted game this paper provides the first systematic evaluation of abstraction algorithm two family of algorithm have been proposed the distinguishing feature is the measure used to evaluate the strategic similarity between game state one algorithm us the probability of winning a the similarity measure the other us a potential aware similarity measure based on probability distribution over future state we conduct experiment on rhode island hold em poker we compare the algorithm against each other against optimal play and against each agent s nemesis we also compare them based on the resulting game s value interestingly for very coarse abstraction the expectation based algorithm is better but for moderately coarse and fine abstraction the potential aware approach is superior furthermore agent constructed using the expectation based approach are highly exploitable beyond what their performance against the game s optimal strategy would suggest 
partially observable markov decision process have been studied widely a a model for decision making under uncertainty and a number of method have been developed to find the solution for such process such study often involve calculation of the value function of a specific policy given a model of the transition and observation probability and the reward these model can be learned using labeled sample of on policy trajectory however when using empirical model some bias and variance term are introduced into the value function a a result of imperfect model in this paper we propose a method for estimating the bias and variance of the value function in term of the statistic of the empirical transition and observation model such error term can be used to meaningfully compare the value of different policy this is an important result for sequential decision making since it will allow u to provide more formal guarantee about the quality of the policy we implement to evaluate the precision of the proposed method we provide supporting experiment on problem from the field of robotics and medical decision making 
in many real world planning scenario the user are interested in optimizing multiple objective such a makespan and execution cost but are unable to express their exact tradeoff between those objective when a planner encounter such partial preference model rather than look for a single optimal plan it need to present the pareto set of plan and let the user choose from them this idea of presenting the full pareto set is fraught with both computational and user interface challenge to make it practical we propose the approach of finding a representative subset of the pareto set we measure the quality of this representative set using the integrated convex preference icp model originally developed in the or community we implement several heuristic approach based on the metric lpg planner to find a good solution set according to this measure we present empirical result demonstrating the promise of our approach 
in multi robot setting activity recognition allows a robot to respond intelligently to the other robot in it environment conditional random field are temporal model that are well suited for activity recognition because they can robustly incorporate rich non independent feature computed from sensory data in this work we explore feature selection in conditional random field for activity recognition to choose which feature should be included in the final model we compare two feature selection method grafting a greedy forward selection strategy and l regularization which simultaneously smoothes the model and selects a subset of the feature we use robot data recorded during four game of the small size league of the robocup robot soccer world championship to empirically compare the performance of the two feature selection algorithm in term of accuracy of the final model the number of feature selected in the final model and the time required to train the final model 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
identifying maternal and paternal inheritance is essential to be able to find the set of gene responsible for a particular disease although we have access to genotype data genetic makeup of an individual determining haplotype genetic makeup of the parent experimentally is a costly and time consuming procedure due to technological limitation with these biological motivation we study a computational problem called haplotype inference with pure parsimony hipp that asks for the minimal number of haplotype that form a given set of genotype we introduce a novel approach to solving hipp using answer set programming asp according to our experiment with a large number of problem instance some automatically generated and some real our asp based approach solves the most number of problem compared to other approach based on e g integer linear programming branch and bound algorithm sat based algorithm or pseudo boolean optimization method if g j then either h j and h j or h j and h j if g j then h j and h j and if g j then h j and h j we consider the decision version of hipp called hippdec given a set g of n genotype each with m site and a positive integer k decide whether there is a set h of at most k unique haplotype such that each genotype in g is explained by two haplotype in h hipp dec is proved to be np hard gusfield lancia pinotti rizzi assume h is a set that contains n haplotype h h n and that every genotype gi in g is mapped to two haplotype h i and h i in h then h is a solution to hipp dec if the following hold 
the problem of planning in the presence of sensing ha been addressed in recent year a a nondeterministic search problem in belief space in this work we use idea advanced recently for compiling conformant problem into classical one for introducing a different approach where contingent problem p are mapped into non deterministic problem x p in state space we also identify a contingent width parameter and show that for problem p with bounded contingent width the translation is sound polynomial and complete we then solve x p by using a relaxation x p that is a classical planning problem the formulation is tested experimentally over contingent benchmark where it is shown to yield a planner that scale up better than existing contingent planner 
query translation for cross lingual information retrieval clir ha gained increasing attention in the research area previous work mainly used machine translation system bilingual dictionary or web corpus to perform query translation however most of these approach require either expensive language resource or complex language model and cannot achieve timely translation for new query in this paper we propose a novel solution to automatically acquire query translation pair from the knowledge hidden in the click through data that are represented by the url a user click after submitting a query to a search engine our proposed solution consists of two stage identitying bilingual url pair pattern in the click through data and matching query translation pair based on user click behavior experimental result on a real dataset show that our method not only generates existing query translation pair with high precision but also generates many timely query translation pair that could not be obtained by previous method a comparative study between our system and two commercial online translation system show the advantage of our proposed method 
the myerson satterthwaite theorem is a foundational impossibility result in mechanism design which state that no mechanism can be bayes nash incentive compatible individually rational and not run a deficit it hold universally for prior that are continuous gapless and overlapping using automated mechanism design we investigate how often the impossibility occurs over discrete valuation domain while the impossibility appears to hold generally for setting with large number of possible valuation approaching the continuous case domain with realistic valuation structure circumvent the impossibility with surprising frequency even if the impossibility applies the amount of subsidy required to achieve individual rationality and incentive compatibility is relatively small even over large unstructured domain 
an ant deposit pheromone along the path that it travel and is more likely to choose a path with a higher concentration of pheromone the sensing and dropping of pheromone make it easy to understand the trail forming behavior of ant the reinforcement tendency of pheromone following behavior ensures selection of the shortest path from a set of path the reinforcement tendency of pheromone following behavior also ensures a biased selection of the initially followed path over a path which is shorter but discovered through chance at a later point in time under what condition and limit can this initial bias be reversed in this paper we answer this question based on a theoretical analysis of the trail forming behavior of ant we believe our result to contribute to the overall area of understanding how to build scalable system that evolve to solve complex problem e g point covering or the traveling salesman problem without the necessity of central command and control 
in this paper we provide a new method to generate hard k sat instance we incrementally construct a high girth bipartite incidence graph of the k sat instance having high girth assures high expansion for the graph and high expansion implies high resolution width we have extended this approach to generate hard n ary csp instance and we have also adapted this idea to increase the expansion of the system of linear equation used to generate xorsat instance being able to produce harder satisfiable instance than former generator 
systematic search and local search paradigm for combinatorial problem are generally believed to have complementary strength nevertheless attempt to combine the power of the two paradigm have had limited success due in part to the expensive information communication overhead involved we propose a hybrid strategy based on shared memory ideally suited for multi core processor architecture this method enables continuous information exchange between two solver without slowing down either of the two such a hybrid search strategy is surprisingly effective leading to substantially better quality solution to many challenging maximum satisfiability maxsat instance than what the current best exact or heuristic method yield and it often achieves this within second this hybrid approach is naturally best suited to maxsat instance for which proving unsatisfiability is already hard otherwise the method fall back to pure local search 
we present a computational model moraldm which integrates several ai technique in order to model recent psychological finding on moral decision making current theory of moral decision making extend beyond pure utilitarian model by relying on contextual factor that vary with culture moraldm us a natural language system to produce formal representation from psychological stimulus to reduce tailorability the impact of secular versus sacred value are modeled via qualitative reasoning using an order of magnitude representation moraldm us a combination of first principle reasoning and analogical reasoning to determine consequence and utility when making moral judgment we describe how moraldm work and show that it can model psychological result and improve it performance via accumulating example 
dimensionality reduction is an essential step in high dimensional data analysis many dimensionality reduction algorithm have been applied successfully to multi class and multi label problem they are commonly applied a a separate data preprocessing step before classification algorithm in this paper we study a joint learning framework in which we perform dimensionality reduction and multi label classification simultaneously we show that when the least square loss is used in classification this joint learning decouples into two separate component i e dimensionality reduction followed by multi label classification this analysis partially justifies the current practice of a separate application of dimensionality reduction for classification problem we extend our analysis using other loss function including the hinge loss and the squared hinge loss we further extend the formulation to the more general case where the input data for different class label may differ overcoming the limitation of traditional dimensionality reduction algorithm experiment on benchmark data set have been conducted to evaluate the proposed joint formulation 
a recurring theme in the mathematical social science is how to select the most desirable element given a binary dominance relation on a set of alternative schwartz s tournament equilibrium set teq rank among the most intriguing but also among the most enigmatic tournament solution proposed so far due to it unwieldy recursive definition little is known about teq in particular it monotonicity remains an open problem to date yet if teq were to satisfy monotonicity it would be a very attractive solution concept refining both the bank set and dutta s minimal covering set we show that the problem of deciding whether a given alternative is contained in teq is np hard and thus doe not admit a polynomial time algorithm unless p equal np furthermore we propose a heuristic that significantly outperforms the naive algorithm for computing teq 
email occupies a central role in the modern workplace this ha led to a vast increase in the number of email message that user are expected to handle daily furthermore email is no longer simply a tool for asynchronous online communication email is now used for task management personal archiving a well both synchronous and asynchronous online communication whittaker and sidner this explosion can lead to email overload many user are overwhelmed by the large quantity of information in their mailbox in the human computer interaction community there ha been much research on tackling email overload recently similar effort have emerged in the artificial intelligence ai and machine learning community to form an area of research known a intelligent email in this paper we take a user oriented approach to applying ai to email we identify enhancement to email user interface and employ machine learning technique to support these change we focus on three task summary keyword generation reply prediction and attachment prediction and summarize recent work in these area 
modal logic represents knowledge that agent have about other agent knowledge probabilistic modal logic further capture probabilistic belief about probabilistic belief model in those logic are useful for understanding and decision making in conversation bargaining situation and competition unfortunately probabilistic modal structure are impractical for large real world application because they represent their state space explicitly in this paper we scale up probabilistic modal structure by giving them a factored representation this representation applies conditional independence for factoring the probabilistic aspect of the structure a in bayesian network bn we also present two exact and one approximate algorithm for reasoning about the truth value of probabilistic modal logic query over a model encoded in a factored form the first exact algorithm applies inference in bns to answer a limited class of query our second exact method applies a variable elimination scheme and is applicable without restriction our approximate algorithm us sampling and can be used for application with very large model given a query it computes an answer and it confidence level efficiently 
conflict driven clause learning one of the most important component of modern sat solver is also recognized a very important in parallel sat solving indeed it allows clause sharing between multiple processing unit working on related sub problem however without limitation sharing clause might lead to an exponential blow up in communication or to the sharing of irrelevant clause this paper proposes two innovative policy to dynamically adjust the size of shared clause between any pair of processing unit the first approach control the overall number of exchanged clause whereas the second additionally exploit the relevance quality of shared clause experimental result show important improvement of the state of the art parallel sat solver 
sensor provide computer system with a window to the outside world activity recognition see what is in the window to predict the location trajectory action goal and plan of human and object building an activity recognition system requires a full range of interaction from statistical inference on lower level sensor data to symbolic ai at higher level where prediction result and acquired knowledge are passed up each level to form a knowledge food chain in this article i will give an overview of some of the current activity recognition research work and explore a life cycle of learning and inference that allows the lowest level radio frequency signal to be transformed into symbolic logical representation for ai planning which in turn control the robot or guide human user through a sensor network thus completing a full life cycle of knowledge 
we carry out a comparative study of the expressive power of different ontology of matter in term of the ease with which simple physical knowledge can be represented in particular we consider five ontology of model of matter particle model field two ontology for continuous material and a hybrid model we evaluate these in term of how easily eleven benchmark physical law and scenario can be represented 
the recent year have witnessed a surge of interest in semi supervised learning method a common strategy for these algorithm is to require that the predicted data label should be sufciently smooth with respect to the intrinsic data manifold in this paper we argue that rather than penalizing the label smoothness we can directly punish the discriminality of the classication function to achieve a more powerful predictor and we derive two specic algorithm semisupervised discriminative regularization ssdr and semi parametric discriminative semi supervised classication sdsc finally many experimental result are presented to show the effectiveness of our method 
permutation modeling is challenging because of the combinatorial nature of the problem however such modeling is often required in many real world application including activity recognition where subactivities are often permuted and partially ordered this paper introduces a novel hidden permutation model hpm that can learn the partial ordering constraint in permuted state sequence the hpm is parameterized a an exponential family distribution and is flexible so that it can encode constraint via different feature function a chain flipping metropolis hastings markov chain monte carlo mcmc is employed for inference to overcome the o n complexity gradient based maximum likelihood parameter learning is presented for two case when the permutation is known and when it is hidden the hpm is evaluated using both simulated and real data from a location based activity recognition domain experimental result indicate that the hpm performs far better than other baseline model including the naive bayes classifier the hmm classifier and kirshner s multinomial permutation model our presented hpm is generic and can potentially be utilized in any problem where the modeling of permuted state from noisy data is needed 
area coverage is one of the emerging problem in multi robot coordination in this task a team of robot is cooperatively trying to observe or sweep an entire area possibly containing obstacle with their sensor or actuator the goal is to build an ecient path for each robot which jointly ensure that every single point in the environment can be seen or swept by at least one of the robot while performing the task 
the precise specification of reward function for markov decision process mdps is often extremely difficult motiv ating research into both reward elicitation and the robust sol ution of mdps with imprecisely specified reward irmdps we develop new technique for the robust optimization of irmdps using the minimax regret decision criterion that exploit the set of nondominated policy i e policy that are optimal for some instantiation of the imprecise reward function drawing parallel to pomdp value function we devise a witness style algorithm for identifying nondominated policy we also examine several new algorithm for computing minimax regret using the nondominated set and examine both practically and theoretically the impact of approximating this set our result suggest that a small subset of the nondominated set can greatly speed up computation yet yield very tight approximation to minimax regret 
we present and analyze coalitional affinity game a family of hedonic game that explicitly model the value that an agent receives from being associated with other agent we provide a characterization of the social welfare maximizing coalition structure and study the stability property of affinity game using the core solution concept interestingly we observe that member of the core do not necessarily maximize social welfare we introduce a new measure the stability gap to capture this difference using the stability gap we show that for an interesting class of coalitional affinity game the difference between the social welfare of a stable coalition structure and a social welfare maximizing coalition structure is bounded by a factor of two and that this bound is tight 
in this paper we provide a logical framework for using computer to discover theorem in two person finite game in strategic form and apply it to discover class of game that have unique pure nash equilibrium payoff we consider all possible class of game that can be expressed by a conjunction of two binary clause and our program rediscovered kat and thisse s class of weakly unilaterally competitive two person game and came up with several other class of game that have unique pure nash equilibrium payoff it also came up with new class of strict game that have unique pure nash equilibrium where a game is strict if for both player different profile have different payoff 
ontology mapping seek to find semantic correspondence between similar element of different ontology ontology mapping is critical to achieve semantic interoperability in the www due to the fact that ubiquitous constraint e g hierarchical restriction in rdfs caused by the characteristic of ontology and their representation exist in ontology constraint satisfaction ha become an intriguing research problem in ontology mapping area though different technique have been examined to find mapping little work is made to solve constraint satisfaction problem for ontology mapping currently most approach simply validate ontology constraint using isolate heuristic rule instead of comprehensively considering them in a global view this paper proposes a neural network based approach to search for a global optimal solution that can satisfy ontology constraint a many a possible experimental result on oaei benchmark test show the approach is promising it dramatically improves the performance of preliminary mapping result 
legacy system data model can interoperate only if their syntactic and semantic difference are resolved to address this problem we developed the intelligent mapping toolkit imt which enables mixed initiative mapping of metadata and instance between relational data model imt employ a distributed multi agent architecture so that unlike many other effort it can perform mapping task that involve thousand of schema element this architecture includes a novel federation of matching agent that leverage case based reasoning method a part of our pre deployment evaluation for ustranscom and other dod agency we evaluated imt s mapping performance and scalability we show that combination of it matching agent are more effective than any that operate independently and that they scale to realistic problem 
reasoning over complex query in the dl underlying owl is of importance in several application domain we provide decidability and tight upper bound for the problem of checking entailment and containment of positive regular path query under various combination of construct used in such expressive dl specifically regular expression and safe booleans over role and allowing for the combination of any two construct among inverse role qualified number restriction and nominal our result carry over also to the dl of the sr family and thus have a direct impact on owl 
previous work by talbot and osborne explored the use of randomized storage mechanism in language modeling these structure trade a small amount of error for significant space saving enabling the use of larger language model on relatively modest hardware going beyond space efficient count storage here we present the talbot osborne morris bloom tomb counter an extended model for performing space efficient counting over stream of finite length theoretical and experimental result are given showing the promise of approximate counting over large vocabulary in the context of limited space 
dynamic programming method including value iteration lao rtdp and derivative are popular algorithm for solving markov decision process mdps unfortunately however these technique store the mdp model extensionally in a table and thus are limited by the amount of main memory available since the required space is exponential in the number of domain feature these dynamic programming method are ineffective for large problem to address this problem edelcamp et al devised the external memory value iteration emvi algorithm which us a clever sorting scheme to efficiently move part of the model between disk and main memory while emvi can handle larger problem than previously addressed the need to repeatedly perform external sort still limit scalability this paper proposes a new approach we partition an mdp into smaller piece block keeping just the relevant block in memory and performing bellman backup block by block experiment show that our algorithm is able to solve large mdps an order of magnitude faster than emvi 
we study propagation algorithm for the conjunction of two alldifferent constraint solution of an alldifferent constraint can be seen a perfect matchings on the variable value bipartite graph therefore we investigate the problem of finding simultaneous bipartite matchings we present an extension of the famous hall theorem which characterizes when simultaneous bipartite matchings exists unfortunately finding such matchings is np hard in general however we prove a surprising result that finding a simultaneous matching on a convex bipartite graph take just polynomial time based on this theoretical result we provide the first polynomial time bound consistency algorithm for the conjunction of two alldifferent constraint we identify a pathological problem on which this propagator is exponentially faster compared to existing propagator our experiment show that this new propagator can offer significant benefit over existing method 
in this paper we present an approach to the task of generating and resolving referring expression re for conversational mobile robot it is based on a spatial knowledge base encompassing both robotand human centric representation existing algorithm for the generation of referring expression gre try to find a description that uniquely identifies the referent with respect to other entity that are in the current context mobile robot however act in large scale space that is environment that are larger than what can be perceived at a glance e g an office building with different floor each containing several room and object one challenge when referring to elsewhere is thus to include enough information so that the interlocutor can extend their context appropriately we address this challenge with a method for context construction that can be used for both generating and resolving re two previously disjoint aspect our approach is embedded in a bi directional framework for natural language processing for robot 
in this paper we consider a general problem of semi supervised preference learning in which we assume that we have the information of the extreme case and some ordered constraint our goal is to learn the unknown preference of the other place taking the potential housing place selection problem a an example we have many candidate place together with their associated information e g position environment and we know some extreme example i e several place are perfect for building a house and several place are the worst that cannot build a house there and we know some partially ordered constraint i e for two place which place is better then how can we judge the preference of one potential place whose preference is unknown beforehand we propose a bayesian framework based on gaussian process to tackle this problem from which we not only solve for the unknown preference but also the hyperparameters contained in our model 
knowledge based recommenders support user in the identification of interesting item from large and potentially complex assortment in case where no recommendation could be found for a given set of requirement such system propose explanation that indicate minimal set of faulty requirement unfortunately such explanation are not personalized and do not include repair proposal which trigger a low degree of satisfaction and frequent cancellation of recommendation session in this paper we present a personalized repair approach that integrates the calculation of explanation with collaborative problem solving technique in order to demonstrate the applicability of our approach we present the result of an empirical study that show significant improvement in the accuracy of prediction for interesting repair 
since it introduction in the mid ninety dung s theory of abstract argumentation framework ha been influential in artificial intelligence dung viewed argument a abstract entity with a binary defeat relation among them this enabled extensive analysis of different semantic argument acceptance criterion however little attention wa given to comparing such criterion in relation to the preference of selfinterested agent who may have conflicting preference over the final status of argument in this paper we define a number of agent preference relation over argumentation outcome we then analyse different argument evaluation rule taking into account the preference of individual agent our framework and result inform the mediator e g judge to decide which argument evaluation rule i e semantics to use given the type of agent population involved 
computer agent are increasingly deployed in setting in which they make decision with people such a electronic commerce collaborative interface and cognitive assistant however the scientific evaluation of computational strategy for human computer decision making is a costly process involving time effort and personnel this paper investigates the use of peer designed agent pda computer agent developed by human subject a a tool for facilitating the evaluation process of automatic negotiator that were developed by researcher it compare the performance between automatic negotiator that interacted with pda to automatic negotiator that interacted with actual people in different domain the experiment included more than human subject and pda developed by student result showed that the automatic negotiator outperformed pda in the same situation in which they outperformed people and that on average they exhibited the same measure of generosity towards their negotiation partner these pattern occurred for all type of domain and for all type of automated negotiator despite the fact that there were individual difference between the behavior of pda and people the study thus provides an empirical proof that pda can alleviate the evaluation process of automatic negotiator and facilitate their design 
this paper deal with evolutionary clustering which refers to the problem of clustering data with distribution drifting along time starting from a density estimation view to clustering problem we propose two general on line framework in the first framework i e historical data dependent hdd current model distribution is designed to approximate both current and historical data distribution in the second framework i e historical model dependent hmd current model distribution is designed to approximate both current data distribution and historical model distribution both framework are based on the general exponential family mixture efm model a a result all conventional clustering algorithm based on efms can be extended to evolutionary setting under the two framework empirical result validate the two framework 
resource allocation in computing cluster is traditionally centralized which limit the cluster scale effective resource allocation in a network of computing cluster may enable building larger computing infrastructure we consider this problem a a novel application for multiagent learning mal we propose a mal algorithm and apply it for optimizing online resource allocation in cluster network the learning is distributed to each cluster using local information only and without access to the global system reward experimental result are encouraging our multiagent learning approach performs reasonably well compared to an optimal solution and better than a centralized myopic allocation approach in some case 
adopting a decision theoretic perspective we investigate the problem of optimal testing of structured knowledge the canonical example being a qualifying examination of a graduate student the setting is characterized by several factor examinee s knowledge structured around several inter dependent topic a limited budget of question available to the examiner a decision to be made pas fail and an utility for good and bad decision the existence of multiple professor brings up additional issue such a committee formation and the existence of multiple student brings up issue such a fairness 
farm manager have to deal with many conflicting objective when planning which crop to cultivate soil characteristic are extremely important when determining yield potential fertilization and liming are commonly used to adapt soil to the nutritional requirement of the crop to be cultivated planting the crop that will best fit the soil characteristic is an interesting alternative to minimize the need for soil treatment reducing cost and potential environmental damage in addition farmer usually look for investment that offer the greatest potential earnings with the least possible risk according to the objective to be considered the crop selection problem can be difficult to solve using traditional tool therefore this work proposes an approach based on multiobjective evolutionary algorithm to help in the selection of an appropriate cultivation plan considering five crop alternative and five objective simultaneously 
there is increasing interest in building system that can automatically interpret hand drawn sketch however many challenge remain in term of recognition accuracy robustness to different drawing style and ability to generalize across multiple domain to address these challenge we propose a new approach to sketched symbol recognition that focus on the visual appearance of the symbol this allows u to better handle the range of visual and stroke level variation found in freehand drawing we also present a new symbol classifier that is computationally efficient and invariant to rotation and local deformation we show that our method exceeds state of the art performance on all three domain we evaluated including handwritten digit powerpoint shape and electrical circuit symbol 
changing preference is very common in real life the expressive power of the operation of preference change introduced so far in the literature is limited to adding new information about preference and equivalence here we discus the operation of discarding preference preference contraction we argue that the property of minimality and the preservation of strict partial order are crucial for contraction contraction can be further constrained by specifying which preference should not be contracted we provide algorithm for computing minimal and minimal preference protecting contraction we also show some preference query optimization technique which can be used in the presence of contraction 
in bartholdi tovey and trick opened the study of control attack on election attempt to improve the election outcome by such action a adding deleting candidate or voter that work ha led to many result on how algorithm can be used to find attack on election and how complexity theoretic hardness result can be used a shield against attack however all the work in this line ha assumed that the attacker employ just a single type of attack in this paper we model and study the case in which the attacker launch a multipronged i e multimode attack we do so to more realistically capture the richness of reallife setting for example an attacker might simultaneously try to suppress some voter attract new voter into the election and introduce a spoiler candidate our model provides a unified framework for such varied attack and by constructing polynomial time multiprong attack algorithm we prove that for various election system even such concerted flexible attack can be perfectly planned in deterministic polynomial time 
many existing explanation method in bayesian network such a maximum a posteriori map assignment and most probable explanation mpe generate complete assignment for target variable a priori the set of target variable is often large but only a few of them may be most relevant in explaining given evidence generating explanation with all the target variable is hence not always desirable this paper address the problem by proposing a new framework called most relevant explanation mre which aim to automatically identify the most relevant target variable we will also discus in detail a specific instance of the framework that us generalized bayes factor a the relevance measure finally we will propose an approximate algorithm based on reversible jump mcmc and simulated annealing to solve mre empirical result show that the new approach typically find much more concise explanation than existing method 
mechanism especially on the internet have begun allowing people or organization to express richer preference in order to provide for greater level of overall satisfaction in this paper we develop an operational methodology for quantifying the expected gain in economic efficiency associated with different form of expressiveness we begin by proving that the sponsored search mechanism gsp used by google yahoo msn etc can be arbitrarily inefficient we then experimentally compare it efficiency to a slightly more expressive variant pgsp which solicits an extra bid for a premium class of position we generate random preference distribution based on published industry knowledge we determine ideal strategy for the agent using a custom tree search technique and we also benchmark using straightforward heuristic bidding strategy the gsp s efficiency loss is greatest in the practical case where some advertiser brand advertiser prefer top position while others value advertiser prefer middle position and that loss can be dramatic it is also worst when agent have small profit margin while the pgsp is only slightly more expressive and thus not much more cumbersome it remove almost all of the efficiency loss in all of the setting we study 
recent research ha shown the benefit of framing problem of imitation learning a solution to markov decision problem this approach reduces learning to the problem of recovering a utility function that make the behavior induced by a near optimal policy closely mimic demonstrated behavior in this work we develop a probabilistic approach based on the principle of maximum entropy our approach provides a well defined globally normalized distribution over decision sequence while providing the same performance guarantee a existing method we develop our technique in the context of modeling real world navigation and driving behavior where collected data is inherently noisy and imperfect our probabilistic approach enables modeling of route preference a well a a powerful new approach to inferring destination and route based on partial trajectory 
in this paper we address the problem of designing an interruptible system in a setting in which n problem instance all equally important must be solved the system involves scheduling execution of contract algorithm which offer a trade off between allowable computation time and quality of the solution in m identical parallel processor when an interruption occurs the system must report a solution to each of the n problem instance the quality of this output is then compared to the best possible algorithm that ha foreknowledge of the interruption time and must likewise produce solution to all n problem instance this extends the well studied setting in which only one problem instance is queried at interruption time we propose a schedule which we prove is optimal for the case of a single processor for multiple processor we show that the quality of the schedule is within a small factor from optimal 
complex multi agent system often are not amenable to standard game theoretic analysis i study method for strategic reasoning that scale to more complex interaction drawing on computational and empirical technique several recent study have applied simulation to estimate game model using a methodology known a empirical game theoretic analysis i report a successful application of this methodology to the trading agent competition supply chain management game game theory ha previously played little if any role in analyzing this scenario or others like it in the rest of the thesis i perform broader evaluation of empirical game analysis method using a novel experimental framework i introduce meta game to model situation where player make strategy choice based on estimated game model each player chooses a meta strategy which is a general method for strategy selection that can be applied to a class of game these meta strategy can be used to select strategy based on empirical model such a an estimated payoff matrix i investigate candidate meta strategy experimentally testing them across different class of game and observation model to identify general performance pattern for example i show that the strategy choice made using a na ve equilibrium model quickly degrade in quality a observation noise is introduced i analyze three family of meta strategy that predict distribution of play each interpolating between uninformed and na ve equilibrium prediction using a single parameter these strategy space improve on the na ve method capturing to some degree the effect of observation uncertainty of these candidate i identify logit equilibrium a the champion supported by considerable evidence that it prediction generalize across many context i also evaluate exploration policy for directing game simulation on two task equilibrium confirmation and strategy selection policy based on computing best response are able to exploit a variety of structural property to confirm equilibrium with limited payoff evidence a novel policy i propose subgame best response dynamic improves previous method for this task by confirming mixed equilibrium in addition to pure equilibrium i apply meta strategy analysis to show that these exploration policy can improve the strategy selection of logit equilibrium 
we introduce dtproblog a decision theoretic extension of prolog and it probabilistic variant problog dtproblog is a simple but expressive probabilistic programming language that allows the modeling of a wide variety of domain such a viral marketing in dtproblog the utility of a strategy a particular choice of action is defined a the expected reward for it execution in the presence of probabilistic effect the key contribution of this paper is the introduction of exact a well a approximate solver to compute the optimal strategy for a dtproblog program and the decision problem it represents by making use of binary and algebraic decision diagram we also report on experimental result that show the effectiveness and the practical usefulness of the approach 
in this paper we formulate the problem of early classification of time series data which is important in some time sensitive application such a health informatics we introduce a novel concept of mpl minimum prediction length and develop ect early classification on time series an effective nearest neighbor classification method ect make early prediction and at the same time retains the accuracy comparable to that of a nn classifier using the full length time series our empirical study using benchmark time series data set show that ect work well on the real data set where nn classification is effective 
matrix factorization technique have been frequently applied in information processing task among them non negative matrix factorization nmf have received considerable attention due to it psychological and physiological interpretation of naturally occurring data whose representation may be part based in human brain on the other hand from geometric perspective the data is usually sampled from a low dimensional manifold embedded in high dimensional ambient space one hope then to find a compact representation which uncovers the hidden topic and simultaneously respect the intrinsic geometric structure in this paper we propose a novel algorithm called locality preserving non negative matrix factorization lpnmf for this purpose for two data point we use kl divergence to evaluate their similarity on the hidden topic the optimal map are obtained such that the feature value on hidden topic are restricted to be non negative and vary smoothly along the geodesic of the data manifold our empirical study show the encouraging result of the proposed algorithm in comparison to the state of the art algorithm on two large high dimensional database 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
protocol specifying business interaction among autonomous party enable reuse and promote interoperability a protocol is specified from a global viewpoint but enacted in a distributed manner by agent playing different role each role describes a local representation an ill specified protocol may yield role that fail to produce correct enactment of the protocol existing approach lack a formal and comprehensive treatment of this problem building on recent work on declaratively specifying a protocol a a set of rule of causal logic this paper formally defines the enactability of protocol it present necessary and sufficient condition for the enactability of a protocol a well a a decision procedure for extracting correct role from enactable protocol 
the problem of dividing a sequence of value into segment occurs in database system information retrieval and knowledge management the challenge is to select a finite number of boundary for the segment so a to optimize an objective error function defined over those segment although this optimization problem can be solved in polynomial time the algorithm which achieves the minimum error doe not scale well hence it is not practical for application with massive data set there is considerable research with numerous approximation and heuristic algorithm still none of those approach ha resolved the quality efficiency tradeoff in a satisfactory manner in halim karras and yap we obtain near linear time algorithm which achieve both the desired scalability and near optimal quality thus dominating earlier approach in this paper we show how two idea from artificial intelligence an efficient local search and recombination of multiple solution reminiscent of genetic algorithm are combined in a novel way to obtain state of the art histogram construction algorithm 
current search engine cannot effectively rank those relational data which exists on dynamic website supported by online database in this study to rank such structured data we propose a new model relaxed dominant relationship rdr which extends the state of the art work by incorporating rank aggregation method we propose efficient strategy on building compressed data structure to encode the core part of rdr between item efficient querying approach are devised to facilitate the ranking process and to answer the rdr query extensive experiment are conducted and the result illustrate the effectiveness and efficiency of our method 
in a yes no voting game a set of voter must determine whether to accept or reject a given alternative weighted voting game are a well studied subclass of yes no voting game in which each voter ha a weight and an alternative is accepted if the total weight of it supporter exceeds a certain threshold weighted voting game are naturally extended to k vector weighted voting game which are intersection of k different weighted voting game a coalition win if it win in every component game the dimensionality k of a kvector weighted voting game can be understood a a measure of the complexity of the game in this paper we analyse the dimensionality of such game from the point of view of complexity theory we consider the problem of equivalence checking whether two given voting game have the same set of winning coalition and minimality checking whether a given k vector voting game can be simplified by deleting one of the component game or more generally is equivalent to a k weighted voting game with k k we show that these problem are computationally hard even if k or all weight are or however we provide efficient algorithm for case where both k is small and the weight are polynomially bounded we also study the notion of monotonicity in voting game and show that monotone yes no voting game are essentially a hard to represent and work with a general game 
unifying first order logic and probability is a long standing goal of ai and in recent year many representation combining aspect of the two have been proposed however inference in them is generally still at the level of propositional logic creating all ground atom and formula and applying standard probabilistic inference method to the resulting network ideally inference should be lifted a in first order logic handling whole set of indistinguishable object together in time independent of their cardinality poole and braz et al developed a lifted version of the variable elimination algorithm but it is extremely complex generally doe not scale to realistic domain and ha only been applied to very small artificial problem in this paper we propose the first lifted version of a scalable probabilistic inference algorithm belief propagation loopy or not our approach is based on first constructing a lifted network where each node represents a set of ground atom that all pas the same message during belief propagation we then run belief propagation on this network we prove the correctness and optimality of our algorithm experiment show that it can greatly reduce the cost of inference 
logic reasoning approach to fault diagnosis account for the fact that a component cj may fail intermittently by introducing a parameter gj that express the probability the component exhibit correct behavior this component parameter gj in conjunction with a priori fault probability is used in a bayesian framework to compute the posterior fault candidate probability usually information on gj is not known a priori while proper estimation of gj can have a great impact on the diagnostic accuracy at present only approximation have been proposed we present a novel framework barinel that computes exact estimation of gj a integral part of the posterior candidate probability computation barinel s diagnostic performance is evaluated for both synthetic and real software system our result show that our approach is superior to approach based on classical persistent fault model a well a previously proposed intermittent fault model 
we develop and test computational method for guiding collaboration that demonstrate how shared plan can be created in real world setting where agent can be expected to have diverse and varying goal preference and availability the method are motivated and evaluated in the realm of ridesharing using gps log of commuting data we consider challenge with coordination among self interested people aimed at minimizing the cost of transportation and the impact of travel on the environment we present planning optimization and payment mechanism that provide fair and efficient solution to the rideshare collaboration challenge we evaluate different vcg based payment scheme in term of their computational efficiency budget balance incentive compatibility and strategy proofness we present the behavior and analysis provided by the abc ridesharing prototype system the system learns about destination and preference from gps trace and calendar and considers time fuel environmental and cognitive cost we review how abc generates rideshare plan from hundred of real life gps trace collected from a community of commuter and reflect about the promise of employing the abc method to reduce the number of vehicle on the road thus reducing co emission and fuel expenditure 
understanding the computational complexity of manipulation in election is arguably the most central agenda in computational social choice one of the influential variation of the the problem involves a coalition of manipulator trying to make a favorite candidate win the election although the complexity of the problem is well studied under the assumption that the voter are weighted there were very few successful attempt to abandon this strong assumption in this paper we study the complexity of the unweighted coalitional manipulation problem ucm under several prominent voting rule our main result is that ucm is np complete under the maximin rule this resolve an enigmatic open question we then show that ucm is np complete under the ranked pair rule even with respect to a single manipulator furthermore we provide an extreme hardness of approximation result for an optimization version of ucm under ranked pair finally we show that ucm under the bucklin rule is in p 
hierarchy are one of the most common organizational structure observed in multi agent system in this paper we study vertical specialization a a reason for hierarchical structure in vertically specialized system more highly skilled agent are also more costly by using le capable agent to initially process task and forwarding only exceptional task to more capable agent such system may be able to economize on the number of highly capable agent the result is a hierarchical structure with least capable agent at the bottom however such a structure increase the delay in completing some task because they must pas through multiple level of control thus vertical specialization present a tradeoff between economizing on skilled agent and increasing task completion time we find that for a wide range of setting vertical specialization induces an optimal hierarchy of height at most three this suggests that a multi agent system designer interested in exploiting vertical specialization need to use at most three level of specialization in order to reap most of the benefit 
software developer increasingly rely on information from the web such a document or code example on application programming interface apis to facilitate their development process however api document often do not include enough information for developer to fully understand the api usage while searching for good code example requires non trivial effort to address this problem we propose a novel code search engine combining the strength of browsing document and searching for code example by returning document embedded with high quality code example summary mined from the web our evaluation result show that our approach provides code example with high precision and boost programmer productivity 
in hand sketched drawing nearly identical stroke may have different meaning to a user for instance a scribble could signify either that a shape should be filled in or that it should be deleted this work describes a method for determining user intention in drawing scribble in the context of a pen based computer sketch our study show that given two stroke a circle and a scribble two feature bounding ratio and density can quickly and effectively determine a user s intention 
there is now extensive interest in reasoning about moving object a pst knowledge base is a set of pst atom which are statement of the form object o is wa will be at location l at time t with probability in the interval l u in this paper we study mechanism for belief revision in pst kb we propose multiple method for revising pst kb these method involve finding maximally consistent subset a well a changing the spatial temporal and probabilistic component of the atom we show that some method cannot satisfy the agm axiom for belief revision while others do but are conp hard finally we present an algorithm for revision through probability change which run in polynomial time and satisfies the agm axiom 
domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain intuitively discovering a good feature representation across domain is crucial in this paper we first propose to find such a representation through a new learning method transfer component analysis tca for domain adaptation tca try to learn some transfer component across domain in a reproducing kernel hilbert space using maximum mean miscrepancy in the subspace spanned by these transfer component data property are preserved and data distribution in different domain are close to each other a a result with the new representation in this subspace we can apply standard machine learning method to train classifier or regression model in the source domain for use in the target domain furthermore in order to uncover the knowledge hidden in the relation between the data label from the source and target domain we extend tca in a semisupervised learning setting which encodes label information into transfer component learning we call this extension semisupervised tca the main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domain in a latent space for domain adaptation we propose both unsupervised and semisupervised feature extraction approach which can dramatically reduce the distance between domain distribution by projecting data onto the learned transfer component finally our approach can handle large datasets and naturally lead to out of sample generalization the effectiveness and efficiency of our approach are verified by experiment on five toy datasets and two real world application cross domain indoor wifi localization and cross domain text classification 
we present a generalization of the magic set technique to datalog program with possibly unstratified negation under the stable model semantics originally defined in faber greco leone the technique optimizes datalog program by mean of a rewriting algorithm that preserve query equivalence under the proviso that the original program is consistent the approach is motivated by recently proposed method for query answering in data integration and inconsistent database which use cautious reasoning over consistent datalog program under the stable model semantics in order to prove the correctness of our magic set transformation we have introduced a novel notion of modularity for datalog under the stable model semantics which is more suitable for query answering than previous module definition and which is also relevant per se a module under this definition guarantee independent evaluation of query if the full program is consistent otherwise it guarantee soundness under cautious and completeness under brave reasoning 
web search engine respond to a query by returning more result than can be reasonably reviewed these result typically include the title link and snippet of content from the target link each result ha the potential to be useful or useless and thus reviewing it ha a cost and potential benefit this paper study the behavior of a rational agent in this setting whose objective is to maximize the probability of finding a satisfying result while minimizing cost we propose two similar agent with different capability one that only compare result snippet relatively and one that predicts from the result snippet whether the result will be satisfying we prove that the optimal strategy for both agent is a stopping rule the agent review a fixed number of result until the marginal cost is greater than the marginal expected benefit maximizing the overall expected utility finally we discus the relationship between rational agent and search user and how our finding help u understand reviewing behavior 
this paper deal with multiobjective optimization in the context of multiattribute utility theory the alternative feasible solution are seen a element of a product set of attribute and preference over solution are represented by generalized additive decomposable gai utility function modeling individual preference or criterion due to decomposability utility vector attached to solution can be compiled into a graphical structure closely related to junction tree the so called gai net we first show how the structure of the gai net can be used to determine efficiently the exact set of pareto optimal solution in a product set and provide numerical test on random instance since the exact determination of the pareto set is intractable in worst case we propose a near admissible algorithm with performance guarantee exploiting the gai structure to approximate the set of pareto optimal solution we present numerical experimentation showing that both utility decomposition and approximation significantly improve resolution time in multiobjective search problem 
much of the work on using markov decision process mdps in artificial intelligence ai focus on solving a single problem however ai agent often exist over a long period of time during which they may be required to solve several related task this type of scenario ha motivated a significant amount of recent research in knowledge transfer method for mdps the idea is to allow an agent to continue to re use the expertise accumulated while solving past task over it lifetime see taylor stone for a comprehensive survey 
kernel discriminant analysis kda is an effective approach for supervised nonlinear dimensionality reduction probabilistic model can be used with kda to improve it robustness however the state of the art of such model could only handle binary class problem which confines their application in many real world problem to overcome this limitation we propose a novel nonparametric probabilistic model based on gaussian process for kda to handle multiclass problem the model provides a novel bayesian interpretation for kda which allows it parameter to be automatically tuned through the optimization of the marginal log likelihood of the data empirical study demonstrates the efficacy of the proposed model 
poirot is an integration framework for combining machine learning mechanism to learn hierarchical model of web service procedure from a single or very small set of demonstration example the system is organized around a shared representation language for communication with a central hypothesis blackboard component learning system share semantic representation of their hypothesis generalization and inference about demonstration trace to further the process component may generate learning goal for other learning component poirot s learner or hypothesis former develop workflow that include order dependency subgoals and decision criterion for selecting or prioritizing subtasks and service parameter hypothesis evaluator guided by poirot s meta control component plan experiment to confirm or disconfirm hypothesis extracted from these learning product collectively they create method that poirot can use to reproduce the demonstration and solve similar problem after it first phase of development poirot ha demonstrated it can learn some moderately complex hierarchical task model from semantic trace of user generated service transaction sequence at a level that is approaching human performance on the same learning task 
the sparsity problem in collaborative filtering cf is a major bottleneck for most cf method in this paper we consider a novel approach for alleviating the sparsity problem in cf by transferring useritem rating pattern from a dense auxiliary rating matrix in other domain e g a popular movie rating website to a sparse rating matrix in a target domain e g a new book rating website we do not require that the user and item in the two domain be identical or even overlap based on the limited rating in the target matrix we establish a bridge between the two rating matrix at a cluster level of user item rating pattern in order to transfer more useful knowledge from the auxiliary task domain we first compress the rating in the auxiliary rating matrix into an informative and yet compact cluster level rating pattern representation referred to a a codebook then we propose an efficient algorithm for reconstructing the target rating matrix by expanding the codebook we perform extensive empirical test to show that our method is effective in addressing the data sparsity problem by transferring the useful knowledge from the auxiliary task a compared to many state of the art cf method 
we study the computational complexity of conjunctive query answering w r t ontology formulated in fragment of the description logic shiq our main result is the identification of two new source of complexity the combination of transitive role and role hierarchy which result in exptime hardness and transitive role alone which result in co nexptime hardness these bound complement the existing result that inverse role make query answering in shiq exptime hard we also show that conjunctive query answering with transitive role but without inverse role and role hierarchy remains in exptime if the abox is tree shaped 
many application are facing the problem of learning from an objective dataset whereas information from other auxiliary source may be beneficial but cannot be integrated into the objective dataset for learning in this paper we propose an omni view learning approach to enable learning from multiple data collection the theme is to organize heterogeneous data source into a unified table with global data view to achieve the omni view learning goal we consider that the objective dataset and the auxiliary datasets share some instance level dependency structure we then propose a relational k mean to cluster instance in each auxiliary dataset such that cluster can help build new feature to capture correlation between the objective and auxiliary datasets experimental result demonstrate that omni view learning can help build model which outperform the one learned from the objective dataset only comparison with the co training algorithm further assert that omni view learning provides an alternative yet effective way for semi supervised learning 
linear causal model known a structural equation model sems are widely used for data analysis in the social science economics and artificial intelligence in which random variable are assumed to be continuous and normally distributed this paper deal with one fundamental problem in the application of sems parameter identification the paper us the graphical model approach and provides a procedure for solving the identification problem in a special class of sems 
most sketch recognition system are accurate in recognizing either text or shape graphic ink stroke but not both distinguishing between shape and text stroke is therefore a critical task in recognizing hand drawn digital ink diagram that contain text label and annotation we have found the entropy rate to be an accurate criterion of classification we found that the entropy rate is significantly higher for text stroke compared to shape stroke and can serve a a distinguishing factor between the two using a single feature zero order entropy rate our system produced a correct classification rate of on test data belonging to diagrammatic domain for which the threshold wa trained on it also performed favorably on an unseen domain for which no training example were supplied 
reconstructing gene network from micro array data can provide information on the mechanism that govern cellular process numerous study have been devoted to addressing this problem a popular method is to view the gene network a a bayesian inference network and to apply structure learning method to determine the topology of the gene network there are however several shortcoming with the bayesian structure learning approach for reconstructing gene network they include high computational cost associated with analyzing a large number of gene and inefficiency in exploiting prior knowledge of co regulation that could be derived from gene ontology go information in this paper we present a knowledge driven matrix factorization kmf framework for reconstructing modular gene network that address these shortcoming in kmf gene expression data is initially used to estimate the correlation matrix the gene module and the interaction among the module are derived by factorizing the correlation matrix the prior knowledge in go is integrated into matrix factorization to help identify the gene module an alternating optimization algorithm is presented to efficiently find the solution experiment show that our algorithm performs significantly better in identifying gene module than several state of the art algorithm and the interaction among the module uncovered by our algorithm are proved to be biologically meaningful 
the interpreted system model offer a computationally grounded model in term of the state of computer process to s epistemic logic this paper extends the interpreted system model and provides a computationally grounded one called the interpreted perception system model to those episternic logic other than s it is usually assumed in the interpreted system model that those part of the environment that are visible to an agent are correctly perceived by the agent a a whole the essential idea of the interpreted perception system model is that an agent may have incorrect perception or observation to the visible part of the environment and the agent may not be aware of this the notion of knowledge can be defined so that an agent know a statement iff the statement hold in those state that the agent can not distinguish from the current state by using only her correct observation we establish a logic of knowledge and certainty called kc logic with a sound and complete proof system the knowledge modality in this logic is s valid it becomes s if we assume an agent always ha correct observation and more interestingly it can be s or s under other natural constraint on agent and their sensor to the environment 
planning a satisfiability is a principal approach to planning with many eminent advantage the existing planning a satisfiability technique usually use encoding compiled from the strip formalism we introduce a novel sat encoding scheme based on the sa formalism it exploit the structural information in the sa formalism resulting in more compact sat instance and reducing the number of clause by up to fold our result show that this encoding scheme improves upon the strip based encoding in term of both time and memory efficiency 
the canadian traveller problem is a stochastic shortest path problem in which one learns the cost of an edge only when arriving at one of it endpoint the goal is to find an optimal policy that minimizes the expected cost of travel the problem is known to be p hard since there ha been no significant progress on approximation algorithm for several decade we have chosen to seek out special case for which exact solution exist in the hope of demonstrating technique that could lead to further progress applying a mix of technique from algorithm analysis and the theory of markov decision process we provide efficient exact algorithm for directed acyclic graph and undirected graph of disjoint path from source to destination with random two valued edge cost we also give worst case performance analysis and experimental data for two natural heuristic 
direction relation between extended spatial object are important commonsense knowledge recently goyal and egenhofer proposed a formal model called cardinal direction calculus cdc for representing direction relation between connected plane region cdc is perhaps the most expressive qualitative calculus for directional information and ha attracted increasing interest from area such a artificial intelligence geographical information science and image retrieval given a network of cdc constraint the consistency problem is deciding if the network is realizable by connected region in the real plane this paper provides a cubic algorithm for checking consistency of basic cdc constraint network a one by product we also show that any consistent network of cdc constraint ha a canonical realization in digital plane the cubic algorithm can also been adapted to cope with disconnected region in which case the current best algorithm is of time complexity o n 
probabilistic modeling ha been a dominant approach in machine learning research a the field evolves thc problem of interest become increasingly challenging and complex making complex decision in real world problem often involves assigning value to set of interdependent variable where the expressive dependency structure can influence or even dictate what assignment are possible however incorporating nonlocal depcndencies in a probabilistic model can lead to intractable training and inference this paper present constraint conditional model ccms a framework that augments probabilistic model with declarative constraint a a way to support decision in an expressive output space while maintaining modularity and tractability of training we further show that declarative constraint can be used to take advantage of unlabeled data when training the probabilistic model 
the core computational step in spectral learning finding the projection of a function onto the eigenspace of a symmetric operator such a a graph laplacian generally incurs a cubic computational complexity o n this paper describes the use of lanczos eigenspace projection for accelerating spectral projection which reduces the complexity to o nt op n n operation where n is the number of distinct eigenvalue and t op is the complexity of multiplying t by a vector this approach is based on diagonalizing the restriction of the operator to the krylov space spanned by the operator and a projected function even further saving can be accrued by constructing an approximate lanczos tridiagonal representation of the krylov space restricted operator a key novelty of this paper is the use of krylov subspace modulated lanczos acceleration for multi resolution wavelet analysis a challenging problem of learning to control a robot arm is used to test the proposed approach 
recent research ha shown that surprisingly rich model of human behavior can be learned from gps positional data however most research to date ha concentrated on modeling single individual or aggregate statistical property of group of people given noisy real world gps data we in contrast consider the problem of modeling and recognizing activity that involve multiple related individual playing a variety of role our test domain is the game of capture the flag an outdoor game that involves many distinct cooperative and competitive joint activity we model the domain using markov logic a statistical relational language and learn a theory that jointly denoises the data and infers occurrence of high level activity such a capturing a player our model combine constraint imposed by the geometry of the game area the motion model of the player and by the rule and dynamic of the game in a probabilistically and logically sound fashion we show that while it may be impossible to directly detect a multi agent activity due to sensor noise or malfunction the occurrence of the activity can still be inferred by considering both it impact on the future behavior of the people involved a well a the event that could have preceded it we compare our unified approach with three alternative both probabilistic and nonprobabilistic where either the denoising of the gps data and the detection of the high level activity are strictly separated or the state of the player are not considered or both we show that the unified approach with the time window spanning the entire game although more computationally costly is significantly more accurate 
planning with temporally extended goal and uncontrollable event ha recently been introduced a a formal model for system reconfiguration problem an important application is to automatically reconfigure a real life system in such a way that it subsequent internal evolution is consistent with a temporal goal formula in this paper we introduce an incremental search algorithm and a search guidance heuristic two generic planning enhancement an initial problem is decomposed into a series of subproblems providing two main way of speeding up a search firstly a subproblem focus on a part of the initial goal secondly a notion of action relevance allows to explore with higher priority action that are heuristically considered to be more relevant to the subproblem at hand even though our technique are more generally applicable we restrict our attention to planning with temporally extended goal and uncontrollable event our idea are implemented on top of a successful previous system that performs online learning to better guide planning and to safely avoid potentially expensive search in experiment the system speed performance is further improved by a convincing margin 
in machine learning problem labeled data are often in short supply one of the feasible solution for this problem is transfer learning it can make use of the labeled data from other domain to discriminate those unlabeled data in the target domain in this paper we propose a transfer learning framework based on similarity matrix approximation to tackle such problem two practical algorithm are proposed which are the label propagation and the similarity propagation in these method we build a hybrid graph based on all available data then the information is transferred cross domain through alternatively constructing the similarity matrix for different part of the graph among all related method similarity propagation approach can make maximum use of all available similarity information across domain this lead to more efficient transfer and better learning result the experiment on real world text mining application demonstrates the promise and effectiveness of our algorithm 
in this paper we present a normal form for concept expression in the description logicalc which is based on a recently introduced notion of prime implicate for the modal logick we show that concept in prime implicate normal form enjoy a number of interesting property for one thing they do not contain any unnecessary atomic concept or role not only doe this make the concept more readable but it also help u to identify the part of a concept which are relevant to a given subject matter another feature of concept in prime implicate normal form is that they can be easily approximated over a sublanguage or up to a xed depth these operation may prove useful when a concept description is too large to be fully understood or when data need to be exchanged between system using dierent language perhaps the most remarkable property of prime implicate normal form is that subsumption betweenalc concept in this form can be carried out in quadratic time using a simple structural subsumption algorithm reminiscent of those used for le expressive description logic this property make prime implicate normal form interesting for the purpose of knowledge compilation of course in order to take advantage of all of these nice property we need a way to transform concept into equivalent concept in prime implicate normal form we provide a sound and complete algorithm for putting concept into prime implicate normal form and we investigate the spatial complexity of this transformation showing there to be an at most doubly exponential blowup in concept size at the end of the paper we compare prime implicate normal form to two other normal form for alc concept that have been proposed in the literature discussing the relative merit of the dierent approach 
a central goal of transfer learning is to enable learning when training data from the domain of interest is limited yet work on transfer across relational domain ha so far focused on the case where there is a significant amount of target data this paper bridge this gap by studying transfer when the amount of target data is minimal and consists of information about just a handful of entity in the extreme case only a single entity is known we present the sr lr algorithm that find an effective mapping of predicate from a source model to the target domain in this setting and thus render pre existing knowledge useful to the target task we demonstrate sr lr s effectiveness in three benchmark relational domain on social interaction and study it behavior a information about an increasing number of entity becomes available 
probabilistic programming language allow a modeler to build probabilistic model using complex data structure with all the power of a programming language we present ctppl an expressive probabilistic programming language for dynamic process that model process using continuous time time is a first class element in our language the amount of time taken by a subprocess can be specified using the full power of the language we show through example that ctppl can easily represent existing continuous time framework and make it easy to represent new one we present semantics for ctppl in term of a probability measure over trajectory we present a particle filtering algorithm for the language that work for a large and useful class of ctppl program 
statistical machine learning continues to show promise a a tool for addressing complex problem in a variety of domain an increasing number of developer are therefore looking to use statistical machine learning algorithm within application we have conducted two initial study examining the difficulty that developer encounter when creating a statistical machine learning component of a larger application we first interviewed researcher with experience integrating statistical machine learning into application we then sought to directly observe and quantify some of the behavior described in our interview using a laboratory study of developer attempting to build a simple application that us statistical machine learning this paper present the difficulty we observed in our study discus current challenge to developer adoption of statistical machine learning and proposes potential approach to better supporting developer creating statistical machine learning component of application 
human have an amazing ability to perceive depth from a single still image however it remains a challenging problem for current computer vision system in this paper we will present algorithm for estimating depth from a single still image there are numerous monocular cue such a texture variation and gradient defocus color haze etc that can be used for depth perception taking a supervised learning approach to this problem in which we begin by collecting a training set of single image and their corresponding ground truth depth we learn the mapping from image feature to the depth we then apply these idea to create d model that are visually pleasing a well a quantitatively accurate from individual image we also discus application of our depth perception algorithm in robotic navigation in improving the performance of stereovision and in creating large scale d model given only a small number of image 
this paper address the problem of activity and event discovery in multi dimensional time series data by proposing a novel method for locating multi dimensional motif in time series while recent work ha been done in finding single dimensional and multi dimensional motif in time series we address motif in general case where the element of multi dimensional motif have temporal length and frequency variation the proposed method is validated by synthetic data and empirical evaluation ha been done on several wearable system that are used by real subject 
the uct algorithm us monte carlo simulation to estimate the value of state in a search tree from the current state however the first time a state is encountered uct ha no knowledge and is unable to generalise from previous experience we describe two extension that address these weakness our first algorithm heuristic uct incorporates prior knowledge inthe form of avalue function the value function can be learned offline using a linear combination of a million binary feature with weight trained by temporal difference learning our second algorithm uct rave form a rapid online generalisation based on the value of move we applied our algorithm to the domain of computer go using the program mogo using both heuristic uct and rave mogo became the first program to achieve human master level in competitive play 
recent result have shown the interest of tree of bdds subbarayan et al a a suitable target language for propositional knowledge compilation from the practical side in the present paper the concept of tree of bdds is extended to additional class of data structure c thus leading to tree of c representation toc we provide a number of generic result enabling one to determine the query transformation satisfied by toc depending on those satisfied by c we also present some result about the spatial efficiency of the toc language focusing on the toobdd language and other related language we address a number of issue that remained open in subbarayan et al we show that beyond co and va the toobdd fragment satisfies im and me but satisfies neither cd nor any query among ce se unless p np among other result we prove that toobdd is not comparable w r t succinctness with any of cnf dnf dnnf unless the polynomial hierarchy collapse this contributes to the explanation of some empirical result reported in subbarayan et al 
sparse coding is an unsupervised learning algorithm for finding concise slightly higher level representation of input and ha been successfully applied to self taught learning where the goal is to use unlabeled data to help on a supervised learning task even if the unlabeled data cannot be associated with the label of the supervised task raina et al however sparse coding us a gaussian noise model and a quadratic loss function and thus performs poorly if applied to binary valued integer valued or other non gaussian data such a text drawing on idea from generalized linear model glms we present a generalization of sparse coding to learning with data drawn from any exponential family distribution such a bernoulli poisson etc this give a method that we argue is much better suited to model other data type than gaussian we present an algorithm for solving the l regularized optimization problem defined by this model and show that it is especially efficient when the optimal solution is sparse we also show that the new model result in significantly improved self taught learning performance when applied to text classification and to a robotic perception task 
email client were not designed to serve a a task management tool but a high volume of task relevant information in email lead many people to use email client for this purpose such usage aggravates a user s experience of email overload and reduces productivity prior research system have sought to address this problem by experimentally adding task management capability to email client software radar reflective agent with distributed adaptive reasoning take a different approach in which a software agent act like a trusted human assistant many radar component employ machine learning to improve their performance human participant study showed a clear impact of learning on usei peiformance metric 
we develop a framework for forgetting concept and role aka uniform interpolation in terminology in the lightweight description logic l extended with role inclusion and domain and range restriction three different notion of forgetting preserving respectively concept inclusion concept instance and answer to conjunctive query with corresponding language for uniform interpolants are investigated experiment based on snomed ct systematised nomenclature of medicine clinical term and nci national cancer institute ontology demonstrate that forgetting is often feasible in practice for large scale terminology 
a machine learning ml system emerge in end user application learning algorithm and classifier will need to be robust to an increasingly unpredictable operating environment in many case the parameter governing a learning system cannot be optimized for every user scenario nor can user typically manipulate parameter defined in the space and terminology of ml conventional approach to user oriented ml system have typically hidden this complexity from user by automating parameter adjustment we propose a new paradigm in which model and algorithm parameter are exposed directly to end user with intuitive label suitable for application where parameter cannot be automatically optimized or where there is additional motivation such a creative flexibility to expose rather than fix or automatically adapt learning parameter in our chi paper we introduced and evaluated mysong a system that us a hidden markov model to generate chord to accompany a vocal melody the present paper formally describes the learning underlying mysong and discus the mechanism by which mysong s learning parameter are exposed to user a a case study in making ml system user configurable we discus the generalizability of this approach and propose that intuitively exposing ml parameter is a key challenge for the ml and human computer interaction community 
we discus challenge and opportunity for developing generalized task market where human and machine intelligence are enlisted to solve problem based on a consideration of the competency availability and pricing of different problemsolving resource the approach couple human computation with machine learning and planning and is aimed at optimizing the flow of subtasks to people and to computational problem solver we illustrate key idea in the context of lingua mechanica a project focused on harnessing human and machine translation skill to perform translation among language we present infrastructure and method for enlisting and guiding human and machine computation for language translation including detail about the hardness of generating plan for assigning task to solver finally we discus study performed with machine and human solver focusing on component of a lingua mechanica prototype 
matrix factorization algorithm are frequently used in the machine leaming community to find low dimensional representation of data we introduce a novel generative bayesian probabilistic model for unsupervised matrix and tensor factorization the model consists of several interacting lda model one for each modality we describe an efficient collapsed gibbs sampler for inference we also derive the non parametric form of the model where interacting lda model are replaced with interacting hdp model experiment demonstrate that the model is useful for prediction of missing data with two or more modality a well a learning the latent structure in the data 
we propose a novel method for approximate inference in bayesian network bns the idea is to sample data from a bn learn a latent tree model ltm from the data offline and when online make inference with the ltm instead of the original bn because ltm are tree structured inference take linear time in the meantime they can represent complex relationship among leaf node and hence the approximation accuracy is often good empirical evidence show that our method can achieve good approximation accuracy at low online computational cost 
traditional ai search method search in a state space typically modelled a a directed graph prohibitively large size of state space graph make complete or optimal search expensive a key observation a exemplified by the sa formalism for planning is that most commonly a state space graph can be decomposed into subgraphs linked by constraint we propose a novel space reduction algorithm that exploit such structure the result reveals that standard search algorithm may explore many redundant path our method provides an automatic way to remove such redundancy at each state we expand only the subgraphs within a dependency closure satisfying certain sufficient condition instead of all the subgraphs theoretically we prove that the proposed algorithm is completeness preserving a well a optimality preserving we show that our reduction method can significantly reduce the search cost on a collection of planning domain 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
when applied to numerical csps the branch and prune algorithm bpa computes a sharp covering of the solution set the bpa is therefore impractical when the solution set is large typically when it ha a dimension larger than four or five which is often met in underconstrained problem the purpose of this paper is to present a new search tree exploration strategy for bpa that hybridizes depth first and breadth first search this search strategy allows the bpa discovering potential solution in different area of the search space in early stage of the exploration hence allowing an anytime usage of the bpa the merit of the proposed search strategy are experimentally evaluated 
this paper summarizes recent advance in the application of multiagent coordination algorithm to air traffic flow management indeed air traffic flow management is one of the fundamental challenge facing the federal aviation administration faa today this problem is particularly complex a it requires the integration and or coordination of many factor including new data e g changing weather info potentially conflicting priority e g different airline limited resource e g air traffic controller and very heavy traffic volume e g over flight over the u airspace the multiagent approach assigns an agent to a navigational fix a specific location in d space and us three separate action to control the airspace setting the separation between airplane setting ground hold that delay aircraft departure and rerouting aircraft agent then use reinforcement learning to learn the best set of action result based on facet a commercial simulator show that agent receiving personalized reward reduce congestion by up to over agent receiving a global reward and by up to over a current industry approach monte carlo estimation these result show that with proper selection of agent their action and their reward structure multiagent coordination algorithm can be successfully applied to complex real world domain 
finding correct semantic correspondence between ontology is one of the most challenging problem in the area of semantic web technology experience with benchmarking matching system revealed that even the manual revision of automatically generated mapping is a very difficult problem because it ha to take the semantics of the ontology a well a interaction between correspondence into account in this paper we propose method for supporting human expert in the task of revising automatically created mapping in particular we present non standard reasoning method for detecting and propagating implication of expert decision on the correctness of a mapping we show that the use of these reasoning method significantly reduces the effort of mapping revision in term of the number of decision that have to be made by the expert 
we address the problem of learning preference relation on multi attribute or combinatorial domain we do so by making a very simple hypothesis about the dependence structure between attribute that the preference relation enjoys namely separability no preferential dependency between attribute given a set of example consisting of comparison between alternative we want to output a separable cp net consisting of local preference on each of the attribute that fit the example we consider three form of compatibility between a cp net and a set of example and for each of them we give useful characterization a well a complexity result 
we show how to translate htn domain description if they satisfy certain restriction into pddl so that they can be used by classical planner we provide correctness result for our translation algorithm and show that it run in linear time and space we also show that even small and incomplete amount of htn knowledge when translated into pddl using our algorithm can greatly improve a classical planner s performance in experiment on several thousand randomly generated problem in three different planning domain such knowledge speeded up the well known fast forward planner by several order of magnitude and enabled it to solve much larger problem than it could otherwise solve 
we propose a perceptron style algorithm for fast discriminative training of structured latent variable model and analyzed it convergence property our method extends the perceptron algorithm for the learning task with latent dependency which may not be captured by traditional model it relies on viterbi decoding over latent variable combined with simple additive update compared to existing probabilistic model of latent variable our method lower the training cost significantly yet with comparable or even superior classification accuracy 
arrow s impossibility theorem is one of the landmark result in social choice theory over the year since the theorem wa proved in quite a few alternative proof have been put forward in this paper we propose yet another alternative proof of the theorem the basic idea is to use induction to reduce the theorem to the base case with alternative and agent and then use computer to verify the base case this turn out to be an effective approach for proving other impossibility theorem such a sen s and muller satterthwaite s theorem a well furthermore we believe this new proof open an exciting prospect of using computer to discover similar impossibility or even possibility result in this paper we propose yet another alternative proof of this result with the help of computer briefly arrow s theorem say that in a society with at least three possible outcome alternative for each agent it is impossible to have a social welfare function that satisfies the following three condition unanimity pareto efficiency independent of irrelevant alternative iia and non dictatorship we shall show by induction that this result hold if and only if it hold for the base case when there are exactly two agent and three alternative the single agent case is trivial for the base case we verify it using computer in two way one view the problem a a constraint satisfaction problem csp and us a depth first search algorithm to generate all social welfare function that satisfy the first two condition and then verifies that all of them are dictatorial the other translates these condition to a logical theory and us a sat solver to verify that the resulting logical theory is not satisfiable either way it took le than one second on an amd opteronbased server with ghz cpu and gb ram for the base case to be verified a it turn out this strategy work not just for proving arrow s theorem the same inductive proof can be adapted for proving other impossibility result such a sen s and muller satterthwaite s theorem we shall outline our proof for muller satterthwaite s theorem along this line but leave sen s theorem to the full version of the paper these proof suggest that many of the impossibility result in social choice theory are all rooted in some small base case thus an interesting thing to do is to use computer to explore these small base case to try to come up with other impossibility or possibility result and to understand the boundary between these two type of result this is what we think the long term implication of our new proof of arrow s and other impossibility theorem will lie and the main reason why we want to formulate the condition in these theorem in a logical language and use a sat solver to check their consistency our work in this direction is still preliminary however we do have two result to report one try to relax the unanimity condition and the other the iia condition in arrow s theorem the rest of the paper is organized a follows we next review arrow s theorem and then describe our new inductive proof of this result we then briefly describe how this proof can be adapted to prove muller satterthwaite s theorem we 
in this paper a bottom up hierarchical genetic algorithm is proposed to visualize clustered data into a planar graph to achieve global optimization by accelerating local optimization process we introduce subgraph rotating and scaling process into the genetic algorithm compared with existing method the proposed approach is more feasible and promising with more accurate graph layout and more satisfiable computationally efficient performance a demonstrated by the experimental result 
splitting a logic program allows u to reduce the task of computing it stable model to similar task for smaller program this idea is extended here to the general theory of stable model that replaces traditional logic program by arbitrary first order sentence and distinguishes between intensional and extensional predicate we discus two kind of splitting a set of intensional predicate can be split into subset and a formula can be split into it conjunctive term 
in this paper we review a series of agent behavior synthesis problem under full observability and nondeterminism partial controllability ranging from conditional planning to recently introduced agent planning program and to sophisticated form of agent behavior composition and show that all of them can be solved by model checking two player game structure these structure are akin to transition system kripke structure usually adopted in model checking except that they distinguish and hence allow to separately quantify between the action move of two antagonistic player we show that using them we can implement solver for several agent behavior synthesis problem 
in bayesian network a most probable explanation mpe is a most likely instantiation of all network variable given a piece of evidence recent work proposed a branch and bound search algorithm that find exact solution to mpe query where bound are computed on a relaxed network obtained by a technique known a node splitting in this work we study the impact of variable and value ordering on such a search algorithm we study several heuristic based on the entropy of variable and on the notion of nogoods and propose a new meta heuristic that combine their strength experiment indicate that search efficiency is significantly improved allowing many hard problem to be solved for the first time 
when controlling dynamic system such a mobile robot in uncertain environment there is a trade off between risk and reward for example a race car can turn a corner faster by taking a more challenging path this paper proposes a new approach to planning a control sequence with a guaranteed risk bound given a stochastic dynamic model the problem is to find a control sequence that optimizes a performance metric while satisfying chance constraint i e constraint on the upper bound of the probability of failure we propose a two stage optimization approach with the upper stage optimizing the risk allocation and the lower stage calculating the optimal control sequence that maximizes reward in general the upper stage is a non convex optimization problem which is hard to solve we develop a new iterative algorithm for this stage that efficiently computes the risk allocation with a small penalty to optimality the algorithm is implemented and tested on the autonomous underwater vehicle auv depth planning problem and demonstrates a substantial improvement in computation cost and suboptimality compared to the prior art 
abstract the berkeley drosophila genome project bdgp ha produced a large number of gene expression pattern many of which have been annotated textually with anatomical and developmental term these term spatially correspond to local region of the image however they are attached collectively to group of image such that it is unknown which term is assigned to which region of which image in the group this pose a challenge to the development of the computational method to automate the textual description of expression pattern contained in each image in this paper we show that the underlying nature of this task match well with a new machine learning framework multi instance multi label learning miml we propose a new miml support vector machine to solve the problem that beset the annotation task empirical study show that the proposed method outperforms the state of the art drosophila gene expression pattern annotation method 
in this article we discus two alternative proposal for neighbourhood semantics which we call strict and loose neighbourhood semantics n andn respectively that have been previously introduced in the literature our main tool are suitable notion of bisimulation while an elegant notion of bisimulation exists forn the required bisimulation for n is rather involved we propose a simple extension ofn with a universal modality that we call n e which come together with a natural notion of bisimulation we also investigate the complexity of the satisfiability problem forn andn e 
while in most planning approach goal and plan are different object it is often useful to specify goal that combine declarative condition with procedural plan in this paper we propose a novel language for expressing temporally extended goal for planning in nondeterministic domain the key feature of this language is that it allows for an arbitrary combination of declarative goal expressed in temporal logic and procedural goal expressed a plan fragment we provide a formal definition of the language and it semantics and we propose an approach to planning with this language in nondeterministic domain we implement the planning framework and perform a set of experimental evaluation that show the potentiality of our approach 
we consider setting in which voter vote in sequence each voter know the vote of the earlier voter and the preference of the later voter and voter are strategic this can be modeled a an extensive form game of perfect information which we call a stackelberg voting game we first propose a dynamic programming algorithm for finding the backward induction outcome for any stackelberg voting game when the rule is anonymous this algorithm is efficient if the number of alternative is no more than a constant we show how to use compilation function to further reduce the time and space requirement our main theoretical result are paradox for the backwardinduction outcome of stackelberg voting game we show that for any n and any voting rule that satisfies nonimposition and with a low domination index there exists a profile consisting of n voter such that the backwardinduction outcome is ranked somewhere in the bottom two position in almost every voter s preference moreover this outcome loses all but one of it pairwise election furthermore we show that many common voting rule have a very low domination index including all majority consistent voting rule for the plurality and nomination rule we show even stronger paradox finally using our dynamic programming algorithm we run simulation to compare the backward induction outcome of the stackelberg voting game to the winner when voter vote truthfully for the plurality and veto rule surprisingly our experimental result suggest that on average more voter prefer the backward induction outcome 
evaluating an agent s performance in a stochastic setting is necessary for agent development scientific evaluation and competition traditionally evaluation is done using monte carlo estimation the magnitude of the stochasticity in the domain or the high cost of sampling however can often prevent the approach from resulting in statistically significant conclusion recently an advantage sum technique ha been proposed for constructing unbiased low variance estimate of agent performance the technique requires an expert to define a value function over state of the system essentially a guess of the state s unknown value in this work we propose learning this value function from past interaction between agent in some target population our learned value function have two key advantage they can be applied in domain where no expert value function is available and they can result in tuned evaluation for a specific population of agent e g novice versus advanced agent we demonstrate these two advantage in the domain of poker we show that we can reduce variance over state of the art estimator for a specific population of limit poker player a well a construct the first variance reducing estimator for no limit poker and multi player limit poker 
program debugging is one of the most time consuming part of the software development cycle in recent year automatic debugging ha been an active research area in software engineering it ha also attracted attention from the ai community however existing approach are mostly expenential moreover those model based approach are based on abstract model of program which lends an experiential flavor to the approach due to the heuristic nature of choosing an abstract model we believe that it is necessary to establish a precise theoretical foundation for debugging from first principle in this paper we present a first step towards this foundation using reiter s theoretical framework of model based diagnosis we give a clean formalization of the program debugging task in the situation calculus a logical language suitable for describing dynamic world example are given to illustrate our formalization 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
abstract ontology mapping is a complex and necessary task for many semantic web sw application the perspective user are faced with a number of challenge including the difficulty of capturing semantics in this paper we present a threedimensional ontology mapping model this model reflects the engineering step needed to materialise a versatile mapping system in order to meet the demand on semantic interoperability in the sw environment we solidify the formalisation with specialised algorithm and we analyse their effectiveness and performance by way of benchmark test 
in this work novelty detection identifies salient image feature to guide autonomous robotic exploration there is little advance knowledge of the feature in the scene or the proportion that should count a outlier a new algorithm address this ambiguity by modeling novel data in advance and characterizing regular data at run time detection threshold adapt dynamically to reduce misclassification risk while accommodating homogeneous and heterogeneous scene experiment demonstrate the technique on a representative set of navigation image from the mar exploration rover opportunity an efficient image analysis procedure filter each image using the integral transform pixel level feature are aggregated into covariance descriptor that represent larger region finally a distance metric derived from generalized eigenvalue permit novelty detection with kernel density estimation result suggest that exploiting training example of novel data can improve performance in this domain 
canonical correlation analysis cca and partial least square pls are well known technique for feature extraction from two set of multidimensional variable the fundamental difference between cca and pls is that cca maximizes the correlation while pls maximizes the covariance although both cca and pls have been applied successfully in various application the intrinsic relationship between them remains unclear in this paper we attempt to address this issue by showing the equivalence relationship between cca and orthonormalized partial least square opls a variant of pls we further extend the equivalence relationship to the case when regularization is employed for both set of variable in addition we show that the cca projection for one set of variable is independent of the regularization on the other set of variable we have performed experimental study using both synthetic and real data set and our result confirm the established equivalence relationship the presented analysis provides novel insight into the connection between these two existing algorithm a well a the effect of the regularization 
powerful consistency technique such a ac and fdac have been developed for weighted constraint satisfaction problem wcsps to reduce the space in solution search but are restricted to only unary and binary constraint on the other hand van hoeve et al developed efficient graph based algorithm for handling soft constraint a classical constraint optimization problem we prove that naively incorporating van hoeve s method into the wcsp framework can enforce a strong form of inverse consistency which can prune infeasible value and deduce good lower bound estimate we further show how van hoeve s method can be modified so a to handle cost projection and extension to maintain the stronger ac and fdac generalized for non binary constraint using the soft alldifferent constraint a a testbed preliminary result demonstrate that our proposal give improvement up to an order of magnitude both in term of time and pruning 
decentralized markov decision process are a powerful general model of decentralized cooperative multi agent problem solving the high complexity of the general problem lead to a focus on restricted model while worst case hardness of such reduced problem is often better le is known about the actual difficulty of given instance we show tight connection between the structure of agent interaction and the essential dimensionality of various problem bound are placed on problem difficulty given restriction on the type and number of interaction between agent these bound arise from a bilinear programming formulation of the problem from such a formulation a more compact reduced form can be automatically generated and the original problem rewritten to take advantage of the reduction 
goal driven learning gdl focus on system that determine by themselves what ha to be learnt and how to learn it typically gdl system use meta reasoning capability over a base reasoner identifying learning goal and devising strategy in this paper we present a novel gdl technique to deal with complex ai system where the meta reasoning module ha to analyze the reasoning trace of multiple component with potentially different learning paradigm our approach work by distributing the generation of learning strategy among the different module instead of centralizing it in the meta reasoner we implemented our technique in the gila system that work in the airspace task order domain showing an increase in performance 
in many planning application one can find action with overlapping effect if for optimally reaching the goal all that matter is within this overlap there is no need to consider all these action for the task at hand they are equivalent using this structure for speed up ha previously been proposed in the context of least commitment planning of a similar spirit is the approach for improving best first search based planning we present here intuitively given a set of start state reachable from the initial state we plan in parallel for all of them exploiting the similarity between them to gain computational saving since the similarity of two state is problem specific we explicitly infer it by regressing all relevant entity goal heuristic function action precondition and cost over the action sequence considered in planning if the resulting formula mention only fluents whose value the two state have in common it suffices to evaluate the formula in one of them this lead to computational saving over conventional best first search 
we study a problem of dynamic allocation without money agent have arrival and departure and strict preference over item strategyproofness requires the use of an arrivalpriority serial dictatorship apsd mechanism which is ex post pareto efficient but ha poor ex ante efficiency a measured through average rank efficiency we introduce the scoring rule sr mechanism which bias in favor of allocating item that an agent value above the population consensus the sr mechanism is not strategyproof but ha tolerable manipulability in the sense that i if every agent o ptimally manipulates it reduces to apsd and ii it significantly outperforms apsd for rank efficiency when only a fraction of agent are strategic the performance of sr is also robust to mistake by agent that manipulate on the basis of inaccurate information about the popularity of item 
dung s abstract theory of argumentation ha become established a a general framework for various specie of non monotonic reasoning and reasoning in the presence of conflict a dung framework consists of argument related by attack and the extension of a framework and so the status of argument are defined under different semantics development of dung s work have also defined argument labellings a an alternative way of characterising extension and dialectical argument game proof theory for establishing the status of individual argument recently extended argumentation framework extend dung s theory so that argument not only attack argument but attack themselves in this way the extended theory provides an abstract framework for principled integration of meta level argumentation about defeasible preference applied to resolve conflict between object level argument in this paper we formalise labellings and argument game for a selection of dung s semantics defined for the extended framework 
recent advance in linear classification have shown that for application such a document classification the training can be extremely efficient however most of the existing training method are designed by assuming that data can be stored in the computer memory these method cannot be easily applied to data larger than the memory capacity due to the random access to the disk we propose and analyze a block minimization framework for data larger than the memory size at each step a block of data is loaded from the disk and handled by certain learning method we investigate two implementation of the proposed framework for primal and dual svms respectively a data cannot fit in memory many design consideration are very different from those for traditional algorithm experiment using data set time larger than the memory demonstrate the effectiveness of the proposed method 
set variable in constraint satisfaction problem csps are typically propagated by enforcing set bound consistency together with cardinality reasoning which us some inference rule involving the cardinality of a set variable to produce more pruning than set bound propagation alone multiset variable are a generalization of set variable by allowing the element to have repetition in this paper we generalize cardinality reasoning for multiset variable in addition we propose to exploit the variety of a multiset the number of distinct element in it to improve modeling expressiveness and further enhance constraint propagation we derive a number of inference rule involving the variety of multiset variable the rule interact variety with the traditional component of multiset variable such a cardinality to obtain stronger propagation we also demonstrate how to apply the rule to perform variety reasoning on some common multiset constraint experimental result show that performing variety reasoning on top of cardinality reasoning can effectively reduce more search space and achieve better runtime in solving multiset csps 
almost all approach to model based diagnosis presume that the system being diagnosed behaves non intermittently and analyze behavior over a small number often only one of time instant in this paper we show how existing approach to model based diagnosis can be extended to diagnose intermittent failure a they manifest themselves over time in addition we show where to insert probe point to best distinguish among the intermittent fault those that best explain the symptom and isolate the fault in minimum expected cost 
an unsupervised probabilistic learning framework for normalizing product record across different retailer web site is presented our framework decomposes the problem into two task to achieve the goal the first task aim at extracting attribute value of product from different site and normalizing them into appropriate reference attribute this task is challenging because the set of reference attribute is unknown in advance besides the layout format are different in different web site the second task is to conduct product record normalization aiming at identifying product record referring to the same reference product based on the result of the first task we develop a graphical model for the generation of text fragment in web page to accomplish the two task one characteristic of our model is that the product attribute to be extracted are not required to be specified in advance and an unlimited number of previously unseen product attribute can be handled we compare our framework with existing method extensive experiment using over web page from over real world web site from three different domain have been conducted demonstrating the effectiveness of our framework 
we introduce a framework so that community can exchange reputation information about agent in environment where agent are migrating between community we view the acquisition of the reputation information a a purchase and focus on the design of a payment function to facilitate the payment for information in a way that motivates community to truthfully report reputation information for agent we prove that in our proposed framework honesty is the optimal policy and demonstrate the value of using a payment function approach for the exchange of reputation information about agent between community in multiagent environment using our payment function each community is strengthened it is able to reason more effectively about which agent to accept and can enjoy agent that are motivated to contribute strongly to the benefit of the community 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
we focus on the random generation of sat instance that have computational property that are similar to real world instance it is known that industrial instance even with a great number of variable can be solved by a clever solver in a reasonable amount of time this is not possible in general with classical randomly generated instance we provide different generation model of sat instance extending the uniform and regular cnf model they are based on the use of non uniform probability distribution to select variable our last model also us a mechanism to produce clause of different length a in industrial instance we show the existence of the phase transition phenomenon for our model and we study the hardness of the generated instance a a function of the parameter of the probability distribution we prove that with these parameter we can adjust the difficulty of the problem in the phase transition point we measure hardness in term of the performance of different solver we show how these model will allow u to generate random instance similar to industrial instance of interest for testing purpose 
to do list have been found to be the most popular personal information management tool yet there is no automated system to interpret and act upon them when appropriate on behalf of the user automating to do list is challenging not only because they are specified a free text but also because most item contain abbreviated task many do not specify an action to be performed and often refer to unrelated personal item this paper present our approach and an implemented system to process to do list entry and map them to task that can be automated for the user by a set of agent since the format of to do entry is not very amenable to natural language processing tool that can parse and create a structured interpretation our approach is to exploit paraphrase of the target task that the agent can perform and that specify how the free text map to the task argument a user manually assign to do to agent for automation our system improves it performance by learning new paraphrase we show an evaluation of our approach in a corpus of to do entry collected from user of an office assistant multi agent system 
this work address the problem of efficiently learning action schema using a bounded number of sample interaction with the environment we consider schema in two language traditional strip and a new language strip w that extends strip to allow for the creation of new object when an action is executed this modification allows strip w to model web service and can be used to describe web service composition planning problem we show that general strip operator cannot be efficiently learned through raw experience though restricting the size of action precondition yield a positive result we then show that efficient learning is possible without this restriction if an agent ha access to a teacher that can provide solution trace on demand we adapt this learning algorithm to efficiently learn web service description in strip w 
this paper debut a novel application of speech recognition to foreign language learning we present a generic framework for developing user customizable card game designed to aid learner in the difficult task of vocabulary acquisition we also describe a prototype game built on this framework that using a mandarin speech recognizer provides a student of chinese with opportunity to speak vocabulary item in a meaningful context the system dynamically load only the necessary vocabulary for each game in an effort to maintain robust recognition performance without limiting the lexical domain to ass the sentence error rate ser of our prototype we asked college age student from various university in the united state and beyond to participate in a web based user study the three central concept in the game were recognized with a ser of illustrating the feasibility of deploying this system in a university curriculum via the internet finally to ensure that our recognizer is behaving appropriately with regard to learner speech we perform a rigorous analysis of the recognition error to determine their underlying cause 
answer set programming asp is a form of declarative programming oriented towards difficult search problem a an outgrowth of research on the use of nonmonotonic reasoning in knowledge representation it is particularly useful in knowledge intensive application asp program consist of rule that look like prolog rule but the computational mechanism used in asp are different they are based on the idea that have led to the creation of fast satisfiability solver f or propositional logic 
while there are several language for representing combinatorial preference over set of alternative none of these are well suited to the representation of ordinal preference over set of good which are typically required to be monotonic we propose such a language taking inspiration from previous work on graphical language for preference representation specifically cp net and introduce conditional importance network ci net a ci net includes statement of the form if i have a set a of good and i do not have any of the good from some other set b then i prefer the set of good c over the set of good d we investigate expressivity and complexity issue for ci net then we show that ci net are well suited to the description of fair division problem 
computing the semantic distance between realworld concept is crucial for many intelligent application we present a novel method that leverage data from wikispeedia an online game played on wikipedia player have to reach an article from another unrelated article only by clicking link in the article encountered in order to automatically infer semantic distance between everyday concept our method effectively extract the common sense displayed by human during play and is thus more desirable from a cognitive point of view than purely corpus based method we show that our method significantly outperforms latent semantic analysis in a psychometric evaluation of the quality of learned semantic distance 
location play a very important role in the retail business due to it huge and long term investment in this paper we propose a novel semisupervised regression model for evaluating convenience store location based on spatial data analysis first the input feature for each convenience store can be extracted by analyzing the element around it based on a geographic information system and the turnover is used to evaluate it performance second considering the practical application scenario a manifold regularization model with one semi supervised performance information constraint is provided the promising experimental result in the real world dataset demonstrate the effectiveness of the proposed approach in performance prediction of certain candidate location for new convenience store opening 
we propose an approach for automatically ranking structured document applied to patent prior art search our model svm patent ranking svmpr incorporates margin constraint that directly capture the specificity of patent citation ranking our approach combine patent domain knowledge feature with meta score feature from several different general information retrieval method the training algorithm is an extension of the pegasos algorithm with performance guarantee effectively handling hundred of thousand of patent pair judgement in a high dimensional feature space experiment on a homogeneous essential wireless patent dataset show that svmpr performs on average better than many other state of the art general purpose information retrieval method in term of the ndcg measure at different cut off position 
the result of the latest international probabilistic planning competition ippc indicate that the presence of dead end state with no trajectory to the goal make mdps hard for modern probabilistic planner implicit dead end state with executable action but no path to the goal are particularly challenging existing mdp solver spend much time and memory identifying these state a a first attempt to address this issue we propose a machine learning algorithm called sixthsense sixthsense help existing mdp solver by finding nogoods conjunction of literal whose truth in a state implies that the state is a dead end importantly our learned nogoods are sound and hence the state they identify are true dead end sixthsense is very fast need little training data and take only a small fraction of total planning time while ippc problem may have million of dead end they may typically be represented with only a dozen or two no good thus nogood learning efficiently produce a quick and reliable mean for dead end recognition our experiment show that the nogoods found by sixthsense routinely reduce planning space and time on ippc domain enabling some planner to solve problem they could not previously handle 
since learning is typically very slow in boltzmann machine there is a need to restrict connection within hidden layer however the resulting state of hidden unit exhibit statistical dependency based on this observation we propose using l l regularization upon the activation possibility of hidden unit in restricted boltzmann machine to capture the loacal dependency among hidden unit this regularization not only encourages hidden unit of many group to be inactive given observed data but also make hidden unit within a group compete with each other for modeling observed data thus the l l regularization on rbms yield sparsity at both the group and the hidden unit level we call rbms trained with the regularizer emph sparse group rbms the proposed sparse group rbms are applied to three task modeling patch of natural image modeling handwritten digit and pretaining a deep network for a classification task furthermore we illustrate the regularizer can also be applied to deep boltzmann machine which lead to sparse group deep boltzmann machine when adapted to the mnist data set a two layer sparse group boltzmann machine achieves an error rate of which is to our knowledge the best published result on the permutation invariant version of the mnist task 
with recent advance in motion detection and tracking in video more effort are being directed at higher level video analysis such a recognizing action event and activity one of the more challenging problem is recognizing activity that involve multiple people and or vehicle whose relationship change over time when motion detection and tracking are unreliable a commonly occurs in busy scene we describe an approach to this problem based on dynamic bayesian network and show how dbns can be extended to compensate for track failure we also show that defining dbns with semantic concept improves robustness v direct observable and discus implication and idea for incorporating semantic symbolic knowledge into the perceptual domain of activity recognition 
we propose a new local search platform that split a cnf formula into three sub component i a minimal dependency lattice representing the core connection between logic gate ii a conjunction of equivalence clause and iii the remaining clause we also adopt a new hierarchical cost function that focus on solving the core component of the problem first we then show experimentally that our platform not only significantly outperforms existing local search approach but is also competitive with modern systematic solver on highly structured problem 
we present a streaming model for large scale classification in the context of svm by leveraging connection between learning and computational geometry the streaming model imposes the constraint that only a single pas over the data is allowed the svm is known to have an equivalent formulation in term of the minimum enclosing ball meb problem and an efficient algorithm based on the idea of core set exists cvm tsang et al cvm learns a approximate meb for a set of point and yield an approximate solution to corresponding svm instance however cvm work in batch mode requiring multiple pass over the data this paper present a single pas svm which is based on the minimum enclosing ball of streaming data we show that the meb update for the streaming case can be easily adapted to learn the svm weight vector in a way similar to using online stochastic gradient update our algorithm performs polylogarithmic computation at each example and requires very small and constant storage experimental result show that even in such restrictive setting we can learn efficiently in just one pas and get accuracy comparable to other stateof the art svm solver batch and online we also give an analysis of the algorithm and discus some open issue and possible extension 
alpha beta is the most common game tree search algorithm due to it high performance and straightforward implementation in practice one must find the best trade off between heuristic evaluation time and bringing the subset of node explored closer to a minimum proof graph in this paper we present a series of structural property of minimum proof graph that help u to prove that finding such graph is np hard for arbitrary dag input but can be done in linear time for tree we then introduce the class of fastest cut first search heuristic that aim to approximate minimum proof graph by sorting move based on approximation of sub dag value and size to explore how various aspect of the game tree such a branching factor and distribution of move value affect the performance of alpha beta we introduce the class of prefix value game tree that allows u to label interior node with true minimax value on the fly without search using these tree we show that by explicitly attempting to approximate a minimum game tree we are able to achieve performance gain over alpha beta with common extension 
we address the problem of providing a logical formalization of arithmetic in declarative modelling language for np search problem the challenge is to simultaneously allow quantification over an infinite domain such a the natural number provide natural modelling facility and control expressive power of the language to address the problem we introduce an extension of the model expansion mx based framework to finite structure embedded in an infinite secondary structure together with double guarded logic for representing mx specification for these structure the logic also contain multi set function aggregate operation our main result is that these logic capture the complexity class np on small cost arithmetical structure 
strategy proof classification deal with a setting where a decision maker must classify a set of input point with binary label while minimizing the expected error the label of the input point are reported by self interested agent who might lie in order to obtain a classifier that more closely match their own label thus creating a bias in the data this motivates the design of truthful mechanism that discourage false report previous work meir et al investigated both decision theoretic and learning theoretic variation of the setting but only considered classifier that belong to a degenerate class in this paper we assume that the agent are interested in a shared set of input point we show that this plausible assumption lead to powerful result in particular we demonstrate that variation of a truthful random dictator mechanism can guarantee approximately optimal outcome with respect to any class of classifier 
we introduce coalitional game with belief cgbs a natural generalization of coalitional game to environment where agent posse private belief regarding the capability or type of others we put forward a model to capture such agent type uncertainty and study coalitional stability in this setting specifically we introduce a notion of the core for cgbs both with and without coalition structure for simple game without coalition structure we then provide a characterization of the core that match the one for the full information case and use it to derive a polynomial time algorithm to check core nonemptiness in contrast we demonstrate that in game with coalition structure allowing belief increase the computational complexity of stability related problem in doing so we introduce and analyze weighted voting game with belief which may be of independent interest finally we discus connection between our model and other class of coalitional game 
for many election system bribery and related attack have been shown np hard using construction on combinatorially rich structure such a partition and cover it is important to learn how robust these hardness protection result are in order to find whether they can be relied on in practice this paper show that for voter who follow the most central politicalscience model of electorate single peaked preference those protection vanish by using single peaked preference to simplify combinatorial covering challenge we show that nphard bribery problem including those for kemeny and llull election fall to polynomial time by using single peaked preference to simplify combinatorial partition challenge we show that np hard partition of voter problem fall to polynomial time we furthermore show that for single peaked electorate the winner problem for dodgson and kemeny election though p complete in the general case fall to polynomial time and we completely classify the complexity of weighted coalition manipulation for scoring protocol in single peaked electorate 
this study examines the ability of a semantic space model to represent themeaning of noun compound such a information gathering or weather forecast a new algorithm comparison is proposed for computing compound vector from constituent word vector and compared with other algorithm i e predication and centroid in term of accuracy of multiple choice synonym test and similarity judgment test the result of both test is that the comparison algorithm is on the whole superior to other algorithm and in particular achieves the best performance when noun compound have emergent meaning furthermore the comparison algorithm also work for novel noun compound that do not occur in the corpus these finding indicate that a semantic space model in general and the comparison algorithm in particular ha sufficient ability to compute the meaning of noun compound 
branch and bound is an effective technique for solving constraint optimization problem cop s however it search space expands very rapidly a the domain size of the problem variable grow in this paper we present an algorithm that cluster the value of a variable s domain into set branch and bound can then branch on these set of value rather than on individual value thereby reducing the branching factor of it search space the aim of our clustering algorithm is to construct a collection of set such that branching on these set will still allow effective bounding in conjunction with the reduced branching factor the size of the explored search space is thus significantly reduced we test our method and show empirically that it can yield significant performance gain over existing state of the art technique 
we develop a point based method for solving finitely nested interactive pomdps approximately analogously to point based value iteration pbvi in pomdps we maintain a set of belief point and form value function composed of those value vector that are optimal at these point however a we focus on muitiagent setting the belief are nested and computation of the value vector relies on predicted action of others consequently we develop a novel interactive gen eralization of pbvi applicable to muitiagent setting 
there ha been intense interest in hierarchical reinforcement learning a a way to make markov decision process planning more tractable but there ha been relatively little work on autonomously learning the hierarchy especially in continuous domain in this paper we present a method for learning a hierarchy of action in a continuous environment our approach is to learn a qualitative representation of the continuous environment and then to define action to reach qualitative state our method learns one or more option to perform each action each option is learned by first learning a dynamic bayesian network dbn we approach this problem from a developmental robotics perspective the agent receives no extrinsic reward and ha no external direction for what to learn we evaluate our work using a simulation with realistic physic that consists of a robot playing with block at a table 
spatial process are typically used to analyse and predict geographic data this paper adapts such model to the prediction of a user s interest or item rating in recommender system we present the theoretical framework for a model based on gaussian spatial process and discus ecient algorithm for parameter estimation our model wa evaluated with simulated data and a real world dataset collected by tracking visitor in a museum and achieves a higher predictive accuracy than a non personalised baseline additionally in the real world scenario the model attains a higher predictive accuracy than state of the art collaborative lters 
planning in realistic domain involves reasoning under uncertainty operating under time and resource constraint and finding the optimal set of goal to be achieved in this paper we provide an ao based algorithm that can deal with durative action concurrent execution over subscribed goal and probabilistic outcome in a unified way we explore plan optimization by introducing two novel aspect to the model first we introduce parallel step that serve the same goal and increase the probability of success in addition to parallel step that serve different goal and decrease execution time second we introduce plan step to terminate concurrent step that are no longer useful so that resource can be conserved our algorithm called cpoao concurrent probabilistic oversubscription ao can deal with the aforementioned extension and relies on the ao framework to reduce the size of the search space using informative heuristic function we describe our framework implementation the heuristic function we use the experimental result and potential research on heuristic that can further reduce the size of search space 
with the aim of fluency and efficiency in human robot team we have developed a cognitive architecture based on the neuro psychological principle of anticipation and perceptual simulation through top down biasing an instantiation of this architecture wa implemented on a nonanthropomorphic robotic lamp performing in a human robot collaborative task in a human subject study in which the robot work on a joint task with untrained subject we find our approach to be significantly more efficient and fluent than in a comparable system without anticipatory perceptual simulation we also show the robot and the human to be increasingly contributing at a similar rate through self report we find significant difference between the two condition in the sense of team fluency the team s improvement over time and the robot s contribution to the efficiency and fluency we also find difference in verbal attitude towards the robot most notably subject working with the anticipatory robot attribute more positive and more human quality to the robot but display increased self blame and self deprecation 
recommender system attempt to highlight item that a target user is likely to find interesting a common technique is to use collaborative filtering cf where multiple user share information so a to provide each with effective recommendation a key aspect of cf system is finding user whose taste accurately reflect the taste of some target user typically the system look for other agent who have had experience with many of the item the target user ha examined and whose classification of these item ha a strong correlation with the classification of the target user since the universe of item may be enormous and huge data set are involved sophisticated method must be used to quickly locate appropriate other agent we present a method for quickly determining the proportional intersection between the item that each of two user ha examined by sending and maintaining extremely concise sketch of the list of item these sketch enable the approximation of the proportional intersection within a distance of with a high probability of our sketching technique are based on random min wise independent hash function and use very little space and time so they are well suited for use in large scale collaborative filtering system 
a a i algorithm are applied to more complex domain that involve high dimensional data set there is a need to more saliently represent the data however most dimension reduction approach are driven by objective function that may not or only partially suit the end user requirement in this work we show how to incorporate general purpose domain expertise encoded a a graph into dimension reduction in way that lends itself to an elegant generalized eigenvalue problem we call our approach graph driven constrained dimension reduction via linear projection gcdr lp and show that it ha several desirable property 
machine learning approach to indoor wifi localization involve an offline phase and an online phase in the offline phase data are collected from an environment to build a localization model which will be applied to new data collected in the online phase for location estimation however collecting the labeled data across an entire building would be too time consuming in this paper we present a novel approach to transferring the learning model trained on data from one area of a building to another we learn a mapping function between the signal space and the location space by solving an optimization problem based on manifold learning technique a low dimensional manifold is shared between data collected in different area in an environment a a bridge to propagate the knowledge across the whole environment with the help of the transferred knowledge we can significantly reduce the amount of labeled data which are required for building the localization model we test the effectiveness of our proposed solution in a real indoor wifi environment 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
transfer learning address the problem of how to utilize plenty of labeled data in a source domain to solve related but different problem in a target domain even when the training and testing problem have different distribution or feature in this paper we consider transfer learning via dimensionality reduction to solve this problem we learn a low dimensional latent feature space where the distribution between the source domain data and the target domain data are the same or close to each other onto this latent feature space we project the data in related domain where we can apply standard learning algorithm to train classification or regression model thus the latent feature space can be treated a a bridge of transferring knowledge from the source domain to the target domain the main contribution of our work is that we propose a new dimensionality reduction method to find a latent space which minimizes the distance between distribution of the data in different domain in a latent space the effectiveness of our approach to transfer learning is verified by experiment in two real world application indoor wifi localization and binary text classification 
a folksonomy refers to a collection of user defined tag with which user describe content published on the web with the flourish of web folksonomies have become an important mean to develop the semantic web because tag in folksonomies are authored freely there is a need to understand the structure and semantics of these tag in various application in this paper we propose a learning approach to create an ontology that capture the hierarchical semantic structure of folksonomies our experimental result on two different genre of real world data set show that our method can effectively learn the ontology structure from the folksonomies 
over the past few year point based pomdp solver scaled up to produce approximate solution to mid sized domain however to solve real world problem solver must exploit the structure of the domain in this paper we focus on the topological structure of the problem where the state space contains layer of state we present here the topological order planner top that utilizes the topological structure of the domain to compute belief space trajectory top rapidly produce trajectory focused on the solveable region of the belief space thus reducing the number of redundant backup considerably we demonstrate top to produce good quality policy faster than any other pointbased algorithm on domain with sufficient structure 
partially observable markov decision process pomdps are a well established and rigorous framework for sequential decision making under uncertainty pomdps are well known to be intractable to solve exactly and there ha been significant work on finding tractable approximation method one well studied approach is to find a compression of the original pomdp by projecting the belief state to a lower dimensional space we present a novel dimensionality reduction method for pomdps based on locality preserving non negative matrix factorization unlike previous approach such a krylov compression and regular non negative matrix factorization our approach preserve the local geometry of the belief space manifold we present result on standard benchmark pomdps showing improved performance over previously explored compression algorithm for pomdps 
the main result of this paper is to show that the problem of instantiating a finite and path consistent constraint network of line in the euclidean space is np complete indeed we already know that reasoning with line in the euclidean space is np hard in order to prove that this problem is np complete we first establish that a particular instance of this problem can be solved by a nondeterministic polynomial time algorithm and then we show that solving any finite and path consistent constraint network of line in the euclidean space is at most a difficult a solving that instance 
in an interlinked corpus of document the context in which a citation appears provides extra information about the cited document however associating term in the context to the cited document remains an open problem we propose a novel document generation approach that statistically incorporates the context in which a document link to another document we quantitatively show that the proposed generation scheme explains the linking phenomenon better than previous approach the context information along with the actual content of the document provides signicant improvement over the previous approach for various real world evaluation task such a link prediction and log likelihood estimation on unseen content the proposed method is more scalable to large collection of document compared to the previous approach 
we explore the relationship between property of the network defined by connected agent and the global system performance this is achieved by mean of a novel class of optimization algorithm this new class make explicit use of an underlying network that structure the information flow between multiple agent performing local search we show that this new class of algorithm is competitive with respect to other population based optimization technique finally we demonstrate by numerical simulation that change in the way the network is built lead to variation in the system s performance in particular we show how constrained hub highly connected agent can be beneficial in particular optimization problem 
when building an application that requires object class recognition having enough data to learn from is critical for good performance and can easily determine the success or failure of the system however it is typically extremely labor intensive to collect data a the process usually involves acquiring the image then manual cropping and hand labeling preparing large training set for object recognition ha already become one of the main bottleneck for such emerging application a mobile robotics and object recognition on the web this paper focus on a novel and practical solution to the dataset collection problem our method is based on using a green screen to rapidly collect example image we then use a probabilistic model to rapidly synthesize a much larger training set that attempt to capture desired invariant in the object s foreground and background we demonstrate this procedure on our own mobile robotics platform where we achieve x saving in the time effort needed to obtain a training set our data collection method is agnostic to the learning algorithm being used and applies to any of a large class of standard object recognition method given these result we suggest that this method become a standard protocol for developing scalable object recognition system further we used our data to build reliable classifier that enabled our robot to visually recognize an object in an office environment and thereby fetch an object from an office in response to a verbal request 
the tactical language and culture training system tlcts help people quickly acquire communicative skill in foreign language and culture more than learner worldwide have used tlcts course tlcts utilizes artificial intelligence technology in multiple way during the authoring process and at run time to process learner speech interpret learner action control the response of non player character and evaluate and ass learner performance and proficiency this paper describes the architecture of tlcts and the artificial intelligence technology that it employ and present result from multiple evaluation study that demonstrate the benefit of learning foreign language and culture using this approach 
probabilistic logic programming is a powerful technique to represent and reason with imprecise probabilistic knowledge a probabilistic logic program plp is a knowledge base which contains a set of conditional event with probability interval in this paper we investigate the issue of revising such a plp in light of receiving new information we propose postulate for revising plps when a new piece of evidence is also a probabilistic conditional event our postulate lead to jeffrey s rule and bayesian conditioning when the original plp defines a single probability distribution furthermore we prove that our postulate are extension to darwiche and pearl dp postulate when new evidence is a propositional formula we also give the representation theorem for the postulate and provide an instantiation of revision operator satisfying the proposed postulate 
proactive assessment of computer network vulnerability to unknown future attack is an important but unsolved computer security problem where ai technique have significant impact potential in this paper we investigate the use of reinforcement learning rl for proactive security in the context of denial of service do attack in peer to peer p p network such a tool would be useful for network administrator and designer to ass and compare the vulnerability of various network configuration and security measure in order to optimize those choice for maximum security we first discus the various dimension of the problem and how to formulate it a rl next we introduce compact parametric policy representation for both single attacker and botnets and derive a policy gradient rl algorithm we evaluate these algorithm under a variety of network configuration that employ recent fair use do security mechanism the result show that nur rl based approach is able to significantly outperform a number of heuristic strategy in term of the severity of the attack discovered the result also suggest some possible network design lesson for reducing the attack potential of an intelligent attacker 
this paper explores the application of knowledgebased word sense disambiguation system to specific domain based on our state of the art graphbased wsd system that us the information in wordnet evaluation wa performed over a publicly available domain specific dataset of word related to sport and finance comprising example drawn from three corpus one balanced corpus bnc and two domain specific corpus news related to sport and finance the result show that in all three corpus our knowledge based wsd algorithm improves over previous result and also over two state of the art supervised wsd system trained on semcor the largest publicly available annotated corpus we also show that using related word a context instead of the actual occurrence context yield better result on the domain datasets but not on the general one interestingly the result are higher for domain specific corpus than for the general corpus raising prospect for improving current wsd system when applied to specific domain 
in model based diagnosis mbd the problem of computing a diagnosis in a strong fault model sfm is computationally much harder than in a weak fault model wfm for example in propositional horn model computing the first minimal diagnosis in a weak fault model wfm is in p but is np hard for strong fault model a a result sfm problem of practical significance have not been studied in great depth within the mbd community in this paper we describe an algorithm that render the problem of computing a diagnosis in several important sfm subclass no harder than a similar computation in a wfm we propose an approach for efficiently computing minimal diagnosis for these subclass of sfm that extends existing conflict based algorithm like gde sherlock and cda experiment on iscas combinational circuit show inference speedup with cda of up to a factor of and an average of reduction in the average conflict size at the price of an extra low polynomial time consistency check for a candidate diagnosis 
captcha ha been widely deployed by commercial web site a a security technology for purpose such a anti spam a common approach to evaluating the robustness of captcha is the use of machine learning technique critical to this approach is the acquisition of an adequate set of labeled sample on which the learning technique are trained however such a sample labeling task is difficult for computer since the strength of captchas stem exactly from the difficulty computer have in recognizing either distorted text or image content therefore until now researcher have to manually label their sample which is tedious and expensive in this paper we present magic bullet a computer game that for the first time turn such sample labeling into a fun experience and that achieves a labeling accuracy of a high a for free the game leverage human computation to address a task that cannot be easily automated and it effectively streamlines the evaluation of captchas the game can also be used for other constructive purpose such a developing better machine learning algorithm for handwriting recognition and training people s typing skill 
we consider the problem of grasping novel object in cluttered environment if a full d model of the scene were available one could use the model to estimate the stability and robustness of different grasp formalized a form force closure etc in practice however a robot facing a novel object will usually be able to perceive only the front visible face of the object in this paper we propose an approach to grasping that estimate the stability of different grasp given only noisy estimate of the shape of visible portion of an object such a that obtained from a depth sensor by combining this with a kinematic description of a robot arm and hand our algorithm is able to compute a specific positioning of the robot s finger so a to grasp an object we test our algorithm on two robot with very different arm manipulator including one with a multifingered hand we report result on the task of grasping object of significantly different shape and appearance than one in the training set both in highly cluttered and in uncluttered environment we also apply our algorithm to the problem of unloading item from a dishwasher 
recognition of chatting activity in social interaction is useful for constructing human social network however the existence of multiple people involved in multiple dialogue present special challenge to model the conversational dynamic of concurrent chatting behavior this paper advocate factorial conditional random field fcrfs a a model to accommodate co temporal relationship among multiple activity state in addition to avoid the use of inefficient loopy belief propagation lbp algorithm we propose using iterative classification algorithm ica a the inference method for fcrfs we designed experiment to compare our fcrfs model with two dynamic probabilistic model parallel condition random field pcrfs and hidden markov model hmms in learning and decoding based on auditory data the experimental result show that fcrfs outperform pcrfs and hmm like model we also discover that fcrfs using the ica inference approach not only improves the recognition accuracy but also take significantly le time than the lbp inference method 
we propose a probabilistic transfer learning model that us task level feature to control the task mixture selection in a hierarchical bayesian model these task level feature although rarely used in existing approach can provide additional information to model complex task distribution and allow effective transfer to new task especially when only limited number of data are available to estimate the model parameter we develop an empirical bayes method based on variational approximation technique our experiment on information retrieval show that the proposed model achieves significantly better performance compared with other transfer learning method 
in this paper we propose a new approach for iterated revision in possibilistic logic by applying a one step revision operator we first argue that the set of km postulate for revision is too strong to define a practical one step revision operator and some of them should be weakened we then present a semantic approach for iterated revision in possibilistic logic using a one step revision operator the computation of the semantic approach is given we show that our revision approach satisfies almost all the dp postulate for iterated revision and some other important logical property 
we introduce wiktionary a an emerging lexical semantic resource that can be used a a substitute for expert made resource in ai application we evaluate wiktionary on the pervasive task of computing semantic relatedness for english and german by mean of correlation with human ranking and solving word choice problem for the first time we apply a concept vector based measure to a set of different concept representation like wiktionary pseudo gloss the first paragraph of wikipedia article english wordnet gloss and germanet pseudo gloss we show that i wiktionary is the best lexical semantic resource in the ranking task and performs comparably to other resource in the word choice task and ii the concept vector based approach yield the best result on all datasets in both evaluation 
in this article we work on certain aspect of the belief change theory in order to make them suitable for argumentation system this approach is based on defeasible logic programming a the argumentation formalism from which we ground the definition the objective of our proposal is to define an argument revision operator that insert a new argument into a defeasible logic program in such a way that this argument end up undefeated after the revision thus warranting it conclusion in order to ensure this warrant the defeasible logic program ha to be changed in concordance with a minimal change principle finally we present an algorithm that implement the argument revision operation 
in this work we propose gregress a new algorithm which given set a of labeled graph and a real value associated with each graph extract the complete set of subgraphs such that a each subgraph in this set ha correlation with the real value above a user specified threshold and b each subgraph in this set ha correlation with any other subgraph in the set below a user specified threshold gregress incorporates novel pruning mechanism based on correlation of a subgraph feature with the output and correlation with other subgraph feature these pruning mechanism lead to significant speedup experimental result indicate that in term of run time gregress substantially outperforms gspan often by an order of magnitude while the regression model produced by both approach have comparable accuracy 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
in a class of game known a stackelberg game one agent the leader must commit to a strategy that can be observed by the other agent the adversary follower before the adversary chooses it own strategy we consider bayesian stackelberg game in which the leader is uncertain about the type of the adversary it may face such game are important in security domain where for example a security agent leader must commit to a strategy of patrolling certain area and an adversary follower can observe this strategy over time before choosing where to attack we present here two different mip formulation asap providing approximate policy with controlled randomization and dobss providing optimal policy for bayesian stackelberg game dobss is currently the fastest optimal procedure for bayesian stackelberg game and is in use by police at the los angeles international airport lax to schedule their activity 
building and using agent based model is often impractical in part due to the cost of including expensive subject matter expert smes in the development process in this paper we describe a method for bootstrapping model building to lower the cost of overall model development the model we are interested in here capture dynamic phenomenon related to international and subnational conflict the method of acquiring these model begin with event data drawn from news report about a conflict region and infers model characteristic particular to a conflict modeling framework called the power structure toolkit pstk we describe the toolkit and how it ha been used prior to this work we then describe the current problem of modeling conflict and the empirical data available to learn model and extension to the pstk for model generation from this data we also describe a formative evaluation of the system that compare the performance and cost of model built entirely by an sme against model built with an sme aided by the automated model generation process early result indicate at least equivalent prediction rate with significant saving in model generation cost 
in this paper we study a sequential decision making problem the objective is to maximize the average reward accumulated over time subject to temporal cost constraint the novelty of our setup is that the reward and constraint are controlled by an adverse opponent to solve our problem in a practical way we propose an expert algorithm that guarantee both a vanishing regret and a sublinear number of violated constraint the quality of this solution is demonstrated on a real world power management problem our result support the hypothesis that online learning with convex cost constraint can be performed successfully in practice 
we propose a new topic model for tracking timevarying consumer purchase behavior in which consumer interest and item trend change over time the proposed model can adaptively track change in interest and trend based on current purchase log and previously estimated interest and trend the online nature of the proposed method mean we do not need to store past data for current inference and so we can considerably reduce the computational cost and the memory requirement we use real purchase log to demonstrate the effectiveness of the proposed method in term of the prediction accuracy of purchase behavior and the computational cost of the inference 
a method is given for the extraction of large number of semantic class along with their corresponding instance based on the recombination of element clustered through distributional similarity experimental result show the procedure allows for a parametric trade off between high precision and expanded recall 
city wide urban infrastructure are increasingly reliant on network technology to improve and expand their service a a side effect of this digitalization large amount of data can be sensed and analyzed to uncover pattern of human behavior in this paper we focus on the digital footprint from one type of emerging urban infrastructure shared bicycling system we provide a spatiotemporal analysis of week of bicycle station usage from barcelona s shared bicycling system called bicing we apply clustering technique to identify shared behavior across station and show how these behavior relate to location neighborhood and time of day we then compare experimental result from four predictive model of near term station usage finally we analyze the impact of factor such a time of day and station activity in the prediction capability of the algorithm 
this paper present an approach to acquire knowledge from wikipedia category and the category network many wikipedia category have complex name which reflect human classification and organizing instance and thus encode knowledge about class attribute taxonomic and other semantic relation we decode the name and refer back to the network to induce relation between concept in wikipedia represented through page or category the category structure allows u to propagate a relation detected between constituent of a category name to numerous concept link the result of the process are evaluated against researchcyc and a subset also by human judge the result support the idea that wikipedia category name are a rich source of useful and accurate knowledge 
this system translates basic english description of a wide range of object in a simplistic zoo environment into plausible three dimensional interactive visualization of their position orientation and dimension it combine a semantic network and contextually sensitive knowledge base a representation for explicit and implicit spatial knowledge respectively it linguistic aspect address underspecification vagueness uncertainty and context with respect to intrinsic extrinsic and deictic frame of spatial reference the underlying commonsense reasoning fomlalism is probability based geometric field that are solved through constraint satisfaction the architecture serf a an extensible test and evaluation framework for a multitude of linguistic and artificial intelligence investigation 
this paper present a new algorithm for plan recognition called elexir engine for lexicalized intent recognition elexir represents the plan to be recognized with a grammatical formalism called combinatory categorial grammar ccg we show that representing plan with ccgs can allow u to prevent early commitment to plan goal and thereby reduce runtime 
the adroit system that we are developing allows automatic discourse analysis of information rich natural language text extracted directly from the web we use guideline and relation of rhetorical structure theory rst to decompose text into elementary segment and to perform the discourse parsing between them in this paper we present version of adroit and focus on the noble technique of cue phrase disambiguation and machine learning for identification and organization of discourse relation 
korf reid and edelkamp introduced a formula to predict the number of node ida will expand given the static distribution of heuristic value their formula proved to be very accurate but it is only accurate under the following limitation the heuristic must be consistent the prediction is for a large random sample of start state or for large threshold in this paper we generalize the static distribution to a conditional distribution of heuristic value we then propose a new formula for predicting the performance of ida that work well for inconsistent heuristic zahavi et al and for any set of start state not just a random sample we also show how the formula can be enhanced to work well for single start state experimental result demonstrate the accuracy of our method in all these situation 
at the turn of the last century constantin stanislavski developed a new system of acting replacing the mannered gesture and forced emotion then popular with a more natural style the core of his system lay in having actor perform a process of scene analysis in which an actor would flesh out the circumstance of the play so that the character s motivation and action would follow logically this paper is an attempt to ground stanislavski s method of scene analysis in a formal theory of action we discus the relation between stanislavskian and formal ai theory of action and planning give a formal definition of the end product of a scene analysis and characterize the condition under which a scene analysis is coherent 
in this paper we discus an application integrating an ontological data model with an argumentation based decision support system showing how the combination of leading technology owl sparql and jena can be used to achieve this in the context of improving a decision support tool that is currently being trialled live in a clinical environment we describe quantitatively how the incorporation of an ontology lead to an improvement over the existing software highlighting the benefit of incorporating an ontology in medical application data and clinical feedback is being collected from a live trial at the john radcliffe hospital in oxford where we are able to test the original decision support tool but also the ontology driven version and thus will be able to demonstrate that any quantitative improvement in the efficacy of the software are a product of the ontological data model alone 
current answer set programming asp solver largely build on logic programming without function symbol this limitation make asp decidable but greatly complicates the modeling of indefinite time recursive data structure e g list and infinite process and object in general recent research thus aim at finding decidable fragment of asp with function symbol and studying their complexity we identify bidirectional asp program a an expressive such fragment that is useful e g for reasoning about action involving both the future and the past we tightly characterize the computational complexity of bidirectional program and of some of their subclass addressing the main reasoning task our result also imply that the recently introduced fdnc program can be extended by inverse predicate while retaining decidability but computational cost are unavoidably higher 
this paper present a novel people detection and tracking method based on a multi modal sensor fusion approach that utilizes d laser range and camera data the data point in the laser scan are clustered using a novel graph based method and an svm based version of the cascaded adaboost classifier is trained with a set of geometrical feature of these cluster in the detection phase the classified laser data is projected into the camera image to define a region of interest for the vision based people detector this detector is a fast version of the implicit shape model ism that learns an appearance codebook of local sift descriptor from a set of hand labeled image of pedestrian and us them in a voting scheme to vote for center of detected people the extension consists in a fast and detailed analysis of the spatial distribution of voter per detected person each detected person is tracked using a greedy data association method and multiple extended kalman filter that use different motion model this way the filter can cope with a variety of different motion pattern the tracker is asynchronously updated by the detection from the laser and the camera data experiment conducted in real world outdoor scenario with crowd of pedestrian demonstrate the usefulness of our approach 
this paper focus on improving branch and bound max sat solver by speeding up the lower bound computation we notice that the existing propagation based computing method and the resolution based computing method which have been studied intensively both suffer from several drawback in order to overcome these drawback we propose a new method with a nice property that guarantee the increment of lower bound the new method exploit within problem learning technique more specifically at each branch point in the search tree the current node is enabled to inherit inconsistency from it parent and learn information about effectiveness of the lower bound computing procedure from previous node furthermore after branching on a new variable the inconsistency may shrink by applying unit propagation to them and such process increase the probability of getting better lower bound we graft the new technique into maxsatz and the experimental result demonstrate that the new solver outperforms the best state of the art solver on a wide range of instance including random and structured one 
conditional random field crf is a popular graphical model for sequence labeling the flexibility of crf pose significant computational challenge for training using existing optimization package often lead to long training time and unsatisfactory result in this paper we develop crfopt a general crf training package to improve the efficiency and quality for training crfs we propose two improved version of the forward backward algorithm that exploit redundancy and reduce the time by several order of magnitude further we propose an exponential transformation that enforces sufficient step size for quasi newton method the technique improves the convergence quality leading to better training result we evaluate crf opt on a gene prediction task on pathogenic dna sequence and show that it is faster and achieves better prediction accuracy than both the hmm model and the original crf model without exponential transformation 
recently the notion of accrual of argument ha received some attention from the argumentation community three principle for argument accrual have been identified a necessary to hold in argumentation framework in this paper we propose an approach to model the accrual of argument in the context of defeasible logic programming a logic programming approach to argumentation which ha proven to be successful for many real world application we will analyze the above mentioned principle in the context of our proposal studying other interesting property 
many ai problem can be recast a finding an optimal path in a discrete state space an abstraction defines an admissible heuristic function a the distance in a smaller state space where arbitrary set of state are aggregated into single state a special case are pattern database pdb heuristic which aggregate state if they agree on the state variable inside the pattern explicit state abstraction is more flexible explicitly aggregating selected pair of state in a process that interleaf composition of abstraction with abstraction of the composite the increased flexibility gain expressive power sometimes the real cost function can be represented concisely a an explicit state abstraction but not a a pdb explicit state abstraction ha been applied to planning and model checking with highly promising empirical result 
moving target search mt or the game of cop and robber ha a broad field of application reaching from law enforcement to computer game within the recent year research ha focused on computing move policy for one or multiple pursuer cop the present work motivates to extend this perspective to both side thus developing algorithm for the target robber we investigate the game with perfect information for both player and propose two new method named trailmax and dynamic abstract trailmax to compute move policy for the target experiment are conducted by simulating game on map of the commercial computer game baldur s gate and measuring survival time and computational complexity we test seven algorithm cover dynamic abstract minimax minimax hill climbing with distance heuristic a random beacon algorithm trailmax and datrailmax analysis show that our method outperform all the other algorithm in quality achieving up to optimality while meeting modern computer game computation time constraint 
least common subsumers lcs have been proposed in description logic dl to capture the commonality between two or more concept since it introduction in lcs have been successfully employed a a logical tool for a variety of application spanning from inductive learning to bottom up construction of knowledge base information retrieval to name a few the best known algorithm for computing lcs us structural comparison on normal form and the most expressive dl it is applied to is alen we provide a general tableau based calculus for computing lcs via substitution on concept term containing concept variable we show the applicability of our method to an expressive dl but without disjunction and full negation discus complexity issue and show the generality of our proposal 
this paper describes our experience in using modern web architecture lightweight python framework and rapid prototyping to create an ai rostering and workforce management system to help prepare for the beijing olympic equestrian event which 
spectral feature selection identifies relevant feature by measuring their capability of preserving sample similarity it provides a powerful framework for both supervised and unsupervised feature selection and ha been proven to be effective in many real world application one common drawback associated with most existing spectral feature selection algorithm is that they evaluate feature individually and cannot identify redundant feature since redundant feature can have significant adverse effect on learning performance it is necessary to address this limitation for spectral feature selection to this end we propose a novel spectral feature selection algorithm to handle feature redundancy adopting an embedded model the algorithm is derived from a formulation based on a sparse multi output regression with a l norm constraint we conduct theoretical analysis on the property of it optimal solution paving the way for designing an efficient path following solver extensive experiment show that the proposed algorithm can do well in both selecting relevant feature and removing redundancy 
pairwise markov network pmn are an important class of markov network which due to their simplicity are widely used in many application such a image analysis bioinformatics sensor network etc however learning of markov network from data is a challenging task there are many possible structure one must consider and each of these structure come with it own parameter making it easy to overfit the model with limited data to deal with the problem recent learning method build upon the l regularization to express the bias towards sparse network structure in this paper we propose a new and more flexible framework that let u bias the structure that can for example encode the preference to network with certain local substructure which a a whole exhibit some special global structure we experiment with and show the benefit of our framework on two type of problem learning of modular network and learning of traffic network model 
multi class problem have a richer structure than binary classification problem thus they can potentially improve their performance by exploiting the relationship among class label while for the purpose of providing an automated classification result this class structure doe not need to be explicitly unveiled for human level analysis or interpretation this is valuable we develop a multi class large margin classifier that extract and take advantage of class relationship we provide a bi convex formulation that explicitly learns a matrix that capture these class relationship and is decoupled from the feature weight our representation can take advantage of the class structure to compress themodel by reducing the number of classifier employed maintaining high accuracy even with large compression in addition we present an efficient formulation in term of speed and memory 
we present the design of a banner advertising auction which is considerably more expressive than current design we describe a general model of expressive ad contract bidding and an allocation model that can be executed in real time through the assignment of fraction of relevant ad channel to specific advertiser contract the uncertainty in channel supply and demand is addresscd by the formulation of a stochastic combinatorial optimization problem for channel allocation that is rerun periodically we solve this in two different way fast deterministic optimization with respect to expectation and a novel online sample based stochastic optimization method that can be applied to continuous decision space which exploit the deterministic optimization a a black box experiment demonstrate the importance of expressive bidding and the value of stochastic optimization 
existing work on workflow mining ignores the dataflow aspect of the problem this is not acceptable for service oriented application that use web service with typed input and output we propose a novel algorithm wit workflow inference from trace which identifies the context similarity of the observed action based on the dataflow and us model merging technique to generalize the control flow and the dataflow simultaneously we identify the class of workflow that wit can learn correctly we implemented wit and tested it on a real world medical scheduling domain where wit wa able to find a good approximation of the target workflow 
access control is an important security activity that prevents undesired person from entering secure building or perimeter the advanced risk analysis presented in this paper enables distinguishing between acceptable and undesired entry based on several entry sensor such a fingerprint reader and intelligent method that learn behavior from previous entry we have extended the intelligent layer in two way first by adding a meta learning layer that combine the output of specific intelligent module and second by constructing a bayesian network to integrate the prediction of learning and meta learning module the obtained result indicate an important increase in detecting security attack 
distributed constraint optimization dcop is a key technique for solving agent coordination problem because finding cost minimal dcop solution is np hard it is important to develop mechanism for dcop search algorithm that trade off their solution cost for smaller runtimes however existing tradeoff mechanism do not provide relative error bound in this paper we introduce three tradeoff mechanism that provide such bound namely the relative error mechanism the uniformly weighted heuristic mechanism and the non uniformly weighted heuristic mechanism for two dcop algorithm namely adopt and bnb adopt our experimental result show that the relative error mechanism generally dominates the other two tradeoff mechanism for adopt and the uniformlyweighted heuristic mechanism generally dominates the other two trade off mechanism for bnb adopt 
we discus cuecard the program that won the computer olympiad computational pool tournament beside addressing intrinsic interest in a complex competitive environment with unique feature our goal is to isolate the factor that contributed to the performance so that the lesson can be transferred to other similar domain specifically we distinguish among pure engineering factor such a using a computer cluster domain specific factor such a optimized break shot and domain independent factor such a state clustering our conclusion is that each type of factor contributed to the performance of the program 
multi agent path planning on grid map is a challenging problem and ha numerous real life application running a centralized systematic search such a a is complete and cost optimal but scale up poorly in practice since both the search space and the branching factor grow exponentially in the number of mobile unit decentralized approach which decompose a problem into several subproblems can be faster and can work for larger problem however existing decentralized method offer no guarantee with respect to completeness running time and solution quality to address such limitation we introduce mapp a tractable algorithm for multi agent path planning on grid map we show that mapp ha low polynomial worst case upper bound for the running time the memory requirement and the length of solution a it run in low polynomial time mapp is incomplete in the general case we identify a class of problem for which our algorithm is complete we believe that this is the first study that formalises restriction to obtain a tractable class of multi agent path planning problem 
a data center energy consumption continues to rise efficient power management is becoming increasingly important in this work we examine the use of a novel market mechanism for finding the right balance between power and performance the market enables a separation between a buyer side that strives to maximize performance and a seller side that strives to minimize power and other cost a concise and scalable description language is defined for agent preference that admits a mixedinteger program for computing optimal allocation experimental result demonstrate the robustness flexibility practicality and scalability of the architecture 
this paper present a novel recursive maximum a posteriori update for the kalman formulation of undelayed bearing only slam the estimation update step is cast a an optimization problem for which we can prove the global minimum is reachable via a bidirectional search using gauss newton s method along a one dimensional manifold while the filter is designed for mapping just one landmark it is easily extended to full scale multiple landmark slam we provide this extension via a formulation of bearing only fastslam with experiment we demonstrate accurate and convergent estimation in situation where an ekf solution would diverge 
we present a formal framework for minimal module extraction based on an abstract notion of inseparability w r t a signature between ontology two instance of this framework are discussed in detail for dl lite ontology concept inseparability when ontology imply the same complex concept inclusion over the signature and query inseparability when they give the same answer to existential query for any instance data over the signature we demonstrate that different type of corresponding minimal module for these inseparability relation can be automatically extracted from large scale dl lite ontology by composing the tractable syntactic locality based module extraction algorithm with intractable extraction algorithm using the multi engine qbf solver aqme the extracted minimal module are compared with those obtained using non logic based approach 
a exact inference for first order probabilistic graphical model at the propositional level can be formidably expensive there is an ongoing effort to design efficient lifted inference algorithm for such model this paper discus directed first order model that require an aggregation operator when a parent random variable is parameterized by logical variable that are not present in a child random variable we introduce a new data structure aggregation parfactors to describe aggregation in directed first order model we show how to extend milch et al s c fove algorithm to perform lifted inference in the presence of aggregation parfactors we also show that there are case where the polynomial time complexity in the domain size of logical variable of the c fove algorithm can be reduced to logarithmic time complexity using aggregation parfactors 
in complex strategic situation decision making agent interact with many other agent and have access to many piece of information throughout their play this usually lead to game solving being a very complex almost intractable procedure moreover algorithm for solving game usually fail to explain how the various equilibrium come about and how plausible they are reasoning pattern try to capture the strategic thinking of agent and formalize the usage of the various information or evidence they obtain during their interaction identifying reasoning pattern can lead to a significant refinement over the full range of equilibrium a well a considerable computational saving in solving the game here we present a polynomial time algorithm that simplifies the original game by iteratively identifying noneffective ignorable decision node and removing redundant information edge in some case this can lead to exponential time saving in computing an equilibrium yet some potentially efficient equilibrium may be lost in the process 
this paper address the task of automatic classification of semantic relation between noun we present an improved wordnet based learning model which relies on the semantic information of the constituent noun the representation of each noun s meaning capture conceptual feature which play a key role in the identification of the semantic relation we report substantial improvement over previous wordnet based method on the semeval data moreover our experiment show that wordnet s is a hierarchy is better suited for some semantic relation compared with others we also compute various learning curve and show that our model doe not need a large number of training example 
in this paper we propose a latent multi task learning algorithm to solve the multi device indoor localization problem traditional indoor localization system often assume that the collected signal data distribution are fixed and thus the localization model learned on one device can be used on other device without adaptation however by empirically studying the signal variation over different device we found this assumption to be invalid in practice to solve this problem we treat multiple device a multiple learning task and propose a multi task learning algorithm different from algorithm assuming that the hypothesis learned from the original data space for related task can be similar we only require the hypothesis learned in a latent feature space are similar to establish our algorithm we employ an alternating optimization approach to iteratively learn feature mapping and multi task regression model for the device we apply our latent multi task learning algorithm to real world indoor localization data and demonstrate it effectiveness 
point based algorithm and rtdp bel are approximate method for solving pomdps that replace the full update of parallel value iteration by faster and more effective update at selected belief an important difference between the two method is that the former adopt sondik s representation of the value function while the latter us a tabular representation and a discretization function the algorithm however have not been compared up to now because they target different pomdps discounted pomdps on the one hand and goal pomdps on the other in this paper we bridge this representational gap showing how to transform discounted pomdps into goal pomdps and use the transformation to compare rtdp bel with point based algorithm over the existing discounted benchmark the result appear to contradict the conventional wisdom in the area showing that rtdp bel is competitive and sometimes superior to point based algorithm in both quality and time 
a general game player is a system that understands the rule of an unknown game and learns to play this game well without human intervention to succeed in this endeavor system need to be able to extract and prove game specific knowledge from the mere game rule we present a practical approach to this challenge with the help of answer set programming the key idea is to reduce the automated theorem proving task to a simple proof of an induction step and it base case we prove correctness of this method and report on experiment with an off the shelf answer set programming system in combination with a successful general game player 
this paper describes our experience in using modern web architecture lightweight python framework and rapid prototyping to create an ai rostering and workforce management system to help prepare for the beijing olympic equestrian event which 
coalition structure generation ha received considerable attention in recent research several algorithm have been proposed to solve this problem in characteristic function game cfgs where every coalition is assumed to perform equally well in any coalition structure containing it in contrast very little attention ha been given to the more general partition function game pfgs where a coalition s effectiveness may change from one coalition structure to another in this paper we deal with pfgs with positive and negative externality in this context we identify the minimum search that is required in order to establish a bound on the quality of the best coalition structure found we then develop an anytime algorithm that improves this bound with further search and show that it out performs the existing state of the art algorithm by order of magnitude 
in many multiagent setting a decision must be made based on the preference of multiple agent and agent may lie about their preference if this is to their benefit in mechanism design the goal is to design procedure mechanism for making the decision that work in spite of such strategic behavior usually by making untruthful behavior suboptimal in automated mechanism design the idea is to computationally search through the space of feasible mechanism rather than to design them analytically by hand unfortunately the most straightforward approach to automated mechanism design doe not scale to large instance because it requires searching over a very large space of possible function in this paper we describe an approach to automated mechanism design that is computationally feasible instead of optimizing over all feasible mechanism we carefully choose a parameterized subfamily of mechanism then we optimize over mechanism within this family finally we analyze whether and to what extent the resulting mechanism is suboptimal outside the subfamily the difficult part of course is to choose a subfamily that allows for both tractable optimization and good mechanism in this paper we discus two case study in the context of strategy proof allocation mechanism mechanism that redistribute vickrey revenue and mechanism that involve no payment at all 
this article fall within the field of abstract argumentation framework in particular we focus on the study of framework using a proof procedure based on dialectical tree these tree rely on a marking procedure to determine the warrant status of their root argument thus our objective is to formulate rationality postulate to characterize the marking criterion over dialectical tree the behavior of the marking procedure is closely tied to the alteration of tree which is the keystone of any model of change based on dialectical argumentation hence the result achieved in this work will benefit research on dynamic in argumentation 
effective communication in open environment relies on the ability of agent to reach a mutual understanding of the exchanged message by reconciling the vocabulary ontology used various approach have considered how mutually acceptable mapping between corresponding concept in the agent own ontology may be determined dynamically through argumentation based negotiation such a meaning based argumentation however the complexity of this process is high approaching p complete in some case a reducing this complexity is non trivial we propose the use of ontology modularization a a mean of reducing the space over which possible concept are negotiated the suitability of different modularization approach a filtering mechanism for reducing the negotiation search space is investigated and a framework that integrates modularization with meaning based argumentation is proposed we empirically demonstrate that some modularization approach not only reduce the number of alignment required to reach consensus but also predict those case where a service provider is unable to satisfy a request without the need for negotiation 
we extend the knowledge compilation map introduced by darwiche and marquis with three influential propositional fragment the krom cnf one also known a the bijunctive fragment the hom cnf fragment and the affine fragment also known a the biconditional fragment a well a seven additional language based on them and composed respectively of krom or hom cnf formula renamable hom cnf formula disjunction of krom cnf formula disjunction of hom cnf formula disjunction of krom or hom cnf formula disjunction of renamable hom cnf formula and disjunction of fine formula each fragment is evaluated w r t several criterion including the complexity of basic quaries and transformation and it spatial efficiency is also analysed 
the selection of the action to do next is one of the central problem faced by autonomous agent in ai three approach have been used to address this problem the programming based approach where the agent controller is given by the programmer the learning based approach where the controller is induced from experience via a learning algorithm and the model based approach where the controller is derived from a model of the problem planning in ai is best conceived a the model based approach to action selection the model represent the initial situation action sensor and goal the main challenge in planning is computational a all the model whether accommodating feedback and uncertainty or not are intractable in the worst case in this article i review some of the model considered in current planning research the progress achieved in solving these model and some of the open problem 
we formalize negotiation using logic programming with consistency restoring rule or cr prolog balduccini and gelfond our formulation deal with incomplete information preference and changing goal we assume that each agent is equipped with a knowledge base for negotiation which consists of a cr program a set of possible assumption and a set of ordered goal we use the notion of an answer set a a mean to formalize the basic notion of negotiation such a proposal response negotiation negotiation tree protocol etc and discus their property 
we formulate loop formula for logic program with arbitrary constraint atom for the semantics based on conditional satisfaction this provides a method for answer set computation by computing model of completion one particular attractive candidate for the latter task is pseudo boolean constraint solver to strengthen this connection we show example of compact encoding of aggregate and global constraint by pseudo boolean constraint 
action language allow to formally represent and reason about action in a highly declarative manner in recent work revision and management of conflict for domain description in such language wrt semantic integrity constraint have been considered in particular their reconciliation however merely ad hoc test and method have been presented to aid the user in analyzing and correcting a flawed description we go beyond this and present a methodology on top of such test for identifying a possible error which work in several stage the issue of such a methodology for action language is novel and ha not been addressed before but is important for building tool and engineering action description in practice 
this paper considers the setting wherein a group of agent e g robot is seeking to obtain a given tangible good potentially available at different location in a physical environment traveling between location a well a acquiring the good at any given location consumes from the resource available to the agent e g battery charge the availability of the good at any given location a well a the exact cost of acquiring the good at the location is not fully known in advance and observed only upon physically arriving at the location however apriori probability on the availability and potential cost are provided given such a setting the problem is to find a strategy plan that maximizes the probability of acquiring the good while minimizing resource consumption sample application include agent in exploration and patrol mission e g rover on mar seeking to mine a specific mineral although this model capture many real world scenario it ha not been investigated so far we focus on the case where location are aligned along a path and study several variant of the problem analyzing the effect of communication and coordination for the case that agent can communicate we present a polynomial algorithm that work for any fixed number of agent for noncommunicating agent we present a polynomial algorithm that is suitable for any number of agent finally we analyze the difference between homogeneous and heterogeneous agent both with respect to their allotted resource and with respect to their capability 
kidney are the most prevalent organ transplant but demand dwarf supply kidney exchange enable willing but incompatible donor patient pair to swap donor these swap can include cycle longer than two pair a well and chain triggered by altruistic donor current kidney exchange address clearing deciding who get kidney from whom a an offline problem they optimize the current batch in reality clearing is an online problem where patient donor pair and altruistic donor appear and expire over time in this paper we study trajectory based online stochastic optimization algorithm which use a recent scalable optimal offline solver a a subroutine for this we identify tradeoff in these algorithm between different parameter we also uncover the need to set the batch size that the algorithm consider an atomic unit we develop an experimental methodology for setting these parameter and conduct experiment on real and generated data we adapt the regret algorithm of bent and van hentenryck for the setting we then develop a better algorithm we also show that the amsaa algorithm of mercier and van hentenryck doe not scale to the nationwide level our best online algorithm save significantly more life than the current practice of solving each batch separately 
we investigate the problem of eliciting cp net in the well known model of exact learning with equivalence and membership query the goal is to identify a preference ordering with a binary valued cp net by guiding the user through a sequence of query each example is a dominance test on some pair of outcome in this setting we show that acyclic cp net are not learnable with equivalence query alone while they are learnable with the help of membership query if the supplied example are restricted to swap a similar property hold for tree cp net with arbitrary example in fact membership query allow u to provide attribute efficient algorithm for which the query complexity is only logarithmic in the number of attribute such result highlight the utility of this model for eliciting cp net in large multi attribute domain 
a common approach to the control problem in partially observable environment is to perform a direct search in policy space a defined over some set of feature of history in this paper we consider predictive feature whose value are conditional probability of future event given history since predictive feature provide direct information about the agent s future they have a number of advantage for control however unlike more typical feature defined directly over past observation it is not clear how to maintain the value of predictive feature over time a model could be used since a model can make any prediction about the future but in many case learning a model is infeasible in this paper we demonstrate that in some case it is possible to learn to maintain the value of a set of predictive feature even when a learning a model is infeasible and that natural predictive feature can be useful for policy search method 
we study path planning on grid with blocked and unblocked cell any angle path planning algorithm find short path fast because they propagate information along grid edge without constraining the resulting path to grid edge incremental pathplanning algorithm solve a series of similar pathplanning problem faster than repeated single shot search because they reuse information from the previous search to speed up the next one in this paper we combine these idea by making the anyangle path planning algorithm basic theta incremental this is non trivial because basic theta doe not fit the standard assumption that the parent of a vertex in the search tree must also be it neighbor we present incremental phi and show experimentally that it can speed up basic theta by about one order of magnitude for path planning with the freespace assumption 
in this paper we introduce an on line decentralised coordination algorithm for monitoring and predicting the state of spatial phenomenon by a team of mobile sensor these sensor have their application domain in disaster response where strict time constraint prohibit path planning in advance the algorithm enables sensor to coordinate their movement with their direct neighbour to maximise the collective information gain while predicting measurement at unobserved location using a gaussian process it build upon the max sum message passing algorithm for decentralised coordination for which we present two new generic pruning technique that result in speed up of up to for sensor we empirically evaluate our algorithm against several on line adaptive coordination mechanism and report a reduction in root mean squared error up to compared to a greedy strategy 
self organizing map can be used to implement an associative memory for an intelligent system that dynamically learns about new high level domain over time som are an attractive option for implementing associative memory they are fast easily parallelized and digest a stream of incoming data into a topographically organized collection of model where more frequent class of data are represented by higher resolution collection of model typically the distribution of model in an som once developed remains fairly stable but developing expertise in a new high level domain requires altering the allocation of model we use a mixture of analysis and empirical study to characterize the behavior of som for high level associative memory finding that new high resolution collection of model develop quickly high resolution area of the som decay rapidly unless actively refreshed but in a large som the ratio between growth rate and decay rate may be high enough to support both fast learning and long term memory 
we introduce a new method to find semantic inconsistency i e concept with erroneous synonymity in the unified medical language system umls the idea is to identify the inconsistency by comparing the semantic group of hierarchically related concept using answer set programming with this method we identified several inconsistent concept in umls and discovered an interesting semantic pattern along hierarchy which seems associated with wrong synonymy 
equilibrium or near equilibrium solution to very large extensive form game are often computed by using abstraction to reduce the game size a common abstraction technique for game with a large number of available action is to restrict the number of legal action in every state this method ha been used to discover equilibrium solution for the game of no limit head up texas hold em when using a solution to an abstracted game to play one side in the un abstracted real game the real opponent action may not correspond to action in the abstracted game the most popular method for handling this situation is to translate opponent action in the real game to the closest legal action in the abstracted game we show that this approach can result in a very exploitable player and propose an alternative solution we use probabilistic mapping to translate a real action into a probability distribution over action whose weight are determined by a similarity metric we show that this approach significantly reduces the exploitability when using an abstract solution in the real game 
event detection is a critical task in sensor network for a variety of real world application many real world event often exhibit complex spatio temporal pattern whereby they manifest themselves via observation over time and space proximity these spatio temporal event cannot be handled well by many of the previous approach in this paper we propose a new spatio temporal event detection sted algorithm in sensor network based on a dynamic conditional random field dcrf model our sted method handle the uncertainty of sensor data explicitly and permit neighborhood interaction in both observation and event label experiment on both real data and synthetic data demonstrate that our sted method can provide accurate event detection in near real time even for large scale sensor network 
plan recognition is the problem of inferring the goal and plan of an agent after observing it behavior recently it ha been shown that this problem can be solved efficiently without the need of a plan library using slightly modified planning algorithm in this work we extend this approach to the more general problem of probabilistic plan recognition where a probability distribution over the set of goal is sought under the assumption that action have deterministic effect and both agent and observer have complete information about the initial state we show that this problem can be solved efficiently using classical planner provided that the probability of a partially observed execution given a goal is defined in term of the cost difference of achieving the goal under two condition complying with the observation and not complying with them this cost and hence the posterior goal probability are computed by mean of two call to a classical planner that no longer ha to be modified in any way a number of example is considered to illustrate the quality flexibility and scalability of the approach avrat and joint plan that pose a problem to library based approach are handled naturally second by building on state of theart planning algorithm the approach scale up well handling domain with hundred of action and fluents quite efficiently an important limitation of our earlier account ramirez and geffner however is the assumption that agent are perfectly rational and thus pursue their goal in an optimal way only the result is that goal g that admit no optimal plan compatible with the observation are excluded and hence noise in the behavior of agent is not tolerated the goal of this work is to introduce a more general formulation that retains the benefit of the generative approach to plan recognition while producing posterior probability p g o rather than boolean judgement for this a prior distribution p g over the goal g is assumed to be given and the likelihood p o g of the observation o given the goal g are defined in term of cost difference computed by a classical planner moreover in contrast to our earlier formulation the classical planner can be used off the shelf and doe not need to be modified in any way the paper is organized a follows we first go over an example and revisit the basic notion of planning and plan recognition from the perspective of planning we then introduce the probabilistic formulation present the experimental result and discus related and future work 
in this paper we describe the system that we have developed to solve a new variant of the periodic vehicle routing problem with time window pvrptw for one of the largest food and restaurant chain in hong kong the extension is to limit the number of driver that any store should see during the fixed period we name this constraint a limited visiting quota lvq we devise a new method to solve this problem our experimental result indicate that our method is able to reduce the number of vehicle used by and thus bring substantial saving to our client the solver ha been integrated into an existing vehicle routing product called vroom for the daily usage of our client 
human computation is a recent approach that extract information from large number of web user recaptcha is a human computation project that improves the process of digitizing book by getting human to read word that are difficult for ocr algorithm to read von ahn et al in this paper we address an interesting strategic control problem inspired by the recaptcha project given a large set of word to transcribe within a time deadline how can we choose the difficulty level such that we maximize the probability of successfully transcribing a document on time our approach is inspired by previous work on timed zero sum game a we face an analogous timed policy decision on the choice of word to present to user however our web based word transcribing domain is particularly challenging a the reward of the action is not known i e there is no knowledge if the spelling provided by a human is actually correct we contribute an approach to solve this problem by checking a small fraction of the answer at execution time obtaining an estimate of the cumulative reward we present experimental result showing how the number of sample and time between sample affect the probability of success we also investigate the choice of aggressive or conservative action with regard to the bound produced by sampling we successfully apply our algorithm to real data gathered by the recaptcha project 
in this paper we propose a notion of question utility for studying usefulness of question and show how question utility can be integrated into question search a static ranking to measure question utility we examine three method a a method of employing the language model to estimate the probability that a question is generated from a question collection and then using the probability a question utility b a method of using the lexrank algorithm to evaluate centrality of question and then using the centrality a question utility and c the combination of a and b to use question utility in question search we employ a log linear model for combining relevance score in question search and utility score regarding question utility our experimental result with the question about travel from yahoo answer show that question utility can be effective in boosting up rank of generally useful question 
previous work on information extraction from unstructured ungrammatical text e g classified ad showed that exploiting a set of background knowledge called a reference set greatly improves the precision and recall of the extraction however finding a source for this reference set is often difficult if not impossible further even if a source is found it might not overlap well with the text for extraction in this paper we present an approach to building the reference set directly from the text itself our approach eliminates the need to find the source for the reference set and ensures better overlap between the text and reference set starting with a small amount of background knowledge our technique construct tuples representing the entity in the text to form a reference set our result show that our method outperforms manually constructed reference set since hand built reference set may not overlap with the entity in the unstructured ungrammatical text we also ran experiment comparing our method to the supervised approach of conditional random field crfs using simple generic feature these result show our method achieves an improvement in f measure for attribute and is competitive in performance on the others and this is without training data 
in this paper we present an integrated planning and robotic architecture that actively directs an agent engaged in an urban search and rescue usar scenario we describe three salient feature that comprise the planning component of this system namely the ability to plan in a world open with respect to object execution monitoring and replanning ability and handling soft goal and detail the interaction of these part in representing and solving the usar scenario at hand we show that though insufficient in an individual capacity the integration of this trio of feature is sufficient to solve the scenario that we present we test our system with an example problem that involves soft and hard goal a well a goal deadline and action cost and show via an included video that the planner is capable of incorporating sensing action and execution monitoring in order to produce goal fulfilling plan that maximize the net benefit accrued 
in many real world situation we are charged with detecting change a soon a possible important example include detecting medical condition detecting security breach and updating cache of distributed database in those situation sensing can be expensive but it is also important to detect change in a timely manner in this paper we present tractable greedy algorithm and prove that they solve this decision problem either optimally or approximate the optimal solution in many case our problem model is a pomdp that includes a cost for sensing a cost for delayed detection a reward for successful detection and no cost partial observation making optimal decision is difficult in general we show that our tractable greedy approach find optimal policy for sensing both a single variable and multiple correlated variable further we provide approximation for the optimal solution to multiple hidden or observed variable per step our algorithm outperform previous algorithm in experiment over simulated data and live wikipedia www page 
current feature based method for sketch recognition system rely on human selected feature certain machine learning technique have been found to be good nonhnear feature extractor in this paper we apply a manifold learning method kernel isomap with a new algorithm for multi stroke sketch recognition which significantly outperforms the standard feature based technique 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
we consider the following setting a decision maker must make a decision based on reported data point with binary label subset of data point are controlled by different selfish agent which might misreport the label in order to sway the decision in their favor we design mechanism both deterministic and randomized that reach an approximately optimal decision and are strategyproof i e agent are best off when they tell the truth we then recast our result into a classical machine learning classification framework where the decision maker must make a decision choose between the constant positive hypothesis and the constant negative hypothesis based only on a sampled subset of the agent point 
in a recent paper ferraris lee and lifschitz conjectured that the concept of a stable model of a first order formula can be used to treat some answer set programming expression a abbreviation we follow up on that suggestion and introduce an answer set programming language that defines the meaning of counting and choice by reducing these construct to first order formula for the new language the concept of a safe program is defined and it semantic role is investigated we compare the new language with the concept of a disjunctive program with aggregate introduced by faber leone and pfeifer and discus the possibility of implementing a fragment of the language by translating it into the input language of the answer set solver dlv the language is also compared with cardinality constraint program defined by syrj nen 
artificial society distributed system of autonomous agent are becoming increasingly important in e commerce agent base their decision on trust and reputation in way analogous to human society many different definition for trust and reputation have been proposed that incorporate many source of information however system design have tended to focus much of their attention on direct interaction furthermore trust updating scheme for direct interaction have tended to uncouple update for positive and negative feedback consequently behaviour in which cycle of positive feedback followed by a single negative feedback result in untrustworthy agent remaining undetected this con man style of behaviour is formally described and desirable characteristic of con resistant trust scheme proposed a conresistant scheme is proposed and compared with fire regret and yu and singh s model yu and singh simulation experiment demonstrate the utility of the con resistant scheme 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
many real world application such a supply chain management scm can be modeled using multi agent system one shortcoming of current scm model is that their trust model are ad hoc and do not have a strong theoretical basis we propose a trust model for scm that is grounded in probabilistic game theory in this model trust can be gained through direct interaction and or by asking for information from other trustworthy agent we will use this model to simulate and study supply chain market behavior supply chain network have often been modeled in the research literature with multi agent system in which the agent need to collaborate with one or more partner this collaboration becomes more effective when agent have the ability to choose their partner based on the trustworthiness of the candidate trust is defined a the belief an agent ha that the other party will fulfill it promise given the possibility that the partner may defect to get higher benefit dasgupta a major shortcoming in previous research on trust in supply chain management is that their trust based decision making is not grounded in a formal trust model in this work we propose a trust model for scms that incorporates trust factor specific to scm represented in probabilistic and utility based term our model also take into account the effect of variable payoff we will investigate how market behavior is affected by different trust factor we will identify optimal strategy for different situation i e those strategy that result in the best performance and overall return this abstract describes our initial framework which we are currently implementing 
existing method for single document keyphrase extraction usually make use of only the information contained in the specified document this paper proposes to use a small number of nearest neighbor document to provide more knowledge to improve single document keyphrase extraction a specified document is expanded to a small document set by adding a few neighbor document close to the document and the graph based ranking algorithm is then applied on the expanded document set to make use of both the local information in the specified document and the global information in the neighbor document experimental result demonstrate the good effectiveness and robustness of our proposed approach 
a key trend in electronic commerce is a demand for higher level of expressiveness in the mechanism that mediate interaction we develop a theory that tie the expressiveness of mechanism to their efficiency in a domain independent manner we introduce two new expressiveness measure maximum impact dimension which capture the number of way that an agent can impact the outcome and shatterable outcome dimension which is based on the concept of shattering from computational learning theory we derive an upper bound on the expected efficiency of any mechanism under it most efficient nash equilibrium remarkably it depends only on the mechanism s expressiveness we prove that the bound increase strictly a we allow more expressiveness we also show that in some case a small increase in expressiveness yield an arbitrarily large increase in the bound finally we study channel based mechanism which subsume most combinatorial auction multi attribute mechanism and the vickrey clarke grove scheme we show that our domain independent measure of expressiveness appropriately relate to the natural measure of expressiveness of channel based mechanism the number of channel allowed using this bridge our general result yield interesting implication for example any channel based multi item auction that doe not allow rich combinatorial bid can be arbitrarily inefficient unless agent have no private information 
standard belief contraction assumes an underlying logic containing full classical propositional logic but there are good reason for considering contraction in le expressive logic in this paper we focus on horn logic in addition to being of interest in it own right our choice is motivated by the use of horn logic in several area including ontology reasoning in description logic we consider three version of contraction entailment based and inconsistency based contraction e contraction and i contraction resp introduced by delgrande for horn logic and package contraction p contraction studied by fuhrmann and hansson for the classical case we show that the standard basic form of contraction partial meet is too strong in the horn case we define more appropriate notion of basic contraction for all three type above and provide associated representation result in term of postulate our result stand in contrast to delgrande s conjecture that orderly maxichoice is the appropriate contraction for both eand i contraction our interest in p contraction stem from it relationship with an important reasoning task in ontological reasoning repairing the subsumption hierarchy in el this is closely related to p contraction with set of basic horn clause horn clause of the form p q we show that this restricted version of p contraction can also be represented a i contraction 
traditionally text categorization ha been studied a the problem of training of a classifier using labeled data however people can categorize document into named category without any explicit training because we know the meaning of category name in this paper we introduce dataless classification a learning protocol that us world knowledge to induce classifier without the need for any labeled data like human a dataless classifier interprets a string of word a a set of semantic concept we propose a model for dataless classification and show that the label name alone is often sufficient to induce classifier using wikipedia a our source of world knowledge we get accuracy on task from the newsgroup dataset and accuracy on task from a yahoo answer dataset without any labeled or unlabeled data from the data set with unlabeled data we can further improve the result and show quite competitive performance to a supervised learning algorithm that us labeled example 
in this paper we present a general data clustering algorithm which is based on the asymmetric pairwise measure of markov random walk hitting time on directed graph unlike traditional graph based clustering method we do not explicitly calculate the pairwise similarity between point instead we form a transition matrix of markov random walk on a directed graph directly from the data our algorithm construct the probabilistic relation of dependence between local sample pair by studying the local distribution of the data such dependence relation are asymmetric which is a more general measure of pairwise relation than the similarity measure in traditional undirected graph based method in that it considers both the local density and geometry of the data the probabilistic relation of the data naturally result in a transition matrix of markov random walk based on the random walk viewpoint we compute the expected hitting time for all sample pair which explores the global information of the structure of the underlying directed graph an asymmetric measure based clustering algorithm called k destination is proposed for partitioning the node of the directed graph into disjoint set by utilizing the local distribution information of the data and the global structure information of the directed graph our method is able to conquer some limitation of traditional pairwise similarity based method experimental result are provided to validate the effectiveness of the proposed approach 
the problem of fairly dividing a cake a a metaphor for a heterogeneous divisible good ha been the subject of much interest since the s and is of importance in multiagent resource allocation two fairness criterion are usually considered proportionality in the sense that each of the n agent receives at least n of the cake and the stronger property of envy freeness namely that each agent prefers it own piece of cake to the others piece for proportional division there are algorithm that require o n log n step and recent lower bound imply that one cannot do better in stark contrast known discrete algorithm for envy free division require an unbounded number of step even when there are only four agent in this paper we give an n lower bound for the number of step required by envy free cake cutting algorithm this result provides for the first time a true separation between envy free and proportional division thus giving a partial explanation for the notorious difficulty of the former problem 
coronary heart disease chd is a global epidemic that is the leading cause of death worldwide chd can be detected by measuring and scoring the regional and global motion of the left ventricle lv of the heart this project describes a novel automatic technique which can detect the regional wall motion abnormalitie of the lv from echocardiogram given a sequence of endocardial contour extracted from lv ultrasound image the sequence of contour moving through time can be interpreted a a three dimensional d surface from the d surface we compute several geometry based feature shape index value curvedness surface normal etc to obtain histogram based similarity function that are optimally combined using a mathematical programming approach to learn a kernel function designed to classify normal v abnormal heart wall motion in contrast with other state of the art method our formulation also generates sparse kernel kernel sparsity is directly related to the computational cost of the kernel evaluation which is an important factor when designing classifier that are part of a real time system experimental result on a set of echocardiogram collected in routine clinical practice at one hospital demonstrate the potential of the proposed approach 
a fundamental task for reasoning with preference is the following given input preference information from a user and outcome and should we infer that the user will prefer to for cp net and related comparative preference formalism inferring a preference of over using the standard definition of derived preference appears to be extremely hard and ha been proved to be pspace complete in general for cp net such inference is also rather conservative only making the assumption of transitivity this paper defines a le conservative approach to inference which can be applied for very general form of input it is shown to be efficient for expressive comparative preference language allowing comparison between arbitrary partial tuples including complete assignment and with the preference being ceteris paribus or not 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
machine translation service available on the web are becoming increasingly popular however a pivot translation service is required to realize translation between non english language by cascading different translation service via english a a result the meaning of word often drift due to the inconsistency asymmetry and intransitivity of word selection among translation service in this paper we propose context based coordination to maintain the consistency of word meaning during pivot translation service first we propose a method to automatically generate multilingual equivalent term based on bilingual dictionary and use generated term to propagate context among combined translation service second we show a multiagent architecture a one way of implementation wherein a coordinator agent gather and propagates context from to a translation agent we generated trilingual equivalent noun term and implemented a japanese to german and back translation cascading into four translation service the evaluation result showed that the generated term can cover over of all noun the translation quality wa improved by for all sentence and the quality rating for all sentence increased by an average of point on a five point scale these result indicate that we can realize consistent pivot translation service through context based coordination based on existing service 
most algorithm for computing diagnosis within a model based diagnosis framework are deterministic such algorithm guarantee soundness and completeness but are p hard to overcome this complexity problem which prohibits the computation of high cardinality diagnosis for large system we propose a novel approximation approach for multiple fault diagnosis based on a greedy stochastic algorithm called safari stochastic fault diagnosis algo rithm we prove that safari can be configured to compute diagnosis which are of guaranteed minimality under subsumption we analytically model safari search a a markov chain and show a probabilistic bound on the minimality of it minimal diagnosis approximation we have applied this algorithm to the xxx and iscas suite of benchmark combinatorial circuit demonstrating order of magnitude speedup over two state of the art deterministic algorithm cda and ha for multiple fault diagnosis 
in this paper we present a novel approach to the evaluation of propositional answer set program in particular for program with bounded treewidth our algorithm is capable of i computing the number of answer set in linear time and ii enumerating all answer set with linear delay our algorithm relies on dynamic programming therefore our approach significantly differs from standard asp system which implement technique stemming from sat or csp and thus usually do not exploit fixed parameter property of the program we provide first experimental result which underline that for program with low treewidth even a prototypical implementation is competitive compared to state of the art system 
optimizing a combination of local cost function on discrete variable is a central problem in many formalism such a in probabilistic network maximum satisfiability weighted csp or factor graph recent result have shown that maintaining a form of local consistency in a branch and bound search provides bound that are strong enough to solve many practical instance in this paper we introduce virtual arc consistency vac which iteratively identifies and applies sequence of cost propagation over rational cost that are guaranteed to transform a wcsp in another wcsp with an improved constant cost although not a strong a optimal soft arc consistency vac is faster and powerful enough to solve submodular problem maintaining vac inside branch and bound lead to important improvement in efficiency on large difficult problem and allowed u to close two famous frequency assignment problem instance 
diagnosability is an essential property that determines how accurate any diagnostic reasoning can be on a system given any sequence of observation an unobservable fault event in a discrete event system is diagnosable iff it occurrence can always be deduced once sufficiently many subsequent observable event have occurred a classical approach to diagnosability checking construct a finite state machine known a a twin plant for the system which ha a critical path iff some fault event is not diagnosable recent work attempt to avoid the often impractical construction of the global twin plant by exploiting system structure specifically local twin plant are constructed for component of the system and synchronized with each other until diagnosability is decided unfortunately synchronization of twin plant can remain a bottleneck for large system in the worst case in particular all local twin plant would be synchronized again producing the global twin plant we solve the diagnosability problem in a way that exploit the distributed nature of realistic system in our algorithm consistency among twin plant is achieved by message passing on a jointree scalability is significantly improved a the message computed are generally much smaller than the synchronized product of the twin plant involved moreover we use an iterative procedure to search for a subset of the jointree that is sufficient to decide diagnosability finally our algorithm is scalable in practice it provides an approximate and useful solution if the computational resource are not sufficient 
we present a method to decompose a declarative knowledge base given by a logic program under answer set semantics with access to external source it overcomes the ineffectiveness of current method due to a lack of structural information about these source viewed a black box by exploiting independency information in access to them to this end we develop a generic notion of domain independence that allows to restrict the evaluation domain and a a consequence to prune unnecessary dependency assumption between atom this lead to increased decomposability we demonstrate this by an evaluation method for hex program based on program rewriting which may yield large performance gain while developed for a particular formalism the notion and idea of this paper might be adapted to related formalism a well 
this paper describes our experience in using modern web architecture lightweight python framework and rapid prototyping to create an ai rostering and workforce management system to help prepare for the beijing olympic equestrian event which 
mechanical design tool would be considerably more useful if we could interact with them in the way that human designer communicate design idea to one another i e using crude sketch and informal speech those crude sketch frequently contain pen stroke of two different sort one type portraying device structure the other denoting gesture such a arrow used to indicate motion we report here on technique we developed that use information from both sketch and speech to distinguish gesture stroke from non gesture a critical first step in understanding a sketch of a device we collected and analyzed unconstrained device description which revealed six common type of gesture guided by this knowledge we developed a classifier that us both sketch and speech feature to distinguish gesture stroke from nongestures experiment with our technique indicate that the sketch and speech modality alone produce equivalent classification accuracy but combining them produce higher accuracy 
this paper discus a process of argumentation we propose an algorithm for dynamic treatment of argumentation in which all line of argumentation are executed in succession and the agent s knowledge base can change during argumentation we show that there exists a case in which an agent dynamically loses argumentation that would be considered won by a static analysis we also show that the algorithm terminates and describe acceptable argument that are obtained after the argumentation 
calendar management tool assist user with coordinating their daily life different task have to be scheduled according to the user preference in many case task are at different location and travel time have to be considered therefore these kind of calendar management problem can be regarded a spatio temporal optimisation problem and are often variant of traveling salesman problem tsp or vehicle routing problem while standard tsps require a solution to include all task prize collecting tsps are more suited for calendar management problem a they require a solution that optimises the total sum of prize we assigned to task at different location if we now add time window that limit when task can occur these prize collecting tsps with time window tw tsp are excellent abstraction of spatio temporal optimisation problem such a calendar management due to the inherent complexity of tw tsps the existing literature considers mainly approximation algorithm or special case we present a novel algorithm for tw tsps that enables u to find the optimal solution to tw tsp problem occurring in real world calendar management application efficiently our algorithm is a fixed parameter tractable algorithm that depends on the maximal number of task that can be revisited from some other task a parameter which is small in the application scenario we consider 
a seed based framework for textual information extraction allows for weakly supervised acquisition of open domain class attribute over conceptual hierarchy from a combination of web document and query log automatically extracted labeled class consisting of a label e g painkiller and an associated set of instance e g vicodin oxycontin are linked under existing conceptual hierarchy e g brain disorder and skin disease are linked under the concept braindisorder and skindisease respectively attribute extracted for the labeled class are propagated upwards in the hierarchy to determine the attribute of hierarchy concept e g disease from the attribute of their subconcepts e g braindisorder and skindisease 
question answer pair extracted from email thread can help construct summary of the thread a well a inform semantic based assistance with email previous work dedicated to email thread extract only question in interrogative form we extend the scope of question and answer detection and pairing to encompass also question in imperative and declarative form and to operate at sentence level fidelity building on prior work our method are based on learned model over a set of feature that include the content context and structure of email thread for two large email corpus we show that our method balance precision and recall in extracting question answer pair while maintaining a modest computation time 
we extend the model minimization technique for partially observable markov decision process pomdps to handle symmetry in the joint space of state action and observation the pomdp symmetry we define in this paper cannot be handled by the model minimization technique previously published in the literature we formulate the problem of finding the symmetry a a graph automorphism ga problem and although not yet known to be tractable we experimentally show that the sparseness of the graph representing the pomdp allows u to quickly find symmetry we show how the symmetry in pomdps can be exploited for speeding up point based algorithm we experimentally demonstrate the effectiveness of our approach 
answering conjunctive query cqs ha been recognized a a key task for the usage of description logic dl in a number of application and ha thus been studied by many author in this paper we present an algorithm for this problem in the dl alch which work in exponential time it improves over previous algorithm which require double exponential time and is worst case optimal a already satisfiability testing in alc is exptime complete furthermore it show that inverse role cause an exponential jump in complexity a recently shown the problem is exptimecomplete for alci the algorithm is based on a technique that compiles knowledge base into set of tree of depth it is in conp under data complexity i e if the taxonomy part and the query are fixed thus worst case optimal an extension from alch to dl with further construct is possible 
we study pursuit evasion problem where a number of pursuer have to clear a given graph we study when polynomial time algorithm exist to determine how many pursuer are needed to clear a given graph and how a given number of pursuer should move on the graph to clear it with either a minimum sum of their travel distance or minimum task completion time we generalize prior work to both unit width arbitrary length and unit length arbitrary width graph and derive both algorithm and complexity result for a variety of graph topology in this context we describe a polynomial time algorithm called clearthetree that is much shorter and algorithmically simpler than the state of the art algorithm for the minimum pursuer problem on tree our theoretical research lay a firm theoretical foundation for pursuit evasion on graph and informs practitioner about which problem are easy and which one are hard 
we introduce in this paper two new complete propositional language and study their property in term of their support for polytime operation and their ability to represent boolean function compactly the new language are based on a structured version of decomposability a property that underlies a number of tractable language the key characteristic of structured decomposability is it support for a polytime conjoin operation which is known to be intractable for unstructured decomposability we show that any cnf can be compiled into formula in the new language whose size is only exponential in the treewidth of the cnf our study also reveals that one of the language we identify is a powerful a obdds in term of answering key inference query yet is more succinct than obdds 
multiple instance learning mil is a branch of machine learning that attempt to learn information from bag of instance many real world application such a localized content based image retrieval and text categorization can be viewed a mil problem in this paper we propose a new graph based semi supervised learning approach for multiple instance learning by defining an instance level graph on the data we first propose a new approach to construct an optimization framework for multiple instance semi supervised learning and derive an efficient way to overcome the non convexity of mil we empirically show that our method outperforms state of the art mil algorithm on several real world data set 
we study the problem of multi robot perimeter patrol in adversarial environment under uncertainty of adversarial behavior the robot patrol around a closed area using a nondeterministic patrol algorithm the adversary s choice of penetration point depends on the knowledge it obtained on the patrolling algorithm and it weakness point previous work investigated full knowledge and zero knowledge adversary and the impact of their knowledge on the optimal algorithm for the robot however realistically the knowledge obtained by the adversary is neither zero nor full and therefore it will have uncertainty in it choice of penetration point this paper considers these case and offer several approach to bounding the level of uncertainty of the adversary and it influence on the optimal patrol algorithm we provide theoretical result that justify these approach and empirical result that show the performance of the derived algorithm used by simulated robot working against human playing the role of the adversary is several different setting 
automatic recognition of human activity is among the key capability of many intelligent system with vision perception most existing approach to this problem require sophisticated feature extraction before classification can be performed this paper present a novel approach for human action recognition using only simple low level visual feature motion captured from direct frame differencing a codebook of key pose is first created from the training data through unsupervised clustering video of action are then coded a sequence of super frame defined a the key pose augmented with discriminative attribute a weighted sequence distance is proposed for comparing two super frame sequence which is further wrapped a a kernel embedded in a svm classifier for the final classification compared with conventional method our approach provides a flexible non parametric sequential structure with a corresponding distance measure for human action representation and classification without requiring complex feature extraction the effectiveness of our approach is demonstrated with the widely used kth human activity dataset for which the proposed method outperforms the existing state of the art 
we consider the problem of tactical assault planning in real time strategy game where a team of friendly agent must launch an assault on an enemy this problem offer many challenge including a highly dynamic and uncertain environment multiple agent durative action numeric attribute and different optimization objective while the dynamic of this problem are quite complex it is often possible to provide or learn a coarse simulation based model of a tactical domain which make monte carlo planning an attractive approach in this paper we investigate the use of uct a recent monte carlo planning algorithm for this problem uct ha recently shown impressive success in the area of game particularly go but ha not yet been considered in the context of multiagent tactical planning we discus the challenge of adapting uct to our domain and an implementation which allows for the optimization of user specified objective function we present an evaluation of our approach on a range of tactical assault problem with different objective in the rts game wargus the result indicate that our planner is able to generate superior plan compared to several baseline and a human player 
decentralized agent group typically require complex mechanism to accomplish coordinated task in contrast biological system can achieve intelligent group behavior with each agent performing simple sensing and action we summarize our recent paper on a biologically inspired control framework for multi agent task that is based on a simple and iterative control law we theoretically analyze important aspect of this decentralized approach such a the convergence and scalability and further demonstrate how this approach applies to real world application with a diverse set of multi agent application these result provide a deeper understanding of the contrast between centralized and decentralized algorithm in multi agent task and autonomous robot control 
we study under what condition bound consistency bc and arc consistency ac two form of propagation used in constraint solver are equivalent to each other we show that they prune exactly the same value when the propagated constraint is connected row convex closed under median and it complement is row convex this characterization is exact for binary constraint since row convexity depends on the order of the value in the domain we give polynomial algorithm for computing order under which bc and ac are equivalent if any 
exploration for robotic mapping is typically handled using greedy entropy reduction here we show how to apply information lookahead planning to a challenging instance of this problem in which an autonomous underwater vehicle auv map hydrothermal vent given a simulation of vent behaviour we derive an observation function to turn the planning for mapping problem into a pomdp we test a variety of information state mdp algorithm against greedy systematic and reactive search strategy we show that directly rewarding the auv for visiting vent induces effective mapping strategy we evaluate the algorithm in simulation and show that our information lookahead method outperforms the others 
we explore equivalence relation between state in markov decision process and partially observable markov decision process we focus on two different equivalence notion bisimulation givan et al and a notion of trace equivalence under which state are considered equivalent if they generate the same conditional probability distribution over observation sequence where the conditioning is on action sequence we show that the relationship between these two equivalence notion change depending on the amount and nature of the partial observability we also present an alternate characterization of bisimulation based on trajectory equivalence 
given an imagebase with tagged image four type of task can be executed i e content based image retrieval image annotation text based image retrieval and query expansion for any of these task the similarity on the concerned type of object is essential in this paper we propose a framework to tackle these four task from a unified view the essence of the framework is to estimate similarity by exploiting the interaction between object of different modality experiment show that the proposed method can improve similarity estimation and based on the improved similarity estimation some simple method can achieve better performance than some state of the art technique 
partially observable markov decision process pomdps provide a powerful model for sequential decision making problem with partially observed state and are known to have approximately optimal dynamic programming solution much work in recent year ha focused on improving the efficiency of these dynamic programming algorithm by exploiting symmetry and factored or relational representation in this work we show that it is also possible to exploit the full expressive power of first order quantification to achieve state action and observation abstraction in a dynamic programming solution to relationally specified pomdps among the advantage of this approach are the ability to maintain compact value function representation abstract over the space of potentially optimal action and automatically derive compact conditional policy tree that minimally partition relational observation space according to distinction that have an impact on policy value this is the first lifted relational pomdp solution that can optimally accommodate action with a potentially infinite relational space of observation outcome 
this paper present a novel approach to the estimation of user s affective state in human computer interaction most of the present approach divide emotion strictly between positive or negative however recent discovery in the field of emotional intelligence show that emotion should be rather perceived a context sensitive engagement with the world this lead to a need to specify whether the emotion conveyed in a conversation are appropriate for a situation they are expressed in in the proposed method we use a system for affect analysis on textual input to recognize user emotion and a web mining technique to verify the contextual appropriateness of those emotion on this basis a conversational agent can choose to either sympathize with the user or help them manage their emotion finally the result of evaluation of the proposed method with two different conversational agent are discussed and perspective for further development of the method are proposed 
we investigate prediction of user desktop activity in the unix domain the learning technique we explore do not require explicit user teaching we show that simple efficient many class learning can perform well for action prediction significantly improving over previously published result and baseline this finding is promising for various human computer interaction scenario where a rich set of potentially predictive feature is available where there can be many different action to predict and where there can be considerable nonstationarity 
qualitative constraint calculus are representation formalism that allow for efficient reasoning about spatial and temporal information many of the calculus discussed in the field of qualitative spatial and temporal reasoning can be defined a combination of other simpler and more compact formalism on the other hand existing calculus can be combined to a new formalism in which one can represent and reason about different aspect of a domain at the same time for example gerevini and renz presented a loose combination of the region connection calculus rcc and the point algebra the resulting formalism integrates topological and qualitative size relation between spatially extended object in this paper we compare the approach by gerevini and renz to a method that generates a new qualitative calculus by exploiting the semantic interdependency between the component calculus we will compare these two method and analyze some formal relationship between a combined calculus and it component the paper is completed by an empirical case study in which the reasoning performance of the suggested method is compared on random test instance 
first order decision diagram fodd were recently introduced a a compact knowledge representation expressing function over relational structure fodds represent numerical function that when constrained to the boolean range use only existential quantification previous work developed a set of operation over fodds showed how they can be used to solve relational markov decision process rmdp using dynamic programming algorithm and demonstrated their success in solving stochastic planning problem from the international planning competition in the system fodd planner a crucial ingredient of this scheme is a set of operation to remove redundancy in decision diagram thus keeping them compact this paper make three contribution first we introduce generalized fodds gfodd and combination algorithm for them generalizing fodds to arbitrary quantification second we show how gfodds can be used in principle to solve rmdps with arbitrary quantification and develop a particularly promising case where an arbitrary number of existential quantifier is followed by an arbitrary number of universal quantifier third we develop a new approach to reduce fodds and gfodds using model checking this yield a reduction that is complete for fodds and provides a sound reduction procedure for gfodds 
with the large volume of alert produced by low level detector management of intrusion alert is becoming more challenging alert correlation address this issue by providing a condensed yet more useful view of the network from the intrusion standpoint in this paper we propose a new framework for real time alert correlation that incorporates novel technique for aggregating alert into structured pattern and incremental mining of frequent structured pattern in the proposed framework time sensitive statistical relationship between alert are maintained in an efficient data structure and are updated incrementally to reflect the latest trend of pattern the result of experiment with synthetic and real world datasets demonstrate the efficiency of the proposed technique our frequent structure mining algorithm scale linearly with the size of the dataset and the proposed framework can cope with the throughput of a large scale network the ability to answer time sensitive query about pattern is another advantage of this work compared to other method 
horn to horn belief revision asks for the revision of a horn knowledge base such that the revised knowledge base is also horn horn knowledge base are important whenever one is concerned with efficiency of computing inference of knowledge acquisition etc horn to horn belief revision could be of interest in particular a a component of any efficient system requiring large commonsense knowledge base that may need revision because for example new contradictory information is acquired recent result on belief revision for general logic show that the existence of a belief contraction operator satisfying the generalized agm postulate is equivalent to the existence of a complement here we provide a first step towards efficient horn to horn belief revision by characterizing the existence of a complement of a horn consequence of a horn knowledge base a complement exists if and only if the horn consequence is not the consequence of a modified knowledge base obtained from the original by an operation called body building this characterization lead to the efficient construction of a complement whenever it exists 
we consider the problem of planning in domain with continuous linear numeric change such change cannot always be adequately modelled by discretisation and is a key facet of many interesting problem we show how a forward chaining temporal planner can be extended to reason with action with continuous linear effect we extend a temporal planner to handle numeric value using linear programming we show how linear continuous change can be integrated into the same linear program and we discus how a temporal numeric heuristic can be used to provide the search guidance necessary to underpin continuous planning we present result to show that the approach can effectively handle duration dependent change and numeric variable subject to continuous linear change 
the aim of general game playing ggp is to create intelligent agent that automatically learn how to play many different game at an expert level without any human intervention the most successful ggp agent in the past have used traditional game tree search combined with an automatically learned heuristic function for evaluating game state in this paper we describe a ggp agent that instead us a monte carlo uct simulation technique for action selection an approach recently popularized in computer go our ggp agent ha proven it effectiveness by winning last year s aaai ggp competition furthermore we introduce and empirically evaluate a new scheme for automatically learning search control knowledge for guiding the simulation playouts showing that it offer significant benefit for a variety of game 
forming effective coalition is a major research challenge in ai and multi agent system coalition structure generation csg involves partitioning a set of agent into coalition so that social surplus is maximized a partition is called a coalition structure c in this paper we propose a novel formalization of csg i e we assume that the value of a characteristic function is given by an optimal solution of a distributed constraint optimization problem dcop among the agent of a coalition also we develop an algorithm with parameter k that can find a c whose social surplus is at least max k w k n of the optimal c where w is the tree width of a constraint graph these result illustrate that the locality of interaction among agent which is explicitly modeled in the dcop formalization is quite useful in developing an efficient csg algorithm with quality guarantee 
web scale data ha been used in a diverse range of language research most of this research ha used web count for only short fixed span of context we present a unified view of using web count for lexical disambiguation unlike previous approach our supervised and unsupervised system combine information from multiple and overlapping segment of context on the task of preposition selection and context sensitive spelling correction the supervised system reduces disambiguation error by over the current state of the art 
one of the core goal of the semantic web is to store data in distributed location and use ontology and reasoning to aggregate it social networking is a large movement on the web and social networking data using the friend of a friend foaf vocabulary make up a significant portion of all data on the semantic web many traditional web based social network share their member information in foaf format while this is by far the largest source of foaf online there is no information about whether the social network model from each network overlap to create a larger unified social network or whether they are simply isolated component if there are intersection it is evidence that semantic web representation and technology are being used to create interesting useful data model in this paper we present a study of the intersection of foaf data found in many online social network using the semantics of the foaf ontology and applying semantic web reasoning technique we show that a significant percentage of profile can be merged from multiple network we present result on how this affect network structure and what it say about the success of the semantic web 
quantitative modeling play a key role in the natural science and system that address the task of inductive process modeling can assist researcher in explaining their data in the past such system have been limited to data set that recorded change over time but many interesting problem involve both spatial and temporal dynamic to meet this challenge we introduce scism an integrated intelligent system which solves the task of inducing process model that account for spatial and temporal variation we also integrate scism with a constraint learning method to reduce computation during induction application to ecological modeling demonstrate that each system fare well on the task but that the enhanced system doe so much faster than the baseline version 
consider the setting where a panel of judge is repeatedly asked to partially rank set of object according to given criterion and assume that the judge expertise depends on the object domain learning to aggregate their ranking with the goal of producing a better joint ranking is a fundamental problem in many area of information retrieval and natural language processing amongst others however supervised ranking data is generally difficult to obtain especially if coming from multiple domain therefore we propose a framework for learning to aggregate vote of constituent ranker with domain specific expertise without supervision we apply the learning framework to the setting of aggregating full ranking and aggregating top k list demonstrating significant improvement over a domain agnostic baseline in both case 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
weighted voting game provide a popular model of decision making in multiagent system such game are described by a set of player a list of player weight and a quota a coalition of the player is said to be winning if the total weight of it member meet or exceeds the quota the power of a player in such game is traditionally identified with her shapley shubik index or her banzhaf index two classical power measure that reflect the player s marginal contribution under different coalition formation scenario in this paper we investigate by how much the central authority can change a player s power a measured by these index by modifying the quota we provide tight upper and lower bound on the change in the individual player s power that can result from a change in quota we also study how the choice of quota can affect the relative power of the player from the algorithmic perspective we provide an efficient algorithm for determining whether there is a value of the quota that make a given player a dummy i e reduces his power a measured by both index to on the other hand we show that checking which of the two value of the quota make this player more powerful is computationally hard namely complete for the complexity class pp which is believed to be significantly more powerful than np 
in several application of logic programming and transaction logic such a planning trust management and independent semantic web service an action might produce incomplete fact and leave existential value in an incrementally generated data structure the same action or other producer or consumer action might read modify or communicate through these fact making this technique a powerful communication technique in this poster we present a definite semantics for these existentially quantified value that occur only in fact query and update of fact although this simple semantics applies only to fact and not to clause it is relevant to many application including artificial intelligence planning workflow modeling and verification and update of fact in the semantic web 
in model based control a planner us a system description to create a plan that achieves production goal fikes nilsson the same description can be used by model based diagnosis to infer the condition of component in a system from partially informative sensor prior work ha demonstrated that diagnosis can be used to adapt the control of a system to change in it component however diagnosis must either make inference from passive observation of production or production must be halted to take diagnostic action we observe that the declarative nature of model based control allows the planner to achieve production goal in multiple way this exibility can be exploited with a novel paradigm we call pervasive diagnosis which produce diagnostic production plan that simultaneously achieve production goal while uncovering additional information about component health we present an efficient heuristic search for these diagnostic production plan and show through experiment on a model of an industrial digital printing press that the theoretical increase in information can be realized on practical real time system we obtain higher long run productivity than a decoupled combination of planning and diagnosis 
this paper describes our experience in using modern web architecture lightweight python framework and rapid prototyping to create an ai rostering and workforce management system to help prepare for the beijing olympic equestrian event which 
the goal of my ongoing work is to provide an architecture for developing and manipulating modular ontology in such a way that each ontology module can plug into or unplug from an ontology this architecture build on top of a fundamental formalism for modular ontology through this formalism we are able to define mechanism for integrating different module and develop algorithm for reasoning over the integrated module the resolution of inconsistency arisen by conflicting axiom in different module a well a the investigation of the impact of change in a module on the other ontology module are two important issue that need to be taken into consideration during the development of the formalism here we briefty review the overall structure of the research work that i intended to conduct 
automated generator for synthetic model and data can playa crucial role in designing new algorithm model framework given the sparsity of benchmark model for empirical analysis and the cost of generating model by hand we describe an automated generator for benchmark model that is based on using a compositional modeling framework and employ random graph model for the system topology we choose the system topology that best match the topology of the real world system using a domain analysis algorithm to show the range of model for which this approach is applicable we demonstrate our model generation process using two example of model generation optimized for a specific domain model based diagnosis for discrete boolean circuit and e coli trn network for simulating gene expression 
potential based shaping wa designed a a way of introducing background knowledge into model free reinforcement learning algorithm by identifying state that are likely to have high value this approach can decrease experience complexity the number of trial needed to find near optimal behavior an orthogonal way of decreasing experience complexity is to use a model based learning approach building and exploiting an explicit transition model in this paper we show how potential based shaping can be redefined to work in the model based setting to produce an algorithm that share the benefit of both idea 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable 
assembly line balancing problem albp are of capital importance for the industry since the first assembly line for the ford t by henry ford their objective is to optimize the design of production line while satisfying the various constraint precedence constraint among the task are always present in albp the objective is then to place the task among various workstation such that the production rate is maximized this problem can be modeled a a bin packing problem with precedence constraint bppc where the bin are the workstation and the item are the task paul shaw introduced a global constraint for bin packing without precedence unfortunately this constraint doe not capture the precedence constraint of bppc in this paper we first introduce redundant constraint for bppc combining the precedence and the bin packing allowing to solve instance which are otherwise intractable in constraint programming we also design a global constraint for bppc introducing even more pruning in the search tree we finally used our cp model for bppc to solve albp we propose two search heuristic and show the efficiency of our approach on standard albp benchmark compared to standard non cp approach our method is more flexible a it can handle new constraint that might appear in real application 
buoyed by recent success in the area of distributed constraint optimization problem dcops this paper address challenge faced when applying dcops to real world domain three fundamental challenge must be addressed for a class of real world domain requiring novel dcop algorithm first agent may not know the payoff matrix and must explore the environment to determine reward associated with variable setting second agent may need to maximize total accumulated reward rather than instantaneous final reward third limited time horizon disallow exhaustive exploration of the environment we propose and implement a set of novel algorithm that combine decision theoretic exploration approach with dcop mandated coordination in addition to simulation result we implement these algorithm on robot deploying dcops on a distributed mobile sensor network 
blog classification e g identifying blogger gender or age is one of the most interesting current problem in blog analysis although this problem is usually solved by applying supervised learning technique the large labeled dataset required for training is not always available in contrast unlabeled blog can easily be collected from the web therefore a semi supervised learning method for blog classification effectively using unlabeled data is proposed in this method entry from the same blog are assumed to have the same characteristic with this assumption the proposed method capture the characteristic of each blog such a writing style and topic and us these characteristic to improve the classification accuracy 
spatial scaffolding is a naturally occurring human teaching behavior in which teacher use their body to spatially structure the learning environment to direct the attention of the learner robotic system can take advantage of simple highly reliable spatial scaffolding cue to learn from human teacher we present an integrated robotic architecture that combine social attention and machine learning component to learn task effectively from natural spatial scaffolding interaction with human teacher we evaluate the performance of this architecture in comparison to human learning data drawn from a novel study of the use of embodied cue in human task learning and teaching behavior this evaluation provides quantitative evidence for the utility of spatial scaffolding to learning system in addition this evaluation supported the construction of a novel interactive demonstration of a humanoid robot taking advantage of spatial scaffolding cue to learn from natural human teaching behavior working together these mechanism take advantage of the structure of nonverbal human teaching behavior allowing the robot to learn from natural spatial scaffolding interaction we evaluated the performance of this integrated learning architecture in comparison to human learning data on benchmark task drawn from the study providing quantitative evidence for the utility of the identified cue additionally we constructed a novel interactive demonstration of a humanoid robot learning task from natural human teaching 
coalition structure generation involves partitioning a set of agent into exhaustive and disjoint coalition so a to maximize the social welfare what make this such a challenging problem is that the number of possible solution grows exponentially a the number of agent increase to date two main approach have been developed to solve this problem each with it own strength and weakness the state of the art in the first approach is the improved dynamic programming idp algorithm due to rahwan and jennings that is guaranteed to find an optimal solution in o n but which cannot generate a solution until it ha completed it entire execution the state of the art in the second approach is an anytime algorithm called ip due to rahwan et ai that provides worst case guarantee on the quality of the best solution found so far but which is o nn in this paper we develop a novel algorithm that combine both idp and ip resulting in a hybrid performance that exploit the strength of both algorithm and at the same avoids their main weakness our approach is also significantly faster e g given agent it take only of the time required by ip and of the time required by idp 
in this paper we consider semi supervised classification on evolutionary data where the distribution of the data and the underlying concept that we aim to learn change over time due to short term noise and long term drifting making a single aggregated classifier inapplicable for long term classification the drift is smooth if we take a localized view over the time dimension which enables u to impose temporal smoothness assumption for the learning algorithm we first discus how to carry out such assumption using temporal regularizers defined in a structural way with respect to the hilbert space and then derive the online algorithm that efficiently find the closed form solution to the classification function experimental result on real world evolutionary mailing list data demonstrate that our algorithm outperforms classical semi supervised learning algorithm in both algorithmic stability and classification accuracy 
this paper study the solving of finite domain action planning problem with discrete action cost and soft constraint for sequential optimal planning a symbolic perimeter database heuristic is addressed in a bucket implementation of a for computing net benefit we propose symbolic branch and bound search together with some search refinement the net benefit we optimize is the total benefit of satisfying the goal minus the total action cost to achieve them this result in an objective function to be minimized that is a linear expression over the violation of the preference added to the action cost total 
we determine the implicit assumption and the structure of the single and double cross calculus within euclidean geometry and use these result to guide the construction of analogous calculus in mereogeometry the system thus obtained have strong semantic and deductive similarity with the euclidean based cross calculus although they rely on a different geometry this fact suggests that putting too much emphasis on usual classification of qualitative space may hide important commonality among space living in different class 
fourier based learning algorithm rely on being able to efficiently find the large coefficient of a function s spectral representation in this paper we introduce and analyze technique for finding large coefficient we show how a previously introduced search technique can be generalized from the boolean case to the real valued case and we apply it in branch and bound and beam search algorithm that have significant advantage over the best first algorithm in which the technique wa originally introduced 
the length lex representation ha been recently proposed for representing set in constraint satisfaction problem the length lex representation directly capture cardinality information provides a total ordering for set and allows bound consistency on unary constraint to be enforced in time c where c is the cardinality of the set however no algorithm were given to enforce bound consistency on binary constraint this paper address this open issue it present algorithm to enforce bound consistency on disjointness and cardinality constraint in time o c moreover it present a generic bound consistency algorithm for any binary constraint s which requires c call to a feasibility subroutine for s 
