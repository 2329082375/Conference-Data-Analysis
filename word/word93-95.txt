in conventional supervised learning one search for vertical pattern coupling input directly to output one can instead search for horizontal pattern which go across the input space coupling output value on one part of the input space with output value on another one way to do this is to pre process the problem in a manner similar to the embedding process of non linear time series analysis the training set produced by this pre processing is constructed solely from the output component of the original training set the input component of the original training set are used in concert with cross validation to determine the detail of the processing of those output component this paper present three set of experiment of the efficacy of such pre processing involving numerical boolean and visual task the first set involves small bit problem in all but one of the experiment in that set the pre processing improved the generalization performance often inducing perfect generalization the average ratio of the generalization error rate with the pre processing to the error rate without it is the second set of experiment involve a bit input space for a number of different target function a training set is actively constructed by sampling the target function at pre specified point in that space a decisiondirected version of the pre processing is then used to extrapolate from that training set to the remaining point in the input space the average error rate across here is the third set of experiment is a variation of the robot arm problem recently investigated by mackay here the rms error rate for extrapolation wa only time the noise level 
we extend work on difference identification and reduction a a technique for automated reason ing we generalise unification so that term are made equal not only by finding substitution for variable but also by hiding term structure this annotation of structural difference serf to direct rippling a kind of rewriting designed to remove difference in a controlled way on the technical side we give a rule based algo rithm for difference unification and analyze it correctness completeness and complexity on the practical side we present a novel search strategy for efficiently applying these rule fi nally we show how this algorithm can be used in new way to direct rippling and how it can play an important role in theorem proving and other kind of automated reasoning 
a consultant system s main task is to provided helpful advice to the user consultant system should not only find solution to user problem but should also inform the user of potential problem with these solution expressing such potential caveat is a difficult process due to the many potential plan failure for each particular plan in a particular planning situation a commonsense planner called kip knowledge intensive planner is described kip is the planner for the unix consultant system kip detect potential plan failure using a new knowledge structure termed a concern concern allow kip to detects plan failure due to unsatisfied condition or goal conflict kip s concern algorithm also is able to provide information to the expression mechanism regarding potential plan failure concern information is passed to the expression mechanism when kip s selected plan might not work in this case kip pass information regarding both the suggested plan and the potential caveat in that plan to the expression mechanism this is an efficient approach since kip must make such decision in the context of it planning process a concern s declarative structure make it easier to express than procedural description of plan failure used by earlier system 
machine discovery is concerned with the taskof finding law from experimental and or observationaldata existing machine discoverysystems have mostly generated law describingstatic situation the paper present lagrange a system that construct a set ofdifferential and or algebraic equation thatdescribe an observed behavior of a dynamicsystem a such lagrange extends thescope of machine discovery to dynamic system we show that lagrange is ableto generate appropriate 
we discus the persistence of the indirect ef fects of an action the question when such ef fects are subject to the commonsense law of in ertia and how to describe their evolution in the case when inertia doe not apply our model of nonpersistent effect involves the assumption that the value of the fluent in question is deter mined by the value of other fluents although the dependency may be partially or completely unknown this view lead u to a new highlevel action language ard for action ram ifications and dependency that is capable of describing both persistent and nonpersistent ef fects unlike the action language introduced in the past ard is non markovia n in the sense that the evolution of the fluents described in this language may depend on their history and not only on their current value 
many learning situation involve multiple set of training example drawn from different but related underlying model family dis covery is the task of discovering a parameterized family of model from this kind of training set the task naturally arises in density estimation classification regression manifold learning reinforce ment learning clustering hmm learning and other setting we describe three family discovery algorithm which are based on tech niques for manifold learning we compare these algorithm on a classification task against two alternative approach and find sig nificant performance improvement 
model based diagnosis algorithm face a combi natorial explosion to combat this explosion this paper present a fundamentally new architecture implode which construct an abstract representa tion of the environment the conflict and the diag nosis space using a sensitivity analysis of assump tions experimental result show that the most dra matic improvement is obtained for circuit which are the most difficult to diagnose using previous algorithm moreover typical source of combina torial explosion such a reconvergent fanout are a source of combinatorial implosion for implode 
starling with a first order modal conditional logic which allows unlimited nesting of default and emheddings into any context analyzable in possibleworlds theory i introduce two simple notion of default reasoning the syntactic notion of pnorniied allowed consequence and the semantic notion of prioritized allowed entailment i prove that the one is sound and complete relative to the other 
mechanical linkage are used to transmit and transform motion in this paper we investigate what it mean for a program to understand a linkage our system extract itsunderstanding by analyzing the result of a numerical simulation of the mechanism finding interestingqualitativefeatures looking forsymbolic relationship between these feature and conjecturing a causal relationship between them our system is capable of understanding a varietyof mechanism producing explanation very much like those in standard text 
geographical database system deal with certainbasic topological relation such a quot a overlapsb quot and quot b contains c quot between simplyconnected region in the plane it is of greatinterest to make sound inference from elementarystatements of this form this problem hasbeen identified extensively in the recent literature but very limited progress ha been madetowards addressing the considerable technicaldifficulties involved in this paper we studythe computational problem involved in 
we present a semantic and pragmatic account of the anaphoric property of past and perfect that improves on previous work by integrating discourse structure aspectual type surface structure and commonsense knowledge a novel aspect of our account is that we distinguish between two kind of temporal interval in the interpretation of temporal operator discourse reference interval and event interval this distinction make it possible to develop an analogy between centering and temporal centering which operates on discourse reference interval our is a defeasible inference rule on the logical form along with lexical and causal reasoning it play a role in incrementally resolving underspecified aspect of the event structure representation of an utterance against the current context 
a knowledge representation problem can be sometimesviewed a an element of a family of problem with parameter corresponding to possibleassumptions about the domain under consideration when additional assumption are made the class of domain that are being described becomessmaller so that the class of conclusion thatare true in all the domain becomes larger asa result a satisfactory solution to a parametricknowledge representation problem on the basis ofsome nonmonotonic 
we provide a method based on the theory of markov decision problem for efficient planning in stochastic domain goal are encoded a reward function expressing the desirability of each world state the planner must find a policy mapping from state to action that maximizes future reward standard goal of achievement a well a goal of maintenance and prioritized combination of goal can be specified in this way an optimal policy can be found using existing method but these method are at best polynomial in the number of state in the domain where the number of state is exponential in the number of proposition or state variable by using information about the starting state the reward function and the transition probability of the domain we can restrict the planner s attention to a set of world state that are likely to be encountered in satisfying the goal furthermore the planner can generate more or le complete plan depending on the time it ha available we describe experiment involving a mobile robotics application and consider the problem of schedulilng different phase of the planning algorithm given time constraint 
ftp cmu c p in many real world task the ability to focus attention on the important feature of the input is crucial for good performance in this paper a mechanism for achieving task specific focus of attention is presented a saliency map which is based upon a computed expectation of the content of the input at the next time step indicates which region of the input retina are important for performing the task the saliency map can be used to accentuate the feature which are important and de emphasize those which are not the performance of this method is demonstrated on a real world robotics task autonomous road following the applicability of this method is also demonstrated in a non visual domain architectural and algorithmic detail are provided a well a empirical result 
we discus the use of database method for data mining recently impressive result have been achieved for some data mining problem using highly specialized and clever data structure we study how well one can manage by using general purpose database management system we illustrate our idea by investigating the use of a dbms for a well researched area the discovery of association rule we present a simple algorithm consisting of only union and intersection operation and show that it achieves quite good performance on an efficient dbms our method can incorporate inheritance hierarchy to the association rule algorithm easily we also present a technique that effectively reduces the number of database operation when searching large search space that contain only few interesting item our work show that database technique are promising for data mining general architecture can achieve reasonable result 
this paper present an analysis of static and dynamic organizational structure for naturally distributed homogeneous cooperative problem solving environment exemplified by distributed sensor network we first show how the performance of any static organization can be statistically described and then show under what condition dynamic organization do better and worse than static one finally we show how the variance in the agent performance lead to uncertainty about whether a dynamic organization will perform better than a static one given only agent a priori expectation in these case we show when meta level communication about the actual state of problem solving will be useful to agent in constructing a dynamic organizational structure that outperforms a static one viewed in it entirety this paper also present a methodology for answering question about the design of distributed problem solving system by analysis and simulation of the characteristic of a complex environment rather than by relying on single instance example 
a new boosting algorithm of freund and schapire is used to improve the performance of decision tree which are constructed usin the information ratio criterion of quinlan s c algorithm this boosting algorithm iteratively construct a series of decision tress each decision tree being trained and pruned on example that have been filtered by previously trained tree example that have been incorrectly classified by the previous tree in the ensemble are resampled with higher probability to give a new probability distribution for the next ace in the ensemble to tnin on result from optical cha xc er reco tion ocr and knowledge discovery and data mining problem show that in comparison to single tree or to tree trained independenrly or to tree trained on subset of the feature space the boosring ensemble is much better 
this paper study property of iterated revision first rationality result show that in the agm original framework the only revision operation that satisfies two resonable property is the trivial revision then an altenative to the agm framework for studying belief revision and probability postulate is proposed iterated revision are the object of this formalism and the rationality postulate deal with property of iterated revision a set of rationality postulate is presented closely related to the agm postulate a representation result show that those postulate simply serious limitation to the way revision cell dope with the principle of minimal change those postulate are not suitable for belief update but then consideration raise doubt about the adequacy of previous treatment of belief implate 
we describe a series of three experiment in which supervised learning technique were used to acquire three different type of grammar for english news story the acquired grammar type were context free context dependent and probabilistic context free training data were derived from university of pennsylvania treebank par of wall street journal article in each case the system started with essentially no grammatical knowledge and learned a set of grammar rule exclusively from the training data performance for each grammar type wa then evaluated on an independent set of test sentence using parseval a standard measure of parsing accuracy these experimental result yield a direct quantitative comparison between each of the three method 
we introduce an active data mining paradigm thatcombines the recent work in data mining with therich literature on active database system in thisparadigm data is continuously mined at a desiredfrequency a rule are discovered they are addedto a rulebase and if they already exist the historyof the statistical parameter associated withthe rule is updated when the history start exhibitingcertain trend specified a shape queriesin the user specified trigger the trigger are 
we present a silicon model of an axon which show promise a abuilding block for pulse based neural computation involving correlationsof pulse across both space and time the circuit sharesa number of feature with it biological counterpart including anexcitation threshold a brief refractory period after pulse completion pulse amplitude restoration and pulse width restoration weprovide a simple explanation of circuit operation and present datafrom a chip fabricated in a standard 
this paper along with the following paper by john mccarthy introduces some of the topic to be discussed at the ijcai event a philosophical encounter an interactive presentation of some of the key philosophical problem in ai and ai problem in philosophy philosophy need ai in order to make progress with many difficult question about the nature of mind and ai need philosophy in order to help clar ify goal method and concept and to help with several specific technical problem whilst philosophical attack on ai continue to be wel comed by a significant subset of the general public ai defender need to learn how to avoid philosophically naive rebuttal 
accuracy play a central role in developing model of continuous physical system both in the context of developing a new model to fit observation or approximating an existing model to make analysis faster the need for simple yet sufficiently accurate model pervades engineering analysis design and diagnosis task this paper focus on two issue related to this topic first it examines the process by which idealized model are derived second it examines the problem of determining when an idealized model will be sufficiently accurate for a given task in a way that is simple and doesn t overwhelm the benefit of having a simple model it describes ideal a system which generates idealized version of a given model and specifies each idealized model s credibility domain this allows valid future use of the model without resorting to more expensive measure such a search or empirical confirmation the technique is illustrated on an implemented example 
we deal with the calibration problem of an active head eye system which consists of a pair of camera mounted on a head with degree of freedom the aim of the calibration is to establish relative position of different d system between camera and neck eye and neck etc so that we can keep track of the camera position in a fixed calibration reference system a a function of the visual parameter of the head eye system we formulate the problem and propose both closed form and nonlinear optimization approach to solve it experiment were carried out and comparison of result with other algorithm were made on both simulated and real data 
visual representation are chosen to meet the requirement of specified task such a object recognition or siereoscopic matching when vision is regarded a a haptic sense a new operational description is called for in support of dezirous manipulation in the contett of two fingered grasp an appropriate description can be constructed in term of local reflectional and rotational symmetry relaied to sgmmetry based representation that have been used in pattern and object recognition ifs siructure is deiemined not heuristically but precisely by the natun of ihe grasping iask it value is demonstrated by it incorporation into the control of a robot that can manipulate object under visual guidance 
hobbs j r m e stickel d e appelt and p martin interpretation a abduction artificial intelligence abduction is inference to the best explanation in the tacitus project at sri we have developed an approach to abductive inference called weighted abduction that ha resulted in a significant simplification of how the problem of interpreting text is conceptualized the interpretation of a text is the minimal explanation of why the text would be true more precisely to interpret a text one must prove the logical form of the text from what is already mutually known allowing for coercion merging redundancy where possible and making assumption where necessary it is shown how such local pragmatic problem a reference resolution the interpretation of compound nominal the resolution of syntactic ambiguity and metonymy and schema recognition can be solved in this manner moreover this approach of interpretation a abduction can be combined with the older view of parsing a deduction to produce an elegant and thorough integration of syntax semantics and pragmatic one that span the range of linguistic phenomenon from phonology to discourse structure finally we discus mean for making the abduction process efficient possibility for extending the approach to other pragmatic phenomenon and the semantics of the weight and cost in the abduction scheme 
intermediate decision tree are the subtrees of the full unpruned decision tree generated in a breadth first order an extensive empirical in vestigation evaluates the classification error of intermediate decision tree and compare their performance to full and pruned tree em pirical result were generated using c with database from the uci machine learning database repository result show that when attempting to minimize the error of the pruned tree produced by c the best intermediate tree performs significantly better in of the database these and other result question the effectiveness of decision tree pruning strategy and suggest further consideration of the full tree and it intermediate also the result reveal specific property satisfied by database in which the intermediate full tree performs best such relationship improve guideline for selecting appropriate inductive strategy based on domain property 
endowing a computer with an ability to reason with diagram could be of great benefit in term of both human computer interaction and computational efficiency through explicit representation to date research in diagrammatic reasoning ha dealt with intra diagrammatic reasoning reasoning with a single diagram almost to the exclusion of inter diagrammatic reasoning reasoning with related group of diagram we postulate a number of general inter diagrammatic operator and show how such operator can be useful in various diagrammatic domain we develop a heuristic in the domain of game notation derive fingering information in the domain of musical notation and infer new information from related cartograms 
influence diagram id are a graphic formal ism able to provide a compact representation of decision problem id are based on the ax ioms of probability and decision theory and they define a normative framework to model decision making unfortunately id require a large amount of information that is not always available to the decision maker this paper in troduces a new class of id called ignorant in fluence diagram iids able to reason on the basis of incomplete information and to improve the accuracy of their decision a a monotonically increasing function of the available infor mation iids represent a net gain with respect to the traditional id since they are able to ex plicitly represent lack of information without loosing any capability of traditional id when the required information is available further more i id provide a new method to ass the reliability of the decision by replacing the tra ditional sensitivity analysis with a single ana lytical measure 
morphotactics and allomorphy are usually modeled in different component leading to interface problem to describe both uniformly we define finite automaton fa for allomorphy in the same feature description language used for morphotactics nonphonologically conditioned allomorphy is problematic in fa model but submits readily to treatment in a uniform formalism 
this paper compare the account based on a simple feature extension to lambek categorial grammar lcg we show that the lcg treatment account for construction that have been recognized a problematic for unification based treatment 
this paper present the expected solution quality esq method for statistically characterizing scheduling problem and the performance of scheduler the esq method is demonstrated by applying it to a practical telescope scheduling problem the method address the important and difficult issue of how to meaningfully evaluate the performance of a scheduler on a constrained optimization problem for which an optimal solution is not known at the heart of esq is a monte carlo algor ithm that estimate a problem s probability density function with respect to solution quality this quality density function provides a useful characterization of a scheduling problem and it also provides a background against which scheduler performance can be meaningfully evaluated esq provides a unitless measure that combine both schedule quality and the amount of time to generate a schedule 
many real world domain bless u with a wealth of attribute to use for learning this blessing is often a curse most inductive method generalize worse given too many attribute than if given a good subset of those attribute we examine this problem for two learning task taken from a calendar scheduling domain we show that id c generalizes poorly on these task if allowed to use all available attribute we examine five greedy hillclimbing procedure that search for attribute set that generalize well with id c experiment suggest hillclimbing in attribute space can yield substantial improvement in generalization performance we present a caching scheme that make attribute hillclimbing more practical computationally we also compare the result of hillclimbing in attribute space with focus and relief on the two task 
although communication is generally considered to dominate overprocessing cost in distributed system the problem of communicationcost in multiagent planning ha not been sufficiently addressed onemethod for reducing both communication cost and planning time isthe use of social law social law however can be too restrictive limiting soundness flexible social law can enable multiagent systemsto reap the benefit of reduced communication cost and planning time except in the worst 
formalizing the ontological commitment of a logical language mean offering a way to specify the intended meaning of it vocabulary by constraining the set of it model giving explicit information about the intended nature of the modelling primitive and their a priori relationship we present here a formal definition of ontological commitment which aim to capture the very basic ontological assumption about the intended domain related to issue such a identity and internal structure to tackle such issue a modal framework endowed with mereo topological primitive ha been adopted the paper is mostly based on a revisitation of philosophical and linguistic literature in the perspective of knowledge representation 
a new approach to tracking weakly modeled object in a semantically rich domain is presented we define a closed world a a space time region of an image sequence in which the complete taxonomy of object is known and in which each pixel should be explained a belonging to one of those object given contextual object information context specific feature can be dynamically selected a the basis for tracking a context specific feature is one that ha been chosen based upon the context to maximize the chance of successful tracking between frame our work is motivated by the goal of video annotation the semi automatic generation of symbolic description of action taking place in a contextually rich dynamic scene we describe how contextual knowledge in the football domain can be applied to closed world football player tracking and present the detail of our implementation we include tracking result based on hundred of image that demonstrate the wide range of tracking situation the algorithm successfully handle a well a a few example of where the algorithm fails 
the facial action coding system facs devised by ekman andfriesen provides an objective mean for measuring the facialmuscle contraction involved in a facial expression in this paper we approach automated facial expression analysis by detecting andclassifying facial action we generated a database of over image sequence of subject performing over distinct facialactions or action combination we compare three different approachesto classifying the facial 
interface agent are semi intelligent systemswhich assist user with daily computer basedtasks recently various researcher have proposeda learning approach towards building suchagents and some working prototype have beendemonstrated such agent learn by watchingover the shoulder of the user and detecting patternsand regularity in the user s behavior despitethe success booked a major problem withthe learning approach is that the agent ha tolearn from scratch and thus 
in a previous paper we defined the deep structure of a constraint satisfaction problem to be that set system produced by collecting the nogood ground instance of each constraint and keeping only those that are not supersets of any other we then showed how to use such deep structure to predict where in a space of problem instance an abrupt transition in computational cost is to be expected this paper explains how to augment this model with enough extra detail to make more accurate estimate of the location of these phase transition we also show that the phase transition phenomenon exists for a much wider class of search algorithm than had hitherto been thought and explain theoretically why this is the case 
dtg are designed to share some of the advantage of tag while overcoming some of it limitation dtg involve two composition operation called subsertion and sister adjunction the most distinctive feature of dtg is that unlike tag there is complete uniformity in the way that the two dtg operation relate lexical item subsertion always corresponds to complementation and sister adjunction to modification furthermore dtg unlike tag can provide a uniform analysis for element in kashmiri appears in sentence second position and not sentence initial position a in english 
visual motion boundary provide a powerful cue for the perceptual organization of scene motion boundary are present when surface in motion occlude one another conventional approach to motion analysis have relied on assumption of data conservation and smoothness which ha made analysis of motion boundary difficult we show that a common source of motion boundary kinetic occlusion can be detected using spatiotemporal junction analysis junction analysis is accomplished by utilizing distributed representation of motion used in model of human visual motion sensing by detecting change in the direction of motion in these representation spatiotemporal junction are detected in a manner which differentiates accretion from deletion we demonstrate successful occlusion detection on spatiotemporal imagery containing occluding surface in motion 
this paper present a method for recognizing human handgestures using a model based approach a finite state machine is used tomodel four qualitatively distinct phase of a generic gesture fingertipsare tracked in multiple frame to compute motion trajectory which arethen used for finding the start and stop position of the gesture gesturesare represented a a list of vector and are then matched to stored gesturevector model using table lookup based on vector displacement 
most artificial intelligence program lack generalitybecause they reason with a single domain theory thatis tailored for a specific task and embodies a host ofimplicit assumption context have been proposedas an effective solution to this problem by providing amechanism for explicitly stating the assumption underlyinga domain theory in addition context canbe used to focus reasoning allow the representation ofmutually incoherent domain theory lift axiom fromone context into 
we explore algorithm for learning classification procedure that attempt to minimize the cost of misclassifying example first we consider inductive learning of classificatio n rule the reduced cost ordering algorithm a new method for creating a decision list i e an ordered set of rule is described and compared to a variety of inductive learning approach next we describe approach that attempt to minimize cost while avoiding overfitting and introduce the clause prefix method for pruning decision list finally we consider reducing misclassification cost when a prior domain theory is available 
we develop a model based approach to reasoning in which the knowledge base is representedas a set of model satisfying assignment rather than a logical formula and the set of queriesis restricted we show that for every propositional knowledge base kb there exists a set ofcharacteristic model with the property that a query is true in kb if and only if it is satisfied bythe model in this set we characterize a set of function for which the model based representationis compact and 
the location of object in image is difficult owing to the view variance of geometric feature but can be determined by developing view insensitive description of the intensity local to image point view insensitive description are achieved in this work by describing point in term of the response of steerable filter at multiple scale owing to the use of multiple scale the vector for each point is for all practical purpose unique and thus can be easily matched to other instance of the point in other image we show that this method can be extended to handle the case where the area near a point of interest is partially occluded the method us a description of the occluder in the form of a template that can be obtained easily via active vision system using a method such a disparity filtering 
for a logical database to faithfully represent our beliefsabout the world one should not only insist onits logical consistency but also on it causal consistency intuitively a database is causally inconsistentif it support belief change that contradict with ourperceptions of causal influence for example comingto conclude that it must have rained only becausethe sprinkler wa observed to be on in this paper we suggest the notion of a causal structure to represent 
a new technique called shape from photomotion is introduced it us a series of d lambertian image generated by moving a light source around a scene to recover the depth map in each of the image the object in the scene remains at a fixed position and the only variable is the light source direction the movement of the light source cause a change in the intensity of any given point in the image the change in intensity is what enables recovery of the unknown parameter the depth map since it remains constant in each of the input image the author method differs from photometric stereo in the sense that the shape estimate is not only computed for each light source orientation but also gradually refined by photomotion 
steerable filter a developed by freeman and adelson are a class of rotation invariant linear operator that may be used to analyze local orientation pattern in imagery the most common example of such operator are directional derivative of gaussians and their d hilbert transforms the inherent symmetry of these filter produce an orientation response that is periodic with period spl pi even when the underlying image structure doe not have such symmetry this problem may be alleviated by reconsidering the full class of steerable filter we develop a family of evenand odd symmetric steerable filter that have a spatially asymmetric wedge like shape and are optimally localized in their orientation response unlike the original steerable filter these filter are not based on directional derivative and the hilbert transform relationship is imposed on their angular component we demonstrate the ability of these filter to properly represent oriented structure 
the paper present a typologically adaptable snake model for image segmentation and object representation the model is embedded in the framework of domain subdivision using simplicial decomposition this framework extends the geometric and topological adaptability of snake while retaining all of the feature of traditional snake such a user interaction and overcoming many of the limitation of traditional snake by superposing a simplicial grid over the image domain and using this grid to iteratively reparameterize the deforming snake model the model is able to flow into complex shape even shape with significant protrusion or branch and to dynamically change topology a necessitated by the data snake can be created and can split into multiple part or seamlessly merge into other snake the model can also be easily converted to and from the traditional parametric snake model representation we apply a d model to various synthetic and real image in order to segment object with complicated shape and topology 
we present new strategy for quot probably approximatelycorrect quot pac learning that usefewer training example than previous approach the idea is to observe training examplesone at a time and decide quot on line quot when toreturn a hypothesis rather than collect a largefixed size training sample this yield sequential learning procedure that pac learn by observinga small random number of example we provide theoretical bound on the expectedtraining sample size of our procedure but 
three different algorithm for qualitative obstacle detection are presented in this paper each one is based on different assumption the first two algorithm are aimed at yes no obstacle detection without indicating which point are obstacle they have the advantage of fast determination of the existence of obstacle in a scene based on the solvability of a linear system the first algorithm us information about the ground plane while the second algorithm only assumes that the ground is planar the third algorithm continuously estimate the ground plane and based on that determines the height of each matched point in the scene experimental result are presented for real and simulated data and performance of the three algorithm under different noise level are compared in simulation we conclude that in term of the robustness of performance the third one work best 
topographic mapping occur frequently in the brain a popular approach to understanding the structure of such mapping is to map point representing input feature in a space of a few dimension to point in a dimensional space using some selforganizing algorithm we argue that a more general approach may be useful where similarity between feature are not constrained to be geometric distance and the objective function for topographic matching is chosen explicitly rather than being specified implicitly by the self organizing algorithm we investigate analytically an example of this more general approach applied to the structure of interdigitated mapping such a the pattern of ocular dominance column in primary visual cortex 
abstract 
many problem of practical interest can be solved using tree search method because carefully tuned successor ordering heuristic guide the search toward region of the space that are likely to contain solution for some problem the heuristic often lead directly to a solution but not always limited discrepancy search address the problem of what to do when the heuristic fail our intuition is that a failing heuristic might well have succeeded if it were not for a small number of 
modelling and analyzing pushbroom sensor commonly used in satellite imagery is difficult and computationally intensive due to the motion of an orbiting satellite with respect to the rotating earth and the non linearity of the mathematical model involving orbital dynamic in this paper a simplified model of a pushbroom sensor the linear pushbroom model is introduced it ha the advantage of computational simplicity while at the same time giving very accurate result compared with the full orbiting pushbroom model besides remote sensing the linear pushbroom model is also useful in many other imaging application simple non iterative method are given for solving the major standard photogrammetric problem for the linear pushbroom model computation of the model parameter from ground control point determination of relative model parameter from image correspondence between two image and scene reconstruction given image correspondence and ground control point the linear pushbroom model lead to theoretical insight that are approximately valid for the full model a well the epipolar geometry of linear pushbroom camera in investigated and shown to be totally different from that of a perspective camera nevertheless a matrix analogous to the fundamental matrix of perspective camera is shown to exist for linear pushbroom sensor from this it is shown that a scene is determined up to an affine transformation from two view with linear pushbroom camera 
in this paper the problem of learning appropriate domain specificbias is addressed it is shown that this can be achieved by learningmany related task from the same domain and a theorem is givenbounding the number task that must be learnt a corollary of thetheorem is that if the task are known to posse a common internalrepresentation or preprocessing then the number of examplesrequired per task for good generalisation when learning n task simultaneouslyscales like o a bn 
we introduce a new algorithm designed to learn sparse perceptrons over input representation which include high order feature our algorithm which is based on a hypothesis boosting method is able to pac learn a relatively natural class of target concept moreover the algorithm appears to work well in practice on a set of three problem domain the algorithm produce classiflers that utilize small number of feature yet exhibit good generalization performance perhaps most importantly our algorithm generates concept description that are easy for human to understand 
interface agent are computer program that employ artificial intelligence technique in order to provide assistance to a user dealing with a particular computer application the paper discus an interface agent which ha been modelled closely after the metaphor of a personal assistant the agent learns how to assist the user by i observing the user s action and imitating them ii receiving user feedback when it take wrong action and iii being trained by the user on the basis of hypothetical example the paper discus how this learning agent wa implemented using memory based learning and reinforcement learning technique it present actual result from two prototype agent built using these technique one for a meeting scheduling application and one for electronic mail it argues that the machine learning approach to building interface agent is a feasible one which ha several advantage over other approach it provides a customized and adaptive solution which is le costly and ensures better user acceptability the paper also argues what the advantage are of the particular learning technique used 
this video demonstrates the program mu the mathematics understander which learns university level pure mathematics the video is both concerned with mu s performance in learning and performing mathematics and it underlying cognitive architecture the contextual memory system cm the change in knowledge representation during proof checking and problem solving are demonstrated graphically 
this paper introduces to the calculus of regular expression a replace operator and defines a set of replacement expression that concisely encode alternate variation of the operation replace expression denote regular relation defined in term of other regular expression operator the basic case is unconditional obligatory replacement we develop several version of conditional replacement that allow the operation to be constrained by context 
in a dynamic environment relatedto earlier work on goal directed diagnosis rymon a major issue being addressed in this research ishow contextual change influence the ongoing explanationprocess a an example of the issue to address consider thefollowing scenario implemented in our system theplanner is trying to achieve the goal of catching a planeand generates two possible plan driving to the airportor taking a taxi choosing the option of driving the plan step are 
this paper describes a method of detecting speech repair that us a part of speech tagger the tagger is given knowledge about category transition for speech repair and so is able to mark a transition either a a likely repair or a fluent speech other contextual clue such a editing term word fragment and word matchings are also factored in by modifying the transition probability 
the overall movement of articulated body such asthe human body is enabled by the coordinated movementof it rigid body part the body part are connectedby joint and in general move differently forinterpreting these movement a the movement of onesingle body it seems to be necessary to incorporateknowledge in the analysis process therefore in orderto recognize pedestrian from monocular image sequence we introduce a model based approach werepresent the human body by a 
we consider the problem of assigning probabilistic rating to hypothesis in a natural language interpretation system to facilitate integrating syntactic semantic and conceptual constraint we allow a fully compositional frame representation which permit co indexed syntactic constituent and or semantic entity filling multiple role in addition the knowledge base contains probabilistic information encoded by marginal probability on frame these probability are used to specify typicality of realworld scenario on one hand and conventionality of linguistic usage pattern on the other because the theoretical maximum entropy solution is infeasible in the general case we propose an approximate method this method s strength are it ability to rate compositional structure and it flexibility with respect to the input chosen by the system it is embedded in arbitrary set of hypothesis from the front end processor can be accepted a well a arbitrary subset of constraint heuristically chosen from the long term knowledge base 
this paper describes an uncertainty model ofstereo vision and it application to a visionmotionplanning for a mobile robot in general recognition of an environment requiresmuch computation and the recognition resultincludes uncertainty in planning therefore atrade off must be considered between the costof visual recognition and the effect of informationobtained by recognition such a trade offmust be formulated on the basis of a model ofvision which describes the required time for 
our experience in the ida natural language generation project ha shown u that ida s klone like classifier originally built solely to hold a domain knowledge base could also be used to perform many of the computation required by a natural language generation system in fact it seems possible to use the classifier to encode and execute arbitrary program we discus ida s classification system and how it differs from other such system perhaps most notably in the presence of template construct that enable recursion to be encoded give example of program fragment encoded in the classification system and compare the classification approach to other ai programming paradigm e g logic programming 
what real time qualitative viewpoint control behaviorsare important for performing global visual exploration taskssuch a searching for specific surface marking buildinga global model of an arbitrary object or recognizing anobject in this paper we consider the task of purposefully controlling the motion of an active monocular observerin order to recover a global description of a smooth arbitrarily shaped object using the occluding contour bystudying the epipolar parameterization 
observation based modeling can reduce the cost and effort of model construction for task such a virtual reality environment object modeling from a sequence of range image ha been formulated a a problem of principal component analysis with missing data pcamd which can be generalized a a weighted least square wls minimization problem after all visible region appeared over the whole sequence are segmented and tracked a normal measurement matrix of surface normal and a distance measurement matrix of normal distance to the origin are constructed respectively these two measurement matrix with possibly many missing element due to occlusion and mismatching enable u to formulate multiple view merging a a combination of two wls problem the solution to the first wls problem which employ the quaternion representation of the rotation matrix yield surface normal and rotation matrix subsequently the normal distance and translation vector are computed by solving the second wls problem experiment using synthetic data and real range image show that our approach is robust against noise and mismatch because it produce a statistically optimal object model by making use of redundancy from multiple view a toy house model from a sequence of real range image is presented 
we report progress on a new approach to combatting illiteracy getting computer to listen to child read aloud we describe a fully automated prototype coach for oral reading it display a story on the screen listens a a child read it and decides whether and how to intervene we report on pilot experiment with low reading second grader to test whether these intervention are technically feasible to automate and pedagogically effective to perform by adapting a continuous speech recognizer we detected of the misread word with a false alarm rate under by incorporating the intervention in a simulated coach we enabled the child to read and comprehend material at a reading level year higher than what they could read on their own we show how the prototype us the recognizer to trigger these intervention automatically 
the lure of using motion vision a a fundamental element in the perception of space drive this effort to use flow feature a the sole cue for robot mobility real time estimate of image flow and flow divergence provide the robot s sense of space the robot steer down a conceptual corridor comparing left and right peripheral flow large central flow divergence warns the robot of impending collision at dead end when this occurs the robot turn around and resume wandering behavior is generated by directly using flow based information in the d image sequence no d reconstruction is attempted active mechanical gate stabilization simplifies the visual interpretation problem by reducing camera rotation by combining corridor following and dead end deflection the robot ha wandered around the lab at cm s for a long a minute without collision the ability to support this behavior in real time with current equipment promise expanded capability a computational power increase in the future 
we present an unsupervised technique for visual learning which is based on density estimation in high dimensional space using an eigenspace decomposition two type of density estimate are derived for modeling the training data a multivariate gaussian for a unimodal distribution and a multivariate mixture of gaussians model for multimodal distribution these probability density are then used to formulate a maximum likelihood estimation framework for visual search and target detection for automatic object recognition this learning technique is tested in experiment with modeling and subsequent detection of human face and non rigid object such a hand 
we present a new algorithm for finding low complexity network with high generalization capability the algorithm search for large connected region of so called quot flat quot minimum of the error function in the weight space environment of a quot flat quot minimum the error remains approximately constant using an mdl based argument flat minimum can be shown to correspond to low expected overfitting although our algorithm requires the computation of second order derivative it ha backprop s order of 
learning system that express theory in firstorderlogic must ensure that the theory areexecutable and in particular that they do notlead to infinite recursion this paper presentsa heuristic method for preventing infinite recursionin the multi clause definition of arecursive relation the method ha been implementedin the latest version of foil but couldalso be used with any learning method thatgrows clause from ground fact by repeatedspecialization result on several 
in previous application bilateral symmetry of object wasused either a a descriptive feature in domain such a recognition andgrasping or a a way to reduce the complexity of structure from motion in this paper we propose a novel application using the symmetry propertyto quot symmetrize quot data before and after reconstruction we first showhow to compute the closest symmetric d and d configuration givennoisy data this give u a symmetrization procedure which we applyto image 
it is genermly agreed that coherent li ourse consists of segment tha t are related to one another a number of researcher have a rgm d lbr the use of rhetorica l ri or coherence relation i iol and the rhetorical relation specified by r s i mt have i e used in structuring text tlov mp in this l a l el we exa mine rh q ori al relation in the cold xt of dia logue ra tlmr than single speaker te t we argue that rea s millg about relational l r lmsit ion is necessary but not sufficient ior structuring lialogtm oiht out s v q al prol h m of a pplyiug rst to dialogue a nd argue tbr the necessity of recognizillg the intention underlying utteratlces a lld i he rich relationshil s among these intention our research on recognizing expression of hml t and interpreting ilmirect reply provides evidence that what moore a nd polla ck call i lfformal itmal level rela ti m ml play a n illlporta llt role in identil ing intention ill lia h gue r llsi l l s me ontiilua l i s of i h iblh wing liah gm sequence 
in del val and shoham we showed that the postulate for belief update recently proposed by katsuno and mendelzon can be analytically derived using the formal theory of action proposed by lin and shoham the contribution of this paper is twofold whereas in del val and shoham we only showed that our encoding of the update problem satisfied the km postulate here we use an independently motivated generalization of the theory of action used in that paper to provide a one to one correspondence between our construction and km update semantics we show how the km semantics can be generalized by relaxing our construction in a number of way each justified in certain intuitive circumstance and each corresponding to one specific postulate it follows that there are reasonable update operator outside the km family 
a new technique for the calibration of the intrinsic parameter of camera for active vision system is presented by making deliberate camera motion the intrinsics of the camera can be calibrated based either on the positional difference of optical flow field pdoff or the trajectory of feature tof on the image plane a way to detect the distortion of a camera lens is also presented the calibration method is simple fast reliable and very easy to combine with task performing process the performance of the technique is illustrated the method can be used for general calibration of camera intrinsics 
the first order spatial derivative of optic flow dilation shear and rotation provide powerful information about motion and surface layout the log polar sampled image lsi is of increasing interest for active vision and is particularly well suited to the measurement of local first order flow we explain why this is propose a simple least square method for measuring first order flow in an lsi sequence and demonstrate that the method work well when applied to real image 
while exploring to find better solution an agent performing onlinereinforcement learning rl can perform worse than is acceptable in some case exploration might have unsafe or even catastrophic result often modeled in term of reaching failure statesof the agent s environment this paper present a method that usesdomain knowledge to reduce the number of failure during exploration this method formulates the set of action from which therl agent composes a control policy 
a method for computing the d camera motion the ego motion in a static scene is introduced whichis based on computing the d image motion of a singleimage region directly from image intensity thecomputed image motion of this image region is used toregister the image so that the detected image regionappears stationary the resulting displacement fieldfor the entire scene between the registered frame is affectedonly by the d translation of the camera aftercanceling the effect 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
this paper is concerned with the problem of determining the indirect effect or ramification of action we argue that the standard framework in which background knowledge is given in the form of state constraint is inadequate and that background knowledge should instead be given in the form of causal law we represent causal law first a inference rule and later a sentence in a modal conditional logic gflatfor the framework with causal law we propose a simple fixpoint condition defining the possible next state after performing an action this fixpoint condition guarantee minimal change between state but also enforces the requirement that change be caused ramification and qualification constraint can be expressed a causal law 
to appear in g tesauro d s touretzky and t k leen ed advance in neuralinformation processing system mit press cambridge ma a straightforward approach to the curse of dimensionality in reinforcementlearning and dynamic programming is to replace thelookup table with a generalizing function approximator such a a neuralnet although this ha been successful in the domain of backgammon there is no guarantee of convergence in this paper we showthat the combination of 
new approach to solving constraint satisfaction problem using iterative improvement technique have been found to be successful on certain very large problem such a the million queen however on highly constrained problem it is possible for these method to get caught in local minimum in this paper we present genet a connectionist architecture for solving binary and general constraint satisfaction problem by iterative improvement genet incorporates a learning strategy to escape from 
a method of determining the motion of a camera from it image velocity is described that is insensitive to noise and intrinsic camera parameter this algorithm is based on a novel extension of motion parallax which doe not require the instantaneous alignment of feature but us sparse visual motion estimate to extract the direction of translation of the camera directly after which determination of the camera rotation and the depth of the image feature follows easily a method for calculating the expected uncertainty in the estimate is also described which allows optimal estimation and can also detect and reject independent motion and false correspondence experiment using small perturbation analysis show a favourable comparison with existing method and specifically the fundamental matrix method 
agent may sub contract some of their task to other agent s even when they don t share a common goal an agent try to contract some of it task that it can t perform by itself or when the task may be performed more efficiently or better by other agent a selfish agent may convince another selfish agent to help it with it task even if the agent are not assumed to be benevolent by promise of reward we propose technique that provide efficient way to reach subcontracting in varied situation the agent have full information about the environment and each other v subcontracting when the agent don t know the exact state of the world we consider situation of repeated encounter case of asymmetric information situation where the agent lack information about each other and case where an agent subcontract a task to a group of agent we also consider situation where there is a competition either among contracted agent or contracting agent in all situation we would like the contracted agent to carry out the task efficiently without the need of close supervision by the contracting agent the contract that are reached are simple pareto optimal and stable 
the dempster shafer theory give a solid ba si for reasoning application characterized by uncertainty a key feature of the theory is that proposition are represented a subset of a set which represents a hypothesis space this power set along with the set operation is a boolean algebra can we generalize the theory to cover arbitrary boolean algebra we show that the answer is yes the theory then cover for example infinite set the practical advantage of generalizatio n are that increased flexibility of representatio n is al lowed and that the performance of evidence ac cumulation can be enhanced in a previous paper we generalized the dempster shafer orthogonal sum operation to support practical evidence pooling in the present paper we provide the theoretical underpinning of that procedure by systemat ically considering familiar evidential function in turn for each we present a weaker form and we look at the relationship between these variation of the function the relationship are not so strong a for the conventional func tions however when we specialize to the fa miliar case of subset we do indeed get the wellknown relationship 
though dealing with different number of agent share negotiation among multiple agent remains an important topic of research in distributed artificial intelligence dai most previous work this subject however ha focused on bilateral negotiation deal that are reached between two agent there ha also been research on n agent agreement which ha considered consensus mechanism such a voting that allow the full group to coordinate itself these group decision making technique however assume that the entire group will or ha to coordinate it action sub group cannot make sub agreement that exclude other member of the group in some domain however it may be possible for beneficial agreement to be reached among subgroup of agent who might be individually motivated to work together to the exclusion of others outside the group this paper considers this more general case of n agent coalition formation we present a simple coalition formation mechanism that us cryptographic technique for subadditive task oriented domain the mechanism is efficient symmetric and individual rational when the domain is also concave the mechanism also satisfies coalition rationality 
the hypothesis selection problem or the k armed bandit problem is central to the realizationof many learning system thispaper study the minimization of samplingcost in hypothesis selection under a probablyapproximately optimal pao learningframework hypothesis selection algorithmscould be exploration oriented or exploitationoriented 
a recursive estimation technique for recovering the d motion and pointwise structure of an object is presented it is based on the use of relative orientation constraint in a local coordinate frame by carefully formulating the problem to propagate all constraint and to use the minimal number of parameter an estimator is obtained which is remarkably accurate stable and fast conveying numerous experiment using both real and synthetic data demonstrate structure recovery with a typical error of and typical motion recovery error of in translation and in rotation 
this paper present an alternating minimization algorithm used to train radial basisfunction network the algorithm is a modification of an interior point method used insolving primal linear program the resulting algorithm is shown to have a convergencerate on the order ofpnl iteration where n is a measure of the network size and l isa measure of the resulting solution s accuracy introductionin recent year considerable research ha investigated the use of alternating 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
this paper describes an automated process for the dynamic creation of a pattern recognizing computer program consisting of initially unknown detector an initially unknown iterative calculation incorporating the a yet uncreated detector and an initially unspecified final calculation incorporating the result of the a yetuncreated iteration the program s goal is to recognize a given protein segment a being a transmembrane domain or non transmembrane area the recognizing program to solve this problem will be evolved using the recentlydeveloped genetic programming paradigm genetic programming start with a primordial ooze of randomly generated computer program composed of available programmatic ingredient and then genetically breed the population using the darwinian principle of survival of the fittest and the genetic crossover sexual recombination operation automatic function definition enables genetic programming to dynamically create subroutine detector when cross validated the best genetically evolved recognizer achieves an out of sample correlation of and an outof sample error rate of this error rate is better than that recently reported for five other method statement of the problem the goal in this paper is to use genetic programming with automatically defined function adfs to create a computer program for recognizing a given subsequence of amino acid in a protein a being a transmembrane domain or non transmembrane area of the protein the automated process that will create the recognizing program for this problem will be given a set of differently sized protein segment and the correct classification for each segment the recognizing program will consist of initiallyunspecified detector an initially unspecified iterative calculation incorporating the a yet undiscovered detector and an initially unspecified final calculation incorporating the result of the a yet undiscovered iteration although genetic programming doe not know the chemical characteristic or biological meaning of the sequence of amino acid appearing in the protein segment we will show that the result have an interesting biological interpretation of course the reader may ignore the biological interpretation and view this problem a a onedimensional pattern recognition problem genetic programming is a domain independent method for evolving computer program that solve or approximately solve problem to accomplish this genetic programming start with a primordial ooze of randomly generated computer program composed of the available programmatic ingredient and breed the population or program using the darwinian principle of survival of the fittest and an analog of the naturally occurring genetic operation of crossover sexual recombination automatic function definition enables genetic programming to dynamically create subroutine dynamically during the run the question arises a to whether genetic programming can evolve a recognizing program consisting of initially unspecified detector an initially unspecified iterative calculation incorporating the a yet undiscovered detector and an initially unspecified final calculation incorporating the result of the a yet undiscovered iteration the genetically evolved program in this paper accomplishes this it achieves a better error rate than all four algorithm described in wei et al when analyzed the genetically evolved program ha a simple biological interpretation 
this paper deal with the qualitative visualrecognition of continuous human action sequence based on an analysis of the structureof physical action a framework for temporalsegmentation and qualitative classificationof physical action is proposed in orderto achieve correctness a well a efficiencyin real time action recognition a hierarchicalspatio temporal attention control method is developed a the integration of these idea acognitive architecture for action recognition is 
this paper present the hdg learning algorithm which us a hierarchical decomposition of the state space to make learning to achieve goal more efficient with a small penalty in path quality special care must be taken when performing hierarchical planning and learning in stochastic domain because macro operator cannot be executed ballistically the hdg algorithm which is a descendent of watkins q learning algorithm is described here and preliminary empirical result are presented 
in this paper area preserving multi scale representation of planar curve are described this allows smoothing without shrinkage at the same time preserving all the scale space property the representation are obtained deforming the curve via geometric heat flow while simultaneously magnifying the plane by a homethety which keep the enclosed area constant when the euclidean geometric heat flow is used the resulting representation is euclidean invariant and similarly it is affine invariant when the affine one is used the flow are geometrically intrinsic to the curve and exactly satisfy all the basic requirement of scale space representation in the case of the euclidean heat flow it is completely local a well the same approach is used to define length preserving geometric flow a similarity scale invariant geometric heat flow is studied a well in this work 
a planner in the real world must be able to handle uncertainty it must be able to reason about the effect of uncertainty on it plan select plan that avoid uncertain outcome when possible and make contingency plan against different possible outcome when uncertainty cannot be avoided we have constructed such a planner cassandra which ha these property using cassandra we have produced the ant general solution to the key and box challenge problem proposed by michie over twenty year ago 
genetic programming gp is an automatic programming technique that ha recently been applied to a wide range of problem including block world planning this paper describes a series of illustrative experiment in which gp technique are applied to traditional block world planning problem we discus genetic planning in the context of traditional ai planning system and comment on the cost and benefit to be expected from further work 
a new representation for object with multiple colour the colour adjacency graph cag is proposed each node of the cag represents a single chromatic component of the image defined a a set of pixel forming a unimodal cluster in the chromatic scattergram edge encode information about adjacency of colour component and their reflectance ratio the cag is related to both the histogram and region adjacency graph representation it is shown to be preserving and combining the best feature of these two approach while avoiding their drawback the proposed approach is tested on a range of difficult object recognition and localisation problem involving complex imagery of non rigid d object under varied viewing condition with excellent result 
the problem of scale in shape from textureis addressed the need for at least two scale parametersis emphasized a local scale describing the amount ofsmoothing used for suppressing noise and irrelevant detailswhen computing primitive texture descriptor fromimage data and an integration scale describing the sizeof the region in space over which the statistic of thelocal descriptor is accumulated a novel mechanism for automatic scale selection isproposed based on normalized 
a number of recent paper have argued that invariantsdo not exist for three dimensional point set in generalposition this ha often been misinterpretedto mean that invariant cannot be computed for anythree dimensional structure this paper prof by examplethat although the general statement is true invariantsdo exist for structured three dimensional pointsets projective invariant are derived for two class ofobject the first is for point that lie on the vertex of 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
novelty detection technique are concept learning method that proceed by recognizing positive instance of a concept rather than differentiating between it positive and negative instance novelty detection approach consequently require very few if any negative training instance this paper present a particular novelty detection approach to classification that us a redundancy compression and non redundancy differentiation technique based on the gluck myers model of the hippocampus a part of the brain critically involved in learning and memory in particular this approach consists of training an autoencoder to reconstruct positive input instance at the output layer and then using this autoencoder to recognize novel instance classification is possible after training because positive instance are expected to be reconstructed accurately while negative instance are not the purpose of this paper is to compare hippo the system that implement this technique to c and feedforward neural network classification on several application 
sensor based navigation is fundamental to any mobile robot conventional statistical approach to the navigation problem maintain an exact global description of environment geometry however in practise the behaviour of real physical sensor and the observation they make of the environment make such central geometric representation extremely fragile to overcome such problem this paper proposes the use of qualitative model of physical sensor observation these aim to describe the world in term of local sensor centric representation of the observed environment each representation exploit those landmark most natural to the physical sensor involved and no explicit geometric representation of the world is assumed this lead naturally to a navigation process defined in term of relationship between different sensor observables an intrinsically more robust mechanism than found in conventional navigation algorithm the representation and navigation methodology proposed is illustrated using sonar data from a real vehicle 
the problem of making optimal decision in uncertain condition is central to artificial intelligence if the state of the world is known at all time the world can be modeled a a markov decision process mdp mdps have been studied extensively and many method are known for determining optimal course of action or policy the more realistic case where state information is only partially observable partially observable markov decision process pomdps have received much le attention the best exact algorithm for these problem can be very inefficient in both space and time we introduce smooth partially observable value approximation spova a new approximation method that can quickly yield good approximation which can improve over time this method can be combined with reinforcement learning meth od a combination that wa very effective in our test case 
we have analyzed the relationship between correlated spike count and the peak in the cross correlation of spike train for pair of simultaneously recorded neuron from a previous study of area mt in the macaque monkey zohary et al we conclude that common input responsible for creating peak on the order of ten millisecond wide in the spike train cross correlograms ccgs is also responsible for creating the correlation in spike count observed at the two second time scale of the trial we argue that both common excitation and inhibition may play signican t role in establishing this correlation 
we describe a technique for surface recovery of a rotating object illuminated under a collinear light source where the light source lie on or near the optical axis we show that the surface reflectance function can be directly estimated from the image sequence without any assumption on the reflectance property of the object surface from the image sequence the d location of some singular surface point are calculated and their brightness value are extracted for the estimation of the reflectance function we also show that the surface can be recovered by using shading information in two image of the rotating object iteratively using the first order taylor series approximation and the estimated reflectance function the depth and orientation of the surface can be recovered simultaneously the experimental result on real image sequence of both matte and specular surface demonstrate that the technique is feasible and robust 
this paper describes a heuristic based approach to word sense disambiguation the heuristic that are applied to disambiguate a word depend on it part of speech and on it relationship to neighboring salient word in the text part of speech are found through a tagger and related neighboring word are identified by a phrase extractor operating on the tagged text to suggest possible sens each heuristic draw on semantic relation extracted from a webster s dictionary and the semantic thesaurus wordnet for a given word all applicable heuristic are tried and those sens that are rejected by all heuristic are discarded in all the disambiguator us heuristic based on relationship 
we propose strategy for selecting a good neural network architecture for modeling any specificdata set our approach involves efficiently searching the space of possible architecturesand selecting a quot best quot architecture based on estimate of generalization performance sincean exhaustive search over the space of architecture is computationally infeasible we proposeheuristic strategy which dramatically reduce the search complexity these employ directedsearch algorithm including 
current computer aided engineering paradigm for supporting synthesis activity in engineering design require the designer to use analysis simulator iteratively in an optimization loop while optimization is necessary to achieve a good final design it ha a number of disadvantage during the early stage of design in the inverse engineering methodology machine learning technique are used to learn a multidirectional model that provides vastly improved synthesis and analysis support to the designer this methodology is demonstrated on the early design of a diesel engine combustion chamber for a truck 
we have implemented a comprehensive constraint based programming language a an extension to common lisp this constraint package provides a unified framework for solving both numeric and non numeric system of constraint using a combination of local propagation technique including binding propagation boolean constraint propagation generalized forward checking propagation of bound and unification the backtracking facility of the nondeterministic dialect of common lisp used to implement this constraint package act a a general fallback constraint solving method mitigating the incompleteness of local propagation 
we introduce a general framework for con straint solving where classical csps fuzzy csps weighted csps partial constraint sat isfaction and others can be easily cast the framework is based on a semiring structure where the set of the semiring specifies the val ues to be associated to each tuple of value of the variable domain and the two semiring op erations and x model constraint projec tion and combination respectively local con sistency algorithm a usually used for clas sical csps can be exploited in this general framework a well provided that some condi tions on the semiring operation are satisfied we then show how this framework can be used to model both old and new constraint solving scheme thus allowing one both to formally justify many informally taken choice in exist ing scheme and to prove that the local con sistency technique can be used also in newly defined scheme 
a solution to the ramification problem caused by underlying domain constraint in strip like approach is presented we introduce the notion of causal relationship which are used in a post processing step after having applied an action description moreover we show how the information needed for these post computation can be automatically extracted from the domain constraint plus general knowledge of which fluents can possibly affect each other we illustrate the necessity of causal relationship by an example that show the limitedness of a common method to avoid unintended ramification namely the distinction between so called frame and non frame fluents finally we integrate our solution into a recently developed strip iike yet purely deductive approach to reasoning about action based on equational logic programming 
this paper give a new method for image rectification the process of resampling pair of stereo image taken from widely differing viewpoint in order to produce a pair of matched epipolar projection these are projection in which the epipolar line run parallel with the x axis and consequently disparity between the image are in the x direction only the method is based on an examination of the essential matrix of longuet higgins which describes the epipolar geometry of the image pair the approach taken is consistent with that recently advocated strongly by faugeras of avoiding camera calibration the paper us method of projective geometry to define a matrix called the epipolar transformation matrix used to determine a pair of d projective transforms to be applied to the two image in order to match the epipolar line the advantage include the simplicity of the d projective transformation which allows very fast resampling a well a subsequent simplification in the identification of matched point and scene reconstruction 
this paper present a computational model of how conversational participant collaborate in order to make a referring action successful the model is based on the view of language a goal directed behavior we propose that the content of a referring expression can be accounted for by the planning paradigm not only doe this approach allow the process of building referring expression and identifying their referent to be captured by plan construction and plan inference it also allows u to account for how participant clarify a referring expression by using meta action that reason about and manipulate the plan derivation that corresponds to the referring expression to account for how clarification goal arise and how inferred clarification plan affect the agent we propose that the agent are in a certain state of mind and that this state includes an intention to achieve the goal of referring and a plan that the agent are currently considering it is this mental state that sanction the adoption of goal and the acceptance of inferred plan and so act a a link between understanding and generation 
flexibility and e ciency these are the two con icting requirement for a multilingualtext planning component generating a natural language text requires a greatamount of exibility in the rule set employed and subsequently in the control mechanism for any local input semantic structure there are in general manyways toimplement it eachchoice produce ripple that move out and a ect other part of thetext a choice that is locally optimal may in the big picture constrain 
the paper evaluates the effectiveness of learning for speeding up the solution of constraint satisfaction problem it extends previous work dechter by introducing a new and powerful variant of learning and by presenting an extensive empirical study on much larger and more difficult problem instance our result show that learning can speed up backjumping when using either a fixed or dynamic variable ordering however the improvement with a dynamic variable ordering is not a great 
integrated mean squared error imse is a version of the usual mean squared errorcriterion averaged over all possible training set of a given size if it could be observed it could be used to determine optimal network complexity or optimal data subset forefficient training we show that two common method of cross validating average squarederror deliver unbiased estimate of imse converging to imse with probability one theseestimates thus make possible approximate imse based choice of 
this thesis present em sodabot a general purpose software agent userenvironment and construction system it primary component is the em basic software agent a computational framework for building agent which is essentially an em agent operating system we also present a new language for programming the basic software agent whose primitive are designed around human level description of agent activity via this programming language em user can easily implement a wide range of typical software agent application e g personal on line assistant and meeting scheduling agent the sodabot system ha been implemented and tested and it description comprises the bulk of this thesis 
inductive logic programming ilp involves the construction of first order definite clause theory from example and background knowledge unlike both traditional machine learning and computational learning theory ilp is based on lock step development of theory implementation and application ilp system have successful application in the learning of structure activity rule for drug design semantic grammar rule finite element mesh design rule and rule for prediction of protein structure and mutagenic molecule the strong application in ilp can be contrasted with relatively weak pac learning result even highly restricted form of logic program are known to be prediction hard it ha been recently argued that the mismatch is due to distributional assumption made in application domain these assumption can be modelled a a bayesian prior probability representing subjective degree of belief other author have argued for the use of bayesian prior distribution for reason different to those here though this ha not lead to a new model of polynomial time learnability incorporation of bayesian prior distribution over time bounded hypothesis in pac lead to a new model called u learnability it is argued that u learnability is more appropriate than pac for universal turing computable language time bounded logic program have been shown to be polynomially u learnable under certain distribution the use of time bounded hypothesis enforces decidability and allows a unified characterization of speed up learning and inductive learning u learnability ha a special case pac and natarajan s model of speed up learning 
query expansion method have been studied for a long time with debatable success in many instance in this paper we present a probabilistic query expansion model based on a similarity thesaurus which wa constructed automatically a similarity thesaurus reflects domain knowledge about the particular collection from which it is constructed we address the two important issue with query expansion the selection and the weighting of additional search term in contrast to earlier method our query are expanded by adding those term that are most similar to the concept of the query rather than selecting term that are similar to the query term our experiment show that this kind of query expansion result in a notable improvement in the retrieval effectiveness when measured using both recall precision and usefulness 
instantiation ordering over formula the relation of one formula bemg an instance of an other have long been central to the study of automated deduction and logic programming and are of rapidly growi ng importance in the study of database system and machine learn ing a variety of instantiation ordenngs are now ip use many of which incorporate some kind of background information in the form of a constraint theory even a casual exami nation of these instantiation ordering reveals that they are somehow related but in exactly what way this paper present a general in stantiation ordering of which all these order ings are special case a are other instantia tion ordenngs the paper show that this gen eral ordering ha the semantic property we desire in an instantiation ordering implying that the special case have these property a well the extension to this general ordering is useful in application to inductive logic programming automated deduction and logic programming knowledge base vivification and database sys tems 
semantic cluster of a domain form an important feature that can be useful for performing syntactic and semantic disambiguation several attempt have been made to extract the semantic cluster of a domain by probabilistic or taxonomic technique however not much progress ha been made in evaluating the obtained semantic cluster this paper focus on an evaluation mechanism that can be used to evaluate semantic cluster produced by a system against those provided by human expert 
many existing rule learning system are computationally expensive on large noisy datasets in this paper we evaluate the recently proposed rule learning algorithm irep on a large and diverse collection of benchmark problem we show that while irep is extremely efficient it frequently give error rate higher than those of c and c rule we then propose a number of modification resulting in an algorithm ripperk that is very competitive with c rule with respect to error rate but much more efficient on large sample ripperk obtains error rate lower than or equivalent to c rule on of benchmark problem scale nearly linearly with the number of training example and can efficiently process noisy datasets containing hundred of thousand of example 
reasoning with model based representation is an intuitive paradigm which ha been shown to be theoretically sound and to posse some computational advantage over reasoning with formula based representation of knowledge in this paper we present more evidence to the utility of such representation in real life situation one normally completes a lot of missing context information when answering query we model this situation by augmenting the available knowledge about the world with context specific information we show that reasoning with model based representation can be done efficiently in the presence of varying context information we then consider the task of default reasoning we show that default reasoning is a generalization of reasoning within context in which the reasoner ha many context rule which may be conflicting we characterize the case in which model based reasoning support efficient default reasoning and develop algorithm that handle efficiently fragment of reiter s default logic in particular this includes case in which performing the default reasoning task with the traditional formula based representation is intractable further we argue that these result support an incremental view of reasoning in a natural way 
this paper is to show that propositionalcontextual reasoning is decidable propositional logic of context extends classicalpropositional logic with a new modality ist c oe usedto express that the sentence oe is true in the context 
even though method based on the use of deformable model have become prevalent the quality oftheir output depends critically on the model s initial state the issue of initializing such model however ha not received much attention even though it is often key to the implementation ofa truly useful system 
this paper describes a diagnosis algorithm called structure based abduction sab which wa developed in the framework of constraint network the algorithm exploit the structure of the constraint network and is most efficient for near tree problem domain by analyzing the structure of the problem domain the performance of such algorithm can be bounded in advance we present empirical result comparing sab with two modelbased algorithm mbd and mbd for the task of finding one or all minimal cardinality diagnosis mbd us the same computing strategy a algorithm gde mbd adopts a breadth first search strategy similar to the algorithm diagnose the main conclusion is that for nearly acyclic circuit such a the n bit adder the performance of sab being linear provides definite advantage a the size of the circuit increase 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
planning system often make the assumption that omniscient world knowledge isavailable our approach make the more realistic assumption that the initial knowledgeabout the action is incomplete and us experimentation a a learning mechanismwhen the missing knowledge cause an execution failure previous work on learning byexperimentation ha not addressed the issue of how to choose good experiment andmuch research on learning from failure ha relied on background knowledge to build 
a key to developing computationally efficient stereo vision is the incorporation of intelligent control stereo is most effective when it is able to focus it analysis on region and detail of a scene that are important to the task at hand while avoiding le important region and unnecessary detail the paper describes two method for electronically focusing stereo measurement through simple image pre processing the first allows measurement sensitivity to be adjusted the second allows the shape of the d region in which measurement are gathered to be matched to the shape of surface in the scene 
the view line associated with a family of profile curve of the projection of a surface onto the retina of a moving camera defines a multi valued vector field on the surface the integral curve of this field are called epipolar curve and together with a parametrization of the profile provide a parametrization of region of the surface in addition one ha the epipolar constraint which define curve in the image these image curve are related to the epipolar curve on the surface but not by a simple projection we present an investigation of epipolar curve on the object surface in the spatio temporal surface and the trace in the image we address the question of when there is an epipolar parametrization we have obtained detailed result which depend on a classification cite davydov of vector field on surface with boundary these result give a systematic way of detecting the gap left by reconstruction of a surface from profile they also suggest method for filling in these gap this work wa supported by nato grant crg in addition the second author would like to acknowledge the support of darpa and tacom under contract daae c r and nsf under grant iri and iri 
ever since strip wa first introduced fikes and nilsson it logical semantics ha been problematic there have been many proposal in the literature e g lifschitz erol nau and subrahmanian bacchus and yang these all have in common a reliance on metatheoretic operation on logical theory to capture the add and delete list of strip operator but it ha never been clear exactly what these operation correspond to declaratively especially when they are applied to logically incomplete theory in this paper we provide a semantics for strip like system in term of a purely declarative situation calculus axiomatization for action and their effect on our view strip is a mechanism for computing the progression lin and reiter pednault of an initial situation calculus database under the effect of an action we illustrate this idea by describing two different strip mechanism and proving their correctness with respect to their situation calculus specification 
model of physical system can differ according to computational cost accuracy and precision among other thing depending on the problem solving task at hand different model will be appropriate several investigator have recently developed method of automatically selecting among multiple model of physical system our research is novel in that we are developing model selection technique specifically suited to computer aided design our approach is based on the idea that artifact performance model for computer aided design should be chosen in light of the design decision they are required to support we have developed a technique called gradient magnitude model selection gmms which embodies this principle gmms operates in the context of a hillclimbing search process it selects the simplest model that meet the need of the hillclimbing algorithm in which it operates we are using the domain of sailing yacht design a a testbed for this research we have implemented gmms and used it in hillclimbing search to decide between a computationally expensive potential flow program and an algebraic approximation to analyze the performance of sailing yacht experimental test show that gmms make the design process faster than it would be if the most expensive model were used for all design evaluation gmms achieves this performance improvement with little or no sacrifice in the quality of the resulting design 
supervised classification problem have receivedconsiderable attention from the machinelearning community we propose a novel geneticalgorithm based prototype learning system please for this class of problem givena set of prototype for each of the possibleclasses the class of an input instance is determinedby the prototype nearest to this instance we assume ordinal attribute and prototypesare represented a set of feature value pair agenetic algorithm is used to evolve the 
this article discus the use of analogy to index and organize large database of information we describe the design and implementation of an analogical database supporting ten to hundred of thousand of case the content of the database are parsed news article represented a network of grammatical relation with reference into wordnet for word meaning information the virtue of this approach is it domain independent handling of content analysis efficient algorithm for indexing and matching in this database are described and bneflv discussed and example of their performance are discussed 
predicting the behavior of physical system is essential to both common sense and engineering task it is made especially challenging by the lack of complete precise knowledge of the phenomenon in the domain and the system being modelled we present an implemented approach to automatically building and simulating qualitative model of physical system imprecise knowledge of phenomenais expressed by qualitative representation of monotonic function and variable value incomplete knowledge about the system is either inferred or alternative complete description that will affect behavior are explored the architecture and algorithm used support both effective implementation and formal analysis the expressiveness of the modelling language and strength of the resulting prediction are demonstrated by substantial application to complex system 
abstraction technique are important for solving constraint satisfaction problem with global constraint and low solution density in the presence of global constraint backtracking search is unable to prune partial solution it therefore operates like pure generate and test abstraction improves on generate and test by enabling entire subset of the solution space to be pruned early in a backtracking search process this paper describes how abstraction space can be characterized in term of approximate symmetry of the original concrete search space it defines two special type of approximate symmetry called range symmetry and domain symmetry which apply to function finding problem it also present algorithm for automatically synthesizing hierarchic problem solver based on range or domain symmetry the algorithm operate by analyzing declarative description of class of constraint satisfaction problem both algorithm have been fully implemented this paper concludes by presenting data from experiment testing the two synthesis algorithm and the resulting problem solver on np hard scheduling and partitioning problem 
we address the issue of agent reasoning about other agent nonmonotonic reasoning ability in the framework of a multi agent autoepistemic logic ael in single agent ael nonmonotonic inference are drawn based on all the agent know in a multi agent context such a jill reasoning about jack s nonmonotonic inference this assumption must be abandoned since it cannot be assumed that jill know everything jack know given a specific subject matter like tweety the bird it is more realistic and sufficient if jill only assumes to know all jack know about tweety in order to arrive at jack s nonmonotonic inference about tweety this paper provides a formalization of all an agent know about a certain subject matter based on possible world semantics in a multi agent ael besides discussing various property of the new notion we use it to characterize formula that are about a subject matter in a very strong sense while our main focus is on subject matter that consist of atomic proposition we also address the case where agent are the subject matter 
useful equivalence preserving operation based on antilinks are described these operation eliminate a potentially large number of subsumed path in a negation normal form formula those anti link that directly indicate the presence of subsumed path are characterized these operation are useful for prime implicant implicate algorithm because most of the computational effort in computing the prime implicants and prime implicates of a propositional formula is spent on subsumption check the problem of removing all subsumed path in an nnf formula is shown to be np hard even though such formula may be small relative to the size of their path set the general problem of determining whether a pair of subsumed path is associated with an arbitrary anti link is shown to be np complete further reduction of subsumption check are shown to be available when strictly pure full block are present the effectiveness of operation based on anti link and strictly pure full block is examined with respect to some benchmark example from the literature 
to summarize is to reduce in complexity and hence in length while retaining some of the essential quality of the original this paper focus on document extract a particular kind of computed document summary 
admissible heuristic are worth discovering because they have desirable property in various search algorithm unfortunately effective one one that are accurate and efficiently computable are difficult for human to discover one source of admissible heuristic is from abstraction of a problem the length of a shortest path solution to an abstracted problem is an admissible heuristic for the original problem because the abstraction ha certain detail removed however often too many detail have to be abstracted to yield an efficiently computable heuristic resulting in inaccurate heuristic this paper describes a method to reconstitute the abstracted detail back into the solution to the abstracted problem thereby boosting accuracy while maintaining admissibility our empirical result of applying this paradigm to project scheduling suggest that reconstitution can make a good admissible heuristic even better 
although empirical machine learning ha seen many algorithm one of it most important goal ha been neglected important real world problem often have just a primitive represen tation to which the target concept bear only a remote obscure relationship this considera tion lead to a class of measure that may be ap plied to data to estimate difficulty for standard algorithm a the concept becomes harder current decision tree and decision list method give increasingly poor accuracy though backpropagation doe better a new system for feature construction scale up best the fun damental limitation of standard algorithm is caused by two problem greedy search and representational inadequacy critical analysis and empirical result show that lookahead alle viates the greedy hill climbing problem at high cost but even this is insufficient combining lookahead with feature construction alleviates the complex global replication problem with hard concept for principled algorithm devel opment and good progress researcher need to study hard concept and system behavior using them 
previous approach to bidirectional search require exponential space and they are either le efficient than unidirectional search for finding optimal solution or they cannot even find such solution for difficult problem based on a memory bounded unidirectional algorithm for tree sma we developed a graph search extension and we used it to construct a very efficient memory bounded bidirectional algorithm this bidirectional algorithm can be run for difficult problem with bounded memory in addition it is much more efficient than the corresponding unidirectional search algorithm also for finding optimal solution to difficult problem in summary bidirectional search appears to be the best approach to solving difficult problem and this indicates the extreme usefulness of a paradigm that wa neglected for long 
in this paper we introduce new algorithm for optimizing noisyplants in which each experiment is very expensive the algorithmsbuild a global non linear model of the expected output at the sametime a using bayesian linear regression analysis of locally weightedpolynomial model the local model answer query about confidence noise gradient and hessian and use them to make automateddecisions similar to those made by a practitioner of responsesurface methodology the global and local 
some linguistic constraint cannot be effectively resolved during parsing at the location in which they are most naturally introduced this paper show how constraint can be propagated in a memoizing parser such a a chart parser in much the same way that variable binding are providing a general treatment of constraint coroutining in memoization prolog code for a simple application of our technique to bouma and van noord s categorial grammar analysis of dutch is provided 
instance based learning method explicitly remember all the data that they receive they usually have no training phase and only at prediction time do they perform computation then they take a query search the database for similar datapoints and build an on line local model such a a local average or local regression with which to predict an output value in this paper we review the advantage of instance based method for autonomous system but we also note the ensuing cost hopelessly slow computation a the database grows large we present and evaluate a new way of structuring a database and a new algorithm for accessing it that maintains the advantage ot instance based learning earlier attempt to combat the cost of instancebased learning have sacrificed the explicit retention of all data or been applicable only to instancebased prediction based on a small number of near neighbor or have had to reintroduce an explicit training phase in the form of an interpolative data structure our approach build a multiresolution data structure to summarize the database of experience at all resolution of interest simultaneously this permit u to query the database with the same flexibility a a conventional linear search but at greatly reduced computational cost 
this paper address the problem of computing the minimal model of a given cnf propositional theory we present two group of algorithm algorithm in the first group are efficient when the theory is almost horn that is when there are few non horn clause and or when the set of all literal that appear positive in any non horn clause is small algorithm in the other group are efficient when the theory can be represented a an acyclic network of low arity relation our algorithm suggest several characterization of tractable subset for the problem of finding minimal model 
we address the problem of optical flow reconstruction and in particular the problem of resolving ambiguity near edge they occur due to i the aperture problem and ii the occlusion problem where pixel on both side of an intensity edge are assigned the same velocity estimate and confidence however these measurement are correct for just one side of the edge the non occluded one we note that the confidence measure are large at intensity edge and larger at the convex side of the edge i e inside corner than at the concave side we resolve the ambiguity through local interaction via coupled markov random field mrf the result is the detection of motion for region of image with large global convexity 
these note discus formalizing context a first class object the basic relation is ist c p it asserts that the proposition p is true in the context c the most important formula relate the proposition true in different context introducing context a formal object will permit axiomatizations in limited context to be expanded to transcend the original limitation this seems necessary to provide ai program using logic with certain capability that human fact representation and human reasoning posse fully implementing transcendence seems to require further extension to mathematical logic i e beyond the nonmonotonic inference method first invented in ai and now studied a a new domain of logic various notation are considered but these note are tentative in not proposing a single language with all the desired capability 
current computing system are just beginning to enable the computational manipulation of temporal medium like video and audio because of the opacity of these medium they must be represented in order to be manipulable according to their content knowledge representation technique have been implicitly designed for representing the physical world and it textual representation temporal medium pose unique problem and opportunity for knowledge representation which challenge many of it assumption about the structure and function of what is represented the semantics and syntax of temporal medium require representational design which employ fundamentally different conception of space time identity and action in particular the effect of the syntax of video sequence on the semantics of video shot demand a representational design which can clearly articulate the difference between the context dependent and contextindependent semantics of video data this paper outline the theoretical foundation for designing representation of video discus medium stream an implemented system for video representation and retrieval and critique related effort in this area 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
we describe a new representation for learningconcepts that differs from the traditional decisiontree and rule approach this representation called prototypical concept description can represent several prototype for aconcept we also describe pl our algorithmfor learning these prototype and demonstratethat prototypical concept descriptionscan in some situation classify more accuratelythan standard machine learning algorithm more importantly we show that theyyield 
most research on machine learning ha focused on scenario in which a learner face a single isolated learning task the lifelong learning framework assumes instead that the learner encounter a multitude of related learning task over it lifetime providing the opportunity for the transfer of knowledge this paper study lifelong learning in the context of binary classification it present the invariance approach in which knowledge is transferred via a learned model of the invariance of the domain result on learning to recognize object from color image demonstrate superior generalization capability if invariance are learned and used to bias subsequent learning 
temporal difference method solve the temporalcredit assignment problem for reinforcementlearning an important subproblem ofgeneral reinforcement learning is learning toachieve dynamic goal although existing temporaldifference method such a q learning can be applied to this problem they do nottake advantage of it special structure this paperpresents the dg learning algorithm whichlearns efficiently to achieve dynamically changinggoals and exhibit good knowledge transfer 
collection fusion is a data fusion problem in which the result of retrieval run on separate autonomous document collection must be merged to produce a single effective result this paper explores two collection fusion technique that learn the rmrnber of document to retrieve from each collection using only the ranked list of document returned in response to past query and those document relevance judgment retrieval experiment using the trec test co lection demonstrate that the effectiveness of the fusion technique is within of the effectiveness of a run in which the entire set of document is treated a a single collection 
lexicalized context free grammar lcfg is an attractive compromise between the parsing efficiency of context free grammar cfg and the elegance and lexical sensitivity of lexicalized tree adjoining grammar ltag lcfg is a restricted form of ltag that can only generate context free language and can be parsed in cubic time however lcfg support much of the elegance of ltag s analysis of english and share with ltag the ability to lexicalize cfgs without changing the tree generated 
when human tutor engage in dialogue they freely exploit all aspect of the mutually known context including the previous discourse utterance that do not draw on previous discourse seem awkward unnatural or even incoherent previous discourse must be taken into account in order to relate new information effectively to recently conveyed material and to avoid repeating old material that would distract the student from what is new producing a system that display such behavior involves finding an efficient way to identify which previous explanation if any are relevant to the current explanation task thus we are implementing a system that us a case based reasoning approach to identify previous situation and explanation that could potentially affect the explanation being constructed we have identified heuristic for constructing explanation that exploit this information in way similar to what we have observed in human human tutorial dialogue 
this paper outline some problem that may occur with reduced errorpruning in inductive logic programming most notably efficiency thereaftera new method incremental reduced error pruning is proposed thatattempts to address all of these problem experiment show that in manynoisy domain this method is much more efficient than alternative algorithm along with a slight gain in accuracy however the experimentsshow a well that the use of this algorithm cannot be recommended for 
we propose a plan based approach for respondingto user query in a collaborative environment weargue that in such an environment the system shouldnot accept the user s query automatically but shouldconsider it a proposal open for negotiation in this paperwe concentrate on case in which the system anduser disagree and discus how this disagreement canbe detected negotiated and how final modificationsshould be made to the existing plan introductionin task oriented 
in recent publication about data compression arithmetic code are often suggested asthe state of the art rather than the more popular huffman code while it is true that huffmancodes are not optimal in all situation we show that the advantage of arithmetic code in compressionperformance is often negligible referring also to other criterion we conclude that for many application huffman code should still remain a competitive choice introductionit is paradoxical that a the 
ambiguity is a notorious problem for natural language processing according to result obtained by schmitz and quantz i see disambiguation a a process in which contextual default are used to derive the most preferred interpretation of an expression i show how contextual information comprising grammatical a well a conceptual knowledge can be modeled in a homogeneous manner using terminological logic tl i slightly modify the default extension to tl presented by quantz and royer to allow a relevance ordering between multisets of default the preferred interpretation is the one containing the fewest exception with respect to such an ordering interpretation is thus achieved by exception minimization i combine this idea with deductive and abductive approach to interpretation and show how they can be formalized in term of tl entailment furthermore i obtain a variable depth of analysis by controling the granularity of interpretation via a set of relevant feature 
this paper present a model for generating prosodically appropriate synthesized response to database query using combinatory categorial grammar ccg cf a formalism which easily integrates the notion of syntactic constituency prosodic phrasing and information structure the model determines accent location within phrase on the basis of contrastive set derived from the discourse structure and a domain independent knowledge base 
this paper present an algorithm called justin case schedulkg for building robust schedule that tend not to break the algorithm implement the common sense idea of being prepared for likely error just in case they should occur the just in case algorithm analyzes a given nominal schedule determines the most likely break and reinvokes a scheduler to generate a contingent schedule to cover that break after a number of iteration the just in case algorithm produce a multiply contingent schedule that is more robust than the original nominal schedule the algorithm ha been developed for a real telescope scheduling domain in order to proactively manage schedule break that are due to an inherent uncertainty in observation duration the paper present empirical result showing that the algorithm performs extremely well on a representative problem from this domain 
this paper describes a simple construction for building a combinatorial model of a smooth manifold solid from a labeled figure representing it occluding contour the motivation is twofold first deriving the combinatorial model is an essential intermediate step in the visual reconstruction of solid shape from image contour a description of solid shape consists of a metric and a topological component both are necessary the metric component specifies how the topological component is embedded in three dimensional space the it paneling construction described in this paper is a procedure for generating the topological component from a labeled figure representing the occluding contour second the existence of this construction establishes the sufficiency of a labeling scheme for line drawing of smooth solid object originally proposed by huffman by sufficiency it is meant that every set of closed plane curve satisfying this labeling scheme is shown to correspond to a generic view of a manifold solid together with the whitney theorem this confirms that huffman s labeling scheme correctly distinguishes possible from impossible solid object 
deformable model are an attractive approach to recognizing nonrigidobjects which have considerable within class variability however there are severe search problem associated with fitting themodels to data we show that by using neural network to providebetter starting point the search time can be significantly reduced the method is demonstrated on a character recognition task in previous work we have developed an approach to handwritten character recognitionbased on the use of 
this paper present a procedure to generate judgment determiner e g many few although such determiner carry very little objective information they are extensively used in everyday language the paper present a precise characterization of a class of such determiner using three semantic test a conceptual representation for set is then derived from this characterization which can serve a an input to a generator capable of producing judgment determiner in a second part a set of syntactic feature controlling the realization of complex determiner sequence is presented the mapping from the conceptual input to this set of syntactic feature is then presented the presented procedure relies on a description of the speaker s argumentative intent to control this mapping and to select appropriate judgment determiner 
in this paper we show how geometry driven diffusion can be used to develop a system of curve evolution that is able to preserve salient feature of closed curve such a corner and straight line segment while simultaneously suppressing noise and irrelevant detail the idea is to characterise the curve by mean of it angle function i e the angle between the tangent and a fixed axis and to apply geometry driven diffusion to this one dimensional representation 
no feature based vision system can work until good feature can be identified and tracked from frame to frame although tracking itself is by and large a solved problem selecting feature that can be tracked well and correspond to physical point in the world is still an open problem we propose a feature selection criterion that is optimal by construction because it is based on how the tracker work a well a a feature monitoring method that can detect occlusion disocclusions and feature that do not correspond to point in the world these method are based on a new tracking algorithm that extends previous newton raphson style search method to work under affine image transformation we test performance with several simulation and experiment on real image 
simard lecun amp denker showed that the performanceof near neighbor classification scheme for handwritten characterrecognition can be improved by incorporating invariance to specifictransformations in the underlying distance metric the socalled tangent distance the resulting classifier however can beprohibitively slow and memory intensive due to the large amountofprototypes that need to be stored and used in the distance comparison in this paper wedevelop rich 
abstract the important scientific challenge of understanding global climate change is one that clearly requires the application of knowledge discovery and datamining technique on a massive scale advance in parallel supercomputing technology enabling high resolution modeling a well a in sensor technology allowing data capture on an unprecedented scale conspire to overwhelm present day analysis approach we present here early experience with a prototype exploratory data analysis environment conquest designed to provide content based access to such massive scientific datasets conquest content based querying in space and time employ a combination of workstation and massively parallel processor mpp s to mine geophysical datasets possessing a prominent tempord component it is designed to enable complex multi modal interactive querying and knowledge discovery while simultaneously coping with the extraordinary computational demand posed by the scope of the datasets involved af 
l to robot action the developmentof this asrl in simulation and with robot demonstratedthat robot could learn to communicate andcould adapt their language to changing circumstance simulated robot have also created a context dependentasrl in a context dependent language robotwords can have different meaning depending on thethis research is supported by the nsf under professorlynn andrea stein s nsf young investigator awardno iri digital equipment corporation the 
in the light of the success of top down inductionsystems tdis the problem to be addressedconcern the structure of description and theexploration of strategy this paper introduces aformal model to describe tdis based on the use oflattice theory and more precisely on the use of agalois connection one of the advantage of thisformalization is that it justifies the extensive use ofattribute value representation in the case of classicaltdis and generalizes this representation to 
a decision method for reiter s default logic is developed it can determine whether a default theory ha an extension whether a formula is in some extension of a default theory and whether a formula is in every extension of a default the ory the method handle full propositional de fault logic it can be implemented to work in polynomial space and by using only a theorem prover for the underlying propositional logic a a subroutine the method divide default rea soning into two major subtasks the search task of examining every alternative for extension which is solved by backtracking search and the classical reasoning task which can be imple mented by a theorem prover for the underly ing classical logic special emphasis is given to the search problem the decision method em ploy a new compact representation of exten sion which reduces the search space efficient technique for pruning the search space further are developed 
we propose a distinction between two kind of metonymy referential metonymy in which the referent of an np is shifted and predicative metonymy in which the referent of the np is unchanged and the argument place of the predicate is shifted instead example are respectively the hamburger is waiting for his check and which airline fly from boston to denver we also show that complication arise for both type of metonymy when multiple coercing predicate are considered finally we present implemented algorithm handling these complexity that generate both type of metonymic reading a well a criterion for choosing one type of metonymic reading over another 
we propose a learning algorithm for a variable memory lengthmarkov process human communication whether given a text handwriting or speech ha multi characteristic time scale onshort scale it is characterized mostly by the dynamic that generatethe process whereas on large scale more syntactic and semanticinformation is carried for that reason the conventionallyused fixed memory markov model cannot capture effectively thecomplexity of such structure on the other hand using 
we propose a novel approach to auditory stream segregation which extract individual sound auditory stream from a mixture of sound in auditory scene analysis the hb harmonic based stream segregation system is designed and developed by employing a multi agent system hb us only harmonic a a clue to segregation and extract auditory stream incrementally when the tracer generator agent detects a new sound it spawn a tracer agent which extract an auditory stream by tracing it harmonic structure the tracer sends a feedforward signal so that the generator and other tracer should not work on the same stream that is being traced the quality of segregation may be poor due to redundant and ghost tracer hb cope with this problem by introducing monitor agent which detect and eliminate redundant and ghost tracer hb can segregate two stream from a mixture of man s and woman s speech it is easy to resynthesize speech or sound from the corresponding stream additionally hb can be easily extended by adding agent of a new capability hb can be considered a the first step to computational auditory scene analysis 
we propose an algorithm ha been proposed the algorithm presented in this paper improves the run time of the recent result using an entirely different approach 
this paper present a class of nonlinear hierarchical algorithm for the fusion of multiresolution image data in low level vision the approach combine nonlinear causal markov model defined on hierarchical graph structure with standard bayesian estimation theory two random process defined on simple hierarchical graph quadtrees or ternary graph are introduced to represent the multiresolution observation at hand and the hidden label to be estimated an optimal algorithm inspired from the viterbi algorithm is developed to compute the bayesian estimate on the hierarchical graph structure estimate are obtained within two pass on the graph structure this algorithm is non iterative and yield a per pixel computational complexity which is independent of image size this approach is compared to the multiscale algorithm proposed by bouman et al for single resolution image segmentation that we have extended for multiresolution data fusion 
a the field of computational vision matures more effort are devoted to vision system that are active and need to interact with their environment in real time a prerequisite for integrating vision and action is the development of a set of representation of the visual system s space time where space includes the system itself thus we are faced with the problem of studying the nature of appropriate representation and also with the computational task of acquiring them in a robust manner and in real time both of these problem are addressed in this paper from a computational point of view in particular we study representation needed by active visual system in order to understand their self motion and the structure of their environment the representation are of le metric information content than the one traditionally used including depth surface normal curvature and d metric value for the parameter of rigid motion etc but they are rich enough to allow the system to perform a large number of action these representation indexed in image coordinate are the direction of translation and the direction of rotation for the case of motion and a monotonic function of the depth value in the case of shape description their advantage come from the fact that they can be computed from minimal and well defined input flow or disparity value along image gradient a opposed to the traditional one which require image correspondence or the utilization of assumption about the environment 
in many computer vision problem it is necessary to robustly estimate parameter value from a large quantity of image data in such problem least square minimization is computationally the most convenient and practical solution method the author show that the least square solution is in general statistically biased in the presence of noise a scheme called renormalization that iteratively remove the statistical bias by automatically adjusting to the image noise is presented it is applied to the problem of estimating vanishing point and focus of expansion and conic fitting 
both the dynamic of belief change and the process of reasoning by default can be based on the conditional bezief set of an agent represented a a set of if then rule in this paper we address the open problem of formalizing the dynamic of revising this conditional belief set by new if then rule be they interpreted a new default rule or new revision policy we start by providing a purely semantic characterization based on the semantics of conditional rule which induces logical constraint on any such revision process we then introduce logical syntax independent and syntax dependent technique and provide a precise characterization of the set of conditionals that hold after the revision in addition to formalizing the dynamic of revising a default knowledge base this work also provides some of the necessary formal tool for establishing the truth of nested conditionals and attacking the problem of learning new default 
this paper describes an interdisciplinary experiment in controlling semi autonomous animated human form with natural language input these computer generated character resemble traditional stage actor in that they are more autonomous than traditional hand guided animated character and le autonomous than fully improvisational agent we introduce the desktop tkeatev metaphor reserving for user the creative role of a theatrical writer or director 
recognition system attempt to recover information about theidentity of observed object and their location in the environment afundamental problem in recognition is pose estimation this is the problem of using a correspondence between some portion of anobject model and some portion of an image to determine whether the imagecontains an instance of the object and in case it doe to determine thetransformation that relates the model to the image the current approachesto this problem are divided into method that use global property of the object e g centroid and moment of inertia and methodsthat use local property of the object e g corner andline segment global property are sensitive to occlusion and specifically to self occlusion local property are difficult to locatereliably and their matching involves intensive computation we present a novel method for recognition that us region information in our approach the model and the image are divided into region given amatch between subset of region without any explicit correspondencebetween different piece of the region the alignment transformation iscomputed the method applies to planar object under similarity affine andprojective transformation and to projection of d object undergoingaffine and projective transformation the new approach combine many of theadvantages of the previous two approach while avoiding some of theirpitfalls like the global method our approach make use of regioninformation that reflects the true shape of the object but like localmethods our approach can handle occlusion 
formal ai system traditionally represent knowledge using logical formula we will show however that for certain kind of information a model based representation is more compact and enables faster reasoning than the corresponding formula based representation the central idea behind our work is to represent a large set of model by a subset of characteristic model more specifically we examine model based representation of horn theory and show that there are large horn theory that can be exactly represented by an exponentially smaller set of characteristic model in addition we will show that deduction based on a set of characteristic model take only linear time thus matching the performance using horn theory more surprisingly abduction can be performed in polynomial time using a set of characteristic model whereas abduction using horn theory is np complete 
in this paper we develop a conceptual framework in which act of manipulation are undertaken for the sake of perceiving material within this frame work we disambiguate different material by ac tively contacting and probing them and by sen ing the resulting force displacement and sound we report experimental result from four separate implementation of this framework using a vari ety of sensory modality including force vision and audition for each implementation we iden tify sensor derived measure that are diagnostic of material property and use those measure to cate gorize object by their material class based on the experimental result we conclude that the issue of shape in variance is of critical importance for future work 
we describe the reinforcement learning problem motivate algorithmswhich seek an approximation to the q function and presentnew convergence result for two such algorithm introduction and backgroundimagine an agent acting in some environment at time t the environment is in somestate x t chosen from a finite set of state the agent perceives x t and is allowed tochoose an action a t from some finite set of action the environment then changesstate so that at time t it is 
abstract discrete relaxation is frequently used to com pute the fixed point of a discrete system where is monotonic with respect to some partial order given an appropriate initial value for x discrete relaxation repeat the assignment until a fixed point for is found monotonicity of with respect to is a sufficient but in general not necessary condition for iterative hill climbing tech niques such a discrete relaxation to find the fixed point of inthis paper we introduce monotonic asynchronous iteration a a novel way of imple menting parallel discrete relaxation in prob lem domain for which monotonicity is a necessary condition this is an optimistic technique that maintains monotonicity without limiting concurrency resulting in good parallel perfor mance we illustrate this technique with the parallel implementation of a constraint satisfac tion system that computes globally consistent solution and present performance number for experiment on a shared memory implementa tion the performance number show that it is indeed possible to obtain a reasonable speedup when parallelizing global constraint satisfac tion we believe that monotonic asynchronous 
machine learning approach to knowledge compilation seek to improve the perfonnance of problem solver by storing solution to previously solved problem in an efficient generalized fonn the problem solver retrieves these learned solution in appropriate later situation to obtain result more efficiently however by relying on it learned knowledge to provide a solution the problem solver may miss an alternative solution of higher quality one that could have been generated using the original non learned problem solving knowledge this phenomenon is referred to a the itulsking effect of learning in this paper we examine a sequence of possible solution for the masking effect each solution refines and build on the previous one the fmal solution is based on cascaded filter when learned knowledge is retrieved these filter alert the system about the inappropriateness of this knowledge so that the system can then derive a better alternative solution we analyze condition under which this solution will perfonn better than the others and present experimental data supportivt of the analysis this investigation is based on a simulated robot domain called groundworld 
additive fashion like in mycin type expert system shortliffe sh the main idea is to allow the abthis paper present decision committee a sence of an underlying rdering in the decision p rodecision committee contains rule each of cedure unlike t e ordering of liter l s m a decision these beeing a couple monomial vector tree or the ordering of rule m a decision list and to each monomial is a condition that when take advantage of multiple knowledge gam gam matched by an instance return it vector kononenko and kovacic kk when each monomi l is tested the sum of decision committee may be viewed a a generalizat e re urned ctors is used to take the clastion of threshold function bruck bru in the sification decision we show h t for eve y multiclass case they are also a very special case constant k the subclass of decision commltof neural network where there would be constraint tee whose element have monomial of length over the activation function and the network architec k is pac learnable and that it properly conture the idea of combining the decision of rule is not tains k dl howe ver we also show th t the new see for example nilsson nii bongard bon problem of mducmg the shortest consistent quinqueton and sallantin q cestnik and bratko decision c mmittee is np hard t s lead cb gam gam gascuel gas kononenko to theore tlcal re llts o non learnabii ty and and kovacic kk decision committee can be to negative consideration for practical opviewed a a formalism that allows to express class of timization proble s on dec sion comm ittees concept using shared knowledge such a sum of disa two stage heuristic algori hm idc is pretribution voting method of kononenko and kovacic sen e that lea n by a particular subclass of kk voting method of gascuel gas our aim decision committee it first chooses monois to use this formalization to provide theoretical pacmials by a breadth first earch inspire d from learn ability result regarding class of decision combranch and bound algorithm then it clusmittees having practical and theoretical relevance ters gradually the resulting rule to form deci sion committee according to the minimizaone essenti al problem m s ystems that use shared tion of empirical risk finally it selects the knowledge is how to combme rule t class f exdecision committee over the final population amples if we restrict the monomials to smgle which is the best according to the learnin mo otono l te al this pro blem is s bsum d by that sample experimental result on artiof linear discrimmation whlthout this restrictlo n t e ficial and real domain tend to show that problem appears to be harder and a good practical idc achieves good result while constructing lustr i on of it d ifliculty is given by kononen o and small and interpretable decision committee ovaclc kk nd ed they observe hat their voting and sum of distribution method give bad or unstable result we propose a two stage algorithm 
since it inception artificial intelligence ha relied upon a theoretical foundation centred around perfect rationality a the desired property of intelligent system we argue a others have done that this foundation is inadequate because it imposes fundamentally unsatisfiable requirement a a result there ha arisen a wide gap between theory and practice in ai hindering progress in the field we propose instead a property called bounded optimality roughly speaking an agent is bounded optimal if it program is a solution to the constrained optimization problem presented by it architecture and the task environment we show how to construct agent with this property for a simple class of machine architecture in a broad class of real time environment we illustrate these result using a simple model of an automated mail sorting facility we also define a weaker property asymptotic bounded optimality abo that generalizes the notion of optimality in classical complexity theory we then construct universal abo program i e program that are abo no matter what real time constraint are applied universal abo program can be used a building block for more complex system we conclude with a discussion of the prospect for bounded optimality a a theoretical basis for ai and relate it to similar trend in philosophy economics and game theory 
we extract from sentence a superstructure made of argumentative operator and connective applying to the remaining set of terminal sub sentence we found the argumentative interpretation of utterance on a semantics defined at the linguistic level we describe the computation of this particular semantics based on the constraint that the superstructure impels to the argumentative power of terminal subsentences 
differentiation between the node of a competitive learning network is conventionally achieved through competition on the basis of neural activity simple inhibitory mechanism are limited to sparse representation while decorrelation and factorization scheme that support distributed representation are computationally unattractive by letting neural plasticity mediate the competitive interaction instead we obtain diffuse nonadaptive alternative for fully distributed representation we use this technique to simplify and improve our binary information gain optimization algorithm for feature extraction schraudolph and sejnowski the same approach could be used to improve other learning algorithm 
several practical inductive logic programming system efficiently learn determinate clause of constant depth recently it ha been shown that while nonrecursive constant depth determinate clause are pat learnable most of the obvious syntactic generalization of this language are not pat learnable in this paper we introduce a new restriction on logic program called locality and present two formal result first the language of nonrecursive clause of constant locality is paclearnable second the language of nonrecursive clause of constant locality is strictly more expressive than the language of nonrecursive determinate clause of constant depth hence constantlocality clause are a pat learnable generalization of constant depth determinate clause a they suggest that efficient general purpose learning algorithm for non determinate or arbitrary depth clause may be difficult to find in this paper we introduce a restriction on logic program called locality and present two new formal result first we show that clause of constant locality are pat learnable second we show that the language of clause of constant locality is strictly more expressive than the language of determinate clause of constant depth hence the language of constant locality clause is a pat learnable generalization of the language of constant depth determinate clause 
in this paper we investigate the simple logical property of context we describe both the syntax and semantics of a general propositional language of context and give a hilbert style proof system for this language a propositional logic of context extends classical propositional logic in two way firstly a new modality ist k is introduced it is used to express that the sentence hold in the context k secondly each context ha it own vocabulary i e a set of propositional atom which are defined or meaningful in that context the main result of this paper are the soundness and completeness of this hilbert style proof system we also provide soundness and completeness result i e correspondence theory for various extension of the general system 
the functionality of system that extract information from text can be specified quite simply the input is a stream of text and the output is some representation of the information to be extracted hence the problem of template design is an instance of the problem of knowledge representation in particular it is the problem of representing essential fact about situation in a way that can mediate between text that describe those situation and a variety of application that involve reasoning about them the research on which we report here is directed at elucidating principle of template design and at compiling these with example in a manual for template designer 
the standard approach in al to knowledge representation is to represent an agent s knowledge symbolically a a collection of formula which we can view a a knowledge base an agent is then said to know a fact if it is provable from the formula in his knowledge base halpem and vardi advocated a model theoretic approach to knowledge representation in this approach the key step is representing the agent s knowledge using an appropriate semantic model here we model knowledge base operationally a multi agent system our result show that this approach offer significant advantage 
since the first shape from shading technique wa developed by horn in the early s different approach have been continuously emerging in the past two decade some of them improve existing technique while others are completely new approach however there is no literature on the comparison and performance analysis of these technique this is exactly what is addressed in this paper introduction shape from shading sfs deal with the recovery of shape from a gradual variation of 
we present a method for learning heuristic employed by an automated proverto control it inference machine the hub of the method is the adaptation of theparameters of a heuristic adaptation is accomplished by a genetic algorithm the necessary guidance during the learning process is provided by a proof problemand a proof of it found in the past the objective of learning consists infinding a parameter configuration that avoids redundant effort w r t this problemand the particular proof 
a major issue in case based system is retrievingthe appropriate case from memory to solve a given problem this implies that a case should be indexed appropriately when stored in memory a case based system being dynamic in that it store case for reuse need to learn indi ce for the new knowledge a the system designer cannot envision that knowledge irrespective of the type of indexing structural or functional a hierarchical organization of the case memory raise two distinct but rela ted issue in index learning learning the indexing vocabularyand learning the right level of generalization in this paper we show how structure behavior function sbf model help in learning structural index to design case in the domain of physical device the sbf model of a design provides the functional and causal explanation of how the structure of the design delivers it function we describe how the sbf model of a design provides both the vocabulary for structural indexing of design case and the inductive bias for index generalizat ion we further discus how model based learning can be integrated with similarity based learning that use s prior design case for learning the level of index generalization 
the problem of interpolating between specified image in an image sequence is a simple but important task in model based vision we describe an approach based on the abstract task of manifold learning and present result on both synthetic and real image se quences this problem arose in the development of a combined lip reading and speech recognition system 
we propose a way of using boolean circuit to perform real valuedcomputation in a way that naturally extends their boolean functionality the functionality of multiple fan in threshold gate inthis model is shown to mimic that of a hardware implementationof continuous neural network a vapnik chervonenkis dimensionand sample size analysis for the system is performed giving bestknown sample size for a real valued neural network experimentalresults confirm the conclusion that the sample 
this paper compare two method for refininguncertain knowledge base using propositionalcertainty factor rule the firstmethod implemented in the rapture system employ neural network training to refinethe certainty of existing rule but usesa symbolic technique to add new rule thesecond method based on the one used inthe kbann system initially add a completeset of potential new rule with very low certaintyand allows neural network training tofilter and adjust these rule 
time varying multispectral observation of cloud from meteorological satellite are used to estimate cloud top height structure and cloud wind semi fluid motion stereo image pair over several time step were acquired by two geostationary satellite with synchronized scanning instrument cloud top height estimation from these image pair is performed using an improved automatic stereo analysis algorithm on a massively parallel maspar computer with k processor a new category of motion behavior known a semi fluid motion is described for modeling cloud motion and an automatic algorithm for extracting semi fluid motion is developed to track cloud wind the time sequential dense estimate of cloud top height depth map in conjunction with intensity data are used to estimate local semi fluid motion parameter for cloud tracking both stereo disparity and motion correspondence are estimated to sub pixel accuracy the interactive image spreadsheet iis is a new versatile visualization tool that wa enhanced to analyze and visualize the result of the stereo analysis and semi fluid motion estimation algorithm experimental result using time varying data of the visible channel from two satellite in geosynchronous orbit is presented for the hurricane frederic 
a practical method for bayesian training of feed forward neural network using sophisticated monte carlo method is presented and evaluated in reasonably small amount of computer time this approach outperforms other state of the art method on datalimited task from real world domain 
we propose a translation approach from modal logic to first order predicate logic which combine advantage from both the standard relational translation and the rather compact functional translation method and avoids many of their respective disadvantage exponential growth versus equality handling in particular in the application to serial modal logic it allows considerable simplification such that often even a simple unit clause suffices in order to express the accessibility relation property although we restrict the approach here to first order modal logic theorem proving it ha been shown to be of wider interest a e g sorted logic or terminological logic 
the traditional goal of computer vision to reconstruct or recover property of the scene ha recently been challenged by advocate of a new purposive approach in which the vision problem is defined in term of the goal of an active agent in the starkest light the debate can be characterized a one about the role of explicit representation the extreme traditionalist strive for a detailed representation of the d world while the other extreme adopts a strict behaviorist stance which eschews representation in favor of direct sensing this panel will explore the role of action representation and purpose in computer vision and in doing so will hopefully discover area of agreement 
the outline and concept of the new year japanese project called real world computing program will be introduced it is the successor to the fifth generation computer project and aim at theoretical and technological foundation for human like flexible information processing and intelligence toward the highly information based society of the st century 
the connection between vision and natural language system in ai research relies on what is often called reference semantics in the situation of a radio reporter for soccer game an utterance must be perceptually anchored and coherent in order to be understandable to a listener not able to see the scene accordingly the speaker must be able to anticipate the listener understanding by mean of mental image in this paper we demonstrate the comparison of mental image and visual perception on the level of spatial relation and show how to employ the result for cooperatively filling optional deep case and for controlling the use of underspecific definite description 
sequel is a new generation functional programming language which allows the specification of type in a notation based on the sequent calculus the sequent calculus notation suffices for the construction of type for type checking and for the specification of arbitrary logic compilation technique derived from both functional and logic programing are used to derive high performance atp from these specification 
the paper present a model of image segmentation that can distinguish foreground from background purely on the basis of motion information the main processing step involved are detection of motion boundary and analysis of figure ground relationship the proposed model utilizes the observation that in kinetic occlusion motion boundary typically display mixture motion information and foreground surface tend to move with motion boundary through distributed probabilistic modeling these constraint can be embedded into computation with efficient network representation the resulting network use spatiotemporal gabor filter a front end and are suitable for parallel distributed processing we demonstrate the application of the model in the decomposition of moving image into surface according to depth 
finding and removing outlier is an important problem in data mining error in large database can be extremely common so an important property of a data mining algorithm is robustness with respect to error in the database most sophisticated method in machine learning address this problem to some extent but not fully and can be improved by addressing the problem more directly in this paper we examine c a decision tree algorithm that is already quite robust few algorithm have been shown to consistently achieve higher accuracy c incorporates a pruning scheme that partially address the outfier removal problem in our robust c algorithm we extend the pruning method to fully remove the effect of outlier and this result in improvement on many database 
we present a machine vision system in which segmentation is computed in conjunction with a structural description of object in the scene it is assumed that contrast edge capture all relevant object information the principle which dictate how edge feature are grouped to infer object are based upon detecting symmetrical enclosing edge configuration these are detected using annular operator applied at multiple scale to edge data which have been extracted at multiple scale from a gray level image the subsequent grouping of symmetry point result in a set of part which make it possible to identify the location of object within an image these part are used a a basis for constructing coarse graph based descriptor for the perceptually significant object found in the scene result are presented to illustrate the method s performance on several image 
abstract this paper present texttiling a method for partitioning full length text document into coherent multiparagraph unit the layout of text tile is meant to reflect the pattern of subtopics contained in an expository text the approach us lexical analysis based on tf idf an information retrieval measurement to determine the extent of the tile incorporating thesaural information via a statistical disambiguation algorithm the tile have been found to correspond well to human judgement of the major subtopic boundary of science magazine article 
interpreting fully natural speech is an important goal for spoken language understanding system however while corpus study have shown that about of spontaneous utterance contain self correction or repair little is known about the extent to which cue in the speech signal may facilitate repair processing we identify several cue based on acoustic and prosodic analysis of repair in a corpus of spontaneous speech and propose method for exploiting these cue to detect and correct repair we test our acoustic prosodic cue with other lexical cue to repair identification and find that precision rate of and recall of can be achieved depending upon the cue employed from a prosodically labeled corpus 
in this paper we describe a fast algorithm for aligning sentence with their translation in a bilingual corpus existing efficient algorithm ignore word identity and only consider sentence length brown et al b gale and church our algorithm construct a simple statistical word to word translation model on the fly during alignment we find the alignment that maximizes the probability of generating the corpus with this translation model we have achieved an error rate of approximately on canadian hansard data which is a significant improvement over previous result the algorithm is language independent 
we describe a case study in data mining for personal loan evaluation performed at the abn amro bank in the netherlands historical data of client and their pay back behaviour are used to learn to predict whether a client will default or not it is shown that due to the pre selection by a credit scoring system the data base is a sample from a different population than the bank is actually interested in this necessarily restricts inference a well furthermore we point out the importance of integrity and consistency checking when the data are entered into the system noise is a serious problem the actual experimental comparison involves a classical statistical method linear discriminant analysis and the classification tree algorithm c both method use one and the same training set drawn from the historical database to learn a classification function the percentage of correct classification on an independent test set are and respectively mcnemar s test show that the null hypothesis of equal performance ha a p value of the classification tree constructed by c us out of attribute to distinguish between defaulter and non defaulter and is consistent with the available theory on credit scoring the linear discriminant function us variable to make the classification both from the viewpoint of predictive accuracy and comprehensibilty the classification tree performs better in this study to make furhter progress the level of noise in the data ha to be reduced and data ha to be collected on loan that are rejected by the credit scoring system 
using the method demonstrated in this paper a robot with an unknown sensorimotor system can learn set of feature and behavior adequate to explore a continuous environment and abstract it to a finitestate automaton the structure of this automaton can then be learned from experience and constitutes a cognitive map of the environment a generate andtest method is used to define a hierarchy of feature defined on the raw sense vector culminating in a set of continuously differentiable local 
we present an implemented unification based parser for relational grammar developed within the stratified feature grammar sfg framework which generalizes kasper round logic to handle relational grammar analysis we first introduce the key aspect of sfg and a lexicalized graph based variant of the framework suitable for implementing relational grammar we then describe a head driven chart parser for lexicalized sfg the basic parsing operation is essentially ordinary feature structure unification augmented with an operation of label unification to build the stratified feature characteristic of sfg 
this paper present a physic based approach for recoveringthe d shape and tracking the motion of nonrigid objectsusing a d elastically deformable balloon model theballoon model is based on a thin plate under tension splinewhich deforms to fit visual data according to internal forcesstemming from the elastic property of the surface and externalforces which are produced from the data we employthe finite element method to represent the model a a continuoussurface we use a 
there are many formal approach to specifying how the mental state of an agent entail that it perform particular action these approach put the agent at the center of analysis for some question and purpose it is more realistic and convenient for the center of analysis to be the task environment domain or society of which agent will be a part this paper present such a task environment oriented modeling framework that can work hand in hand with more agent centered approach our approach feature careful attention to the quantitative computational interrelationship between task to what information is available and when to update an agent s mental state and to the general structure of the task environment rather than single instance example this framework avoids the methodological problem of relying solely on single instance example and provides concrete meaningful characterization with which to state general theory task environment model built in our framework can be used for both analysis and simulation to answer question about how agent should be organized or the effect of various coordination algorithm on agent behavior this paper is organized around an example model of cooperative problem solving in a distributed sensor network 
an extended version of the dual constraint model of motor end plate morphogenesis is presented that includes activity dependent and independent competition it is supported by a wide range of recent neurophysiological evidence that indicates a strong relation ship between synaptic e cacy and survival the computational model is justi ed at the molecular level and it prediction match the developmental and regenerative behaviour of real synapsis 
an application of reinforcement learning to a linear quadratic differential game is presented the reinforcement learning system us a recently developed algorithm the residual gradient form of advantage updating the game is a markov decision process mdp with continuous time state and action linear dynamic and a quadratic cost function the game consists of two player a missile and a plane the missile pursues the plane and the plane evades the missile the reinforcement learning algorithm for optimal control is modified for differential game in order to find the minimax 
a technique is presented for computing d scene structure from point and line feature in monocular image sequence unlike previous method the technique guarantee the completeness of the recovered scene ensuring that every scene feature that is detected in each image is reconstructed the approach relies on the presence of four or more reference feature whose correspondence are known in all the image under an orthographic or affine camera model the parallax of the reference feature provides constraint that simplify the recovery of the rest of the visible scene an efficient recursive algorithm is described that us a unified framework for point and line feature the algorithm integrates the task of feature correspondence and structure recovery ensuring that all reconstructible feature are tracked in addition the algorithm is immune to outlier and feature drift two weakness of existing structure from motion technique experimental result are presented for real image 
suppose we have a feature system and we wish to add default value in a well defined way we might start with kasper round logic and use reiter s example to form it into a default logic giving a node a default value would be equivalent to saying if it is consistent for this node to have that value then it doe then we could use default theory to describe feature structure the particular feature structure described would be the structure that support the extension of the default theory this is in effect what the theory of nonmonotonic sort give you this paper describes how that theory derives from what is described above 
the evidence approximationit ha recently become popular to consider the problem of training neural net from abayesian viewpoint buntine and weigend mackay the usual way of doingthis start by assuming that there is some underlying target function f from rnto r parameterizedby an n dimensional weight vector w we are provided with a training set lof noise corrupted sample of f our goal is to make a guess for w basing that guess onlyon l now assume we have i i d 
abstract in this paper we consider speech coding a a problem of speech modelling in particular prediction of parameterised speech over short time segment is performed using the hierarchical mixture of expert hme jordan jacob the hme give two ad vantage over traditional non linear function approximators such a the multi layer perceptron mlp a statistical understand ing of the operation of the predictor and provision of information about the performance of the predictor in the form of likelihood information and local error bar these two issue are examined on both toy and real world problem of regression and time series prediction in the speech coding context we extend the principle of combining local prediction via the hme to a vector quantiza tion scheme in which xed local codebooks are combined on line for each observation 
we consider the solution to large stochastic control problem by mean of method that rely on compact representation and a variant of the value iteration algorithm to compute approximate costto go function while such method are known to be unstable in general we identify a new class of problem for which convergence a well a graceful error bound are guaranteed this class involves linear parameterizations of the cost to go function together with an assumption that the dynamic programming operator is a contraction with respect to the euclidean norm when applied to function in the parameterized class we provide a special case where this assumption is satisfied which relies on the locality of transition in a state space other case will be discussed in a full length version of this paper 
a new technique exploiting d correlation of d or even d patch between successive frame may be sufficient to compute an estimation of optical flow field sparse measurement are used to compute qualitative property of the flow for different visual task we can combine our technique with a scheme for detecting expansion or rotation in an algorithm which suggests interesting biological implication the algorithm provides a rough estimate of time to crash it is well suited for vlsi implementation it wa tested on real image sequence we show it performance and compare the result to previous approach 
an active area of research in machine learning is learning logic program from example this paper investigates formally the problem of learning a single horn clause we focus on generalization of the language of constant depth determinate clause which is used by several practical learning system we show first that determinate clause of logarithmic depth are not learnable next we show that learning indeterminate clause with at most k indeterminate variable is equivalent to learning dnf finally we show that recursive constant depth determinate clause are not learnable our primary technical tool is the method of predictionpreserving reducibilities introduced by pitt and warmuth a a consequence our result are independent of the representation used by the learning system 
a knowledge representation server is described which provides a fast memory efficient and principled system component modeling the server through intensional algebraic semantics lead naturally to an open architecture class library into which new data type may be plugged in a required without change to the basic deductive engine it is shown that the operation of an existing knowledge representation system classic may be implemented through one data type supporting set with upper and lower set and cardinality bound the architecture developed is cleanly layered by complexity of inference so that fast propagation of constraint is separated from potentially slow model checking search client program may obtain estimate of the complexity of a request and may control the resource allocated to it complete solution 
a technique for representing and learning smooth nonlinear manifold is presented and applied to several lip reading task given a set of point drawn from a smooth manifold in an abstract feature space the technique is capable of determining the structure of the surface and of finding the closest manifold point to a given query point we use this technique to learn the space of lip in a visual speech recognition task the learned manifold is used for tracking and extracting the lip for interpolating between frame in an image sequence and for providing feature for recognition we describe a system based on hidden markov model and this learned lip manifold that significantly improves the performance of acoustic speech recognizers in degraded environment we also present preliminary result on a purely visual lip reader 
difficult concept arise in many complex formative or poorly understood real world domain high interaction among the data attribute cause problem for many learning algorithm including greedy decision tree builder extension of basic method and even backpropagation and mar a new algorithm lfc us directed lookahead search to address feature interaction improving hypothesis accuracy at reasonable cost lfc also address a second problem the general verbosity or global replication problem the algorithm cache search information a new feature for decision tree construction the combination of these two design factor lead to improved prediction accuracy concept compactness and noise tolerance empirical result with synthetic boolean concept bankruptcy prediction and bond rating show typical accuracy improvement of with lfc over several alternative algorithm in case of moderate feature interaction lfc also explicates latent relationship in the training data to provide useful intermediate concept from the perspective of domain expert 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
in this paper we present a logical framework for defining consistent axiomatizations of planning domain a language to define basic action and structured plan is embedded in a logic this allows general property of a whole plan ning scenario to be proved a well a plan to be formed deductively in particular frame a sertions and domain constraint a invariant of the basic action can be formulated and proved even for complex plan most frame assertion are obtained by purely syntactic analysis in such case the formal proof can be generated in a uniform way the formalism we introduce is especially useful when treating recursive plan a tactical theorem prover the karlsruhe inter active verifier kiv is used to implement this logical framework 
we describe an efficient algorithm for recognizing d object by combining photometric and geometric invariant a photometric property is derived that is invariant to the change of illumination and to relative object motion with respect to the camera and or the lighting source in d space we argue that conventional color constancy algorithm can not be used in the recognition of d object further we show recognition doe not require a full constancy of color rather it only need something that remains unchanged under the varying light condition and pose of the object combining the derived color invariant and the spatial constraint on the object surface we identify corresponding position in the model and the data space coordinate using centroid invariance of corresponding group of feature position test are given to show the stability and efficiency of our approach to d object recognition 
junction of line or edge are important visual cue in various field of computer vision they are characterized by the existence of more than one orientation at one single point the so called keypoint in this work we investigate the performance of highly orientation selective function to detect multiple orientation and to characterize junction a quadrature pair of function is used to detect line a well a edge and to distinguish between them an associated one sided function with an angular periodicity of can distinguish between terminating and non terminating line and edge which constitute the junction to calculate the response of these function in a continuum of orientation and scale a method is used that wa introduced recently by p perona 
we present a physically based deformable modelwhich can be used to track and to analyze non rigidmotion of dynamic structure in time sequence of d or d medical image the model considers anobject undergoing an elastic deformation a a set ofmasses linked by spring where the natural length ofthe spring is set equal to zero and is replaced by a setof constant equilibrium force which characterize theshape of the elastic structure in the absence of externalforces this model ha 
we address the problem of representing and recognizing arbitrarily curved d rigid object when the object may vary in shape and complexity and no restrictive assumption are made about the type of surface on the object we propose a new and general surface representation scheme for recognizing object with free form sculpted surface from range data in this scheme an object is described concisely in term of maximal surface patch of constant shape index these maximal patch are mapped onto the unit sphere via their orientation and aggregated via shape spectral function property such a surface area curvedness and connectivity that capture local and global information are also built into the representation the scheme yield not only a meaningful and rich surface description useful for the recoverability of the object but also a set of powerful indexing primitive for object matching we demonstrate the generality and the effectiveness of our scheme using real range image of complex object we also present result on the categorization of object view based on a novel shape spectral matching technique 
we present a logic which allows u to reason about acting and more specifically about sensing i e action that acquire information from the real world and planning i e action that generate and execute plan of action this logic take into account the fact that a it happens in real system action may fail and provides the ability of reasoning about failure handling in acting sensing and planning we see this work a a first step towards a formal account of system which are able to plan to act plan to sense and plan to plan and therefore to integrate action perception and reasoning 
a major problem associated with geometric hashing and method which have emerged from it is the non uniform distribution of invariant over the hash space this problem can affect the performance of the method significantly finding a good geometric hash function which redistributes the invariant uniformly over the hash space is not easy in this paper a new approach is proposed for alleviating the above problem it is based on the use of an elastic hash table which is implemented a a self organizing feature map neural network sofm nn in contrast to existing approach which try to redistribute the invariant over the hash bin we proceed oppositely spreading the hash bin over the invariant during training the sofm nn resembles an elastic net which deforms over the hash space the objective of the deformation process is to spread more hash bin in hash space area which are heavily occupied and le hash bin in lower density area the advantage of the proposed approach is that it is a process that adapts to the invariant through learning hence it make absolutely no assumption about the statistical characteristic of the invariant and the geometric hash function is actually computed through learning furthermore the well known topology preserving property of the sofm nn guarantee that the computed geometric hash function should be well behaved finally the proposed approach is inherently parallelizable 
dynamic programming provides a methodology to develop planner and controllersfor nonlinear system however general dynamic programming is computationallyintractable we have developed procedure that allow more complex planning andcontrol problem to be solved we use second order local trajectory optimization togenerate locally optimal plan and local model of the value function and it derivative we maintain global consistency of the local model of the value function guaranteeing 
the primary goal of inductive learning is to generalize well that is induce a function that accurately produce the correct output for future input hansen and salamon showed that under certain assumption combining the prediction of several separately trained neural network will improve generalization one of their key assumption is that the individual network should be independent in the error they produce in the standard way of performing backpropagation this assumption may be violated because the standard procedure is to initialize network weight in the region of weight space near the origin this mean that backpropagation s gradient descent search may only reach a small subset of the possible local minimum in this paper we present an approach to initializing neural network that us competitive learning to intelligently create network that are originally located far from the origin of weight space thereby potentially increasing the set of reachable local minimum we report experiment on two real world datasets where combination of network initialized with our method generalize better than combination of network initialized the traditional way 
this paper describes the integration of analogical reasoni ng into general problem solving a a method of learning at the strategy level to solve problem more effectively learnin g occurs by the generation and replay of annotated derivational trace of problem solving episode the problem solver is extended with the ability to examine it decision cycle and accumulate knowledge from the chain of success and failure encountered during it search experien ce instead of investing substantial effort deriving general r ules of behavior to apply to individual decision the analogica l reasoner compiles complete problem solving case that are used to guide future similar situation learned knowledge is flexibly applied to new problem solving situation even if only a partial match exists among problem we relate this work with other alternative strategy learning method and also with plan reuse we demonstrate the effectiveness of the analogical replay strategy by providing empiric al result on the performance of a fully implemented system prodigy analogy accumulating and reusing a large case library in a complex problem solving domain 
simulated annealing sa procedure can potentially yield near optimal solution to many difficult combinatorial optimization problem though often at the expense of intensive computational effort the single most significant source of inefficiency in sa search is it inherent stochasticity typically requiring that the procedure be rerun a large number of time before a near optimal solution is found this paper describes a mechanism that attempt to learn the structure of the search space over multiple sa run on a given problem specifically probability distribution are dynamically updated over multiple run to estimate at different checkpoint how promising a sa run appears to be based on this mechanism two type of criterion are developed that aim at increasing search efficiency a cuto criterion used to determine when to abandon unpromising run and restart criterion used to determine whether to start a fresh sa run or restart search in the middle of an earher run experimental result obtained on a class of complex job shop scheduling problem show that sa can produce high quality solution for this class of problem if run a large number of time and that our learning mechanism can significantly reduce the computation time required to find high quality solution to these problem the result further indicate that the closer one want to be to the optimum the larger the speedup 
in this paper we present a content planning system which take into consideration a user s boredom and cognitive overload our system applies a constraint based optimization mechanism which maximizes a probabilistic function of a user s belief and us a representation of boredom and overload a constraint that affect the possible value of this function further we discus two orthogonal policy for relaxing the parameter of the communication process when these constraint are violated conveying le information or breaking up the material into smaller chunk 
we compare two regularization method which can be used to improve the generalizationcapabilities of gaussian mixture density estimate the first method consistsof defining a bayesian prior distribution on the parameter space we derive em expectation maximization update rule which maximize the a posterior parameterprobability in contrast to the usual em rule for gaussian mixture which maximizethe likelihood function in the second approach we apply ensemble averaging to density 
in recent year various formalization of nonmonotonic reasoning and di rent semantics for normal and disjunctive logic program have been proposed including autoepisttic logic circumscription cwa gcwa ecwa epi mic specfications stable well founded stationary and static semantics of normal and disjunctive logic program in this paper we introduce a simple non monotonic knowledge representation framework which isomorphically contains all of the above mentioned nonmonotonic formalism and semantics a special case and yet is significantly more expressive than each one of these formalism considered individually the new formalism called the autoepidemic logic of minimal belief aelb is obtained by augmenting moore s autoepistemic logic ael with an additional minimal belief operator b which allows u to explicitly talk about minimally entailed formula the existence of such a uniform framework not only result in a new powerful non monotonic formalism 
a new learning algorithm is developed for the design of statisticalclassifiers minimizing the rate of misclassification the method which is based on idea from information theory and analogy tostatistical physic assigns data to class in probability the distributionsare chosen to minimize the expected classification errorwhile simultaneously enforcing the classifier s structure and a levelof quot randomness quot measured by shannon s entropy achievement ofthe classifier structure is 
recently a novel shape representation of general curvedobjects which is suitable for object recognition ha beenproposed it is based on a set of surface curve namedhot curve defined by the locus of point where a linehas high order tangency with the surface these curvesdetermine the structure of an object s image contour andtheir catastrophic change a nat ural correspondencebetween a point in an intensity image and some of thesecurves can be directly established this 
motion based image segmentation becomes inherently ambiguous when apparent motion of different object are locally or globally similar during a period to disambiguate the segmentation temporal coherence between the local image motion at each edge point and the apparent motion of every object is examined over a long sequence the point is grouped into that segment of the object whose apparent motion is temporally most coherent with the local image motion at the point 
we define a gesture to be a sequence of state in a measurement or configuration space for a given gesture these state are used to capture both the repeatability and variability evidenced in a training set of example trajectory the state are positioned along a prototype of the gesture and shaped such that they are narrow in the direction in which the ensemble of example is tightly constrained and wide in direction in which a great deal of variability is observed we develop technique for computing a prototype trajectory of an ensemble of trajectory for defining configuration state along the prototype and for recognizing gesture from an unsegmented continuous stream of sensor data the approach is illustrated by application to a range of gesture related sensory data the two dimensional movement of a mouse input device the movement of the hand measured by a magnetic spatial position and orientation sensor and lastly the changing eigenvector projection coefficient computed from an image sequence 
this paper describes a complete face tracking systemthat interprets human head movement in real time the system combine motion analysis with reliable andefficient object recognition strategy it classifies headmovements a quot yes quot nodding head quot no quot shakinghead or quot nothing quot still head the system s skill allowscontactless man machine interaction thus givingaccess to a number of new application introductionas industrialization proceeds the importance of interaction 
we examine the consistency problem for description of tree based on remote dominance and present a consistency checking algorithm which is polynomial in the number of node in the description despite disjunction inherent in the theory of tree the resulting algorithm allows for description which go beyond set of atomic formula to allow certain type of disjunction and negation 
many data mining algorithm developed recently are based on inductive learning method very few are based on similarity based learning however similarity based learning accrues advantage such a simple representation for concept description low incremental learning cost small storage requirement etc we present a similarity based learning method from database in the context of rough set theory unlike the previous similarity based learning method which only consider the syntactic distance between instance and treat all attribute equally important in the similarity measure our method can analyse the attribute in the database by using rough set theory and identify the relevant attribute to the task attribute we also eliminate superfluous attribute for the task attribute and assign a weight to the relevant attribute according to their significance to the task attribute our similarity measure take into account the semantic information embedded in the database 
given a set of low resolution camera image it is possible to reconstruct high resolution luminance and depth information specially if the relative displacement of the image frame are known we have proposed iterative algorithm for recovering high resolution albedo and depth map that require no a priori knowledge of the scene and therefore do not depend on other method a regard boundary and initial condition the problem of surface reconstruction ha been formulated a one of expectation maximization em and ha been tackled in a probabilistic framework asing markov random field mrf a for the depth map our method is directly recovering surface height without refering to surface orientation whale increasing the resolution by camera jittering conventional statistical model have been coupled with geometrical technique to construct a general model of t he world and the imaging process 
we derive from first principle the basic equation for a few of the basic hidden markov model word tagger a well a equation for other model which may be novel the description in previous paper being too spare to be sure we give performance result for all of the model the result from our best model on an unused test sample from the brown corpus with distinct tag is on the upper edge of reported result we also hope these result clear up some confusion in the literature about the best equation to use however the major purpose of this paper is to show how the equation for a variety of model may be derived and thus encourage future author to give the equation for their model and the derivation thereof 
computation quot tal grossman complex system los alamo presenteda neural network algorithm for finding small minimal cover of hypergraphs thenetwork ha two set of unit the first representing the hyperedges to be coveredand the second representing the vertex the connection between the unit aredetermined by the edge of the incidence graph the dynamic of these two typesof unit are different when the parameter of the unit are correctly tuned thestable state of the 
when a lambertian surface is illuminated by severalchromatic light the surface normal may be recoveredfrom a single color image a robust regression is usedto find the ellipsoid in color space on which at leasthalf the pixel lie then the matrix giving the linearrelationship between the color and the surface normal for non outlier point is found a a root of the ellipsoidquadratic form but this root is recovered only upto an arbitrary rotation an integrability condition canbe 
path consistency algorithm which are polynomial for discrete problem are exponential when applied to problem involving quantitative temporal information the source of complexity stem from specifying relationship between pair of time point a disjunction of interval we propose a polynomial algorithm called ult that approximates path consistency in temporal constralllt satisfaction problem tcsps we compare ult empirically to path consistency and directional path consistency algorithm when used a a preprocessing to backtracking ult is shown to be time more effective then either dpc or pc 
precise segmentation of underlying object in an image is very important especially for biomedical image analysis we present an integrated approach for boundary finding using region and curvature information along with the gradient unlike the previous method where smoothing is enforced by penalizing curvature here the grey level curvature is used a an extra source of information however information fusion may not be useful unless used properly to address that we present result that highlight the pro and con of using the various source of information and indicate when one should get precedence over the others 
neuron learning under an unsupervised hebbian learning rule can perform a nonlinear generalization of principal component analysis this relationship between nonlinear pca and nonlinear neuron is reviewed the stable fixed point of the neuron learning dynamic correspond to the maximum of the statistic optimized under nonlinear pca however in order to predict what the neuron learns knowledge of the basin of attraction of the neuron dynamic is required here the correspon dence between nonlinear pca and neural network break down this is shown for a simple model method of statistical mechanic can be used to find the optimum of the objective function of non linear pca this determines what the neuron can learn in order to find how the solution are partitioned amoung the neuron however one must solve the dynamic 
database often inaccurately identify entity of interest two operation consolidation and link formation which complement the usual machine learning technique that use similarity based clustering to discover classification are proposed a essential component of kdd system for certain application consolidation relates identifier present in a database to a set of real world entity rwe s which are not uniquely identified in the database consolidation may also be viewed a a transformation of representation from the identifier present in the original database to the rwe s link formation construct structured relationship between consolidated rwe s through identifier and event explicitly represented in the database consolidation and link formation are easily implemented a index creation in relational database management system an operational knowledge discovery system identifies potential money laundering in a database of large cash transaction using consolidation and link formation 
we present a data driven protocol and a supporting architecture for communication among cooperating intelligent agent in real time diagnostic system the system architecture and the exchange of information among agent are based on simplicity of agent hierarchical organization of agent and modular non overlapping division of the problem domain these feature combine to enable efficient diagnosis of complex system failure in real time environment with high data volume and moderate failure rate preliminary result of the real world application of this work to the monitoring and diagnosis of complex system are discussed in the context of nasa s interplanetary mission operation 
we consider using machine learning technique to help understand a large software system in particular we describe how learning technique can be used to reconstruct abstract datalog specification of a certain type of database software from example of it operation in a case study involving a large more than one million line of c real world software system we demonstrate that off the shelf inductive logic programming method can be successfully used for specification recovery specifically grende can extract specification for about one third of the module in a test suite with high rate of precision and recall we then describe two extension to grende which improve performance on this task one which allows it to output a set of candidate hypothesis and another which allows it to output specification containing determination in combination these extension enable specification to be extracted for nearly two third of the benchmark module with perfect recall and precision of better than 
an important aspect of partial order planning is the resolution of threat between action and causal link in a plan we present a technique for automatically deciding which threat should be resolved during planning and which should be delayed until planning is otherwise complete in particular we show that many potential threat can be provably delayed until the end that is if the planner can find a plan for the goal while ignoring these threat there is a guarantee that the partial ordering in the resulting plan can be extended to eliminate the threat our technique involves construction of an operator graph that capture the interaction between operator relevant to a given goal decomposition of this graph into group of related threat and postponement of threat with certain property 
we present a program for segmenting text according to the separate event they describe a modular architecture is described that allows u to examine the contribution made by particular aspect of natural language to event structuring this is applied in the context of terrorist news article and a technique is suggested for evaluating the resulting segmentation we also examine the usefulness of various heuristic in forming these segmentation 
a method for localization the act of recognizing the environment is presented the method is based on representing the scene a a set of view and predicting the appearance of novel view by linear combination of the model view the method accurately approximates the appearance of scene under weak perspective projection analysis of this projection a well a experimental result demonstrate that in many case this approximation is s uficient to accurately describe the scene when weak perspective approximation is invalid either a larger number of model can be acquired or an iterative solution to account for the perspective distortion can be employed the method ha several advantage over other approach it us relatively rich representation the representation are d rather than d and localization can be done from only a single d view 
this research explores the interaction of textualand photographic information in document understanding the problem of performing generalpurposevision without apriori knowledge is verydifficult at best the use of collateral informationin scene understanding ha been explored in computervision system that use general scene contextin the task of object identification the workdescribed here extends this notion by defining visualsemantics namely technique for systematically 
motivation for including relational constraint other than equality within grammatical formalism ha come from discontinuous constituency and partially free word order for natural language a well a from the need to define combinatory operation at the most basic level for language with a two dimensional syntax e g mathematical notation chemical equation and various diagramming language this paper present f patr a generalization of the patr ii unification based formalism which incorporates relational constraint expressed a user defined function an operational semantics is given for unification that is an adaptation and extension of the approach taken by ait kaci and nasr it is designed particularly for unification based formalism implemented in functional programming environment such a lisp the application of unification in a chart parser for relational set language is discussed briefly 
we suggest a new approach for the study of the non monotonicity of human commonsense reasoning the two main premise that underlie this work are that commonsense reasoning is an inductive phenomenon and that missing information in the interaction of the agent with the environment may be a informative for future interaction a observed information this intuition is normalized and the problem of reasoning from incomplete information is presented a a problem of learning attribute function over a generalized domain we consider example that illustrate various aspect of the non monotonic reasoning phenomenon which have been used over the year a bench mark for various formalism and translate them into learning to reason problem we demonstrate that these have concise representation over the generalized domain and prove that these representation can be learned efficiently the framework developed suggests an operational approach to studying reasoning that is nevertheless rigorous and amenable to analysis we show that this approach efficiently support reasoning with incomplete information and at the same lime match our expectation of plausible pattern of reasoning in case where other theory do not this work continues previous work in the learning to reason framework and support the thesis that in order to develop a computational account for commonsense reasoning one should study the phenomenon of learning and reasoning together 
we describe a method for evaluating a grammar checking application with hand bracketed par a randomly selected set of sentence wa submitted to a grammar checker in both bracketed and unbracketed format a comparison of the resulting error report illuminates the relationship between the underlying performance of the parser grammar system and the error critique presented to the user 
we apply smith s theory of aspect to german a language without any aspectual marker in particular we try to shed more light on the effect aspect can have on discourse structure and show how english and german behave differently in this respect we furthermore describe how smith s notion of a neutral viewpoint can be helpful for the analysis of discourse in german it turned out that proposal claiming that the german preterite cover the progressive a well a the simple aspect can not sufficiently explain the data presented in this paper b uerle finally we give a situation theoretic approach to formalize smith s intuition following glasbey incorporating allen s interval calculus allen 
reinforcement learning method based on approximating dynamicprogramming dp are receiving increased attention due to theirutility in forming reactive control policy for system embeddedin dynamic environment environment are usually modeled ascontrolled markov process but when the environment model isnot known a priori adaptive method are necessary adaptive controlmethods are often classified a being direct or indirect directmethods directly adapt the control policy 
sri international ha a long tradition in the field of qualitative analysis and control of complex system starting with the development of the early mobile robot shakey more recently we have developed a fuzzy controller for our new platform flakey flakey s controller can pursue strategic goal while operating under condition of uncertainty incompleteness and imprecision this controller includes capability for robust uncertainty tolerating goal directed activity real time reactivity to unexpected contingency e g unknown obstacle blending of multiple goal e g reaching a position while avoiding static and moving obstacle 
an automatic treebank conversion method is proposed in this paper to convert a treebank into another treebank a new treebank associated with a different grammar can be generated automatically from the old one such that the information in the original treebank can be transformed to the new one and be shared among different research community the simple algorithm achieves conversion accuracy of when tested on sentence between two major grammar revision of a large mt system 
collaboration to accomplish common goal necessitate negotiation to share and reach agreement on the belief that agent hold a part of the collaboration negotiation in communication can be simulated by a series of exchange in which agent propose reject counterpropose or seek supporting information for belief they wish to be held mutually in an artificial language of negotiation message display the state of the agent belief dialogue consisting of such message clarify the mean by which agent come to agree or fail to agree on mutual belief and individual intention 
speaker recognition is the identification of a speaker from feature of his or her speech this paper describes the use of decision tree induction technique to induce classification rule that automatically identify speaker in a population of speaker the method described ha a recognition rate of for both text dependent and text independent utterance training time scale linearly with the population size 
accurate estimation of heart wall dense field motion and deformation could help to better understand the physiological process associated with ischemic heart disease and to provide significant improvement in patient treatment we present a new method of estimating left ventricular deformation which integrates instantaneous velocity information obtained within the mid wall region with shape information found on the boundary of the left ventricle velocity information is obtained from phase contrast magnetic resonance image and boundary information is obtained from shape based motion tracking of the endoand cardial boundary the integration take place within a continuum biomechanical heart model which is embedded in a finite element framework we also employ a feedback mechanism to improve tracking accuracy the integration of the two disparate but complementary source overcomes some of the limitation of previous work in the field which concentrate on motion estimation from a single image derived source 
this paper present a simple fast coordination algorithm for the dynamic reorganization of agent in a distributed sensor network dynamic reorganization is a technique for adapting to the current local problemsolving situation that can both increase expected system performance and decrease the variance in performance we compare our dynamic organization algorithm to a static algorithm with lower overhead oneshot refers to the fact that the algorithm only us one meta level communication action the other theme of this paper is our methodology for analyzing complex control and coordination issue without resorting to a handful of single instance example using a general model that we have developed of distributed sensor network environment decker and lesser a we present probabilistic performance bound for our algorithm given any number of agent in any environment that fit our assumption this model also allows u to predict exactly in what situation and environment the performance benefit of dynamic reorganization outweigh the overhead 
attitude is the d rotation between the coordinate system of a known object and that of a sensed portion of it surface combination of the support function of a known object with curvature measurement from a visible surface transform attitude determination into optimization problem that can be solved using standard numerical method previous work using the extended gaussian image egi defined for convex polyhedron is extended to the domain of smooth strictly convex object where the egi becomes equivalent to the second curvature function three dimensional shape matching using the first curvature function is new emphasis is placed on theoretical foundation algorithm development and experimental proof of concept using real object and surface data obtained from an existing photometric stereo system 
ideally pattern recognition machine provide constant outputwhen the input are transformed under a group g of desiredinvariances these invariance can be achieved by enhancing thetraining data to include example of input transformed by elementsof g while leaving the corresponding target unchanged alternatively the cost function for training can include aregularization term that penalizes change in the output when theinput is transformed under the group this paper relates the twoapproaches showing precisely the sense in which the regularizedcost function approximates the result of adding transformedexamples to the training data we introduce the notion of aprobability distribution over the group transformation and usethis to rewrite the cost function for the enhanced training data under certain condition the new cost function is equivalent tothe sum of the original cost function plus a regularizer forunbiased model the regularizer reduces to the intuitively obviouschoice a term that penalizes change in the output when theinputs are transformed under the group for infinitesimaltransformations the coefficient of the regularization term reducesto the variance of the distortion introduced into the trainingdata this correspondence provides a simple bridge between the twoapproaches 
incremental concept learning algorithm using backtracking have to store previous data these data can be ordered by the is more specific than relation using this order only the most informative data have to be stored and the le informative data can be discarded moreover under certain condition some data can be replaced by automatically generated more informative data we investigate some condition for data to be discarded independently of the chosen concept learning algorithm or concept representation language then an algorithm for discarding data is presented in the framework of iterative versionspaces which is a depth first algorithm computing versionspaces a introduced by mitchell we update the datastructures used in the iterative versionspaces algorithm while preserving it most important property 
we address the problem of finding the parametersettings that will result in optimalperformance of a given learning algorithmusing a particular dataset a training data we describe a quot wrapper quot method consideringdetermination of the best parametersas a discrete function optimization problem the method us best first search and crossvalidationto wrap around the basic inductionalgorithm the search explores the spaceof parameter value running the basic algorithmmany time on training 
this paper develops a new class of physic based deformable model which can deform both globally and locally their global parameter are function allowing the definition of new parameterized primitive and parameterized global deformation these new global parameter function improve the accuracy of shape description through the use of a few intuitive parameter such a functional bending and twisting using a physic based approach we convert these geometric model into deformable model that deform due to force exerted from the data point so a to conform to the given dataset we present an experiment involving the extraction of shape and motion of the left ventricle lv of a heart from mri spamm data based on a few global parameter function 
sometimes inferencesmade at somespecifictimeare valid at othertimes too inmodel based diagnosisand monitoring a wellasqualitativesimulationinferencesareoftenre done althoughthey have been performedpreviously wepropose a newmethod for sharingpredictions doneat differenttimes thus mutuallycutting down predictioncosts incurringat different time furthermore we generalizethe technique from sharing prediction across time to sharing predictionsacross time and logicalcontexts assumptionbased truth maintenanceis a form of sharing prediction acrosslogicalcontexts becauseof theclose connectionsto the atm we were able to use it a a mean for implementation we reportempiricalresults on monitoring differentconfigurationsof ballast water tank a used on offshoreplatformsand ship 
the problem of checking for consistency of constraint satisfaction problem csps is a fundamental problem in the field of constraint based reasonning moreover it is a hard problem since satisfiability of csps belongs to the class of npcomplete problem so in freuder freuder gave theoretical result concerning consistency of binary csps two variable per constraint in this paper we proposed an extension to these result to general csp n ary constraint on one hand we define a partial consistency well adjusted to general csps called hyper k consistency on the other hand we proposed a measure of the connectivity of hypergraphs called width of hypergraphs using width of hypergraphs and hyper k consistency we derive a theorem defining a sufficient condition for consistency of general csps 
in this paper we investigate enhancement to an upper classifier a decision algorithm generated by an upper classification method which is one of the clsssification method in rough set theory specifically we consider two enhancement first we present a stepwise backward feature selection algorithm to prepro li c n mn co nf fcmt v x t h c ic imnnrtont hclrs rar buy tjd v y cl i iuiyuyauu a yi lu l y l u y iuuuui rough classification method are incapable of removing superfluous feature we prove that the stepwise backward selection algorithm find a small subset of relevant feature that are ideally sufficient and necessary to define target concept with respect to a given threshold this threshold value indicates an acceptable degradation in the quality of an upper classifier second to make an upper classifier adaptive we associate it with some kind of frequency information which we call incremental information an extended decision table is used to represent an adaptive upper classifier it is also used for interpreting an upper classifier either deterministically or nondeterministically 
although many decision combination method have been proposed most of them did not focus on dependency relationship among classifier id combining multiple decision that make classification performance of combining multiple decision be degraded and biased in case of adding highly dependent inferior classifier to overcome such weakness and obtain robust classification performance the present study used dependency relationship for better combining multiple decision in order to identify dependency relationship by observing output of multiple classifier two method are used on the basis of first order dependency relationship one is to use the concept of mutual information and the other one is to use the concept of statistically measured association the first order dependency identified are used to combine multiple decision using bayesian formalism a number of multiple classifier system are applied to totally uncontrained on line handwritten numeral and the english alphabet recognition the experimental result show that the classification performance of a multiple classifier system is superior to that of individual classifier also they show that considering the dependency relationship outperforms others in accuracy when the highly dependent inferior classifier are added 
we propose a kb testing procedure that us a kads conceptual model cm the set of v d inference path is derived from the inference structure and a high level trace representing the czsrrernt inference path is built using the link established between the cm and the kb the comparison of this trace to the vip can lead to modify either the code or the cm 
an important component of an intelligent tutoring system it for teaching geometry is it capacity to transform a figure into a many different figure a possible yet all of which respect the same underlying logical specification given a logical specification for a figure i a figure can be constructed automatically from the object and property in the spepification and ii once constructed one can transform a figure through displacement of any of it object and still obtain a figure that respect the specification for a student user this feature provides an invaluable tool for graphical exploration and discovery of property induced by the logical specification our problem domain is automatic construction of figure and we address this issue in restricted case using constraint logic programming we present solution to case in which figure can be constructed automatically and in which there is also a natural notion of completeness for our system for this automatic figure construction system we describe an implementation written in prolog iii which make use of both constraint and coroutines provided in the language result of experimentation are also included a well a way in which the system can be extended to handle non restricted case 
qualitative spatial reasoning ha many application insuch diverse area a natural language understanding cognitive mapping and reasoning about the physicalworld we address problem whose solution requireintegrated spatial and dynamic reasoning in this paper we present our spatial representation based on the extremal point of object and show thatthis representation is useful for modeling the spatialextent relative position and orientation of object and in reasoning 
we investigate to what extent texture can be distinguished using conditional markov field and small sample we establish that the least square l estimator is the only reasonable choice for this task and we prove it asymptotic consistency and normality for a general class of random field that include gaussian markov field a a special case the performance of this estimator when applied to textured image of real surface is poor if small box are used spl time or le we investigate the nature of this problem by comparing the behavior predicted by the rigorous theory to the one that ha been experimentally observed our analysis reveal that spl time sample contain enough information to distinguish between the texture in our experiment and that the poor performance mentioned above should be attributed to the fact that conditional markov field do not provide accurate model for textured image of many real surface a more general model that exploit more efficiently the information contained in small sample is also suggested 
gsat is a randomized local search procedure for solving propositional satisfiability problem selman et al gsat can solve hard randomly generated problem that are an order of magnitude larger than those that can be handled by more traditional approach such a the davis putnam procedure gsat also efficiently solves encoding of graph coloring problem n queen and boolean induction however gsat doe not perform a well on handcrafted encoding of block world planning problem and formula with a high degree of asymmetry we present three strategy that dramatically improve gsat s performance on such formula these strategy in effect manage to uncover hidden structure in the formula under consideration thereby significantly extending the applicability of the gsat algorithm 
decision tree have provided a classical mechanism for progressively narrowing down a search from a large group of possibility to a single alternative the structuring of a decision tree is based on a heuristic that maximizes the value of the information gained at each level in the hierarchy decision tree are effective when an agent need to reach the goal of complete diagnosis a quickly a possible and cannot accept a partial solution we present an alternative to the decision tree heuristic which is useful when partial solution do have value and when limited resource may require an agent to accept a partial solution our heuristic maximizes the improvement in the value of the partial solution gained at each level in the hierarchy we term the resulting structure an action based hierarchy we present the result of a set of experiment designed to compare these two heuristic for hierarchy structuring finally we describe some preliminary work we have done in applying these idea to a medical domain surgical intensive care unit sicu patient monitoring 
current environmental monitoring system assume particle to bespherical and do not attempt to classify them a laser based systemdeveloped at the university of hertfordshire aim at classifyingairborne particle through the generation of two dimensionalscattering profile the performance of template matching andtwo type of neural network hypernet and semi linear unit arecompared for image classification the neural network approach isshown to be capable of comparable 
address the problem of constructing a complete surface model of an object using a set of registered range image our approach is based on a dynamic balloon model represented by using a triangulated mesh the vertex in the mesh are linked to their neighboring vertex by spring to simulate the surface tension and to keep the shell smooth unlike other dynamic model proposed by previous researcher our balloon model is driven purely by an applied inflation force towards the object surface from inside the object until the mesh element reach the object surface the system includes an adaptive local triangle mesh subdivision scheme that result in an evenly distributed mesh since our approach is not based on global minimization it can handle complex non star shaped object without relying on a carefully selected initial state or encountering a local minimum problem it also allows u to adapt the mesh surface to change in local surface shape and to handle any hole that are present in the input data by adjusting certain system parameter adaptively and locally we present result on some complex non star shaped object from real range image 
in this paper we discus the different strategy used in comet coordinated multimedia explanation testbed for selecting word with which the user is familiar when picture cannot be used to disambiguate a word or phrase comet ha four strategy for avoiding unknown word we give example for each of these strategy and show how they are implemented in comet 
we present and experimentally evaluate the hypothesis that cooperative parallel search is well suited for hard graph coloring problem near a previously identified transition between underand overconstrained instance we find that simple cooperative method can often solve such problem faster than the same number of independent agent 
the aim of the unmanned ground vehicle ugv project at the university of massachusetts is to develop a system capable of navigating both on road and cross country avoiding obstacle and determining it position using landmark complex problem such a driving can be solved more easily by decomposing them into smaller sub problem solving each sub problem and then integrating the solution in case of an autonomous vehicle the integrated system should be able to react in real time to a changing environment and to reason about way to achieve it goal this paper describes the approach taken on the uma mobile perception laboratory mpl to integrate independent process each solving a particular aspect of the navigation problem into a fully capable autonomous vehicle 
the point distribution model derived by analysing the mode of variation of a set of training example can be a useful tool in machine vision one of the drawback of this approach to date is that the training data is acquired with human intervention where fixed point must be selected by eye from example image a method is described for generating a similar flexible shape model automatically from real image data a cubic b spline is used a the shape vector for training the model large training set are used to generate a robust model of the human profile for use in the labelling and tracking of pedestrian in real world scene furthermore an extended model is described which incorporates direction of motion allowing the extrapolation of direction from shape 
many reported discovery system build discrete model of hidden structure property or process in the diverse field of biology chemistry and physic we show that the search space underlying many well known system are remarkably similar when re interpreted a search in matrix space a small number of matrix type are used to represent the input data and output model most of the constraint can be represented a matrix constraint most notably conservation law and their analogue can be represented a matrix equation typically one or more matrix dimension grow a these system consider more complex model after simpler model fail and we introduce a notation to express this the novel framework of matrix space search serf to unify previous system and suggests how at least two of them can be integrated our analysis constitutes an advance toward a generalized account of model building in science 
situated interactive tutorial instruction give flexibility in teaching task by allowingcommunication of a variety of type of knowledge in a variety of situation toexploit this flexibility however an instructable agent must be able to learn differenttypes of knowledge from different instructional interaction this paper present an approachto learning from flexible tutorial instruction called situated explanation thattakes advantage of constraint in different instructional 
abstract on large problem reinforcement learning system must use parameterized function approximators such a neural network in order to generalize between similar situation and action in these case there are no strong theoretical result on the accuracy of convergence and computational result have been mixed in particular boyan and moore reported at last year s meeting a series of negative result in attempting to apply dynamic programming together with function approximation to simple control problem with continuous state space in this paper we present positive result for all the control task they attempted and for one that is significantly larger the most important dierences are that we used sparse coarse coded function approximators cmacs whereas they used mostly global function approximators and that we learned online whereas they learned oine boyan and moore and others have suggested that the problem they encountered could be solved by using actual outcome rollouts a in classical monte carlo method and a in the td algorithm when however in our experiment this always resulted in substantially poorer performance we conclude that reinforcement learning can work robustly in conjunction with function approximators and that there is little justification at present for avoiding the case of general reinforcement learning and function approximation 
we discus the cost and benefit of usingnatural language nl generation technologyto produce documentation and on line helpmessages and propose some intermediate generation technique that provide many althoughnot all of the benefit of more principled deep generation technique but at a significantlylower cost introductionmost research on natural language nl generation hasstressed what might be called deep technique wheretext is generated in a principled way 
most constructive induction researcher focus only on new boolean attribute this paper report a new constructive induction algorithm called x of n that construct new nominal attribute in the form of x of n representation an x of n is a bet containing one or more attribute value pair for a given instance it value corresponds to the number of it attribute value pair that are true the promising preliminary experimental result on both artificial and real world domain show that constructing new nominal attribute in the form of x of n representation can significantly improve the performance of selective induction in term of both higher prediction accuracy and lower theory complexity 
recently several local hill climbing procedure for propositional satisfiability have been proposed which are able to solve large and difficult problem beyond the reach of conventional algorithm like davis putnam by the introduction of some new variant of these procedure we provide strong experimental evidence to support our conjecture that neither greediness nor randomness is important in these procedure one of the variant introduced seems to offer significant improvement over earlier procedure in addition we investigate experimentally how performance depends on their parameter our result suggest that runtime scale le than simply exponentially in the problem size 
we derive sufficient condition on image structure that permitsdetermination of d motion parameter and depth from motionrelative a rigid surface in front of the camera we assume that only thefirst order spatio temporal derivative or of the image is given and thatthe image intensity is continuously differentiable everywhere or that imagecontours are continuously differentiable this mean that only thecomponent of the image motion field orthogonal to iso intensity contour the so 
qualitative probabilistic network qpns are an abstraction of bayesian belief network replacmg numerical relation by qualitative influence and synergy wellman b to reason in a qpn is to find the effect of new evidence on each node in term of the sign of the change in belief increase or decrease we introduce a polynomial time algorithm for reasoning in qpns based on local sign propagation it extends our previous scheme from singly connected to general multiply connected network unlike existing graph reduction algorithm it preserve the network structure and determines the effect of evidence on all node in the network this aid meta level reasoning about the model and automatic generation of intuitive explanation of probabilistic reasoning 
uniformly textured surface in d scene provide important cue for image understanding texture can be used for both segmentation and for d shape inference unfortunately virtually all current algorithm are based on assumption that make it impossible to do texture segmentation and shape from texture in the same image texture segmentation algorithm rely on an absence of d effect that tend to distort the texture shape from texture algorithm depend on these effect relying instead on the texture being already segmented to really understand texture in image texture segmentation and shape from texture must be viewed a a combined problem to be solved simultaneously we present a solution to this problem with a region growing algorithm that explicitly account for perspective distortion of otherwise uniform texture we use the image spectrogram to compute local surface normal which are in turn used to frontalize the texture these frontalized texture patch are then subjected to a region growing algorithm based on similarity in the local frequency domain and a minimum description length criterion we show result of our algorithm on real texture image taken in the lab and outdoors 
the penn treebank ha recently implemented a new syntactic annotation scheme designed to highlight aspect of predicate argument structure this paper discus the implementation of crucial aspect of this new annotation scheme it incorporates a more consistent treatment of a wide range of grammatical phenomenon provides a set of coindexed null element in what can be thought of a underlying position for phenomenon such a wh movement passive and the subject of infinitival construction provides some non context free annotational mechanism to allow the structure of discontinuous constituent to be easily recovered and allows for a clear concise tagging system for some semantic role 
text classification the grouping of text into several cluster ha been used a a mean of improving both the efficiency and the effective des of text retrieval categorization in this paper we propose a hierarchical clustering algorithm that construct a bet of cluster having the maximum bayesian posterior probability the probability that the given text are classified into cluster we call the algorithm hierarchical bayesian clustering hbc the advantage of hbc are experimentally verified from several viewpoint hbc can reconstruct the original cluster more accurately than do other non probabilistic algorithm when a probabilistic text categorization is extended to a cluster based one the use of hbc offer better performance than doe the use of non probabilistic algorithm 
our analysis combine the concern of signature extraction and signature file organization which have usually been treated a separate issue we also relax the uniform frequency and single term query assumption and provide a comprehensive analysis for multiterm query environment where term can be classified based on their query and database occurrence frequency the performance of three superimposed signature generation scheme is explored a they are applied to one dynamic signature file organization based on linear hashing linear hashing with superimposed signature lh first scheme sm allows all term set the same number of bit regardless of their discriminatory power whereas the second and third method mm and mmm emphasize the term with high query and low database occurrence frequency of these three scheme only mmm take the probability distribution of the number of query term into account in finding the optimal mapping strategy derivation of performance evaluation formula is provided together with the result of various experimental setting suggestion a to how to implement the given technique in real life case are also provided result indicate that mmm outperforms the other method a the gap between the discriminatory power of the term get larger the absolute value of the saving provided by mmm reach a maximum for the high query weight case however the extra saving decline sharply for high weight and moderately for the low weight query with the increase in database size 
current approach for detecting periodic motionassume a stationary camera and place limit on an object s motion these approach rely on the assumptionthat a periodic motion project to a set of periodicimage curve an assumption that fails in general using affine invariance we derive necessary and sufficientconditions for an image sequence to be the projectionof a periodic motion no restriction are placedon either the motion of the camera or the object ouralgorithm is shown to 
in this paper we present a method to group adjective according to their meaning a a first step towards the automatic identification of adjectival scale we discus the property of adjectival scale and of group of semantically related adjective and how they imply source of linguistic knowledge in text corpus we describe how our system exploit this linguistic knowledge to compute a measure of similarity between two adjective using statistical technique and without having access to any semantic information about the adjective we also show how a clustering algorithm can use these similarity to produce the group of adjective and we present result produced by our system for a sample set of adjective we conclude by presenting evaluation method for the task at hand and analyzing the significance of the result obtained 
we define the probabilistic planning problem in termsof a probability distribution over initial world state a boolean combination of goal proposition a probabilitythreshold and action whose effect depend onthe execution time state of the world and on randomchance adopting a probabilistic model complicatesthe definition of plan success instead of demandinga plan that provably achieves the goal we seek planswhose probability of success exceeds the threshold this paper describes 
we address the recovery of segmented d description of an object from intensity image we use three view of an object from slightly different viewpoint a our input for each image we extract a hierarchy of group based on proximity parallelism and symmetry in a robust manner the group in the three image are matched by computing the epipolar geometry for each set of matched group from the three image we then label the contour of the group a true or limb edge using the information about group the label associated with their contour and projective property of subclass of generalized cylinder we infer the d structure of these group the proposed method not only allows robust shape recovery but also produce segmented part our approach can also deal with group generated a a result of texture or shadow on the object we present result on real image of moderately complex object 
we describe a multilingual implementation of such a grammar and it advantage over both principlebased parsing and ad hoc grammar design we show how x bar theory and language independent semantic constraint facilitate grammar development our implementation includes innovative handling of syntactic gap logical structure alternation and conjunction each of these innovation enhances performance in both large scale and multilingual natural language processing application phrase structure grammar are hardly new the novelty in this paper come from the use of practical guideline and real number based on our experience with three language and ten of thousand of text the issue of grammar design is worth revisiting because of the increasing bifurcation between semantic phrase grammar on thv one hand and principle based parsing in toy domain on the other semantic grammar are brittle and must be rewritten for each new domain and language principle based parsing is not yet mature enough for our application we offer an extensible multilingual application of the traditional approach that extends theoretical linguistic insight to industrial strength data 
the minimum description length principle mdl can be used totrain the hidden unit of a neural network to extract a representationthat is cheap to describe but nonetheless allows the input tobe reconstructed accurately we show how mdl can be used todevelop highly redundant population code each hidden unit hasa location in a low dimensional implicit space if the hidden unitactivities form a bump of a standard shape in this space they canbe cheaply encoded by the center of 
we present an efficient procedure for cost based abduction which is based on the idea of using chart parser a proof procedure we discus in detail three feature of our algorithm goal driven bottom up derivation tabulation of the partial result and agenda control mechanism and report the result of the preliminary experiment which show how these feature improve the computational efficiency of cost based abduction 
this paper present neurochess a program which learns to play chess from the final outcome of game neurochess learns chess board evaluation function represented by artificial neural network it integrates inductive neural network learning temporal differencing and a variant of explanation based learning performance result illustrate some of the strength and weakness of this approach 
this paper describes an automatic indexing procedure that us the is a relation contained within wordnet and the set of noun contained in a text to select a sense for each plysemous noun in the text the result of the indexing procedure is a vector in which some of the term represent word sens instead of word stem retrieval experiment comparing the effectivenss of these sense based vector v stem based vector show the stem based vector to be superior overall although the sense based vector do improve the performance of some query the overall degradation is due in large part to the difficulty of disambiguating sens in short query statement an analysis of these result suggests two conclusion the is a link define a generalization specialization hierarchy that is not sufficient to reliably select the correct sense of a noun from the set of fine sense distinction in wordnet and missing correct match because of incorrect sense resolution ha a much more deleterious effect on retrieval performance than doe making spurious match 
although backpropagation neural network generally predict better than decision tree do for pattern classification problem they are often regarded a black box i e their prediction are not a interpretable a those of decision tree this paper argues that this is because there ha been no proper technique that enables u to do so with an algorithm that can extract rule by drawing parallel with those of decision tree we show that the prediction of a network can be explained via rule extracted from it thereby the network can be understood experiment demonstrate that rule extracted from neural network are comparable with those of decision tree in term of predictive accuracy number of rule and average number of condition for a rule they preserve high predictive accuracy of original network 
this paper present a method for inducing the part of speech of a language and part of speech label for individual word from a large text corpus vector representation for the part of speech of a word are formed from entry of it near lexical neighbor a dimensionality reduction creates a space representing the syntactic category of unambiguous word a neural net trained on these spatial representation classifies individual context of occurrence of ambiguous word the method classifies both ambiguous and unambiguous word correctly with high accuracy 
the video demonstrates research accomplishment in interactive design and assembly with d computer graphic environment agent technique and dynamic knowledge representation technique are used to process qualitative verbal instruction to quantitative scene change a key idea is to exploit situated perceptive information by inspecting the computer graphic scene model 
an incremental network model is introduced which is able to learnthe important topological relation in a given set of input vector bymeans of a simple hebb like learning rule in contrast to previousapproaches like the quot neural gas quot method of martinetz and schulten this model ha no parameter which change over timeand is able to continue learning adding unit and connection untila performance criterion ha been met application of the modelinclude vector quantization 
the use of primary effect in planning is an effective approach to reducing search the underlying idea of this approach is to select certain important effect among the effect of each operator and to use an operator only for achieving it important effect in the past there ha been little analysis of planning with primary effect and few experimental result we provide empirical and analytical result on the use of primary effect first we experimentally demonstrate that the use of primary effect may lead to an exponential reduction of the planning time second we analytically ex plain the experimental result and identify the factor that influence the efficiency of planning with primary effect third we describe an application of our analysis to predicting the performance of a planner for a given selection of primary effect 
we present a class of region growing algorithmsbased on an analogy to a mass suspended in a field offorces the mass represents the growing region andthe field represents the degree of concordance betweenlocal pixel value and the color characteristic of theregion the algorithm are particularly well suited tosystems that are looking for specific simply connectedshapes in local area of interest for example arectangle of unknown size elongation or orientation comparing 
this paper describes the facility available for knowledge discovery in database using the tetrad ii program while a year or two shy of state of the most advanced research on discovery we believe this program provides the most flexible and reliable suite of procedure so far availabie commercially for discovering causal structure semiautomatically constructing bayes network estimating nmmntnrr in c w k n akmrb nnrl naa nn l h ntrrnrsm lrnn suauiiwly yl u c apiy yxlu yvu qj i ya cu yull also be used to red e the number of variable needed for classification or prediction for example a a neural net preproceesor the theoretical principle on which the program is based are described in detail in spirtes glymour and scheines under assumption described there each of the search and discovery procedure we will describe have been proved to give correct information when statistical decision are made correctly what doe tetrad do this paper describes the facility available for howledge discovery in database using the tetrad ii program while a year or two shy of state of the most advanced research on discovery we believe this program provides the most flexible and reliable suite of procedure so far available commercially for discovering causal structure semiautomatically constructing bayes network estimating parameter in such network and updating the theoretical principle on which the program is based are described in detail in spirtes glymour and scheines under assumption described there each of the search and discovery procedure we will describe have been proved to give correct information when statistical decision about independence and conditional independence research supported by the navy office of personnel research and development and the office of naval research contract n this work is a collaboration with c meek r scheines and p spirtes 
we present the result of an empirical study of severalconstraint satisfaction search algorithm and heuristic using a random problem generator that allows usto create instance with given characteristic we showhow the relative performance of various search methodsvaries with the number of variable the tightnessof the constraint and the sparseness of the constraintgraph a version of backjumping using a dynamicvariable ordering heuristic is shown to be extremely 
this paper investigates alternative estimator of the accuracy of concept learned from example in particular the cross validation and bootstrap estimator are studied using synthetic training data and the foil learning algorithm our experimental result contradict previous paper in statistic which advocate the bootstrap method a superior to cross validation nevertheless our result also suggest that conclusion based on cross validation in previous machine learning paper are unreliable specifically our observation are that i the true error of the concept learned by foil from independently drawn set of example of the same concept varies widely ii the estimate of true error provided by cross validation ha high variability but is approximately unbiased and iii the bootstrap estimator ha lower variability than cross validation but is systematically biased 
traditionally large area of research in machine learning have concentrated on pattern recognition and it application to many diversified problem both within the realm of ai a well a outside of it over several decade of intensified research an array of learning methodology have been proposed accompanied by attempt to evaluate these method with respect to one another on small set of real world problem unfortunately little emphasis wa placed on the problem of learning bias common to all learning algorithm and a major culprit in preventing the construction of a zsniuerscsl pattern recognizer state of the art learning algorithm exploit some inherent bias when performing pattern recognition on yet unseen pattern automatically adapting this learning bias dependent on the type of pattern classification problem seen over time is largely lacking in this paper weakness of the traditional one shot learning environment are pointed out and the move towards a learning method displaying the ability to learn about lecarning is undertaken trans dimensional learning is introduced a a mean to automatically adjust learning bias and empirical evidence is provided showing that in some instance beurning the whole can be simpler than learning a part of it 
the paper attempt to establish a basis upon which it could plausibly be said that knowledge level model typically used in the development of at system such a expert system could have psychological import various modelling methodology are set out and it is shown that these methodology cannot supply psychological explanation of expertise on the basis of ordinary realist assumption about the mind since the knowledge level primitive cannot supply the right sort of link between task and ai method in contrast an anti realist interpretative view of mind is set out and it is shown how ai modelling methodology could in that context be of psychological value finally a short example give some concrete expression to these idea 
in the first part of the paper i present a new treatment of the imperfective paradox dowty for the restricted case of trajectory of motion event this treatment extends and refines those of moens and steedman and jackendoff in the second part i describe an implemented algorithm based on this treatment which determines whether a specified sequence of such event is or is not possible under certain situationally supplied constraint and restrictive assumption 
anytime algorithm whose quality of result improves gradually a computation time increase provide useful performance component for timecritical planning and control of robotic system in earlier work we introduced a compilation scheme for optimal composition of anytime algorithm in this paper we present an implementation of a navigation system in which an off line compilation process and a run time monitoring component guarantee the optimal allocation of time to the anytime module the crucial meta level knowledge is kept in the anytime library in the form of conditional performance profile we also extend the notion of gradual improvement to sensing and plan execution the result is an efficient flexible control for robotic system that exploit the tradeoff between time and quality in planning sensing and plan execution 
belief revision and belief update have been proposed a two type of belief change serving differ ent purpose belief revision is intended to capture change of an agent s belief state reflecting new information about a static world belief update is intended to capture change of belief in response to a changing world we argue that both belief revision and belief update are too restrictive routine belief change involves element of both we present a model for generalized update that allows update in response to external change to inform the agent about it prior belief this model of update combine aspect of revision and update providing a more realistic characterization of belief change we show that under certain assumption the original update postulate are satisfied we also demonstrate that plain revision and plain update are special case of our model in a way that formally verifies the intuition that revision is suitable for static belief change 
in this paper we give a general analysis of dyadic deontir logic that were introduced in the early eventies to formalize deontic reasoning about subideal behavior recently it wa observed that they are closely related to nonmonotonic logic theory of diagnosis and dc cision theory in particular we argue that two type of defeasibihty must be distinguished in a defeasible deontic logic overridden defeasibility that formalizes cancelling of in obligation by other conditional obligation and factual defeasibility that formalizes overshadowing of an obligation by a violating fact we also show that this distinction is essential for an adequate analysis of notorious paradox of deontic logic such a the chisholm and for rester paradox 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality we can construct distributed representation with oriented energy measure used in model of biological vision surface model of orientation velocity and disparity can easily be fit to distributed representation of texture motion and stereo by combining tool of orientation analysis and regularization we describe base representation construction and model fitting process in these domain 
in this paper we investigate the efficiency of subsumption the basic provability relation in ilp a is np complete even if we restrict ourselves to linked horn clause and fix to contain only a small constant number of literal we investigate in several restriction of we first adapt the notion of determinate clause used in ilp and show that subsumption is decidable in polynomial time if is determinate with respect to secondly we adapt the notion of local horn clause and show that subsumption is efficiently computable for some reasonably small we then show how these result can be combined to give an efficient reasoning procedure for determinate local horn clause an ilp problem recently suggested to be polynomial predictable by cohen by a simple counting argument we finally outline how the reduction algorithm an essential part of every lgg ilp learning algorithm can be improved by these idea 
a shape recognition approach is presented uncertainty handling combining and propagation form the heart of the method multiple knowledge source extract information from the segmented image and increase knowledge about undefined shape knowledge source have to be tuned to discriminate shape class and a critical number of independent knowledge source guarantee the classification information provided by the knowledge source is stored in the shafer form of probability mass assignment dempster s rule is used to update belief in class a brief theoretical overview is given combined with a heuristic this method achieves interesting result a well a a short execution time an example derived from an application in the prometheus project consisting of traffic sign recognition on a motorway illustrates this method 
understanding the design of an engineered device requires both knowledge of the general physical principle that determine the behavior of the device and knowledge of what the device is intended to do i e it functional specification however the majority of work in modelbased reasoning about device behavior ha focused on modeling a device in term of general physical principle or intended functionality but not both in order to use both functional and behavioral knowledge in understanding a device design it is crucial that the functional knowledge is represented in such a way that it ha a clear interpretation in term of actual behavior we propose a new formalism for representing device function with well defined semantics in term of actual behavior we call the language cfrl causal functional representation language cfrl allows the specification of condition that a behavior must satisfy such a occurrence of a temporal sequence of expected event and causal relation among the event and the behavior of device component we have used cfrl a the basis for a functional verification program which determines whether a behavior achieves an intended function 
the study of situated system that are capable of reactive and goal directed behaviour ha received increased attention in recent year one approach to the design of such system is based upon agent oriented architecture this approach ha led to the development of expressive but computationally intractable logic for describing or specifying the behaviour of agent oriented system in this paper we present three propositional variant of such logic with different expressive power and analyze the computational complexity of verifying if a given property is satisfied by a given abstract agent oriented system we show the complexity to be linear time for one of these logic and polynomial time for another thus providing encouraging result with respect to the practical use of such logic for verifying agent oriented system 
recent year have seen the increasing development of knowledge discovery or database mining system that combine database management technology with machine learning technique and algorithm to perform the analysis of data many of these system use passive database management system to hold the data rather than active database however application such a battlemanagement situation or stock market trading would benefit from the use of an active database management system since the data is being constantly updated and other action should be triggered based on database event in this paper we present a general description of an active knowledge mining system that combine an active database with machine learning operator we introduce the notion of an intelligent mediator that contains knowledge about both the database and the capability of the learning operator the mediator chooses which of the operator to use to achieve a learning goal then determines how the discovered knowledge should be used and where it should be stored and maintained 
we have written a computer program called trendx for automated trend detection during process monitoring the program us a representation called trend template that define disorder a typical pattern of relevant variable these pattern consist of a partially ordered set of temporal interval with uncertain endpoint attached to each temporal interval are value constraint on real valued function of measurable parameter a trendx receives measured data of the monitored process the program creates hypothesis of how the process ha varied over time we introduce the importance of a distinct trend representation in knowledge based system then we demonstrate how trend template may represent trend that occur at fixed time or at unknown time and their utility for domain that are quantitalively both poorly and well understood finally we present experimental result of trendx diagnosing pediatric growth disorder from height weight bone age and pubertal data of twenty patient seen at boston child s hospital 
we have implemented an incremental lexical acquisition mechanism that learns the meaning of previously unknown word from the context in which they appear a a part of the process of parsing and semantically interpreting sentence implement at ion of this algorithm brought to light a fundamental difference between learning verb and learning noun specifically because verb typically play the predicate role in english sentence whereas noun typically function a argument we found that different mechanism were required to learn verb and noun because of this difference in usage our learning algorithm formulates the most specific hypothesis possible consistent with the data for verb meaning but the most general hypothesis possible for noun subsequent example may falsify a current hypothesis causing verb meaning to be generalized and noun meaning to be made more specific this paper describes the two approach used to learn verb and noun in the system and report on the system s performance in substantial empirical testing resnik yarowsky camille us it domain knowledge when inferring the meaning of unknown word the actual process of meaning inference however is not dependent on any particular domain hierarchy it is a weak method that search the hierarchy for an appropriate node for the meaning of a word 
tracking elementary feature and coherently grouping them is an important problem in computer vision and a real challenging feature extraction problem perceptual grouping technique can be applied to some feature tracking problem such an approach is presented in this paper moreover we show how a perceptual grouping problem can be expressed a a global optimization problem in order to solve it we devise an original neural network called pulsed neural network the specific application concerned here is particle tracking velocimetry in fluid mechanic 
we develop a refined mean field approximation for inference andlearning in probabilistic neural network our mean field theory unlike most doe not assume that the unit behave a independentdegrees of freedom instead it exploit in a principled way theexistence of large substructure that are computationally tractable to illustrate the advantage of this framework we show how toincorporate weak higher order interaction into a first order hiddenmarkov model treating the 
human episodic memory provides a seemingly unlimited storage for everyday experience and a retrieval system that allows u to access the experience with partial activation of their component this paper present a computational model of episodic memory inspired by damasio s idea of convergence zone the model consists of a layer of perceptual feature map and a binding layer a perceptual feature pattern is coarse coded in the binding layer and stored on the weight between layer a partial activation of the stored feature activates the binding pattern which in turn reactivates the entire stored pattern a worst case analysis show that with realistic size layer the memory capacity of the model is several time larger than the number of unit in the model and could account for the large capacity of human episodic memory 
this paper introduces a new measurement robustness to measure the quality of machine discovered knowledge from real world database that change over time a piece of knowledge is robust if it is unlikely to become inconsistent with new database state robustness is different from predictive accuracy in that by the latter the system considers only the consistency of a rule with unseen data while by the former the consistency after deletion and update of existing data is also considered combining robustness with other utility measurement a system can make intelligent decision in learning and maintenance of knowledge learned from changing database this paper defines robustness then present an estimation approach for the robustness of horn clause rule learned from a relational database the estimation approach applies the laplace law of succession which can be efficiently computed the estimation is based on database schema and transaction log no domainspecific information is required however if it is available the approach can exploit it 
in this paper i extend the standard first order resolution method with special reasoning mechanism for sort sort are unary predicate literal built from unary predicate are called sort literal negative sort literal can be compiled into restriction of the relevant variable to sort or can be deleted if they fulfill special condition positive sort literal define the sort theory sorted unification exploit the sort restriction of variable with respect to the sort theory a occurrence of sort literal are not restricted it may be necessary to add additional literal to resolvent and factor and to dynamically change the sort theory used by sorted unification during the deduction process the calculus i propose thus extends the standard resolution method with sorted unification residue literal and a dynamic processing of the sort information i show that this calculus generalizes and improves existing approach to sorted reasoning finally i give some application to automated theorem proving and abduction 
this paper describes a new admissible tree search algorithmcalled iterative threshold search it itscan be viewed a a much simplified version of ma and a generalized version of mrec we alsopresent the following result every node generated by it is also generatedby ida even if it is given no more memory thanida in addition there are tree on which it generates o n node in comparison to o n log n nodesgenerated by ida where n is the number of node 
previous research ha shown that a techniquecalled error correcting output coding ecoc can dramatically improve theclassification accuracy of supervised learningalgorithms that learn to classify datapoints into one of k ae class thispaper present an investigation of why theecoc technique work particularly whenemployed with decision tree learning algorithm it show that the ecoc method like any form of voting or committee canreduce the variance of the learning 
a computational framework is introduced for matching a pair of stereo image which in contrast to existing algorithm feature a self contained local matching module cascaded with a global matching module local matching output a d grey scale image in which each and every point ha an intensity measuring the goodness of a possible match global matching reduces to surface detection in this image to detect the surface it is first enhanced employing a hyperpyramid data structure unlike traditional multiresolution approach which are based on the coarse to fine continuation method the author multilevel method emphasizes a fine to coarse process in which local support is accumulated the algorithm is concise efficient and above all give good result for complex scene 
a given overcomplete discrete oriented pymmid may be converted into a steemble pyramid by interpolation we present a technique for deriving the optimal interpolation function otherwise called steering coeficients the proposed scheme is demonstmted on a computationally efficient oriented pyramid which is a variation on the burt and adelson pyramid we apply the generated steerable pyramid to orientation invarianttexture analysis to demonstrate it excellent rotational isotropy high classijcation rate and precise rotation identification are demonstrated 
we argue that the advent of large volume of full length text a opposed to short text like abstract and newswire should be accompanied by corresponding new approach to information access toward this end we discus the merit of imposing structure on full length text document that is a partition of the text into coherent multi paragraph unit that represent the pattern of subtopics that comprise the text using this structure we can make a distinction between the main topic which occur throughout the length of the text and the subtopics which are of only limited extent we discus why recognition of subtopic structure is important and how to some degree of accuracy it can be found we describe a new way of specifying query on full length document and then describe an experiment in which making use of the recognition of local structure achieves better result on a typical information retrieval task than doe a standard ir measure 
numerical simulation of partial differential equation pdes play a crucial role in predicting the behavior of physical system and in modern engineering design however in order to produce reliable result with a pde simulator a human expert must typically expend considerable time and effort in setting up the simulation most of this effort is spent in generating the grid the discretization of the spatial domain which the pde simulator requires a input to properly design a grid the gridder must not only consider the characteristic of the spatial domain but also the physic of the situation and the peculiarity of the numerical simulator this paper describes an intelligent gridder that is capable of analyzing the topology of the spatial domain and predicting approximate physical behavior based on the geometry of the spatial domain to automatically generate grid for computational fluid dynamic simulator typically gridding program are given a pcsrtitioning of the spatial domain to assist the gridder our gridder is capable of performing this partitioning this enables the gridder to automatically grid spatial domain of arbitrary configuration 
this paper describes the application of a new spatial domain convolution deconvolution transform s transform for determining distance of object and rapid autofocusing of camera system usingimage defocus the method known a stmap involves simple local operation on only two imagestaken with dierent aperture diameter and can be easily implemented in parallel both image canbe arbitrarily blurred and neither of them need to be a focused image taken with a pin hole camera stmap ha 
this paper introduces gnarl an evolutionary program which induces recurrent neural network that are structurally unconstrained in contrast to constructive and destructive algorithm gnarl employ a population of network and us a fitness function s unsupervised feedback to guide search through network space annealing is used in generating both gaussian weight change and structural modification applying gnarl to a complex search and collection task demonstrates that the system is capable of inducing network with complex internal dynamic 
we present a novel motion based approach for the part determination and shape estimation of a human s body part the novelty of the technique is that neither a prior model of the human body is employed nor prior body part segmentation is assumed we present a human body part identification strategy hbpis that recovers all the body part of a moving human based on the spatiotemporal analysis of it deforming silhouette we formalize the process of simultaneous part determination and d shape estimation by employing the supervisory control theory of discrete event system in addition in order to acquire the d shape of the body part we present a new algorithm which selectively integrates the segmented by the hbpis apparent contour from three mutually orthogonal view the effectiveness of the approach is demonstrated through a series of experiment where a subject performs a set of movement according to a protocol that reveals the structure of the human body 
we describe an analog vlsi implementation of the art algorithm carpenter a prototype chip ha been fabricated in a standard low cost m double metal single poly cmos process it ha a die area of cm and is mounted in a pin pga package the chip realizes a modified version of the original art architecture such modification ha been shown to preserve all computational property of the original algorithm serrano a while being more appropriate for vlsi realization the chip implement an art network with f node and f node it can therefore cluster binary pixel input pattern into up to different category modular expansibility of the system is possible by assembling an n m array of chip without any extra interfacing circuitry resulting in an f layer with n node and an f layer with m node pattern classification is performed in le than s which mean an equivalent computing power of connection and connection update per second although internally the chip is analog in nature it interface to the outside world through digital signal thus having a true asynchrounous digital behavior experimental chip test result are available which have been obtained through test equipment for digital chip 
one of the main theme in the area of terminologicalreasoning ha been to identify description logic dl that are both very expressive and decidable arecent paper by schild showed that this issue can beprofitably addressed by relying on a correspondencebetween dl and propositional dynamic logic pdl however schild left open three important problem related to the translation into pdl of functional restrictionson role both direct and inverse numberrestrictions and assertion 
problem solving with incomplete information is usually very costly since multiple alternative must be taken into account in the planning process in this paper we present some pruning rule that lead to substantial cost saving the rule are all based on the simple idea that if goal achievement is the sole criterion for performance a planner need not consider one branch in it search space when there is another branch characterized by equal or greater information the idea is worked out for the case of sequential planning conditional planning and interleaved planning and execution the rule are of special value in this last case a they provide a way for the problem solver to terminate it search without planning all the way to the goal and yet be assured that no important alternative are overlooked 
design to time is an approach to real time scheduling in situation where multiple method exist for many task that the system need to solve often ithese method will have relationship with one other such a the execution of one method enabling the execution of another or the use of a rough approximation by one method affecting the performance of a method that us it result most previous work in the scheduling of real time ai task ha ignored these relationship this paper present an optimal design to time scheduler for particular kind of relationship that occur in an actual ai application and examines the performance of that scheduler in a simulation environment that model the task of that application 
a significant limitation of neural network is that the representation they learn are usually incomprehensible to human we present a novel algorithm trepan for extracting comprehensible symbolic representation from trained neural network our algorithm us query to induce a decision tree that approximates the concept represented by a given network our experiment demonstrate that trepan is able to produce decision tree that maintain a high level of fidelity to their respective 
automating the construction of semantic grammar is a difficult and interesting problem for machine learning this paper show how the semantic grammar acquisition problem can be viewed a the learning of search control heuristic in a logic program appropriate control rule are learned using a new first order induction algorithm that automatically invents useful syntactic and semantic category empirical result show that the learned parser generalize well to novel sentence and out perform previous approach based on connectionist technique 
in order to measure and analyze the performance of rule based expert system it is necessary to explicate the internal structure of their rule base although a number of attempt have been made in the literature to formalize the structure of a rule base using the notion of a rule base execution path none of these are entirely adequate this paper report a new formal definition for the notion of a rule base execution path which adequately support both validation and performance analysis of rule based expert system this definition for the execution path in a rule base ha been embodied in a rule base analysis tool called path hunter path hunter is used to analyse a rule base consisting of clip rule in this analysis the problem of combinatorial explosion which arises during path enumeration is controlled due to the manner in which path are defined the analysis raise several issue which should be taken into account in the engineering of rule based system 
most current method for prediction of protein secondary structureuse a small window of the protein sequence to predict the structureof the central amino acid we describe a new method for predictionof the non local structure called fi sheet which consists of two ormore fi strand that are interconnected by hydrogen bond sincefi strand are often widely separated in the protein chain a networkwith two window is introduced after training on a set of protein the network predicts the 
discus the basic role of the trifocal tensor in scene reconstruction this spl time spl time tensor play a role in the analysis of scene from three view analogous to the role played by the fundamental matrix in the two view case in particular the trifocal tensor maybe computed by a linear algorithm from a set of line correspondence in three view it is further shown in this paper to be essentially identical to a set of coefficient introduced by shashua to effect point transfer in the three view case this observation mean that the line algorithm may be extended to allow for the computation of the trifocal tensor given any mixture of sufficiently many line and point correspondence from the trifocal tensor the camera image matrix may be computed and the scene may be reconstructed for unrelated uncalibrated camera this reconstruction is unique up to projectivity thus projective reconstruction of a set of line and point may be reconstructed linearly from three view 
plausible inference is an essential aspect of logic based information disclosure this paper proposes a context sensitive plausible inference mechanism based on a so called index expression belief network plausible inference is cloaked a probabilistic evidence propagation within this network preliminary experiment show general evidence propagation algorithm to be too inefficient for real life information disclosure application the paper sketch two optimization whereby efficient special purpose evidence propagation may be realized 
we describe irs a program that combine partial order planning with gde style model based diagnosis to achieve an integrated approach to repair our system make three contribution to the field of diagnosis first we provide a unified treatment of both information gathering and state altering action via the uwl representation language second we describe a way to use part replacement operation in addition to probe to gather diagnostic information finally we define a cost function for decision making that account for both the eventual need to repair broken part and the dependence of cost on the device state 
this paper proposes a method of nonmonotonic theory change we first introduce a new form of abduction that can account for observation in nonmonotonic situation then we provide a framework of autoepistemic update which describes nonmonotonic theory change through the extended abductive framework the proposed update semantics is fairly general and provides a unified framework for various update semantics such a first order update view update of database and contradiction removal of nonmonotonic theory 
the assumption based truth maintenance system atm de kleer is the most well known implementation of any dynamic reasoning system some connection have been established between the atm and various nonmonotonic logic e g autoepistemic logic reinfrank et al we describe the relationship between the atm and the agm logic of belief gardenfors and show that it is possible to simulate the behaviour of the atm using the agm logic by encoding the justificational information a an epistemic entrenchment ordering the atm context switching is performed by agm expansion and contraction operation we present an algorithm for calculating this entrenchment ordering and prove it correctness relative to a functional specification of the atm this result demonstrates that the agm logic which is based on the coherence theory of justification is able to achieve both coherence and foundational style behaviour via the choice of epistemic entrenchment 
modeling of a connectionist rule based system or neuro aj hybrid system discussed through the paper will be a fruitful step towards the practical modeling of human cognition this paper investigates a plausible and useful integration method of symbolic ai technique and connectionist model and proposes a practical implementation mainly how variable can be included in the structured information provided a fact and rule in the system 
we address the problem of scene segmentation and shape recovery from a single real intensity image solving this problem is central to obtaining d scene description in realistic application where perfect data cannot be obtained and only one image is available the method we propose address a large class of generic shape namely straight homogeneous generalized cylinder shgcs it consists of the derivation and use of their geometric projective property in a multi level grouping approach we describe an implemented and working system that detects and recovers full shgc description in the presence of image imperfection such a broken contour surface marking shadow and occlusion we demonstrate our method on complex real image 
this paper present a new measure of semantic similarity in an is a taxonomy based on the notion of information content experimental evaluation suggests that the measure performs encouragingly well a correlation of r with a benchmark set of human similarity judgment with an upper bound of r for human subject performing the same task and significantly better than the traditional edge counting approach r 
this paper proposes an effective next view planning strategy for the object recognition and localization task in a model based robot vision system a set of rule are designed to automatically predict new feature and calculate the next optimal placement of the sensor so that the most useful information can be gathered from multi view a state vector i r t is defined to describe the current state of the vision system and each possible state corresponds to a subset of rule to deal with it the recognition and location task can be described a a process of rule calling and state conversion the most suitable rule is selected at each step to try to acquire more useful information a soon a possible experiment are shown in the paper 
we analyze condition that allow for sound and efficient non monotonic inference for that we consider theory comprised of rule and observation and a semantic framework developed elsewhere that allows u to view such theory a dynamic system system with a transition function f that map state to set of possible successor state and a plausibility function that determines the relative likelihood of those transition in this framework the transition function f is determined by the rule and the plausibility function is provided independently in this work we aim to identify plausibility function that have good semantical and computational property we do so by identifying a vet of tore prediction to be accounted for that can be computed in polynomial time can be justified in simple term and are not tied to either horn theory or closed world assumption the resulting function allow u to handle an interesting class of theory in a justifiable and efficient manner 
identifying that part of a knowledge base kb are irrelevant to a specific query is a powerful method of controlling search during problem solving however finding method of such irrelevance reasoning and analyzing their utility are open problem we present a framework based on a proof theoretic analysis of irrelevance that enables u to address these problem within the framework we focus on a class of strong irrelevance claim and show that they have several desirable property for example in the context of horn rule theory we show that strong irrelevance claim can be derived efficiently either by examining the kb or a logical consequence of other strong irrelevance claim an important aspect is that our algorithm reason about irrelevance using only a small part of the kb consequently the reasoning is efficient and the derived irrelevance claim are independent of change to other part of the kb 
implementation result for projective invariant description of planar curve are presented the paper outline method for the generation of projectively invariant representation of curve segment between bitangent point a well a and this for the first time segment between inflection their usefulness for recognition is illustrated the semi local nature of the invariant description allows recognition of object irrespective of overlap and other image degradation 
this work describes a statistical approach to deal with learning and recognition problem in the field of computer vision an abstract theoretical framework is provided which is suitable for automatic model generation from example identification and localization of object both the learning and localization stage are formalized a parameter estimation task the statistical learning phase is unsupervised with respect to the matching of model and scene feature the general mathematical description yield algorithm which can even treat parameter estimation problem from projected data the experiment show that this probabilistic approach is suitable for solving d and d object recognition problem using grey level image the method can also be applied to d image processing issue using range image i e d input data 
model of analog retrieval require a computationally cheap method of estimating similarity between a probe and the candidate in a large pool of memory item the vector dot product operation would be ideal for this purpose if it were possible to encode complex structure s a vector representation in such a way that the superficial similarity of vector representation reflected underlying structural similari ty this paper describes how such an encoding is provided by holographic reduced representation hrrs which are a method for encoding nested relational structure a fixed width distributed representation the condition under which structural similarity is reflected in the dot prod uct ranking of hrrs are discussed 
what is the nature of expertise this paper posit an answer to that question in the domain of geometry problem solving we present a computer program called polya which make use of explicit planning knowledge to solve geometry proof problem integrating the process of parsing the diagram and writing the proof 
the scale space representation and wavelet transform theory have provided u with good singularity detection framework thanks to such method image understanding process have become more and more powerful some computer vision task also require a knowledge on the nature of detected singularity we propose in this paper to take into account these requirement and we present a singularity detection method based on fractional calculus argument 
abstract several study about complexity of nmr showed thatinferring in non monotonic knowledge base is significantlyharder than reasoning in monotonic one thiscontrasts with the general idea that nmr can be usedto make knowledge representation and reasoning simpler not harder in this paper we show that tosome extent nmr ha fulfilled it goal in particularwe prove that circumscription allows for more compactand natural representation of knowledge resultsabout intractability 
we describe a system that can track a hand in a sequence of videoframes and recognize hand gesture in a user independent manner the system locates the hand in each video frame and determinesif the hand is open or closed the tracking system is able to trackthe hand to within sigma pixel of it correct location in ofthe frame from a test set containing video sequence from differentindividuals captured in different room environment thegesture recognition network 
in this paper we present one aspect of our research on machine translation mt defining the relation between the interlingua il and a knowledge representation kr within an mt system our interest lie in the translation of natural language nl sentence where the message contains a spatial relation in particular where the sentence conveys information about the location or path of physical entity in the real physical world we explore several argument for clarifying the source of constraint on the particular il structure needed to translate these sentence this paper develops one approach to defining these constraint and building an mt system where the il structure designed to satisfy these constraint may be tested in this way we have begun to address one of the basic issue in mt research providing independent justification for the il itself 
inducing concept description in first order logic is inherently a complex task there are two main reason on one hand the task is usually formulated a a search problem inside a very large space of logical description which need strong heuristic to be kept to manageable size on the other hand most developed algorithm are unable to handle numerical feature typically occurring in realworld data in this paper we describe the learning system smart that embeds sophisticated knowledge based heuristic to control the search process and is able to deal with numerical feature smart can use different learning strategy such a inductive deductive and abductive one and exploit both backgruond knowledge and statistical evaluation criterion furthermore it can use simple genetic algorithm to refine predicate semantics and this aspect will be described in detail finally an evaluation of smart performance is made on a complex task 
mcallester and rosenblitts systematic nonlinear planner snlp remove threat a they are discovered in other planner such a sipe wilkins and noah sacerdoti threat resolution is partially or completely delayed in this paper we demonstrate that planner efficiency may be vastly improved by the use of alternative to these threat removal strategy we discus five threat removal strategy and prove that two of these strategy dominate the other three resulting in a provably smaller search space furthermore the systematicity of the planning algorithm is preserved for each of the threat removal strategy finally we confirm our result experimentally using a large number of planning example including example from the literature 
this work investigates visual characteristic of specular surface during rotation and give approach to qualitatively identify and quantitatively recover shape of these surface continuous image are taken when an object rotates we look at specularly reflected pattern on the surface and their motion in the epi parallel to the rotation plane from which estimation of each surface point and construction of the object model are carried out we find very simple and direct method to fulfill this objective linear equation for multiple light illumination and a st order differential equation for single light illumination the motion of specular reflection ha nice global characteristic in epi the surface type range from very shiny metal surface to surface with only week specular reflectance we give both simulation and experiment on real object 
scott semantically constrained otter is a resolution based automatic theorem prover for first order logic it is based on the high performance prover otter by w mccune and also incorporates a model generator this find finite model which scott is able to use in a variety of way to direct it proof search clause generated by the prover are in turn used a axiom of theory to be modelled thus prover and model generator inform each other dynamically this paper describes the algorithm and some sample result 
a number of algorithm have recently been proposed that use iterative improvement a form of hill climbing to solve constraint satisfaction problem these technique have had dramatic success on certain problem however one factor limiting their wider application is the possibility of getting stuck at non solution local minimum in this paper we describe an iterative improvement algorithm called breakout that can escape from local minimum we present empirical evidence that this method is very effective in case where previous approach have difficulty although breakout is not theoretically complete in practice it appears to almost always find solution for solvable problem we prove that an idealized but le efficient version of the algorithm is complete 
an algorithm is described which rapidly verifies the potential rigidity of three dimensional point correspondence from a pair of two dimensional view under perspective projection the output of the algorithm is a simple yes or no answer to the question could these corresponding point from two view be the projection of a rigid configuration potential application include d object recognition from a single previous view and correspondence matching for stereo or motion over widely separated view the rigidity checking problem is different from the structure from motion problem because it is often the case that two view cannot provide an accurate structure from motion estimate due to ambiguity and ill conditioning whereas it is still possible to give an accurate yes no answer to the rigidity question rigidity checking verifies point correspondence using d recovery equation a a matching condition the proposed algorithm improves upon other method that fall under this approach because it work with a few a six corresponding point under full perspective projection handle correspondence from widely separated view make full use of the disparity of the correspondence and is integrated with a linear algorithm for d recovery due to kontsevich result are given for experiment with synthetic and real image data a complete implementation of this algorithm is being made publicly available 
when specificity consideration are incorporated in default reasoning system it is hard to ensure that exceptional subclass inherit all legitimate feature of their parent class to reconcile these two requirement specificity and inheritance this paper proposes the addition of a new rule called coherence rule to the desideratum for default inference the coherence rule capture the intuition that formula which are more compatible with the default in the database are more believable we offer a formal definition of this extended desideratum and analyze the behavior of it associated closure relation which we call coference closure we provide a concrete embodiment of a system satisfying the extended desideratum by taking the coherence closure of system z a procedure for computing the unique most compact be lief ranking in the coherence closure of system z is also described 
most existing decision tree system use a greedy approachto induce tree locally optimal split are inducedat every node of the tree although the greedyapproach is suboptimal it is believed to produce reasonablygood tree in the current work we attempt toverify this belief we quantify the goodness of greedytree induction empirically using the popular decisiontree algorithm c and cart we induce decisiontrees on thousand of synthetic data set and comparethem to the 
there is no need to show the importance of arc consistency in constraint network mohr and henderson have proposed ac an algorithm having an optimal worst case time complexity but it ha two drawback it space complexity and it average time complexity in problem with many solution where the size of the constraint is large these drawback become so important that user often replace ac by ac a nonoptimal algorithm in this paper we propose a new algorithm ac which 
various feature description are being employed in logic programming language and constraint based grammar formalism the common notational primitive of these description are functional attribute called feature the description considered in this paper are the possibly quantified first order formula obtained from a signature of binary and unary predicate called feature and sort respectively we establish a first order theory ft by mean of three axiom scheme show it completeness and construct three elementarily equivalent model one of the model consists of the so called feature graph a data structure common in computational linguistics the other two model consist of the so called feature tree a recordlike data structure generalizing the tree corresponding to first order term our completeness proof exhibit a terminating simplification system deciding validity and satisfiability of possibly quantified feature description 
various feature description are being employed in constrained based grammar formalism the common notational primitive of these description are functional attribute called feature the description considered in this paper are the possibly quantified first order formula obtained from a signature of feature and sort we establish a complete first order theory by mean of three axiom scheme and construct three elementarily equivalent model one of the model consists of so called feature graph a data structure common in computational linguistics the other two model consist of so called feature tree a record like data structure generalizing the tree corresponding to first order term our completeness proof exhibit a terminating simplification system deciding validity and satisfiability of possibly quantified feature description 
model generation can be regarded a a special case of the constraint satisfaction problem csp it ha many application in ai computer science and mathematics in this paper we describe sem a system for enumerating finite model of first order many sorted theory to the best of our knowledge sem outperforms any other finite model generation system on many test problem the high performance of sem relies on the following two technique a an efficient implementation of constraint propagation which requires little dynamic allocation of storage b a powerful heuristic which eliminates many isomorphic partial model during the search we will present the basic algorithm of sem along with these two technique our experimental result show that general purpose finite model generator are indeed useful in many application 
exception directed acyclic graph edags are knowledge structure that subsume tree and rule but can be substantially more compact manually constructed and induced edags are compared by reconstructing shapiro s structured induction of a chess end game it is shown that the induced edag is very similar to that produced through consultation with exnerts and that both are small comorehensible 
choosing appropriate model is crucial in analyzing complex physical phenomenon especially when supercomputing resource and complex partial differential equation are involved this paper present an approach to formulating mathematical model guided by the structure of a domain theory and the gross behavior of a physical problem the approach is motivated by the observation that many physical domain though complex and computationally expensive to analyze have strong domain theory based on a few fundamental conservation law and well defined physical process furthermore modeling decision have to be guided by the behavior specific to a physical problem that the system is trying to model by exploiting a domain theory and using problem specific behavior the approach offer an uniform and efficient way of formulating model of various complexity ranging from algebraic ordinary to partial differential equation the approach ha been implemented in a computer program msg and tested in the heat transfer domain 
there is a need to develop a suitable computational grammar formalism for free word order language for two reason first a suitably designed formalism is likely to be more efficient second such a formalism is also likely to be linguistically more elegant and satisfying in this paper we describe such a formalism called the paninian framework that ha been successfully applied to indian language this paper show that the paninian framework applied to modern indian language give an elegant account of the relation between surface form vibhakti and semantic karaka role the mapping is elegant and compact the same basic account also explains active passive and complex sentence this suggests that the solution is not just adhoc but ha a deeper underlying unity a constraint based parser is described for the framework the constraint problem reduces to bipartite graph matching problem because of the nature of constraint efficient solution are known for these problem it is interesting to observe that such a parser designed for free word order language compare well in asymptotic time complexity with the parser for context free grammar cfgs which are basically designed for positional language 
system that learn from example often create a disjunctive concept definition small disjuncts are those disjuncts which cover only a few training example the problem with small disjuncts is that they are more error prone than large disjuncts this paper investigates the reason why small disjuncts are more error prone than large disjuncts it show that when there are rare case within a domain then factor such a attribute noise missing attribute class noise and training set size can result in small disjuncts being more error prone than large disjuncts and in rare case being more error prone than common case this paper also ass the impact that these error prone small disjuncts and rare case have on inductive learning i e on error rate one key conclusion is that when low level of attribute noise are applied only to the training set the ability to learn the correct concept is being evaluated rare case within a domain are primarily responsible for making learning difficult 
several research group are implementing analog integrated circuit model of biological auditory processing the output of these circuit model have taken several form including video format for monitor display simple scanned output for oscilloscope display and parallel analog output suitable for data acquisition system here an alternative output method for silicon auditory model suitable for direct interface to digital computer is described a a prototype of this method an integrated circuit model of temporal adaptation in the auditory nerve that function a a peripheral to a workstation running unix is described data from a working hybrid system that includes the auditory model a digital interface and asynchronous software are given this system produce a real time x window display of the response of the auditory nerve model 
present a new approach for fixing two camera at a single location in a d scene fixation requires the detection of binocular disparity for a single point of interest in the scene so that vergence control can reduce that disparity to zero most existing system use area based matching since feature based matching is computationally prohibitive unfortunately the area based approach doe not perform well when confronted with steeply inclined surface occlusion repeating pattern or featureless image region the method presented in this paper utilizes attentional shift and affine resampling to combat these problem these are integrated with adaptive window size control and coarse to fine correlation based searching the effectiveness of the approach for complex scene is demonstrated with several stereo image pair 
this paper present a novel parameter free techniquefor the segmentation and local description of linestructures on multiple scale both in d and d the algorithm is based on a nonlinear combinationof linear filter and search for elongated symmetricline structure while suppressing the response toedges the filtering process creates one sharp maximumacross the line feature profile and across scalespace the multiscale response reflects local contrastand is independent of the 
this paper present a model based object recognition approach that us a hierarchical gabor wavelet representation the key idea is to use magnitude phase and frequency measure of gabor wavelet representation in an innovative flexible matching approach that can provide robust recognition a gabor grid a topology preserving map efficiently encodes both signal energy and structural information of an object in a sparse multi resolution representation the gabor grid subsamples the gabor wavelet decomposition of an object model and is deformed to allow the indexed object model match with the image data flexible matching between the model and the image minimizes a cost function based on local similarity and geometric distortion of the gabor grid grid erosion and repairing is performed whenever a collapsed grid due to object occlusion is detected the result on infrared imagery are presented where object undergo rotation translation scale occlusion and aspect variation under changing environmental condition 
instead of trying to compare methodology for reasoning about action on the basis of specific example we focus here on a general class of problem expressible in a declarative language a we propose three translation p r and b from a representing respectively the first order method of reasoning about action proposed by pednault and reiter and the circumscriptive approach of baker we then prove the soundness and completeness of these translation relative to the semantics of a this let u compare these three method in a mathematically precise fashion moreover we apply the method of baker in a general setting and prove a theorem which show that if the domain of interest can be expressed in a circumscription yield result which are intuitively expected 
the paper demonstrates that exponential complexity with respect to grammar size and input length have little impact on the performance of three unification based parsing algorithm using a wide coverage grammar the result imply that the study and optimisation of unification based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parser 
this paper describes a discovery system for trigonometric fuctions dst which ha ability to acquire new knowledge in the form of theorcins ancl foriiialas ii a plant geometry cloiiiaiil the systcn is composed of two suhystenls a plaiic geonhry analysis systeiri ad a niatlicniatical fornda t ratnsforlllat ioll system the former change the length and angle of a figure and extract geouietric relation and the lat tcr transforiiis the relation to acquire nsefnl formnlas with little lmsic knowledge such a the clefinitioii of the congruence of triangle aid the dcfiiiition of fiuidameiit al trigonometric fiuictions our system ha recliscovered many trigononietric formula ant geometric theorem including the pythagorean tlieoreui 
semiquantitative model combine both qualitative and quantitative knowledge within a single semiquantitative qualitative differential equation sqde representation with current simulation method the quantitative knowledge is not exploited a fully a possible this paper describes dynamic envelope a method to exploit quantitative knowledge more fully by deriving and numerically simulating an extremal system whose solution is guaranteed to bound all solution of the sqde it is shown that such system can be determined automatically given the sqde and an initial condition a model precision increase the dynamic envelope bound become more precise than those derived by other semiquantitative inference method we demonstrate the utility of our method by showing how it improves the dynamic monitoring and diagnosis of a vacuum pump down system 
change in lighting condition strongly effect the perform ance and reliability of computer vision system we report face recogniti on result under drastically changing lighting condition for a compu ter vision system which concurrently us a contrast sensitive silico n retina and a conventional gain controlled ccd camera for both input device the face recognition system employ an elastic matching algorithm with wavelet based feature to classify unknown face to ass the effect of analog on chip preprocessing by the silicon retina the ccd image have been digitally preprocessed with a bandpass filter to adjust the power spectrum the silicon retina with it ability to adjus t sensitivity increase the recognition rate up to percent these comparative experiment demonstrate that preprocessing with an analog vlsi silicon retina generates image data enriched with object constantfeatures 
we propose a comprehensive framework for modeling and specifying multimodal interaction to this end we employ an extended notion of dialogue act which can be realized by linguistic and non linguistic mean first a set of constraint is presented that describes the temporal structure and all pattern of exchange during a cooperative informationseeking dialogue second we introduce a strategic level of description which allows the specification of the topical structure according to an information seeking strategy the model wa used to design and implement the merit system and led to a reduction in the complexity of the user interface while preserving most of the useful but sometimes confusing dialogue option of advanced direct manipulation interface 
a goal of the multi tac project is to make combinatorial problem solving technologyavailable to user who have no formal training in ai or or to achieve this aim we have built a system vicss visual constraint specification system that enables user to specify problem graphically to simplify the specification process vicss relies heavily on programming by demonstrationtechniques we believe however that one key to making programming by demonstration succeedfor our application is 
in expert consultation dialogue it is inevitable that an agent will at time have insufficient information to determine whether to accept or reject a proposal by the other agent this result in the need tor the agent to initiate an information sharing subdialogue to form a set of shared belief within which the agent can effectively re evaluate the proposal this paper present a computational strategy for initiating such information sharing subdialogues to resolve the system uncertainty regarding the acceptance of a user proposal our model determines when information sharing should be pursued selects a focus of information sharing among multiple uncertain belief chooses the most effective information sharing strategy and utilizes the newly obtained information to re evaluate the user proposal furthermore our model is capable of handling embedded informauon sharing subdialogues 
this paper present an unsupervised learning algorithm for sense disambiguation that when trained on unannotated english text rival the performance of supervised technique that require time consuming hand annotation the algorithm is based on two powerful constraint that word tend to have one sense per discourse and one sense per collocation exploited in an iterative bootstrapping procedure tested accuracy exceeds 
a variety of statistical method for nouncompound analysis are implemented andcompared the result support two mainconclusions first the use of conceptualassociation not only enables a broad coverage but also improves the accuracy second an analysis model based on dependencygrammar is substantially more accuratethan one based on deepest constituent even though the latter is more preva lent in the literature 
we present a new method for analyzing the d motion of the heart s left ventricle lv from tagged magnetic resonance imaging mri data our technique is based on the development of a new class of volumetric physic based deformable model whose parameter are function and can capture the local shape variation of an object these parameter require no complex post processing in order to be used by a physician these volumetric model allow the accurate estimation of the shape and motion of the inner and outer wall of the lv a well a within the wall we also present a new technique for calculating force exerted by tagged mri data to material point of the deformable model furthermore by plotting the variation over time of the extracted lv model parameter from normal heart data we are able to quantitatively analyze and compare the epicardial and endocardial motion 
this paper present an overview of some openresearch problem in the representation of emotionon computer the issue discussed arise inthe context of a broad albeit shallow emotionreasoning platform based originally on the ideasof ortony clore and collins ortony clore amp collins in addressing these problem wehope to correct and expand our content theoryof emotion and pseudo personality which underliesall aspect of the research answer feasibilityquestions 
this paper present the first implementation of explanation based learning technique for a partial order planner we describe the basic learning framework of including regression explanation propagation and rule generation we then concentrate on s ability to learn from failure and present a novel approach that us stronger domain and planner specific consistency check to detect explain and learn from the failure of plan at depth limit we will end with an empirical evaluation of the efficacy of this approach in improving planning performance 
we present new algorithm for local planning over markov decision process the base level algorithm posse several interesting feature for control of computation based on selecting computation according to their expected benefit to decision quality the algorithm are shown to expand the agent s knowledge where the world warrant it with appropriate responsiveness to time pressure and randomness we then develop an introspective algorithm using an internal representation of what computational work ha already been done this strategy extends the agent s knowledge base where warranted by the agent s world model and the agent s knowledge of the work already put into various part of this model it also enables the agent to act so a to take advantage of the computational saving inherent in staying in known part of the state space the control flexibility provided by this strategy by incorporating natural problem solving method directs computational effort towards where it s needed better than previous approach providing grcatcr hope for scalability to large domain assign the goal state a reward of and all other state a reward of problem using such a reward function include the path planning problem on a grid with obstacle and imperfect motor control and the ubiquitous puzzle but with random error associated with action the model can also handle problem having several stop state of differcnt value in this domain a plan take the form of a policy assigning to each state an action choice the agent try to choose a policy maximizing it cumulative reward for domain involving unbounded time it is common to discount future gain by an amount exponential in time to 
the paper proposes a statistical framework that enables d structure and motion to be computed optimally from an image sequence on the assumption that feature measurement error are independent and gaussian distributed the analysis and result demonstrate that computing both camera scene motion and d structure is essential to computing either with any accuracy having computed optimal estimate of structure and motion over a small number of initial image a recursive version of the algorithm previously reported recomputes sub optimal estimate given new image data the algorithm is designed explicitly for real time implementation and the complexity is proportional to the number of tracked feature d projective affine and euclidean model of structure and motion recovery have been implemented incorporating both point and line feature into the computation the framework can handle any feature type and camera model that may be encapsulated a a projection equation from scene to image 
an approach to nonmonotonic inference based on a closure operation on a conditional knowledge base is presented the central idea is that given a theory of default conditionals an extension to the theory le defined that satisfies certain intuitive restriction two notion for forming an extension are given corresponding to the incorporation of irrelevant property in conditionals and of transitivity among conditionals in this approach these notion coincide several equivalent definition for an extension are developed general nonconstructive definition and a general pseudo iterative definition reasoning with irrelevant property is correctly handled a is specificity reasoning within exceptional circumstance and inheritance reasoning tina approach is intented to ultimately serve a the proof theoretic analogue to an extant semantic development based on preference ordering among possible world 
understanding flow in the three dimensional phase space is challenging both to human expert and current computer science technology to break through the barrier we are building a program called psx that can autonomously explore the flow in a three dimensional phase space by integrating ai and numerical technique in this paper i point out that quasi symbolic representation called flow mapping is effective a a mean of capturing qualitative aspect of three dimensional flow and present a method of generating flow mapping for a system of ordinary differential equation with three unknown function the method is based on a finding that geometric cue for generating a set of flow pattern can be classified into five category i demonstrate how knowledge about interaction of geometric cue is utilized for intelligently controlling numerical computation 
symmetry is usually viewed a a discrete feature anobject is either symmetric or non symmetric followingthe view that symmetry is a continuous feature acontinuous symmetry measure csm ha been developedto evaluate symmetry of shape and object inthis paper we extend the symmetry measure to evaluatethe symmetry of occluded shape additionally usingthe symmetry measure we reconstruct occluded shapesby locating the center of symmetry of the shape proc cvpr new york 
we describe a framework for learning saccadic eye movement usinga photometric representation of target point in natural scene therepresentation take the form of a high dimensional vector comprisedof the response of spatial filter at different orientation and scale we first demonstrate the use of this response vector in the task oflocating previously foveated point in a scene and subsequently usethis property in a multisaccade strategy to derive an adaptive motormap for 
most reinforcement learning rl work supposes policy for sequential decisiontasks to be optimal that minimize the expected total discounted cost e g q learning wat ahc bar sut and on the other hand it is well knownthat it is not always reliable and can be treacherous to use the expected value a adecision criterion tha a lot of alternative decision criterion have beensuggested in decision theory to get a more sophisticated considaration of risk butmost rl 
vision system that have successfully supported nontrivial task have invariably taken advantage of constraint derived from the task and environment to increase reliability and lower the complexity of perception we propose that it is possible to build a general purpose vision system that is one that can support a wide variety of task and take advantage of such constraint the central idea within our proposed architecture is the reactive skill skill are concurrent control routine assembled at run time using instruction from a symbolic execution system visual module are used a resource in the construction of these skill skill control the agent a continuous feedback loop but are constructed using discrete symbolic instruction the key to general purpose vision is the ability to parametrize the primitive element of the vision system and to compose visual and control routine in a variety of way we demonstrate the architecture in the context of an implemented example task of a robot collecting trash off a floor and depositing it in a garbage can 
this paper describes how meta level theory are used for analytic learning in multi tac multi tac operationalizes generic heuristic for constraint satisfaction problem in order to create program that are tailored to specific problem for each of it generic heuristic multi tac ha a meta theory specifically designed for operationalising that heuristic we present example of the specialisation process and discus how the theory influence the tractability of the learning process we also describe an empirical study showing that the specialised program produced by multitac compare favorably to hand coded program 
even with significant advance in model based diagnosis methodology it is recognizedthat effectivemodelingis the key to developingefficientdiagnosisalgorithms for complexcontinuous valued system in this paper we developa formal modeling methodology basedon the bondgraphmodelinglanguage andthen presentschemes for focusingthe systemmodelto the diagnosistaskby convertingequationsto conflictsets this representationgreatly facilitates the candidate generation and the measurement selectionprocesses 
the problem of tracking a moving object having marking on it surface in front of a static camera is addressed in this case the motion analysis reduces to the problem of tracking these marking from frame to frame the marking are modeled by b spline the problem of establishing the correspondence between the image marking point in the various frame is bypassed by relating the b spline parameter of the same marking viewed at different frame implicit in this relationship are the motion parameter the complexity of the problem is significantly reduced by first acquiring the d object curve structure before the usual motion estimation 
we describe a method for generating causal explanation in natural language of the simulated behavior of physical device the method is implemented in dme a system that help formulate mathematical simulation model from a library of model fragment using a compositional modeling approach because explanation are generated from model that are dynamically constructed from modular piece several of the limitation of conventional explanation technique are overcome since the explanation system ha access to the derivation of mathematical equation from the original model specification the system can explain low level quantitative behavior predicted by conventional simulation technique in term of salient behavioral abstraction such a physical process idealized component and operating mode instead of relying on ad hoc causal model crafted specifically for the explanation task the program infers causal relationship among parameter in a constraint based equation model rather than using canned top down template the text generator composes textual annotation associated with individual model fragment into coherent sentence we show how these technique can be combined to produce a variety of explanation about simulated system 
we provide syntactic characterization for a number of propositional model based belief revision and update operator proposed in the literature a well a algorithm based on these characterization 
in this paper we suggest an approach to multiagentplanning that contains heuristic element ourmethod make use of subgoals and derived sub plan to construct a global plan agent solve their individualsub plan which are then merged into a globalplan the suggested approach may reduce overallplanning time and derives a plan that approximatesthe optimal global plan that would have been derivedby a central planner given those original subgoals we consider two different scenario 
i present an average case analysis of propositional strip planning the analysis assumes that each possible precondition likewise postcondition is equally likely too appear within an operator under this assumption i derive bound for when it is highly likely that a planning instanee can be efficiently solved either by finding a plan or proving that no plan exists roughly if planning instance have no condition ground atom g goal and o n operator then a simple efficient algorithm can prove that no plan exists for at least of the instance if instance have n ln g ln g operator then a simple efficient algorithm can find a plan for at least of the instance a similar result hold for plan modification i e solving a planning instance that is close too another planning instance with a known plan thus it would appear that propositional strip planning a pspace complete problem is hard only for narrow parameter range which complement previous average case analysis for np complete problem future work is needed to narrow the gap between the bound and to consider more realistic distributional assumption and more sophisticated algorithm 
recently there ha been a growing interest in the use of mosaic image to represent the information contained in video sequence the paper systematically investigates how to go beyond thinking of the mosaic simply a a visualization device but rather a a basis for efficient representation of video sequence we describe two different type of mosaic called the static and the dynamic mosaic that are suitable for different need and scenario we discus a series of extension to these basic mosaic to provide representation at multiple spatial and temporal resolution and to handle d scene information we describe technique for the basic element of the mosaic construction process namely alignment integration and residual analysis we describe several application of mosaic representation including video compression enhancement enhanced visualization and other application in video indexing search and manipulation 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
this paper describes an extension to the constraintsatisfaction problem csp approach called musecsp mu ltiply segmented constraint satisfactionproblem this extension is especially useful for thoseproblems which segment into multiple set of partiallyshared variable such problem arise naturally in signalprocessing application including computer vision speech processing and handwriting recognition forthese application it is often difficult to segment thedata in only one way 
we present a practical pattern recognition system that is invariant with respect to translation scale and rotation of object the system is also insensitive to large variation of the threshold used a feature vector zernike moment are used and we compare them with hu s seven moment invariant for a practical machine vision system three key issue are discussed pattern normalization fast computation of zernike moment and classification using k nn rule a testing result the system recognizes a set of alphanumeric machine printed character with different size at arbitrary orientation and with different threshold where the size of the character varies from to pixel 
the author present a hand model that simultaneously satisfies both the synthesis and analysis requirement of model based compression the model can be fitted to any person s hand and can be done using a single camera once the model is fitted to a real human hand it is then used in several tracking scenario in order to verify it effectiveness with successful tracking achieved the model is ready to be incorporated into a virtual environment or model based compression scheme such a sign language communication over telephone line or virtual teleconference over computer network at very low bit rate and at very high image quality 
this paper we propose to use ordered decision graph odgs a the underlying description in odgs thevariables can only be tested in accordance with a previously specified order by using odgs we can effectively avoidsome of the problem encountered by other researcher when trying to identify common subtrees we describe asimple but relatively effective algorithm that derives an ordered decision graph with minimal description length froma decision tree built using standard technique 
a self organizing neural network for sequence classification called sardnet is described and analyzed experimentally sardnet extends the kohonen feature map architecture with activation retention and decay in order to create unique distributed response pattern for different sequence sardnet yield extremely dense yet descriptive representation of sequential input in very few training iteration the network ha proven successful on mapping arbitrary sequence of binary and real number a well a phonemic representation of english word potential application include isolated spoken word recognition and cognitive science model of sequence processing 
this paper introduces g genetic state space search the integration of two general search paradigm genetic search and state space search provides a general framework which can be applied to a large variety of search problem here we show how g solves constrained optimization problem cop basically it search for promising search state from which good solution can be easily found domain knowledge in the form of constraint is used to limit the space to be searched interestingly our approach allows the handling of constraint within genetic search at a general domain independent level first we introduce a genetic representation of search state next we provide empirical result which compare the relative merit of the introduction of constraint during the generation of the initial population during the fitness calculation and during the application of genetic operator finally we describe some extension to our method which came about when applying it to factory floor scheduling problem 
in previous work bennett dejong and bennetl we proposed a machine learning approach called permissive planning to extend classical planning into the realm of real world plan execution our prior result have been favorable but empirical bennett and dejong here we examine the analytic foundation of our empirical success we advance a formal account of realworld planning adequacy we prove that permissive planning doe what it claim to do it probabilistically achieves adequate real world performance or guarantee that no adequate real world planning behavior is possible within the flexibility allowed we prove that the approach scale tractably we prove that restriction are necessary without them permissive planning is impossible we also show how these restriction can be quite naturally met through schema based planning and explanation based learning 
deformable model have been widely used to approximate object from collected data point but most algorithm based on the deformable model can only handle geometrically and topologically simple object they are inadequate for object with deep cavity or multi part object furthermore they always assume there is only one underlying object for the collected data which mean the segmentation ha been done ahead of time unlike most deformable algorithm which approximate one object at a time our proposed approach can apply simultaneously more than one curve to approximate multiple object using the residual data point the bad part of the fitting curve and appropriate boolean operation our approach is able to detect pattern with hole or cavity and can perform segmentation by itself for more than one underlying object we currently present experiment mainly on d data these d algorithm can be extended to d without theoretical difficulty an experiment on d data composed of two genus i torus is also presented also a new method for defining the external energy is presented which help capture the shape more accurately with low time and reasonable space complexity and a method to prevent self intersection of the curve during evolution is also introduced 
this paper describes a technique whereby anautonomous agent such a a mobile robot canexplore an unknown environment and make atopological map of it it is assumed that the environmentcan be represented a a graph thatis a a fixed set of discrete location or regionswith an ordered set of path between them in previous work it ha been shown that suchworlds can be fully explored and described usinga single movable marker even if there areno spatial metric and almost no sensory 
in this work we describe experiment with eigenfaces for recognition and interactive search ina large scale face database accurate visualrecognition is demonstrated using a database ofo face the problem of recognition undergeneral viewing orientation is also examined a view based multiple observer eigenspacetechnique is proposed for use in face recognitionunder variable pose in addition a modulareigenspace description technique is used whichincorporates salient feature 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
the use of entropy a a distance measure ha several benefit amongst other thing it provides a consistent approach to handling of symbolic attribute real valued attribute and missing value the approach of taking all possible transformation path is discussed we describe k an instance based learner which us such a measure and result are presented which compare favourably with several machine learning algorithm 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
the process of machine learning can be considered in two stage modelselection and parameter estimation in this paper a technique is presentedfor constructing dynamical system with desired qualitative property theapproach is based on the fact that an n dimensional nonlinear dynamicalsystem can be decomposed into one gradient and n gamma hamiltonian system thus the model selection stage consists of choosing the gradient andhamiltonian portion appropriately so that a certain 
resource allocation is a difficult constraint satisfaction problem that ha many practical application fully automatic system are often rejected by the ultimate user because in many real world environment constraint cannot be formalized completely on the other hand human are overwhelmed by the complexity of their task we present a new way of solving the resource allocation where a computer build dynamic abstraction that simplify problem solving to the point that the user can intervene in the solution of the problem these abstraction are based on the concept of interchangeability introduced by freuder in this paper we describe a heuristic for decomposing a resource allocation problem into abstraction that reflect interchangeable set of task or resource we ass the quality of the discovered neighborhood interchangeable set by comparing them to the one obtained by the exact algorithm described by freuder both for data taken from a real world application and for randomly generated problem 
this paper present a method for acquiringa semantic hierarchy and updating an incompletehierarchy the creation of a comprehensivehierarchy is one important step in constructinga system for translating japanesetexts into english the hierarchy is usedto bias the learning of rule that indicatethe english translation of a japanese verb the task is particularly challenging becausetraining example are ambiguous in the sensethat each of the attribute forming an examplemay 
we describe the design of comlex syntax a computational lexicon providing detailed syntactic information for approximately english headword we consider the type of error which arise in creating such a lexicon and how such error can be measured and controlled 
we apply active exemplar selection plutowski amp white to predicting a chaotic time series given a fixed set of example the method chooses a concise subset for training fittingthese exemplar result in the entire set being fit a well a desired the algorithm incorporates a method for regulating networkcomplexity automatically adding exemplar and hidden unit asneeded fitting example generated from the mackey glass equationwith fractal dimension to an rmse 
we examine the issue of evaluation of model specific parameter in a modified vc formalism two example are analyzed the dim ensional homogeneous perceptron and the dimensional higher order neuron both model are solved theoretically and their learning cu rf are compared against true learning curve it is shown that the form alism ha the potential to generate a variety of learning curve incl uding one displaying phase transition 
reinforcement learning rl ha become a central paradigm for solving learning control problem in robotics and artificial intelligence r l researcher have focussed almost exclusively on problem where the controller ha to maximize the discounted sum of payoff however a emphasized by schwartz x in many problem e g those for which the optimal behavior is a limit cycle it is more natural and computationally adva ntageous to formulatae task so that the controller s objective is to ma ximize the avera ge payoff received per time step in this paper i derive new average payofl rl algorithm a stochastic approximation method for solving the system of equation associated with the policy evctl tiot and optimal control question in avera ge payoff rl task these algorithm are analogous to the popular td and q learning a lgorithms a lready developed for the discounted payoff case one of the a lgorit hm clerived here is a significant variation of schwartz s r lea rning algorithni prelimina ry empirica result arc presented to validate these new algorithm 
the purpose of this paper is to suggest that quantifier in natural language do not have a fixed truth functional meaning a ha long been held in logical semantics instead we suggest that quantifier can best be modeled a complex inference procedure that are highly dynamic and sensitive to the linguistic context a well a time and memory constraint 
the wake sleep algorithm hinton dayan frey and neal is a relatively efficient method of fitting a multilayer stochastic generative model to high dimensional data in addition to the top down connection in the generative model it make use of bottom up connection for approximating the probability distribution over the hidden unit given the data and it train these bottom up connection using a simple delta rule we use a variety of synthetic and real data set to compare the performance of the wake sleep algorithm with monte carlo and mean field method for fitting the same generative model and also compare it with other model that are le powerful but easier to fit 
the pile is a new element of the desktop user interface metaphor designed to support the casual organization of document an interface design based on the pile concept suggested us of content awareness for describing organizing and filing textual document we describe a prototype implementation of these capability and give a detailed example of how they might appear to the user we believe the system demonstrates how content awareness can be not only used in a computer filing system but made an integral part of the user s experience 
table lookup with interpolation is used for many learning and adaptation task redundant mapping capture the important concept of motor skill which is important in real behaving system few if any robot skill implementation have dealt with redundant mapping in which the space to be searched to create the table ha much higher dimensionality than the table itself a practical method for inverting redundant mapping is important in physical system with limited time for trial we present the guided table fill in algorithm which us data already stored in the table to guide search through the space of potential table entry the algorithm is illustrated and tested on a robot skill learning task both in simulation and on a robot with a flexible link our experiment show that the ability to search high dimensional action space efficiently allows skill learner to find new behavior that are qualitatively different from what they were presented or what the system designer may have expected thus the use of this technique can allow researcher to seek higher dimensional action space for their system rather than constraining their search space at the risk of excluding the best action 
this paper present an improved backjumping algorithm for the constraint satisfaction problem namely conflict directed backjumping cbj cbj is then modified such that it can detect infeasible value and remove them from the domain of variable once and for all a similar modification is then made to gaschnig s backjumping routine bj and to haralick and elliott s forward checking routine fc empirical analysis show that these modification tend to result in an improvement in average performance the existence of a peculiar phenomenon is then shown the removal of infeasible value may result in a degradation in the performance of intelligent backjumping algorithm and conversely the addition of infeasible value may lead to an improvement in performance 
a framework for knowledge based scientificdiscovery in geological database ha been developed the discovery process consists of twomain step context definition and equationderivation context definition properly definesand formulates homogeneous region each ofwhich is likely to produce a unique and meaningfulanalytic formula for the goal variable clustering technique and a suite of visualizationand interpretation routine make up a toolbox that assist the context definition 
this paper provides a backdrop to my invited talk at the conference the talk itself will focus on selected success of artificial intelligence from the industrial perspective the selected success story will demonstrate that artificial intelligence ha a strong and meaningful influence on our life by impacting development of product and service we use daily they will show how the technical result in the field have been used to make a difference in designing complex artifact in coordinating our action in playing game and in other activity the emphasis is on transformation process rather than on specifies of achievement in knowledge representation case based reasoning or sophisticated search technique i could not have given this type of a talk ten or even five year ago because this wa the initial period of attempt to insert ai into the business process of company today ten year later we have a large number of ai system that are an integral part of critical business process i shall describe some shining example in my talk however the expectation generated ten year ago were much higher than what we were able to achieve in ten year because of this we have pessimist who talk of failure and ai winter since i want to concentrate on the accomplishment in my talk i wish to get the negative aspect out of the way here in this paper taking the metaphor of a partially filled glass of water i will describe the empty portion of it in this paper and only remind you of it during the talk conversely this paper just outline the content of the filled portion while the talk will give you it full taste 
hybrid kl one style logic are knowledge representation formalism of considerable applicative interest a they are specifically oriented to the vast class of application domain that are describable by mean of taxonomic organization of complex object in this paper we consider the problem of endowing such logic with capability for default inheritance reasoning a kind of default reasoning that is specifically oriented to reasoning on taxonomy the formalism that result from our work ha a reasonable and simple behaviour when dealing with the interplay of defeasible and strict inheritance of property of complex object 
wolfgang maassinstitute for theoretical computer sciencetechnische universitaet grazklosterwiesgasse a graz austriae mail maass igi tu graz ac atneurocolt technical report seriesnc tr december produced a part of the esprit working groupin neural and computational learning neurocolt neurocolt coordinating partner department of computer scienceegham surrey tw ex englandfor more information contact john 
we propose a formal approach to the problem of prediction based on the following step first a mental level model is constructed based on the agent s previous action next the model is updated to account for any new observation by the agent and finally we predict the optimal action w r t the agent s mental state a it next action this paper formalizes this prediction process in order to carry out this process we need to understand how a mental state can be ascribed to an agent and how this mental state should be updated in brafman and tennenholtz b we examined the first stage here we investigate a particular update operator and show that it ascription requires making only weak modeling assumption 
estimating d motion parameter from three dimensional point correspondence is considered a minimum error in variable formulation of the motion estimation problem is developed to obtain stable and accurate solution and is connected with the ordinary least square formulation of the problem when covariance matrix of d point are known a closed form matrix weighted solution is derived for estimating the motion parameter in the presence of independent and identically distributed i i d gaussian noise in a general case a closed form approximate solution is presented the experimental result demonstrate that the author solution are accurate and reliable in the presence of noise with different deviation 
abstract an alternative model is proposed for mixture of expert by utiliz ing a di erent parametric form for the gating network the mod i ed model is trained by an em algorithm in comparison with earlier model trained by either em or gradient ascent there is no need to select a learning stepsize to guarantee the convergence of the learning procedure we report simulation experiment which show that the new architecture yield signi cantly faster conver gence we also apply the new model to two problem domain piecewise nonlinear function approximation and combining multi ple previously trained classi er 
this paper present a prototype of an interface system with an active human like agent in usual human communication non verbal expression play important role they convey emotional information and control timing of interaction a well this project attempt to introduce multi modality into computer human interaction our human like agent with it realistic facial expression identifies the user by sight and interacts actively and individually to each user in spoken language that is the agent see human and visually recognizes who is the person keep eye contact in it facial display with human start spoken language interaction by talking to human first 
this paper describes a new approach to the integration and control of continuously operating visual process visual process are expressed a transformation which map signal from virtual sensor into command for device these transformation define reactive process which tightly couple perception and action such transformation may be used to control robotic device including fixation an active binocular head a well a the to select and control the process which interpret visual data this method take inspiration from so called behavioural approach to mobility and manipulation however unlike most previous work we define reactive transformation at the level of virtual sensor and device controller this permit a system to integrate a large number of perceptual process and to dynamically compose sequence of such process to perform visual task the transition between visual process is mediated by signal from a supervisory controller a well a signal obtained from perception this method offer the possibility of constructing vision system with large number of visual ability in a manner which is both scalable and learnable after a review of related work in mobility and manipulation we adapt the reactive process framework to computer vision we define reactive visual process which map information from virtual sensor to device command we discus the selection and control of reactive visual process to accomplish visual task we then illustrate this approach with a system which detects and fixates on different class of moving object 
agent tracking involves monitoring the observable action of other agent a well a inferring their unobserved action plan goal and behavior in a dynamic real time environment an intelligent agent face the challenge of tracking other agent flexible mix of goal driven and reactive behavior and doing so in real time despite ambiguity this paper present resc real time situated commitment an approach that enables an intelligent agent to meet this challenge resc s situatedness derives from it constant uninterrupted attention to the current world situation it always track other agent on going action in the context of this situation despite ambiguity resc quickly commits to a single interpretation of the on going action without an extensive examination of the alternative and us that in service of interpretation of future action however should it commitment lead to inconsistency in tracking it us single state backtracking to undo some of the commitment and repair the inconsistency together resc s situatedness immediate commitment and single state backtracking conspire in providing resc it real time character resc is implemented in the context of intelligent pilot agent participating in a real world synthetic air combat environment experimental result illustrating resc s effectiveness are presented 
we attemped to improve recognition accuracy by reducing the inadequacy of the lexicon and language model specifically we address the following three problem the best size for the lexicon conditioning written text for spoken language recognition and using additional training outside the text distribution we found that increasing the lexicon word to word reduced the percentage of word outside the vocabulary from over to just thereby decreasing the error rate substantially the error rate on word already in the vocabulary did not increase substantially we modified the language model training text by applying rule to simulate the difference between the training text and what people actually said finally we found that using another three year of training text even without the appropriate preprocessing substantially improved the language model we also tested these approach on spontaneous news dictation and found similar improvement 
this paper present an unsupervised learning scheme for categorizing d object from their d projected image the schemeexploits an auto associative network s ability to encode each viewof a single object into a representation that indicates it view direction we propose two model that employ different classificationmechanisms the first model selects an auto associative networkwhose recovered view best match the input view and the secondmodel is based on a modular 
i propose that the characteristic of the scope disambiguation process observed in the literature can be explained in term of the way in which the model of the situation described by a sentence is built the model construction procedure i present build an event structure by identifying the situation associated with the operator in the sentence and their mutual dependency relation a well a the relation between these situation and other situation in the context the procedure take into account lexical semantics and the result of various discourse interpretation procedure such a definite description interpretation and doe not require a complete disambiguation to take place 
this paper address dynamic trajectory planning which is defined a motionplanning for a robot a moving in a dynamic workspace w i e with moving obstacle besides a issubject both to kinematic constraint and dynamic constraint we consider the case of a car likerobot a with bounded velocity and acceleration moving in a dynamic workspace w ir our approachis an extension to the path velocity decomposition kant and zucker we introduce the conceptof adjacent path and 
spatial relation play an important role in the researcharea of connecting visual and verbal space in the last decade several approach to semanticsand computation of spatial relation in dspace have been developed presented here isa new approach to the computation and evaluationof basic spatial relation meaning in dspace we propose the use of various kind of approximationswhen defining the basic semantics the vagueness of the applicability of a spatialrelation is accounted 
one of the hardest problem in reasoning about a physical system is finding an approximate model that is mathematically tractable and yet capture the essence of the problem approximate model in science are often constructed by informal reasoning based on consideration of limiting case knowledge of relative importance of term in the model and understanding of gross feature of the solution we show how an implemented program can combine such knowledge with a heuristic simplification procedure and an inequality reasoner to simplify difficult fluid equation 
one of the open problem listed in rivest andschapire is whether and how that thecopies of lin their algorithm can be combinedinto one for better performance thispaper describes an algorithm called dthatdoes that combination the idea is to representthe state of the learned model using observablesymbols a well a hidden symbol that are constructedduring learning these hidden symbolsare created to reflect the distinct behaviorsof the model state the distinct 
this paper describes the problem faced while using kimmo s two level model to describe certain indian language such a tamil and hindi the two level model is shown to be descriptively inadequate to address these problem a simple extension to the basic two level model is introduced which allows conflicting phonological rule to coexist the computational complexity of the extension is the same a kimmo s two level model 
multi class classification problem can be efficiently solved by partitioning the original problem into sub problem involving only two class for each pair of class a potentially small neural network is trained using only the data of these two class we show how to combine the output of the two class neural network in order to obtain posterior probability for the class decision the resulting probabilistic pairwise classifier is part of a handwriting recognition system which is currently applied to check reading we present result on real world data base and show that from a practical point of view these result compare favorably to other neural network approach 
this article present a new algorithm for segmenting d image it is based on a dynamic triangulated surface and on a pyramidal representation the triangulated surface which can a well modify it geometry a it topology segment image into their component by altering it shape according to internal and external constraint in order to speed up the whole process the surface performs a coarse to fine approach by evolving in a specifically designed pyramid of d image 
we apply smith s theory of aspect to german a language without any aspectual marker in particular we try to shed more light on the effect aspect can have on approach to formalize smith s intuition following glasbey incorporating allen s interval calculus allen 
fuzzy logic method have been used successfully in many real world application but the foundation of fuzzy logic remain under attack taken together these two fact constitute a paradox a second paradox is that almost all of the successful fuzzy logic application are embedded controller while most of the theoretical paper on fuzzy method deal with knowledge representation and reasoning i hope to resolve these paradox by identifying which aspect of fuzzy logic render it useful in practice and which aspect are inessential my conclusion are based on a mathematical result on a survey of literature on the use of fuzzy logic in heuristic control and in expert system and on practical experience in developing expert system 
this paper proposes a method to find the most suitable architecture for a given response time requirement for example retrieval er which search for the best match from a bulk collection of lingusitic example in the example based approach eba which attains substantially higher accuracy than traditional approach er is extensively used to carry out natural language processing task e g parsing and translation er however is so computationally demanding that it often take up most of the total sentence processing time this paper compare several acceleration of er on different architecture i e serial mimd and simd experimental result reveal the relationship between architecture and response time which will allows u to find the most suitable architecture for a given response time requirement 
address the problem of pose estimation and tracking of vehicle in image sequence from traffic scene recorded by a stationary camera in a new algorithm the vehicle pose is estimated by directly fitting image gradient to polyhedral vehicle model without an edge segment extraction process the new approach is significantly more robust than approach that rely on feature extraction because the new approach exploit more information from the image data we can track vehicle that are partially occluded by textured object e g foliage where classical approach based on edge segment extraction fail result from various experiment with real world traffic scene are presented 
this paper present an efficient biologically inspired early vision architecture the dynamicretina that is well suited to highly active and responsive vision platform the dynamic retina exploitsnormally undesirable camera motion a a necessary step in detecting image contrast by using dynamicreceptive field instead of traditional spatial neighborhood operator we analyze the continuous miniature quot noise quot movement made by active imaging system and show that they can be exploited 
a new algorithm is described for refining the pose of a model of a rigid object to conform more accurately to the image structure elemental d force are considered to act on the model these are derived from directional derivative of the image local to the projected model feature the convergence property of the algorithm is investigated and compared to a previous technique it use in a video sequence of a cluttered outdoor traffic scene is also illustrated and assessed 
between sensing the world after every action a in a reactive plan and not sensing at all a in an openloop plan lie a continuum of strategy for sensing during plan execution if sensing incurs a cost in time or resource the most cost effective strategy is likely to fall somewhere between these two extreme yet most work on plan execution assumes one or the other in this paper an efficient anytime planner is described that control the rate of sensing during plan execution the sensing interval is determined by the state during plan execution a well a by the cost of sensing so that an agent can sense more often when necessary the planner is based on a generalization of stochastic dynamic programming 
a family of qualitative method is described that determinestructure of curve in a scene from their imageprojections in two or more view for example planecurves are distinguished from space curve and spacecurves from contour generator the novelty of theapproach is first it is unaffected by camera intrinsic parameter so calibration is not required second is it also unaffected by camera motion extrinsic parameter so viewer motion or epipolar geometry forstereo need not 
for non monotonic reasoning explicit ordering over formula offer an important solution to problem such a multiple extension however a criticism of such a solution is that it is not clear in general from where the ordering should be obtained here we show how ordering can be derived from statistical information about the domain which the formula cover for this we provide an overview of prioritized logic a general class of logic that incorporate explicit ordering over formula this class of logic ha been shown elsewhere to capture a wide variety of proof theoretic approach to non monotonic reasoning and in particular to highlight the role of preference both implicit and explicit in such proof theory we take one particular prioritized logic called sf logic and describe an experimental approach for comparing this logic with an important example of a logic that doe not use explicit ordering of preference namely horn clause logic with negation a failure finally we present the result of this companson showmg how sf logic is more skeptical and more accurate than negation a failure 
we discus the type of functional knowledge about an environment an agent can use in order to act effectively we demonstrate the use of structural regularity for acting efficiently and the use of physical regularity for designing effective sensor these idea are described in the context of an everyday task grocery store shopping we discus how shopper a program us regularity of grocery store in order to act appropriately and sense efficiently in groceryworld a simulated grocery store 
in this paper we present a formalization of behavior based planning for nonholonomic robotic system this work provides a framework that integrates feature of reactive planning model with modern control theory based robotic approach in the area of path planning for nonholonomic robot in particular we introduce a motion description language mdle that provides a formal basis for robot programming using behavior and at the same time permit incorporation of kinematic model of robot given in the form of differential equation the structure of the language mdle is such a to allow description of trigger generated by sensor in the language feedback and feedforward control law are selected and executed by the triggering event we demonstrate the use of mdle in the area of motion planning for nonholonomic robot such model impose limitation on stabilizability via smooth feed back i e piecing together open loop and closed loop trajectory becomes essential in these circumstance and mdle enables one to describe such piecing together in a systematic manner a reactive planner using the formalism of the paper is described we demonstrate obstacle avoidance with limited range sensor a a test of this planner 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
we explore the geometric and algebraic relation that exist between correspondence of point and line in an arbitrary number of image we propose to use the formalism of the grassmann cayley algebra a the simplest way to make both geometric and algebraic statement in a very synthetic and effective way i e allowing actual computation if needed we have a fairly complete picture of the situation in the case of point there are only three type of algebraic relation which are satisfied by the coordinate of the image of a d point bilinear relation arising when we consider pair of image among the n and which are the well known epipolar constraint trilinear relation arising when we consider triple of image among the n and quadrilinear relation arising when we consider four tuples of image among the n in the case of line we show how the traditional perspective projection equation can be suitably generalized and that in the case of three image there exist two independent trilinear relation between the coordinate of the image of a d line 
an approach to analytic learning is described that search for accurate entailment of a horn clause domain theory a hill climbing search guided by an information based evaluation function is performed by applying a set of operator that derive frontier from domain theory the analytic learning system is one component of a multi strategy relational learning system we compare the accuracy of concept learned with this analytic strategy to concept learned with an analytic strategy that operationalizes the domain theory 
abstract when solving homework exercise human student often notice that the problem they are about to solve is similar to an example they then deliberate over whether to refer to the example or to solve the problem without looking at the example we present protocol analysis showing that e ective human learn er prefer not to use analogical problem solv ing for achieving the base level goal of the problem although they do use it occasionally for achieving meta level goal such a check ing solution or resolving certain kind of im pass on the other hand ine ective learn er use analogical problem solving in place of ordinary problem solving and this prevents them from discovering gap in their domain theory an analysis of the task domain col lege physic reveals a testable heuristic for when to use analogy and when to avoid it the heuristic may be of use in guiding mul tistrategy learner 
visual cognition depends critically on the ability to make rapid eye movement known a saccade that orient the fovea over target of interest in a visual scene saccade are known to be ballistic the pattern of muscle activation for foveating a prespecified target location is computed prior to the movement and visual feedback is precluded despite these distinctive property there ha been no general model of the saccadic targeting strategy employed by the human visual system during visual search in natural scene this paper proposes a model for saccadic targeting that us iconic scene representation derived from oriented spatial filter at multiple scale visual search proceeds in a coarse to fine fashion with the largest scale filter response being compared first the model wa empirically tested by comparing it performance with actual eye movement data from human subject in a natural visual search task preliminary result indicate substantial agreement between eye movement predicted by the model and those recorded from human subject 
many learning algorithm form concept description composed of clause each of which cover some proportion of the positive training data and a small to zero proportion of the negative training data this paper present a method using likelihood ratio attached to clause to classify test example one concept description is learned for each class each concept description competes to classify the test example using the likelihood ratio assigned to clause of that concept description by testing on several artificial and real world domain we demonstrate that attaching weight and allowing concept description to compete to classify example reduces an algorithm s susceptibility to noise 
this paper describes a bootstrapping approach to the engineering of appropriate training data representation for inductive learning the central idea is to begin with an initial set of human created feature and then generate additional feature that have syntactic form that are similar to the human engineered feature more specifically we describe a two stage process for the engineering of good representation for learning first generating by hand usually in consultation with domain expert an initial set of feature that seem to help learning and second bootstrapping off of these feature by developing and applying operator that generate new feature that look syntactically like the expertbased feature our experiment in the domain of dna sequence identification show that an initial successful humanengineered representation for data can be expanded in this fashion to yield dramatically improved result for learning 
inductive logic programming is a rapidly growing area of research that center on the development of inductive learning algorithm for first order definite clause theory an obvious framework for inductive logic programming research is the study of the pac learnability of various restricted class of these theory of particular interest are theory that include recursive definite clause because little work ha been done within this framework the need for initial result and technique is great this paper present result about the pac learnability of several class of simple definite clause theory that are allowed to include a recursive clause in so doing the paper us technique that may be useful in studying the learnability of more complex class 
modgen model generation is a complete theoremprover for first order logic with finite herbrand domain modgen take first order formula a input and generates model of the input formula modgenconsists of two major module a module for transformingthe input formula into propositional clause and a module to find model of the propositionalclauses the first module can be used by other researchersso that the sat problem can be easily represented stored and communicated 
the human mind is capable of absorbing and processing large volume of information most of this processing however occurs at a precognitive level the result of which serve to alert the cognitive mind to area of potential interest the multidimensional user oriented synthetic environment fluse is an open ended software shell that provides a new approach to interacting with computer based information by using a real time device independent software design and incorporating both cognitive and experiential model of human perception fluse greatly enhances a person s ability to examine interact with and understand relationship in complex information space a jluse shell may be wrapped around data model simulation or even complete program using a design based on human functionality it provides tool for the presentation exploration navigation manipulation and examination of information user experience a highly interactive environment capable of dynamically mapping information into visual auditory or kinesthetic representation 
this paper present a method for learning phonological rule from sample pair of underlyingand surface form without negative evidence the learned rule are represented asfinite state transducer that accept underlying form a input and generate surface form asoutput the algorithm for learning them is an extension of the ostia algorithm for learninggeneral subsequential finite state transducer although ostia is capable of learningarbitrary s f s t s in the limit large dictionary 
introductionin the conventional bayesian view of backpropagation bp buntine and weigend nowlan and hinton mackay wolpert one start with the quot likelihood quot conditional distribution p training set t weight vector w and the quot prior quot distributionp w a an example in regression one might have a quot gaussian likelihood quot p t w exp c w t o p i exp net w t x i t y i s for some constant s t x i and t y i are the 
vista is a software environment supporting the modular implementation and execution of computer vision algorithm because it is extensible portable and freely available vista is an appropriate medium for the exchange of standard implementation of algorithm this paper an overview of vista describes it file format it data abstraction it c onventions for unix filter program and library routine and it user interface toolkit unlike system that are designe d principally to support image processing vista provides fo r the easy creation and use of arbitrary data type such a are needed for many area of computer vision research 
we present an approach for building an affine representation of an unknown curved object viewed under orthographic projection from image of it occluding contour it is based on the observation that the projection of a point on a curved featureless surface can be computed along a special viewing direction that doe not belong to the point s tangent plane we show that by circumnavigating the object on the tangent plane of selected surface point we can compute two orthogonal projection of every point projecting to the occluding contour during this motion and compute the affine coordinate of these point our approach demonstrates that affine shape of curved object can be computed directly i e without euclidean calibration or image velocity and acceleration measurement 
finding the lowest cost path through a graph is central to many problem including route planning for a mobile robot if arc cost change during the traverse then the remainder of the path may need to be replanned this is the case for a sensor equipped mobile robot with imperfect information about it environment a the robot acquires additional information via it sensor it can revise it plan to reduce the total cost of the traverse if the prior information is grossly incomplete the robot may discover useful information in every piece of sensor data during replanning the robot must either wait for the new path to be computed or move in the wrong direction therefore rapid replanning is essential the d algorithm dynamic a plan optimal traverse in real time by incrementally repairing path to the robot s state a new information is discovered this paper describes an extension to d that focus the repair to significantly reduce the total time required for the initial path calculation and subsequent replanning operation this extension completes the development of the d algorithm a a full generalization of a for dynamic environment where arc cost can change during the traverse of the solution path 
recent work ha pointed out that diagnosis strategy are a necessary tool for the diagnosis of complex system nevertheless though current diagnosis system are able to use explicit system model their representation of diagnosis strategy is only implicit in this paper we introduce a formal meta language to express strategic knowledge in an explicit way this language is sufficient to formalize all strategy introduced in previous work and extends previous diagnosis strategy by the integration of empirical knowledge and by explicit statement about dependency between action we provide a declarative semantics for this language and an architecture for implementation 
this paper rest on several contribution first we introduce the notion of a consequence which is a boolean expression that characterizes consistency based diagnosis second we introduce a basic algorithm for computing consequence when the system description is structured using a causal network we show that if the causal network ha no undirected cycle then a consequence ha a linear size and can be computed in linear time finally we show that diagnosis characterized by a consequence and meeting some preference criterion can be extracted from the consequence in time linear in it size a dual set of result is provided for abductive diagnosis 
we study bayesian network for continuous variable using nonlinear conditional density estimator we demonstrate that useful structure can be extracted from a data set in a self organized way and we present sampling technique for belief update based on markov blanket conditional density model 
this paper present an algorithm for incremental induction of decision tree that is ableto handle both numeric and symbolic variable in order to handle numeric variable a newtree revision operator called slewing is introduced finally a non incremental method isgiven for finding a decision tree based on a direct metric of a candidate tree content introduction design goal an improved algorithm incorporating a training instance 
when training neural network by the classical backpropagation algorithm the whole problem to learn must be expressed by a set of input and desired output however we often have high level knowledge about the learning problem in optical character recognition ocr for instance we know that the classification should be invariant under a set of transformation like rotation or translation we propose a new mo dular classification system based on several autoassociative multilayer perceptrons which allows the efficient incorporation of such knowledge result are reported on the nist database of upper case handwritten letter and compared to other approach to the invariance problem 
a crucial problem in inductive logic programming is learning recursive logic program from example alone current system such a golem and foil often achieve success only for carefully selected set of example we describe a program called force that us the new technique of forced simulation to learn twoclause closed linear recursive ij determinate program although this class of program is fairly restricted it doe include most of the standard benchmark problem experimentally force requires fewer example than foil and is more accurate when learning from randomly chosen datasets formally force is also shown to be a pac learning algorithm in a variant of valiant s model in which we assume the ability to make two type of query one which give an upper bound on the depth of the proof for an example and one which determines if an example can be proved in unit depth 
the problem of automatically constructing algebraic surface model from set of d and d image and using these model in pose computation motion and deformation estimation and object recognition are addressed it is proposed that a combination of constrained optimization and nonlinear least square estimation technique be used to minimize the mean squared geometric distance between a set of point or ray and a parameterized surface in modeling task the unknown parameter are the surface coefficient while in pose and deformation estimation task they represent the transformation mapping the observer s coordinate system onto the modeled surface s own coordinate system this approach is applied to a variety of real range computerized tomography ct and video image 
best first search bfs expands the fewest node among all admissible algorithm using the same cost function but typically requires exponential space depth first search need space only linear in the maximum search depth but expands more node than bfs using a random tree we analytically show that the expected number of node expanded by depth first branch and bound dfbnb is no more than o d n where d is the goal depth and n is the expected number of node expanded by bfs we also show that dfbnb is asymptotically optimal when bfs run in exponential time we then consider how to select a linear space search algorithm from among dfbnb iterative deepening id and recursive best first search rbfs our experimental result indicate that dfbnb is preferable on problem that can be represented by bounded depth tree and require exponential computation and rbfs should be applied to problem that cannot be represented by bounded depth tree or problem that can be solved in polynomial time 
abstract ithas often been thought that word sense ambiguity is a cause of poor performance in information retrieval ir system the belief is that if ambiguous word can be correctly disambiguated ir performance will increase however recent research into the application of a word sense disambiguator to an ir system failed toshow any performance increase from these result it ha become clear that more basic research is needed to investigate the relationship between sense ambiguity disambiguation and ir using a technique that introduces additional sense ambiguity into a collection this paper present research 
most probabilistic classifier used for word sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependency among multiple contextual feature in this paper a different approach to formulating a probabilistic model is presented along with a case study of the performance of model produced in this manner for the disambiguation of the noun interest we describe a method for formulating probabilistic model that use multiple contextual feature for word sense disambiguation without requiring untested assumption regarding the form of the model using this approach the joint distribution of all variable is described by only the most systematic variable interaction thereby limiting the number of parameter to be estimated supporting computational efficiency and providing an understanding of the data 
the range of possible domain model on which an explanation can be based is often large yet human explainers are able to choose model that address a questioner s informative need without undue obscurity however few existing explanation system use knowledge base providing multiple model of their topic of explanation let alone account for the selection of a model for a given explanation this paper demonstrates the utility of a preference based mechanism for model selection selection heuristic are made explicit a preference and can be added modified or removed without modifying the interpreter or other planning knowledge the mechanism is more general than previous mechanism for model selection or perspective and can apply previously identified model selection criterion 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
simple constraint on the set of possible surface reflectance and illuminant are exploited in a new color constancy algorithm that build upon forsyth s theory of color constancy the goal defined for a color constancy algorithm is to discount variation in the color and intensity of the incident illumination and thereby extract illumination independent descriptor of surface color from image forsyth s method is based on two constraint first the surface color under a canonical illuminant all fall within an established maximal convex gamut of possible color and second that a diagonal matrix accurately map color between illuminant these constraint taken together turn out to be very effective in solving for color constancy however other strong assumption about the scene are required for the method to work the illumination must be uniform the surface must be planar and there can be no specularities we show that these restriction are necessary only because forsyth set out to recover the intensity of descriptor at the outset we abandon dimensional descriptor recovery in favor of recovering only orientation i e dimension intensity information is factored out of the problem by mapping dimensional r g b camera response onto dimensional chromaticity specifically r b g b we show that this diagonal chromaticity space ha two important property first gamut convexity is preserved and second illumination change is still described by a diagonal matrix it follows that forsyth s algorithm can be directly applied to the recover chromaticity descriptor and from these the d descriptor orientation can be derived the basic algorithm is then extended to include a maximal gamut constraint on the set of illuminant that is analogous to the gamut constraint on surface color the diagonal chromaticity space facilitates the expression of the illumination constraint in the algorithm test on real image show that the algorithm provides good color constancy 
this paper present a major revision of the either propositional theory refinement system two issue are discussed first we show how run time efficiency can be greatly improved by changing from a exhaustive scheme for computing repair to an iterative greedy method second we show how to extend either to refine m of n rule the resulting algorithm neither new either is more than an order of magnitude faster and produce significantly more accurate result with theory that fit the m of n format to demonstrate the advantage of neither we present preliminary experimental result comparing it to either and various other system on refining the dna promoter domain theory 
the complexity of reasoning is a fundamental issue in ai in many case the fact that an intelligent system need to perform reasoning on line contributes to the difficulty of this reasoning in this paper we investigate a couple of context in which an initial phase of off line preprocessing and design can improve the on line complexity considerably the first context is one in which an intelligent system computes whether a query is entailed by the system s knowledge base we present the notion of an efficient bast for a query language and show that off line preprocessing can be very effective for query language that have an efficient basis the usefulness of this notion is illustrated by showing that a fairly expressive language ha an efficient basis the second context is closely related to the artificial social system approach introduced in mt we present the design of a social law for a multi agent environment a primarily an instance of off line processing and study this problem in a particular model we briefly review the artificial social system approach to design of multi agent system introduced in mt computing or coming up with a social law is viewed a a primarily off line activity that ha major impact on the effectiveness of the on line activity of the agent the tradeoff between the amount of effort invested in computing the social law and the cost of the on line activity can thus be viewed a an off line v on line tradeoff 
knowledge discovery in database ha become an increasingly important research topic with the advent of wide area network computing one of the crucial problem we study in this paper is how to scale machine learning algorithm that typically are designed to deal with main memory based datasets to efficiently learn from large distributed database we have explored an approach called meta learning that is related to the traditional approach of data reduction commonly employed in distributed query processing system here we seek efficient mean to learn how to combine a number of base classifier which are learned from subset of the data so that we scale efficiently to larger learning problem and boost the accuracy of the constituent classifier if possible in this paper we compare the arbiter tree strategy to a new but related approach called the combiner tree strategy 
with increased processor speed and improved robotic and ai technology researcher are beginning to design program that can behave intelligently and interact in the real world a large increase in processing power ha come from parallel machine but taking advantage of this power is challenging in this paper we address the issue in designing planner for real time ai and robotic application and provide guiding principle these principle were designed to minimize the difference between the new real time model and the standard off line model applying these principle yield a better structured application easier design and implementation and improved performance the focus of the paper is on a design methodology for implementing effective planner in real world application using ephor our runtime environment and applying the described planner principle we demonstrate improved performance in a real world shepherding application 
this paper concern the pose determination and recognition of vehicle in traffic scene which under normal condition stand on the ground plane novel linear and closed form algorithm are described for pose determination from an arbitrary number of known line match a form of the generalised hough transform is used in conjuction with explicit probability based voting model to find consistent match the algorithm are fast and robust they cope well with complex outdoor scene 
the paper study the geometry of multi image perspective projection and the matching constraint that this induces on image measurement the combined image projection define a d joint image subspace of the space of combined homogeneous image coordinate this is a complete projective replica of the d world in image coordinate it location encodes the imaging geometry and is captured by the index joint image grassmannian tensor projective reconstruction in the joint image is a canonical process requiring only a simple rescaling of image coordinate reconstruction in world coordinate amount to a choice of basis in the joint image the matching constraint are multilinear tensorial equation in image coordinate that tell whether token in different image could be the projection of a single world token for d image of d point there are exactly three basic type the epipolar constraint a shashua s trilinear one and a new quadrilinear image one for image of line r hartley s trilinear constraint is the only type the coefficient of the matching constraint are tensor built directly from the joint image grassmannian their complex algebraic interdependency is captured by quadratic structural simplicity constraint on the grassmannian 
knowledge based neural network are networkswhose topology is determined by mapping thedependencies of a domain specific rulebase intoa neural network however existing networktraining method lack the ability to add newrules to the reformulated rulebases thus ondomain theory that are lacking rule generalizationis poor and training can corrupt theoriginal rule even those that were initially correct we present topgen an extension to thekbann algorithm that heuristically 
recent work in planning ha focussed on the reuse of previous plan in order to re use a plan in a novel situation the plan ha to be transformed into an applicable plan we describe an approach to plan transformation which utilises reasoning experience a well a planning experience some of the additional information is generated by a series of self generated question and answer a well a appropriate experiment furthermore we show how transformation strategy can be learned 
in this paper we present result from the first use of neuralnetworks for real time control of the high temperature plasma in atokamak fusion experiment the tokamak is currently the principalexperimental device for research into the magnetic confinementapproach to controlled fusion in an effort to improve the energyconfinement property of the high temperature plasma insidetokamaks recent experiment have focused on the use of noncircularcross sectional plasma shape however the accurate generation ofsuch plasma represents a demanding problem involving simultaneouscontrol of several parameter on a time scale a short a a fewtens of microsecond application of neural network to thisproblem requires fast hardware for which we have developed a fullyparallel custom implementation of a multilayer perceptron based ona hybrid of digital and analogue technique 
we analyze the difficulty in applying bayesian belief network to language interpretation domain which typically involve many unification hypothesis that posit variable binding a an alternative we observe that the structure of the underlying hypothesis space permit an approximate encoding of the joint distribution based on marginal rather than conditional probability this suggests an implicit binding approach that circumvents the problem with explicit unification hypothesis while still allowing hypothesis with alternative unification to interact probabilistically the proposed method accepts arbitrary subset of hypothesis and marginal probability constraint is robust and is readily incorporated into standard unification based and frame based model 
most information retrieval system use stopword list and stemming algorithm however we have found that recognizing singular and plural noun verb form negation and preposition can produce dramatically different text classification result we present result from text classification experiment that compare relevancy signature which use local linguistic context with corresponding indexing term that do not in two different domain relevancy signature produced better result than the simple indexing term these experiment suggest that stopword list and stemming algorithm may remove or conflate many word that could be used to create more effective indexing term 
computer based forecasting of weather wa first experimented in at princeton university since then there have been newer and more accurate method to predict the incoming climate one common practice of weather prediction is by using the general circulation model which are based on the law of physic j m moran m d morgan these model are highly complex and computational intensive limiting their use for only short range prediction and that too needing supercomputer the accuracy of forecasting deteriorates rapidly for period longer than hour and it often becomes minimal beyond day due to imperfection in the model the analog technique of weather forecasting is another approach which search for period in the past when the current condition were similar and use the past spatial pattern a analog j t houghton g j jenkins j j ephraums long term trend and recurring event guide the decision this is more relevant for long range prediction a well a in single station prediction the araudog method is relatively simple compared to the complex process of development validation use and maintenance of numerical model 
in this paper a robust method for detecting and recognizing road sign by vision is presented the detection step is based on a geometrical analysis of the edge extracted from monochromatic image and is able to identify triangular and circular road sign from image of city street country road and highway a simple recognition system which validates and classifies the detected road sign greatly improves the reliability and the robustness of the whole system extensive experimentation on real image show that road sign are usually correctly identified even in cluttered image 
one of the challenge in process control is providing reliable control of poorly understood system before such a system can be controlled we must first be able to predict it future behavior so that we know what control action is necessary this paper present two approach to this prediction task both using qualitative model augmented by record of historical system behavior our hypothesis is that qualitative information about a system is more easily available than quantitative equation moreover the information need not be complete or totally correct we restructure the historical information into a case base suitable for the prediction task and use the qualitative model to identify the attribute to use a case index the case base then provides the quantitative information needed for the prediction task our technique are extensively evaluated on data taken from a real world system 
we quantify the observation by kender and freudenstein that degenerate view occupy a significant fraction of the viewing sphere surrounding an object this demonstrates that system for recognition must explicitly account for the possibility of view degeneracy we show that view degeneracy cannot be detected from a single camera viewpoint a a result system designed to recognize object from a single arbitrary viewpoint must be able to function in spite of possible undetected degeneracy or else operate with imaging parameter that cause acceptably low probability of degeneracy to address this need we give a prescription for active control of focal length that allows a principled tradeoff between the camera field of view and probability of view degeneracy 
the problem of driving an autonomous vehicle in normal traffic engages many area of ai research and ha substantial economic significance we describe work in progress on a new approach to this problem that us a decision theoretic architecture using dynamic probabilistic network the architecture provides a sound solution to the problem of sensor noise sensor failure and uncertainty about the behavior of other vehicle and about the effect of one s own action we report on advance in the theory of inference and decision making in dynamic partially observable domain our approach ha been implemented in a simulation system and the autonomous vehicle successfully negotiates a variety of difficult situation 
a framework for tracking pointwise periodic non rigid motion of the heart s left ventricular lv wall is presented which incorporates information from two different magnetic resonance imaging mri technique new development in phase contrast cine mr imaging have produced spatial map of instantaneous velocity that heave proven accuracy within the myocardium or wall of the heart this information is combined with shape based matching technique to provide improved estimate of trajectory especially in region where shape information is limited these raw trajectory act a input to a recursive least square rls filter which applies the constraint of temporal periodicity and spatial smoothness for the final estimate the result of the rls filter are compared with the motion of actual implanted marker comparison are also made between exclusively shape based filtered and phase contrast enhanced trajectory estimate using both phantom and actual canine heart mr image 
admissible heuristic are an important class of heuristic worth discovering they guarantee shortest path solution in search algorithm such a a and they guarantee le expensively produced but boundedly longer solution in search algorithm such a dynamic weighting unfortunately effective accurate and cheap to compute admissible heuristic can take year for people to discover several researcher have suggested that certain transformation of a problem can be used to generate admissible heuristic this article defines a more general class of transformation called abstraction that are guaranteed to generate only admissible heuristic it also describes and evaluates an implemented program absolver ii that us a mean end analysis search control strategy to discover abstracted problem that result in effective admissible heuristic absolver ii discovered several well known and a few novel admissible heuristic including the first known effective one for rubik s cube thus concretely demonstrating that effective admissible heuristic can be tractably discovered by a machine 
in any object recognition system a major and primary task is to associate those image feature within an image of a complex scene that arise from an individual object the key idea here is that a geometric class defined in d induces relationship in the image which must hold between point on the image outline the perspective projection of the object the resulting image constraint enable both identification and grouping of image feature belonging to object of that class the class include surface of revolution canal surface pipe and polyhedron recognition proceeds by first recognising an object a belonging to one of the class for example a surface of revolution and subsequently identifying the object for example a a particular vase this differs from conventional object recognition system where recognition is generally targetted at particular object these class also support the computation of d invariant description including symmetry ax canonical coordinate frame and projective signature the constraint and grouping method are viewpoint invariant and proceed with no information on object pose we demonstrate the effectiveness of this class based grouping on real cluttered scene using grouping algorithm developed for rotationally symmetric surface canal surface and polyhedron 
this paper introduces a sensor placement measure called resolvability the measure provides a technique for estimating the relative ability of various visual sensor including monocular system stereo pair multi baseline stereo system and d rangefinder to accurately control visually manipulated object the resolvability ellipsoid illustrates the directional nature of resolvability and can be used to direct camera motion and adjust camera intrinsic parameter in real time so that the servoing accuracy of the visual servoing system improves with camera lens motion the jacobian mapping from task space to sensor space is derived for a monocular system a stereo pair with parallel optical ax and a stereo pair with perpendicular optical ax resolvability ellipsoid based on these mapping for various sensor configuration are presented visual servoing experiment demonstrate that resolvability can be used to direct camera lens motion in order to increase the ability of a visually servoed manipulator to precisely servo object 
many real life constraint satisfaction problem csps involve some constraint similar to the alldifferent constraint these constraint are called constraint of difference they are defined on a subset of variable by a set of tuples for which the value occuring in the same tuple are all different in this paper a new filtering algorithm for these constraint is presented it achieves the generalized arc consistency condition for these non binary constraint it is based on matching theory and it complexity is low in fact for a constraint defined on a subset of p variable having domain of cardinality at most d it space complexity is o pd and it time complexity is o p d this filtering algorithm ha been successfully used in the system resyn to solve the subgraph isomorphism problem 
an automatic compound retrieval method is proposed to extract compound within a text message it us n gram mutual information relative frequency count and part of speech a the feature for compound extraction the problem is modeled a a two class classification problem based on the distributional characteristic of n gram token in the compound and the non compound cluster the recall and precision using the proposed approach are and for bigram compound and and for trigram compound for a testing corpus of word a significant cutdown in processing time ha been observed 
our work tackle the problem of finding partial determination in databasesand proposes a compression based measure to evaluate them partialdeterminations can be viewed a generalization of both functionaldependencies and association rule in that they are relational in natureand may have exception extending the measure used for evaluatingassociation rule namely support and confidence to partial determinationsleads to a few problem we therefore propose a measure based onthe 
we argue that a quot theory bottleneck quot encountered in the s and early s in attempt to build comprehensivenlu system led to a fragmentation of nlu research which still persists to some extent this fragmentationrepresents an appropriate response to the varietyand subtlety of remaining problem but at this point italso represents a loss of nerve nlu is an organic phenomenon and enough ha been learned about the vexingproblems of the s to try to integrate these insightsand 
this paper describes several mean for sharingbetween related concept to improvelearning in the same domain the sharingcomes in the form of substructure or possiblyentire structure of previous conceptswhich may aid in learning other concept these substructure highlight useful informationin the domain using two domain weevaluate the effectiveness of concept sharingwith respect to accuracy concept size searchcomplexity and noise resistance introductionhumans 
dynamic object such a liquid wave and flame can easily change their position shape and number snapshot image produced by finite element simulator show these change hut lack an explicit representation of the object and their cause for the example of seismic wave we develop a method for interpreting snapshot which is based on hayes concept of a history 
we present episodic logic el a highly expressive knowledge representation well adapted to general commonsense reasoning a well a the interpretive and inferential need of natural language processing one of the distinctive feature of el is it extremely permissive ontology which admits situation episode event state of affair etc proposition possible fact and kind and collection and which allows representation of generic sentence el is natural language like in appearance and support intuitively understandable inference at the same time it is both formally analyzable and mechanizable a an efficient inference engine 
this paper discus the use of artificial neural network for dynamic modelling of time series we argue that multistep prediction is more appropriate to capture the dynamic of the underlying dynamical system because it constrains the iterated model we show how this method can be implemented by a recurrent ann trained with trajectory learning we also show how to select the trajectory length to train the iterated predictor for the case of chaotic time series experimental result corroborate the proposed method 
in this paper we define the task of place learning anddescribe one approach to this problem the frameworkrepresents distinct place using evidence grid a probabilistic description of occupancy place recognitionrelies on case based classification augmentedby a registration process to correct for translation the learning mechanism is also similar to that in casebasedsystems involving the simple storage of inferredevidence grid experimental study with both physicaland simulated 
we describe one approach to build an automatically trainable anaphora resolution system in this approach we use japanese newspaper article tagged with discourse information a training example for a machine learning algorithm which employ the c decision tree algorithm by quinlan quinlan then we evaluate and compare the result of several variant of the machine learning based approach with those of our existing anaphora resolution system which us manually designed knowledge source finally we compare our algorithm with existing theory of anaphora in particular japanese zero pronoun 
a prototyped data mining system dblearn wa developed in simon fraser univ which integrates machine learning methodology with database technology and efficiently and effectively extract characteristic and discriminant rule from relational database further development of dblearn lead to a new generation data mining system dbminer with the following feature mining new kind of rule from large database including multiple level association rule classification rule cluster description rule etc automatic generation and refinement of concept hierarchy high level sql like and graphical data mining interface and client server architecture and performance improvement for large application the major feature of the system are demonstrated with experiment in a research grant information database 
the ability to answer prediction question is crucialto reasoning about physical system a predictionquestion pose a hypothetical scenarioand asks for the resulting behavior of variable ofinterest prediction question can be answeredby simulating a model of the scenario an appropriatesystem boundary which separate aspectsof the scenario that must be modeled fromthose that can be ignored is critical to achievinga simple yet adequate model this paper presentsan efficient 
a description classifier organizes concept and relation into a taxonomy based on the result of subsumption computation applied to pair of relation definition until now description classifier have only been designed to operate over definition phrased in highly restricted subset of the predicate calculus this paper describes a classifier able to reason with definition phrased in the full first order predicate calculus extended with set cardinality equality scalar inequality and predicate variable the performance of the new classifier is comparable to that of existing description classifier our classifier introduces two new technique dual representation and auto socratic elaboration that may be expected to improve the performance of existing description 
we present an approach for identifying the occludingcontour and determining it sidedness using an active i e moving observer it is based on the non stationarity property of the visible rim when the observer s viewpoint ischanged the visible rim is a collection of curve that quot slide quot rigidly or non rigidly over the surface we show that theobserver can deterministically choose three view on thetangent plane of selected surface point to distinguish suchcurves from stationary 
most of the work achieved thus far on aspect graph ha concentrated on the design of algorithm for computing the representation after reviewing how the space of viewpoint can be partitioned in view equivalent cell we work in this paper on a more theoretical level to give enumerative property of the different entity entering in the construction of aspect graph of object bounded by smooth algebraic surface we show how tool from algebraic geometry can be used to compute their 
we introduce a new approach to ga genetic algorithm based problem solving earlier gas did not contain local search i e hill climbing mechanism which led to optimization difficulty especially in higher dimension to overcome such difficulty we introduce a bug based search strategy and implement a system called bug the idea behind this new approach are derived from biologically realistic bug behavior these idea were confirmed empirically by applying them to some optimization and computer vision problem 
eric brill ha recently proposed a simple and powerful corpus based language modeling approach that can be applied to various task including part of speech tagging and building phrase structure tree the method learns a series of symbolic transformational rule which can then be applied in sequence to a test corpus to produce prediction the learning process only requires counting match for a given set of rule template allowing the method to survey a very large space of possible contextual factor this paper analysis brill s approach a an interesting variation on existing decision tree method based on experiment involving part of speech tagging for both english and ancient greek corpus in particular the analysis throw light on why the new mechanism seems surprisingly resistant to overtraining a fast incremental implementation and a mechanism for recording the dependency that underlie the resulting rule sequence are also described 
this paper introduces the recurrence surfaceapproximation an inductive learningmethod based on linear programming thatpredicts recurrence time using censoredtraining example that is example in whichthe available training output may be only alower bound on the quot right answer quot this approachis augmented with a feature selectionmethod that chooses an appropriate featureset within the context of the linear programminggeneralizer computational result inthe field of breast cancer 
an approach to labeling the component of human face from range image is proposed the component of interest are those human usually find significant for recognition to cope with the nonrigidity of face a qualitative approach is used the preprocessing stage employ a multi stage diffusion process to identify convexity and concavity point these point are grouped into component and qualitative reasoning about possible interpretation of the component is performed consistency of hypothesized interpretation is carried out using context based reasoning experimental result on real image of several face are provided 
relevance feedbacvk with too much data a uma technical report james allan ir ref tr modern text collection often contain large document which span several subject area such document are problematic for relevance feedback since inappropriate term can easily be chosen this study explores the highly effective approach of feeding back passage of large document a lessexpensive method which discard long document is also reviewed and found to be effective if there are enough relevant document a hybrid approach which feed back short document and passage of long document may be the best compromise 
many real world planning domain are complex and uncertain preventing complete a priori planning however real world planner can also rely on runtime information to facilitate additional planning during execution the completable approach to planning introduces the idea of completable step which represent deferred planning decision through completable step a planner can defer particular goal until execution time when additional information may be used for their achievement to maintain the provably correct nature of plan afforded by classical planning completable step have the additional requirement of achievability unfortunately without additional higher order knowledge for reasoning about achievability proving achievability becomes infeasible for any real world domain we thus developed an incremental approach for learning completable plan using this approach instead of proving achievability a planner us feedback from it experience with the real world to construct completable plan which cover an increasing space of situation this approach to real world planning ha been successfully tested in a simple simulated robot navigation domain 
discovering conceptually interesting and repetitive substructure in a structural data improves the ability to interpret and compress the data the substructure are evaluated by their ability to describe and compress the original data set using the domain s background knowledge and the minimum description length mdl of the data once discovered the substructure concept is used to simplify the data by replacing instance of the substructure with a pointer to the newly discovered concept the discovered substructure concept allow abstraction over detailed structure in the original data iteration of the substructure discovery and replacement process construct a hierarchical description of the structural data in term of the discovered substructure this hierarchy provides varying level of interpretation that can be accessed based on the goal of the data analysis 
introductiondocument image understanding encompasses thetechnology required to make paper document equivalentto other computer exchange medium like floppy tape and cdroms the physical reader of the paperdocument is the scanner just like the physical reader ofthe floppy is the floppy drive and the physical readerof the tape cartridge is the tape cartridge drive andthe physical reader of the cdrom is the cdrom drive but document image understanding can involvemore than just 
a number of grammatical formalism were introduced to define the syntax of natural language among them are parallel multiple context free grammar pmcfg s and lexical functional grammar lfg s pmcfg s and their subclass called multiple context free grammar mcfg s are natural extension of cfg s and pmcfg s are known to be recognizable in polynomial time some subclass of lfg s have been proposed but they were shown to generate an np complete language finite state translation system ft were introduced a a computational model of transformational grammar in this paper three subclass of lfg s called nc lfg s dc lfg s and fc lfg s are introduced and the generative capacity of the above mentioned grammatical formalism are investigated first we show that the generative capacity of ft is equal to that of nc lfg s a relation among subclass of those formalism it is shown that the generative capacity of deterministic ft dc lfg s and pmcfg s are equal to each other and the generative capacity of fc lfg s is equal to that of mcfg s it is also shown that at least one np complete language is generated by ft consequently deterministic ft dc lfg s and fc lfg s can be recognized in polynomial time however ft and nc lfg s cannot if p np 
with a point matching distance measure which is invariant undertranslation rotation and permutation we learn d point set object by clustering noisy point set image unlike traditional clusteringmethods which use distance measure that operate on featurevectors a representation common to most problem domain thisobject based clustering technique employ a distance measure specificto a type of object within a problem domain formulatingthe clustering problem a two nested 
the use of primary effect of operator in planningis an effective approach to reduce searchcosts however the characterization of quot good quot primary effect ha remained at an informallevel in this paper we present a formal criterionfor selecting useful primary effect whichguarantees planning efficiency completeness and optimality we also describe an inductivelearning algorithm based on this criterion thatautomatically selects primary effect of operator both the sample complexity 
we address the problem to recover the d shape of an unfolded book surface from the shading information in a scanner image from a technical point of view this shape from shading problem in real world environment is characterized by proximal light source interreflection moving light source specular reflection and nonuniform albedo distribution taking all these factor into account we first formulate the problem based on an iterative nonlinear optimization scheme then we introduce piecewise polynomial model of the d shape image restoration experiment for a real book surface demonstrated that geometric and photometric distortion are almost completely removed by the proposed method 
compositional q learning cq l singh is a modular approach to learning to perform composite task made up of several elemental task by reinforcement learning skill acquired while performing elemental task are also applied to solve composite task individual skill compete for the right to act and only winning skill are included in the decomposition of the composite task we extend the original cq l concept in two way a more general reward function and the agent can have more than one actuator we use the cq l architecture to acquire skill for performing composite task with a simulated twolinked manipulator having large state and action space the manipulator is a non linear dynamical system and we require it end effector to be at specific position in the workspace fast function approximation in each of the q module is achieved through the use of an array of cerebellar model articulation controller cmac albus structure 
we present a novel approach to reliable and efficient recovery of part description from range image we show that a set of superquadric model can be directly recovered from unsegmented range data a opposed to method which attempt the recovery of volumetric model only after the data ha been pre segmented using extensive pre processing the approach is based on the recover and select paradigm which consists of two intertwined stage model recovery and model selection at the model recovery stage a redundant set of superquadrics is initiated in the image and allowed to grow which involves an iterative procedure combining data classification and parameter estimation all the recovered model are passed to the model selection procedure where only the model resulting in the simplest overall description are selected 
an optical flow determination method is proposed which derives at a location more than two gradient constraint equation with a set of orientation selective spatial gaussian filter the uncertainty measure of an optical flow is also obtained from the equation 
we present a new technique for tracking d objectsfrom d image sequence through the integrationof qualitative and quantitative technique the deformable model are initializedbased on a previously developed part based qualitativeshape segmentation system using aphysics based quantitative approach object aresubsequently tracked without feature correspondencebased on generalized force computed fromthe stereo image the automatic prediction ofpossible edge occlusion and 
we recently proposed a data structure called associative commutative discrimination net that support efficient algorithm for many to one term matching in the presence of associative commutative function in this paper we discus the integration of such discrimination net into an actual equational theorem prover and report on corresponding experiment the general associative commutative matching problem is known to be np complete but can be solved in polynomial time if the given term are linear i e do not contain multiple occurrence of the same variable we therefore have implemented a two stage matching procedure first we check whether a match exists for the linearized version of the given term where different occurrence of the same variable are replaced by different new variable if a match for the linearized term doe exist we then determine whether there is also a match for the original non linear term i e whether the proposed substitution for different occurrence of the same variable are consistent our experimental result indicate that this approach work very well in theorem proving where most matching attempt actually fail and are filtered out during the first stage so that the second more expensive stage of the algorithm is only needed in comparatively few case 
we study how an autonomous robot can attain a cognitive process that account for it symbolic manipulation of acquired knowledge without generating fatal gap from the reality the paper focus on two essential problem one is the symbol grounding problem and the other is how the internal symbolic process can be situated with respect to the behavioral context we investigate these problem by applying a dynamical system s approach to the robot navigation problem our formulation based on a forward modeling scheme using recurrent neural learning show that the robot is capable of learning grammatical structure hidden in the geometry of the workspace from the local sensory input through it navigational experience furthermore the robot is capable of mentally simulating it own action plan using the acquired forward model our assertion is that the internal representation obtained is grounded since it is self organized solely through interaction with the physical world we also show that structural stability arises in the interaction between the neural dynamic and the environmental dynamic which account for the situatedness of the internal symbolic process 
we describe a supervised learning algorithm eodg that us mutual information to build an oblivious decision tree the tree is then converted to an oblivious read once decision graph oodg by merging node at the same level of the tree for domain that art appropriate for both decision tree and oodgs performance is approximately the same a that of c but the number of node in the oodg is much smaller the merging phase that convert the oblivious decision tree to an oodg provides a new way of dealing with the replication problem and a new pruning mechanism that work top down starting from the root the pruning mechanism is well suited for finding symmetry and aid in recovering from split on irrelevant feature that may happen during the tree construction 
the min conflict heuristic minton et al ha been introduced into backtracking algorithm and iterative improvement algorithm a a powerful heuristic for solving constraint satisfaction problem backtracking algorithm become inefficient when a bad partial solution is constructed since an exhaustive search is required for revising the bad decision on the other hand iterative improvement algorithm do not construct a consistent partial solution and can revise a bad decision without exhaustive search however most of the powerful heuristic obtained through the long history of constraint satisfaction study e g forward checking haralick elliot presuppose the existence of a consistent partial solution therefore these heuristic can not be applied to iterative improvement algorithm furthermore these algorithm are not theoretically complete in this paper a new algorithm called we commitment search which utilizes the min conflict heuristic is developed this algorithm remove the drawback of backtracking algorithm and iterative improvement algorithm i e the algorithm can revise bad decision without exhaustive search the completeness of the algorithm is guaranteed and various heuristic can be introduced since a consistent partial solution is constructed the experimental result on various example problem show that this algorithm is to time more efficient than other algorithm 
conceptual information retrieval system use structured document index domain knowledge and a set of heuristic retrieval strategy to match user query with a set of index describing the document s content such retrieval strategy increase the set of relevant document retrieved increase recall but at the expense of returning additional irrelevant document decrease precision usually in conceptual information retrieval system this tradeoff is managed by hand and with difficulty this paper discus way of managing this tradeoff by the application of standard induction algorithm to refine the retrieval strategy in an engineering design domain we gathered example of query retrieval pair during the system s operation using feedback from a user on the retrieved information we then fed these example to the induction algorithm and generated decision tree that refine the existing set of retrieval strategy we found that induction improved the precision on a set of query generated by another user without a significant loss in recall and in an interactive mode the decision tree pointed out flaw in the retrieval and indexing knowledge and suggested way to refine the retrieval strategy 
we have developed a domain independent systematic methodology for plan merging at the various level of plan abstraction this method manifest itself in the hierarchical plan graph where each level contains a complete partially merged plan the principle advantage of this approach is that once external interaction between node on a given level have been established the continued merging of the plan fragment in one node can take place independently of plan fragment in other node on that level this provides a decomposition or divide and conquer approach to plan merging another advantage to this decomposition approach is that replanning effort is minimized in the presence of the selection of alternative action at some level of the hierarchical plan graph only those plan fragment which are in the same branch a the alternative selection need be considered for replanning also an algorithm is proposed which take a bilateral approach to breaking cyclic dependency between node in the hierarchical plan graph we demonstrate the utility of this hierarchical approach to plan merging through example in the process planning domain 
biological sensorimotor system are not static map that transforminput sensory information into output motor behavior evidencefrom many line of research suggests that their representationsare plastic experience dependent entity while this plasticityis essential for flexible behavior it present the nervous systemwith difficult organizational challenge if the sensorimotor systemadapts itself to perform well under one set of circumstance will itthen perform poorly 
introductionwe have derived a novel approach to the detectionand recognition of human gait in gait detection we find a spatiotemporal pattern thatsignals the presence of a walking person in gaitrecognition we seek to identify the individualwho is walking it is known that human candetect and recognize gait with reduce spatiotemporalsequences such a moving light display and we would like to give similar capabilitiesto machine any reasonable approach to the 
abstract introductionwehave derived a novel approach to the detectionand recognition of human gait in gait detection we nd a spatiotemporal pattern thatsignals the presence of a walking person in gaitrecognition we seek to identify the individualwho is walking it is known that human candetect and recognize gait with reduce spatiotemporalsequences suchasmoving light display and wewould like to give similar capabilitiesto machine any reasonable approach to the interpretation 
this paper discus the important issue of knowledge base comprehensibility and describes a technique for comprehensibility improvement comprehensibility is often measured by simplicity of concept description even in the simplest form however there will be a number of different dnf disjunctive normal form description possible to represent the same concept and each of these will have a different degree of comprehensibility in other word simplification doe not necessarily guarantee improved comprehensibility in this paper the author introduce three new comprehensibility criterion similarity continuity and conformity for use with tabular knowledge base in addition they propose an algorithm to convert a decision table with poor comprehensibility to one with high comprehensibility while preserving logical equivalency in experiment the algorithm generated either the same or similar table to those generated by human 
in this paper we propose a new statistical framework formodeling and extracting d moving deformable object fromimage sequence the object representation relies on a hierarchicaldescription of the deformation applied to a template global deformation are modeled using a karhunenloeve expansion of the distorsions observed on a representativepopulation local deformation are modeled bya first order markov process the optimal bayesian estimateof the global and local deformation is 
we present a new paradigm for minimax search algorithm mt a memory enhanced version of pearl s test procedure by changing the way mt is called a number of practical best first search algorithm can be simply constructed reformulating ss a an instance of mt eliminates all it perceived implementation drawback most assessment of minimax search performance are based on simulation that do not address two key ingredient of high performance game playing program iterative deepening and memory usage instead we use experimental data gathered from tournament checker othello and chess program the use of iterative deepening and memory make our result differ significantly from the literature one new instance of our framework mtd i out performs our best alpha beta searcher on leaf node total node and execution time to our knowledge these are the first reported result that compare both depth first and best first algorithm given the same amount of memory 
new method are reported for the detection of multiple solution degeneracy when estimating the fundamental matrix with specific emphasis on robustness in the presence of data contamination outlier the fundamental matrix can be used a a first step in the recovery of structure from motion if the set of correspondence is degenerate then this structure cannot be accurately recovered and many solution will explain the data equally well it is essential that we are alerted to such eventuality however current feature matcher are very prone to mismatching giving a high rate of contamination within the data such contamination can make a degenerate data set appear non degenerate thus the need for robust method becomes apparent the paper present such method with a particular emphasis on providing a method that will work on real imagery and with an automated non perfect feature detector and matcher it is demonstrated that proper modelling of degeneracy in the presence of outlier enables the detection of outlier which would otherwise be missed result using real image sequence are presented all processing point matching degeneracy detection and outlier detection is automatic 
this paper suggests that it may be easier to learn several hard task at one time than to learn these same task separately in effect the information provided by the training signal for each task serf a a domain specific inductive bias for the other task frequently the world give u cluster of related task to learn when it doe not it is often straightforward to create additional task for many domain acquiring inductive bias by collecting additional teaching signal may be more practical than the traditional approach of codifying domain specific bias acquired from human expertise we call this approach multitask learning mtl since much of the power of an inductive learner follows directly from it inductive bias multitask learning may yield more powerful learning an empirical example of multitask connectionist learning is presented where learning improves by training one network on several related task at the same time multitask decision tree induction is also outlined part of this methodology may be wrong the reductionist method ha caused u to attempt learning on simple isolated task before earnestly attempting learning on larger rich er task by adhering to this method we may be ignoring a critical source of inductive bias for real world problem the inductive bias inherent in the similarity of related ta k drawn from the same domain if an inductive learner is given several related task at the same time these task can be used a valuable source of inductive bias for each other this may make learning faster or more accurate and may allow hard task to be learned that are not learnable in isolation we call this approach multitask learning mtl first we introduce multitask learning at an abstract level and explain how related task can be used a source of mutual inductive bias then we present a concrete example of mtl applied to artificial neural network after this demonstration we discus multitask connectionist learnin g in more detail then we briefly describe multitask decisiontree learning after this we discus related work most notably that on giving hint to network and on transferring knowledge between related task learned one at a time finally we summarize the mtl methodology and highlight the research problem that will have to be tackled for mtl to succeed on complex real world task 
a new method to locate human face in a complex background is proposed this system utilizes a hierarchical knowledge based method and consists of three level the higher two level are based on mosaic image at different resolution in the lower level an improved edge detection method is proposed the problem of scale is solved and the system is made more practical so that it can locate unknown human face spanning a wide range of size in a complex black white picture 
we present a neural network based face detection system a retinally connected neural network examines small window of an image and decides whether each window contains a face the system arbitrates between multiple network to improve performance over a single network we use a bootstrap algorithm for training which add false detection into the training set a training progress this eliminates the difficult task of manually selecting non face training example which must be chosen to span the entire space of non face image comparison with another state of the art face detection system are presented our system ha better performance in term of detection and false positive rate 
we propose a new method for acquiring time sequential range image that provide dense range data in our approach stereo pair of thermal and intensity image are synchronously acquired and are mutually registered the thermal image are segmented into isotemperature region contour based matching is done for the isotemperature region in the thermal image independently at each time instant and by temporal correspondence possible matching pair of contour are generated by evaluating the similarity of the pair consistent and likely pair are chosen to get dense range data intensity profile within the isotemperature region are matched by dynamic programming experiment on real scene including a sequence showing a moving human being show promising result 
we propose an o m n time algorithm for the recognition of tree adjoining language tals where n is the size of the input string and m k is the time needed to multiply two k x k boolean matrix tree adjoining grammar tag are formalism suitable for natural language processing and have received enormous attention in the past among not only natural language processing researcher but also algorithm designer the first polynomial time algorithm for tal parsing wa proposed in and had a run time of o n quite recently an o n m n algorithm ha been proposed the algorithm presented in this paper improves the run time of the recent result using an entirely different approach 
a geometric criterion is developed for establishing shape based non rigid correspondence between plane curve unlike previous effort the criterion doe not use rigid invariant of shape instead shape are compared non rigidly from the vantage point of the correspondence geometric invariant are proposed for curve whose shape can be exactly matched by a non rigid correspondence the invariant are based on angular deviation of convex and concave segment of the curve example of correspondence between curve obtained from medical image are provided 
this paper conjecture a computational account of how child might learn the meaning of word in their native language first a simplified version of the lexical acquisition task faced by child is modeled by a precisely specified formal problem then an implemented algorithm for solving this formal problem is presented key advance of this algorithm over previously proposed algorithm are it ability to learn homonymous word sens in the presence of noisy input and it ability to scale up to problem of the size faced by real child 
in vision guided robotic operation vision is used for extracting necessary information for achieving the task since visual sensing is usually performed with limited resource visual sensing strategy should be planned so that only necessary information is obtained efficiently this paper describes a method of systematically generating visual sensing strategy based on knowledge of the task to be performed the generation of the appropriate visual sensing strategy entail knowing what information to extract where to get it and how to get it this is facilitated by the knowledge of the task which describes what object are involved in the operation and how they are assembled our method ha been implemented using a laser range finder a the sensor experimental result show the feasibility of the method and point out the importance of task oriented evaluation of visual sensing strategy 
this report discus the problem of recovering d motion and structure for rigid curve for this purpose we use long monocular sequence of image of the curve and compute some set of derivative up to the second order that are dened on the so called spatio temporal surface generated by the curve for general d rigid curve there is exactly one constraint for each image point that relates these derivative to the kinematic screw and it first order time derivative these equation derive 
the chou fasman algorithm and it associatedtheory are non learning method to predictthe secondary structure for protein fromthe string of amino acid forming the protein however the overall accuracy of thepredictions is not high and the predictionsfor the important structure are particularlylow therefore a range of method have beenused to improve the prediction of the secondarystructure we have applied our symbolicknowledge refinement system krustto the chou fasman theory to 
a number of reinforcement learning algorithm have been developed that are guaranteed to converge to the optimal solution when used with lookup table it is shown however that these algorithm can easily become unstable when implemented directly with a general function approximation system such a a sigmoidal multilayer perceptron a radial basisfunction system a memory based learning system or even a linear function approximation system a new class of algorithm residual gradient algorithm is proposed which perform gradient descent on the mean squared bellman residual guaranteeing convergence it is shown however that they may learn very slowly in some case a larger class of algorithm residual algorithm is proposed that ha the guaranteed convergence of the residual gradient algorithm yet can retain the fast learning speed of direct algorithm in fact both direct and residual gradient algorithm are shown to be special case of residual algorithm and it is shown that residual algorithm can combine the advantage of each approach the direct residual gradient and residual form of value iteration qlearning and advantage learning are all presented theoretical analysis is given explaining the property these algorithm have and simulation result are given that demonstrate these property 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
we apply dijkstra s semantics for programming language to formalization of reasoning about action and change the basic idea is to view action a formula transformer i e function from formula into formula the major advantage of our proposal is that it is very simple and more effective than most of other approach yet it deal with a broad class of action including those with random and indirect effect also both temporal prediction and postdiction reasoning task can be solved without restricting initial nor final state to completely specified 
it ha been established that certain trilinear form of three perspective view give rise to a tensor of intrinsic coefficient we show in this paper that a permutation of the the trilinear coefficient produce three homography matrix projective transformation of plane of three distinct intrinsic plane respectively this in turn yield the result that d invariant are recovered directly simply by appropriate arrangement of the tensor s coefficient on a secondary level we show new relation between fundamental matrix epipoles euclidean structure and the trilinear tensor on the practical side the new result extend the existing envelope of method of d recovery from d view for example new linear method that cut through the epipolar geometry and new method for computing epipolar geometry using redundancy available across many view 
no finite sample is sufficient to determine the density and therefore the entropy of a signal directly some assumption about either the functional form of the density or about it smoothness is necessary both amount to a prior over the space of possible density function by far the most common approach is to assume that the density ha a parametric form by contrast we derive a differential learning rule called emma that optimizes entropy by way of kernel density estimation entropy and it derivative can then be calculated by sampling from this density estimate the resulting parameter update rule is surprisingly simple and efficient we will show how emma can be used to detect and correct corruption in magnetic resonance image mri this application is beyond the scope of existing parametric entropy model 
to achieve reasonable accuracy in large vocabulary speech recognition system it is important to use detailed acoustic model together with good long span language model for example in the wall street journal wsj task both cross word triphones and a trigram language model are necessary to achieve state of the art performance however when using these model the size of a pre compiled recognition network can make a standard viterbi search infeasible and hence either multiple pas or asynchronous stack decoding scheme are typically used in this paper we show that time synchronous one pas decoding using cross word triphones and a trigram language model can be implemented using a dynamically built tree structured network this approach avoids the compromise inherent in using fast match or preliminary pass and is relatively efficient in implementation it wa included in the htk large vocabulary speech recognition system used for the arpa wsj evaluation and experimental result are presented for that task 
indexing is an efficient method of recovering matchhypotheses in model based object recognition unlikeother method which search for viewpoint invariantshape descriptor to use a index we use a learningmethod to model the smooth variation in appearance oflocal feature set lf indexing from lf effectivelydeals with the problem of occlusion and missing feature the indexing function generated by the learningmethod are probability distribution describing thepossible 
orientation based representation are well suited to vision task including viewpoint independent object recognition and d attitude determination the key property that orientation based representation share is that they rotate in the same way a the object rotates combination of orientation based representation of a model and of a sensed object determine inequality that become equality if and only if the object and model match both in identity and in attitude this result in optimization problem that can be solved by standard numerical method the paper unifies and extends previous work based on the extended gaussian image egi representation it provides the theoretical basis for new approach to object recognition and attitude determination using dense surface data it extends result on convex polyhedron to the domain of smooth strictly convex surface the class of shape covered also is extended to include starshaped set the theoretical result lead to feasible algorithm that are both accurate and robust a proof of concept system ha been implemented and experiment conducted both on synthesized data and on data obtained from real object 
best first search algorithm require exponential memory while depth first algorithm require only linear memory on graph with cycle however depth first search do not detect duplicate node and hence may generate asymptotically more node than best first search we present a technique for reducing the asymptotic complexity of depth first search by eliminating the generation of duplicate node the automatic discovery and application of a finite state machine fsm that enforces pruning rule in a depth first search ha significantly extended the power of search in several domain we have implemented and tested the technique on a grid the fifteen puzzle the twenty four puzzle and two version of rubik s cube in each case the effective branching factor of the depth first search is reduced reducing the asymptotic time complexity 
in this paper we present a meth od for reconstructing the three dimensional scene geometry i e depth surface orientation occluding contour and surface crease from a pair of stereo image this reconstruclion is not done a a post processing step but rather of the above quantity are estimated simultaneously a part of the matching algorith m we argue for an energy functional in which each of the quantity in i he scene geometry is ezplicitly represented for this energy functional we use a smooth ness prior which an addition to it ability to detect surface discontinuity and the accompanying half occluded region is able to reconsiruct steeply sloping surface with sharp crease ezperimental result are pmsented demonstrating the effectiveness of the algorithm 
we analyze how data with uncertain or missing input feature canbe incorporated into the training of a neural network the generalsolution requires a weighted integration over the unknown oruncertain input although computationally cheaper closed form solutionscan be found for certain gaussian basis function gbf network we also discus case in which heuristical solution suchas substituting the mean of an unknown input can be harmful introductionthe ability to learn from data with 
we present a method of representing some class of default theory a normal logic program the main point is that the standard semantics i e sldnf resolution computes answer substitution that correspond exactly to the extension of the represented default theory we explain the step of constructing a logic program logprog p d from a given default theory p d and present the proof idea of the soundness and completeness result for the approach 
in this paper i will describe polly a low cost visionbased robot that give primitive tour the system is very simple robust and efficient and run on a hardware platform which could be duplicated for le than k u the system wa built to explore how knowledge about the structure the environment can be used in a principled way to simplify both visual and motor processing i will argue that very simple and efficient visual mechanism can often be used to solve real problem in real unmodified environment in a principled manner i will give an overview of the robot discus the property of it environment show how they can be used to simplify the design of the system and discus what lesson can drawn for the design of other system 
determination are a useful type of functional knowledge representation application include knowledge based system analogical reasoning database design and robotic sensing system this paper present an efficient batch algorithm for inducing all minimal determination from observed data the algorithm is based on breadth first search and run in polynomial time and space given a user supplied parameter limiting the maximum size of a determination the algorithm us probabilistic measure to induce determination despite noisy data one key contribution is the identification of an enumeration order in the space of possible determination that affords a complete and systematic search another contribution list axiom that relate neighboring state and allow the construction of pruning rule a third contribution formulates a perfect hash function for state in this space and facilitates optimal use of the pruning rule this paper also sketch an algorithm that can incrementally revise a set of determination s given additional data 
since we have investigated the problem and possibility of applying modern information retrieval method to large online public access library catalog opacs in the retrieval experiment virginia tech online catalog revtolc study we carried out a large pilot test in and a larger controlled investigation in with user and roughly marc record result indicated that a form based interface coupled with vector and relevance feedback retrieval method would be well received recent effort developing the multiple access and retrieval of information with annotation marian system have involved used of a specially developed object oriented dbms construction of a client running under nextstep programming of a distributed server with a thread assigned to each user session to increase concurrency on a small network of nexts refinement of algorithm to use object and stopping rule for greater efficiency usability testing and iterative interface refinement 
the people finder is a knowledge based tool to assist user in determining the whereabouts of other staff located in an office or network environment the tool make use of several mode of input and output a well a employing a number of interface and communication medium with which to present information and interconnect geographically distributed system user the accompanying video contains example us of the tool which help illustrate some of it functionality 
at the institute for the learning science we have been developing large scale hypermedia system called ask system that are designed to simulate aspect of conversation with expert they provide access to manually indexed multimedia database of story unit we are particularly concerned with finding a practical solution to the problem of finding index for thes unit when the database grows too large for manual technique our solution is to provide automated assistance that proposes relative link between unit eliminating the need for manual unit to unit comparison in this paper we describe eight class of link and show a representation and inference procedure to assist in locating instance of each 
in the last decade the outline of the neural structure subservingthe sense of direction have begun to emerge several investigationshave shed light on the effect of vestibular input and visual inputon the head direction representation in this paper a model isformulated of the neural mechanism underlying the head directionsystem the model is built out of simple ingredient depending onnothing more complicated than connectional specificity attractordynamics hebbian learning and 
d shape modeling ha been a very prominent part of computer vision over thepast decade several shape modeling technique have been proposed in literature someare local distributed parameter while others are global lumped parameter in termsof the parameter required to describe the shape hybrid model that combine bothends of this parameter spectrum have been in vogue only recently however they donot allow a smooth transition between the two extreme of this parameter 
because complex real world domain defy perfect formalization real world planner must be able to cope with incorrect domain knowledge this paper offer a theoretical framework for permissive planning a machine learning method for improving the real world behavior of planner permissive planning aim to acquire technique that tolerate the inevitable mismatch between the planner s internal belief and the external world unlike the reactive approach to this mismatch permissive planning embrace projection the method is both problem independent and domain independent unlike classical planning permissive planning doe not exclude real world performance from the formal definition of planning 
a new method for acquiring time sequential range image is proposed stereo pair of thermal and intensity image are synchronously acquired and are mutually registered stereo thermal image are segmented into isotemperature region contour based matching is done for the isotemperature region to supplement sparse range data obtained from the contour matching dynamic programming matching is performed for either intensity profile or edge in the stereo intensity image by corresponding pixel pair obtained from the matching process the d coordinate of the point can be calculated experiment with real scene having moving human being show promising result 
whilst much emphasis in ai ha been placed on the use of goal in problem solving le emphasis ha been placed on the role of perception and experience in this paper we show that in the domain that may be considered the most abstract namely mathematics that perception and experience play an important role the mathematician ha a vast amount of mathematical knowledge and yet is able to utilise the appropriate knowledge without difficulty we argue that it is essential to model how well the knowledge is grasped so that mathematical knowledge can grow from partial knowledge to important result that are easily accessed not all knowledge is equal in it importance and we argue that perception and experience play a key role in ordering our knowledge feature play a role in both representing the information from the environment and indexing the knowledge of our memory but a key requirement is that the feature should be dynamic and not be built in this research is implemented in the program mu the mathematics understander which utilises the cm contextual memory system mu ha sucessfully read university level text in pure mathematics checking the proof and solving the simple problem 
approach to text processing that rely on parsing the text with a context free grammar tend to be slow and error prone because of the massive ambiguity of long sentence in contrast fastus employ a nondeterministic nite state language model that produce a phrasal decomposition of a sentence into noun group verb group and particle another nite state machine recognizes domain specic phrase based on combination of the head of the constituent found in the rst pas fastus ha been evaluated on several blind test that demonstrate that state of the art performance on information extraction task is obtainable with surprisingly little computational eort 
this paper describes probabilistic method for novelty detection when using pattern recognitionfor monitoring of dynamic l he of detection is particularlyacute when prior knowledge and data only allow one to construct an incomplete prior of thesystem hence some allowance must be made in so that a classifier will robustto generated by class not in training phase the fault apractical approach is to construct both an input and classconstruction input model for data of is cansolved in 
people need to calibrate camera system in order to determine the relationship between the position of feature in object space and their corresponding position in the image part of camera calibration is the determination of image center but what is the image center ideally the image center is considered to be the point of intersection of the camera s optical axis with the camera s sensing plane in fact there are many possible definition of image center and in real lens most do not have the same coordinate in addition the image center move a lens parameter are changed in this paper we examine why image center are not necessarily the same for different image property and why they vary with lens parameter we then provide a taxonomy of different image center and describe procedure for measuring them finally we examine the calibration of image center for a variable parameter lens several technique are applied to a precision automated zoom lens and experimental result are shown we conclude that the accuracy of the image center can be an important factor in the accuracy of the overall camera calibration and that the large variation in the position of the image center across different definition and different lens setting make the calibration problem much more complex than is conventionally believed with proper modeling by using correct definition for all image center in a system we can improve the accuracy of our camera calibration 
with few exception the study of nonmonotonic reasoning ha been confined to the single agent case however it ha been recognized that intelligent agent often need to reason about other agent and their ability to reason nonmonotonically in this paper we present a formalization of multi agent autoepistemic reasoning which naturally extends earlier work by levesque in particular we propose an n agent modal belief logic which allows u to express that a formula or finite set of them is all an agent know which may include belief about what other agent believe the paper present a formal semantics of the logic in the possible world framework we provide an axiomatization which is complete for a large fragment of the logic and sufficient to characterize interesting form of multi agent autoepistemic reasoning we also extend the stable set and stable expansion idea of single agent autoepistemic logic to the multi agent case 
weak perspective represents a simplified projection model that approximates the imaging process when the scene is viewed under a small viewing angle and it depth relief is small relative to it distance from the viewer we study how to generate dynamic model for estimating rigid d motion from weak perspective a crucial feature in dynamic visual motion estimation is to decouple structure from motion in the estimation model the reason are both geometric to achieve global observability of the model and practical for a structure independent motion estimator allows u to deal with occlusion and appearance of new feature in a principled way it is also possible to push the decoupling even further and isolate the motion parameter that are affected by the so called ba relief ambiguity from the one that are not we present a novel method for reducing the order of the estimator by decoupling portion of the state space from the time evolution of the measurement constraint we use this method to construct an estimator of full rigid motion modulo a scaling factor on a six dimensional state space an approximate estimator for a four dimensional subset of the motion space and a reduced filter with only two state the latter two are immune to the ba relief ambiguity we compare strength and weakness of each of the scheme on real and synthetic image sequence 
compliant control is a standard method for performing fine manipulation task like grasping and assembly but it requires estimation of the state of contact between the robot arm and the object involved here we present a method to learn a model of the movement from measured data the method requires little or no prior knowledge and the resulting model explicitly estimate the state of contact the current state of contact is viewed a the hidden state variable of a discrete hmm the control dependent transition probability between state are modeled a parametrized function of the measurement we show that their parameter can be estimated from measurement concurrently with the estimation of the parameter of the movement in each state of contact the learning algorithm is a variant of the em procedure the e step is computed exactly solving the m step exactly would require solving a set of coupled nonlinear algebraic equation in the parameter instead gradient ascent is used to produce an increase in likelihood 
tree is an optimized rete like pattern matching algorithm it ha been designed for a production system whose restricted data formalism lead to a highly combinatorial join step like in soar tree aim at reducing the join search space without using hashing technique it join strategy us constraint propagation to define the solution space of a join then a constraint relaxation to determine the index to be used in the join computation constraint relaxation is heuristic driven and based on the relational paradigm unlike rete the indexing scheme tree requires is not based on the membership of condition element but on the sharing of reference to symbol on the basis of experimental evidence tree s strategy showed better result than the standard rete one the number of comparison during join step ha been reduced by a factor ranging from to nearly two order of magnitude 
a counterpart to von neumann and morgenstern expected utility theory is proposed in the framework of possibility theory the existence of a utility function representing a preference ordering among possibility distribution on the consequence of decision maker s action that satisfies a series of axiom pertaining to decision maker s behavior is established the obtained utility is a generalization of wald s criterion which is recovered in case of total ignorance when ignorance is only partial the utility take into account the fact that some situation are more plausible than others mathematically the qualitative utility is nothing but the necessity measure of a fuzzy event in the sense of possibility theory a so called sugeno integral the possibilistic representation of uncertainty which only requires a linearly ordered scale is qualitative in nature only max min and order reversing operation are used on the scale the axiom express a risk averse behavior of the decision maker and correspond to a pessimistic view of what may happen the proposed qualitative utility function is currently used in flexible constraint satisfaction problem under incomplete information it can also be used in association with possibilistic logic which is tailored to reasoning under incomplete state of knowledge 
many machine learning algorithm aim at finding quot simple quot rule to explain training data theexpectation is the quot simpler quot the rule the better the generalization on test data occam srazor most practical implementation however use measure for quot simplicity quot that lack the power universality and elegance of those based on kolmogorov complexity and solomonoff s algorithmicprobability likewise most previous approach especially those of the quot bayesian quot kind sufferfrom the problem of 
the frontier of a curved surface is the envelope of contour generator showing the boundary at least locally of the visible region swept out under viewer motion in general the outline of curved surface apparent contour from different viewpoint are generated by different contour generator on the surface and hence do not provide a constraint on viewer motion we show that frontier point however have projection which correspond to a real point on the surface and can be used to constrain viewer motion by the epipolar constraint we show how to recover viewer motion from frontier point for both continuous and discrete motion calibrated and uncalibrated camera we present preliminary result of an iterative scheme to recover the epipolar line structure from real image sequence using only the outline of curved surface a statistical evaluation a also performed to estimate the stability of the solution 
a stereoscopic scene analysis system for d modeling of object from stereoscopic image sequence is described a dense map of d surface point is obtained by image correspondence object segmentation interpolation and triangulation emphasis is put on the accurate measur ement of image correspondence from grey level image the surface geometry of each scene object is approximated by a triangular wire frame which store the surface texture in texture map sequence processing serf to track camera motion and to fuse surface from different view point into a consistent d surface model from the textured d model highly realistic image sequence from arbitrary view point can be synthesized using computer graphic technique 
an approach to nonmonotonic inference based on preference ordering between possible world or state of affair is presented we begin with an extant weak theory of default conditionals using this theory ordering on world are derived the idea is that if a conditional such a bird fly is true then all other thing being equal world in which bird fly are preferred over those where they don t in this case a red bird would fly by virtue of red bird world being among the least exceptional world in which bird fly in this approach irrelevant property are correctly handled a is specificity reasoning within exceptional circumstance and inheritance reasoning a sound proof theoretic characterisation is also given lastly the approach is shown to subsume that of conditional entailment 
a variant is proposed of the preference based semantics for nonmonotonic logic that wa originally considered by shoham in this variant it is not assumed that preference between standard model are aggregated into one preference order this allows the capturing of all main nonmonotonic formalism including default logic on reiter the preferential model introduced in this paper are motivated from an epistemic point of view and are therefore called epistemic preference model the consequence operation induced by epistemic preference model are characterized further the view is defended that the rationality of cumulative monotonicity doe not imply that nonmonotonic logic have to be cumulative but only that a rational agent should not believe a set of default rule that induces a noncumulative consequence operation 
an important problem in geometric reasoning is to find the configuration of a collection of geometric body so a to satisfy a set of given constraint recently it ha been suggested that this problem can be solved efficiently by symbolically reasoning about geometry using a degree of freedom analysis the approach employ a set of specialized routine called plan fragment that specify how to change the configuration of a set of body to satisfy a new constraint while preserving existing constraint in this paper we show how these plan fragment can be automatically synthesized using first principle about geometric body action and topology 
in dague a formal system rom k involving four relation ha been defmed to reason with relative order of magnitude in this paper problem of introducing guantitative informahon and of ensuring validity of the result in ir are tackled correspondent overlapping relation are defmed in r and all rule of rom k are transposed to r unlike other proposed system the obtained system rom r ensures a sound calculus in r while keeping the ability to provide commonsense explanation of the result if needed these result can be refmed by using additional and complementary technique k bound consistency which generalizes interval propagation symbolic computation which considerably improves the result by delaying numeric evaluation symbolic algebra calculus of the root of partial derivative which allows the exact extremum to be obtained transformation of rational function when possible so that each variable occurs only once which allows interval propagation to give the exact result rom r possibly supplemented by these various technique constitutes a rich powerful and flexible tool for performing mixed qualitative and numeric reasoning essential for engineering task 
discus a nonparametric approach for calibrating a ccd camera a constrained topological mapping ctm approach to analyze the systematic imaging error of an image system and compare it with parametric approach which are based on optimization and have been discussed by many other author this nonparametric approach ha several distinct feature in this approach some distortion surface are derived directly from the training sample because no analytical form of these surface is assumed when we modeled the distortion by a nonparametric model the systematic imaging error instead of mere lens distortion are considered this give an new approach to analyze the imaging error of a particular imaging system experimental result are given in detail which indicate that both in image projection and in d reconstruction the accuracy is much improved when the nonparametric approach is employed for calibrating a camera 
many of today s electro mechanical device exhibit both continuous and discrete behavior modeling these hybrid system present special challenge for automated modeling and simulation we show how nonstandard analysis overcomes these challenge provides a firm mathematical foundation and satisfies our intuition about the behavior of hybrid system 
protein structure analysis from dna sequence is an important and fast growing area in both computet science and biochemistry although interesting approach have been studied it is very dificult to capture the characteristic of protein since even a simple protein are made of more than amino acid which make biochemical experiment very dificult to detect functional component for this reason almost all the problem in this field are left unsolved and it is very important to develop a system which assist researcher on molecular biology to remove the dificulties caused by combinatorial explosion in this paper we report a system called mwi molecular biologist workbench version l o which extract knowledge from amino acid sequence by controlling application of domain knowledge automatically we apply this method to comparative analysis of lysozyme and lxlactalbumin the result show that we obtain several interesting result from amino acid sequence which have not been reported before 
we introduce a new subclass of allen s interval algebra we call quot ord horn subclass quot which is a strict superset of the quot pointisable subclass quot we prove that reasoning in the ord horn subclass is a polynomial time problem and show that the path consistency method is sufficient for deciding satisfiability further using an extensive machine generated case analysis we show that the ord horn subclass is a maximal tractable subclass of the full algebra assuming p np in fact it is the unique 
this paper relates the computational power of fahlman s recurrent cascade correlation rcc architecture to that of finite state automaton fsa while some recurrent network are fsa equivalent rcc is not the paper present a theoretical analysis of the rcc architecture in the form of a proof describing a large class of fsa which cannot be realized by rcc 
this paper describes an algorithm for stereo tracking using d affine transfer of a body centred fixation point transfer is based on corner detected in the image and matched over time and in stereo the paper present a method of basing the transfer on all the available data providing immunity to noise and poor conditioning the paper also show an implementation at video rate on a four axis active camera platform graceful degradation in the presence of insufficient data and fixed latency tracking in parallel with the structure calculation provide robust performance recovered trajectory are shown in an approximately euclidean frame while structure transfer is demonstrated by the evolution of the target s convex hull 
we develop in the context of discriminant analysis a general approach to the design of neural architecture it consists in building a neural net around a statistical model family larger network made up of such elementary network are then constructed it is shown that on the one hand the statistical modeling approach provides a systematic way to obtaining good approximation in the neural network context while on the other neural network offer a powerful expansion to classical model family a novel integrated approach emerges which stress both flexibility contribution of neural net and interpretability contribution of statistical modeling a well known data set on birth weight is analyzed by this new approach the result are rather promising and open the way to many potential application 
this paper describes a new discourse module within our multilingual nlp system because of it unique data driven architecture the discourse module is language independent moreover the use of hierarchically organized multiple knowledge source make the module robust and trainable using discourse tagged corpus separating discourse phenomenon from knowledge source make the discourse module easily extensible to additional phenomenon 
while researcher in computer vision and pattern recognition have worked on automatic technique for recognizing face for the last year most system specialize on frontal view of the face we present a face recognizer that work under varying pose the difficult part of which is to handle face rotation in depth building on successful template based system our basic approach is to represent face with template from multiple model view that cover different pose from the viewing sphere our system ha achieved a recognition rate of on a data base of people containing testing and modelling view per person 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
reinforcement learning address the problem of learning to select action in order tomaximize one s performance in unknown environment to scale reinforcement learningto complex real world task such a typically studied in ai one must ultimately be ableto discover the structure in the world in order to abstract away the myriad of detail andto operate in more tractable problem space this paper present the skill algorithm skill discovers skill which are partiallydefined action 
for more than two decade there ha been consensus that bidirectional heuristic search is afflicted by the problem of search wavefront missing each other however our result indicate that a different problem appears to be of primary importance the front typically meet rather early even without using waveshaping technique especially when aiming for optimal solution however much effort ha to be spent for subsequently improving the solution quality and finally for proving that there is indeed no better solution possible therefore only slightly relaxing the requirement on the solution quality already lead to strong improvement in efficiency we describe several new e admissible bidirectional search algorithm which do not use wave shaping technique the most efficient of these use a novel termination criterion designed to address the suspected primary problem of bidirectional heuristic search we prove e admissibility and a dominance result based on this termination criterion in summary we show that and how bidirectional best first search can be more efficient than the corresponding unidirectional counterpart without using computationally very demanding waveshaping technique 
we present a fast algorithm for non linear dimension reduction the algorithm build a local linear model of the data by mergingpca with clustering based on a new distortion measure experimentswith speech and image data indicate that the local linearalgorithm produce encoding with lower distortion than those builtby five layer auto associative network the local linear algorithmis also more than an order of magnitude faster to train introductionfeature set can be more compact 
increasing attention ha been paid to reinforcement learning algorithmsin recent year partly due to success in the theoreticalanalysis of their behavior in markov environment if the markovassumption is removed however neither generally the algorithmsnor the analysis continue to be usable we propose and analyzea new learning algorithm to solve a certain class of non markovdecision problem our algorithm applies to problem in whichthe environment is markov but the learner ha 
this paper proposes fbrl a language for representing function and behavior with the primitive we identified and discus it application to explanation generation fbrl explicitly represents model of each component in a system in term of two element one is a necessary and sufficient information for simulation of the component which we call behavior the other is the interpretation of the behavior under a desirable state which the component is expected to achieve which we call function by identifying primitive necessary for the interpretation of the behavior in various domain we can capture what function is and represent it by selection and combination of them we also investigate the relation between function and behavior based on the primitive of fbrl a fbrl can represent concept at various level of abstraction it contributes to explanation generation by providing information for mapping behavior of a component to a term which represents it function 
we extend two notion of only knowing that of halpern and moses and that of levesque to many agent the main lesson of this paper is that these approach do have reasonable extension to the multi agent case our result also shed light on the single agent case for example it wa always viewed a significant that the hm notion of only knowing wa based on s while levesque s wa based on k in fact our result show that the hm notion is better understood in the context of k indeed in the singleagent case the hm notion remains unchanged if we use k or kd instead of s however in the multiagent case there are significant difference between k and s moreover all the result proved by halpern and moses for the single agent case extend naturally to the multi agent case for k but not for s 
a central issue in non linear planning is the ordering of operator so a to avoid undesirable interaction between their effect the modal truth criterion chapman state the condition under which these interaction will occur non linear planner use the criterion directly or indirectly to promote or demote operator or to co designate variable so a to avoid interaction this abstract describes a method called goal clobbering avoidance gca to avoid some interaction in a partially ordered plan by promoting or demoting a sequence of operator rather than individual operator effectively it simultaneously applies the modal truth criterion to all operator in the sequence using pm compiled information about the domain gca will be illustrated in the familiar blocksworld domain with the operator 
it ha been known that using different representation of either query or document or different retrieval technique retrieves different set of document recent work suggests that significant improvement in retrieval performance can be achieved by combining multiple representation or multiple retrieval technique in this paper we propose a simple method for retrieving different document within a single query representation a single document representation and a single retrieval technique we classify the type of document and describe the property of weighting scheme then we explain that different property of weighting scheme may retrieve different type of document experimental result show that significant improvement can be obtained by combining the retrieval result from different property of weighting scheme 
multi method planning is an approach to using a set of different planning method to simultaneously achieve planner completeness planning time efficiency and plan length reduction although it ha been shown that coordinating a set of method in a coarse grained problem by problem manner ha the potential for approaching this ideal such an approach can waste a significant amount of time in trying method that ultimately prove inadequate this paper investigates an approach to reducing this wasted effort by refining the granularity at which method are switched the experimental result show that the fine grained approach can improve the planning time significantly compared with coarse grained and single method approach 
this paper defines a temporal continuity constraint that express assumption about the evolution of d image velocity or optical flow over a sequence of image temporal continuity is exploited to develop an incremental minimization framework that extends the minimization of a non convex objective function over time within this framework this paper describes an incremental continuation method for recursive non linear estimation that robustly and adaptively recovers optical flow with motion 
this paper present a massively parallel parser that predicts critical attachment behavior of the human sentence processor without the use of explicit preference heuristic or revision strategy the processing of a syntactic ambiguity is modeled a an active distributed competition among the potential attachment for a phrase computationally motivated constraint on the competitive mechanism provide a principled and uniform account of a range of human attachment preference and garden path phenomenon 
prior knowledge constraint are imposed upon a learning problem in the form of distance measure prototypical d point set and graph are learned by clustering with point matching and graph matching distance measure the point matching distance measure is approximately invariant under affine transformation translation rotation scale and shear and permutation it operates between noisy image with missing and spurious point the graph matching distance measure operates on weighted graph and is invariant under permutation learning is formulated a an optimization problem large objective so formulated million variable are efficiently minimized using a combination of optimization technique softassign algebraic transformation clocked objective and deterministic annealing 
real world learning task may involve high dimensional data setswith arbitrary pattern of missing data in this paper we presenta framework based on maximum likelihood density estimation forlearning from such data set we use mixture model for the densityestimates and make two distinct appeal to the expectationmaximization em principle dempster et al in derivinga learning algorithm em is used both for the estimation of mixturecomponents and for coping with missing data 
genetic algorithm gas and heuristic search are shown to bestructurally similar the strength of the correspondence and it practicalconsequences are demonstrated by considering the relationshipbetween fitness function in gas and the heuristic function of ai by examining the extent to which fitness function approximate anai ideal a measure of ga search difficulty is defined and applied topreviously studied problem the success of the measure in predictingga performance 
this paper is a study on lraam based labeling recursive auto associative memory classification of symbolic recursive structure encoding term the result reported here have been obtained by combining an lraam network with an analog perceptron the approach used wa to interleave the development of representation unsupervised learning of the lraam with the learning of the classification task in this way the representation are optimized with respect to the classification task the intended application of the approach described in this paper are hybrid symbolic connectionist system where the connectionist part ha to solve logic oriented inductive learning task similar to the term classification problem used in our experiment these problem range from the detection of a specific subterm to the satisfaction of a specific unification pattern and they can get a very satisfactory solution by our approach 
todate visualization ha not been extensively harnessed in knowledge discovery in database kdd in this paper we show that a multidimensional visualization mdv technique can be used synergistically with a machine learning program like c to uncover new knowledge used together the two approach span the kdd spectrum between complete automation on one hand and fully manual on the other we introduce mdv it implementation in a tool named winviz and show how winviz support the various task in kdd 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
viewpoint are coherent collection of fact thatdescribe a concept from a particular perspective they are essential for a wide variety of task suchas explanation generation and qualitative modeling we have identified many type of viewpointsand developed a program the view retriever for extracting them from knowledge base eithersingly or in combination the view retrieverprovides a general solution to the central problemin extracting viewpoint determining whichfacts are 
we propose a novel approach to extraposition in german within an alternative conception of syntax in which syntactic structure and linear order are mediated not via encoding of hierarchical relation but instead via order domain at the heart of our proposal is a new kind of domain formation which affords analysis of extraposition construction that are linguistically more adequate than those previously suggested in the literature 
in this paper we address the problem of scalability in temporal reasoning in particular new algorithm for efficiently managing large set of relation in the point algebra are provided our representation of time is based on timegraphs graph partitioned into a set of chain on which the search is supported by a rnetagraph data structure the approach is an extension of the time representation proposed by schubert taugher and miller in the context of story comprehension the algorithm presented in this work concern the construction of a timegraph from a given set of relation and are implemented in a temporal reasoning system called tg ii ex perimental result show that our approach is very efficient especially when the given relation admit representation a a collection of chain connected by relatively few cross chain link 
the fundamental matrix is a basic tool in the analysis of scene taken with two uncalibrated camera and the point algorithm is a frequently cited method for computing the fundamental matrix from a set of or more point match it ha the advantage of simplicity of implementation the prevailing view is however that it is extremely susceptible to noise and hence virtually useless for most purpose the paper challenge that view by showing that by preceding the algorithm with a very simple normalization translation and scaling of the coordinate of the matched point result are obtained comparable with the best iterative algorithm this improved performance is justified by theory and verified by extensive experiment on real image 
markov decision process mdps have recently been applied to the problem of modeling decision theoretic planning while traditional method for solving mdps are often practical for small state space their effectiveness for large ai planning problem is questionable we present an algorithm called structured policy iteration spi that construct optimal policy without explicit enumeration of the state space the algorithm retains the fundamental computational step of the commonly used modified policy iteration algorithm but exploit the variable and prepositional independency reflected in a temporal bayesian network representation of mdps the principle behind spi can be applied to any structured representation of stochastic action policy and value function and the algorithm itself can be used in conjunction with recent approximation method 
the fundamental tradeoff that is well known in knowledge representation and reasoning affect concept learning from example too representation of learning example using at tribute value ha proved to support efficient inductive algorithm but limited expressiveness whereas more expressive representation language typically subset of first order logic fol are supported by le efficient algorithm in fact an underlying problem is that of the number of different way of matching example just one in attribute val ue representatio n and potentially large in fol representation this paper describes a novel approach to perform representation shift on learning example the structure of these learning example initially represented using a subset of fol based language is reformulated so a to produce new learning example that are represented using an attribute value language what is considered to be an adequate structure varies according to the learning task we introduce the notion of morion from the greek to qualify this structure and show through a concrete example the advantage it of fers we then describe an algorithm which reformulates learning example automatically and go on to analyze it complexity this approach to deductive reformulation is implemented in the remo system that ha been experimented on the learning of the construction of chinese character 
term of feature and their qualitative geometric relationship we propose to detect group using perceptual organization criterion such a proximity symmetry parallelism and closure the detection of these feature is performed in an efficient way using proximity indexing since many group are created we also perform selection of relevant group by organizing them into set of similar perceptual content finally we present an initial implementation of a recognition system using these set a primitive it is an efficient colored graph matching algorithm using the adjacency matrix representation of a graph using indexing we retrieve matching hypothesis which are verified against each other with respect to topological constraint group of consistent hypothesis represent detected model instance in a scene the complete system is illustrated on real image we also discus further extension most object recognition system today address the problem of finding the location and orientation of an exactly known rigid object in a scene grimson s book give a lucid treatment for the geometric constraint used in these approach the presence of a model is inferred by the verification that such a model could indeed produce some of the observed data under an appropriate geometric transform however this approach cannot be extended to more general scenario containing object which may be very similar while being geometrically different consider for instance two different airplane which have similar feature but different geometry in other word generic recognition obviates the use of method based purely on the exact geometric structure of the object it is clear that the only way to solve this difficult problem is to reason about part and their arrangement this argument is supported by biederman s theory which state the sufficiency of a limited number of volumetric component or geons for the task of recognition recovery of part and their arrangement can help fast recognition of object even if they are occluded novel rotated in depth or extensively degraded we therefore have three problem to solve the extraction of primitive the description of scene in term of these primitive and the actual recognition of object in this paper we propose the use of perceptual grouping to approach the problem of generic recognition use of perceptual group is not new a it wa proposed in the s 
several recurrent network have been proposed a representation for the task of formal language learning after training a recurrent network recognize a formal language or predict the next symbol of a sequence the next logical step is to understand the information processing carried out by the network some researcher have begun to extracting finite state machine from the internal state trajectory of their recurrent network this paper describes how sensitivity to initial condition and discrete measurement can trick these extraction method to return illusory finite state description 
ensemble learning by variational free energy minimization is a tool introduced toneural network by hinton and van camp in which learning is described in term ofthe optimization of an ensemble of parameter vector the optimized ensemble is anapproximation to the posterior probability distribution of the parameter this toolhas now been applied to a variety of statistical inference problem in this paper i study a linear regression model with both parameter and hyperparameters i 
the scatter gather document browsing method us fast document clustering to produce table of content like outline of large document collection previous work developed linear time document clustering algorithm to establish the feasibility of this method over moderately large collection however even linear time algorithm are too slow to support interactive browsing of very large collection such a tipster the darpa standard text retrieval evaluation collection we present a scheme that support constant interaction time scatter gather of arbitrarily large collection after near linear time preprocessing this involves the construction of a cluster hierarchy a modification of scatter gather employing this scheme and an example of it use over the tipster collection are presented 
this paper explores different technique for explanation within the framework of the situation calculus using the so called stolen car problem a it main example two approach to explanation are compared the deductive approach usually found in the literature and a le common abductive approach both approach are studied in the context of two different style of representation 
we evaluate crustacean an inductivelogic programming algorithm that us inverseimplication to induce recursive clausesfrom example this approach is wellsuited for learning a class of self recursiveclauses which commonly appear in logic program because it search for common substructuresamong the example however little evidence exists that inverse implicationapproaches perform well when given onlyrandomly selected positive and negative example we show that crustaceanlearns 
most artificial natural language processing nlp system make use of some simple algorithm for parsing these algorithm overlook the inextricable link between parsing natural language and understanding it human parse language in a linear fashion our goal is to develop an nlp system that par in a linear and psychologically valid fashion when this goal is achieved our nlp system will be efficient and it will generate the correct interpretation in ambiguous situation in this paper we describe two nlp system whose parsing is driven by several heuristic the first is a bottom up system which is based on the work of ford bresnan kaplan the second system is a more expansive attempt incorporating the initial heuristic and several more this system run on a much larger domain and incorporates several new syntactic form it ha it weakness but it show good progress toward the goal of linearity 
roughly speaking adequatness is the property of a theorem proving method to solve simpler problem faster than more difficult one automated inferencing method are often not adequate a they require thousand of step to solve problem which human solve effortlessly spontaneously and with remarkable efficiency l shastri and v ajjanagadde who call this gap the artificial intelligence paradox suggest that their connectionist inference system is a first step toward bridging this gap in this paper we show that their inference method is equivalent to reasoning by reduction in the well known connection method in particular we extend a reduction technique called evaluation of isolated connection such that this technique together with other reduction technique solves all problem which can be solved by shastri and ajjanagadde s system under the same parallel time and space requirement consequently we obtain a semantics for shastri and ajjanagadde s logic but most importantly if shastri and ajjanagadde s logic really capture the kind of reasoning which human can perform efficiently then this paper show that a massively parallel implementation of the connection method is adequate 
the paper address the problem of computing the fundamental matrix which describes a geometric relationship between a pair of stereo image the epipolar geometry we propose a novel method based on virtual parallax instead of computing directly the spl time fundamental matrix we compute a homography with one epipole position and show that this is equivalent to computing the fundamental matrix simple equation are derived by reducing the number of parameter to estimate a a consequence we obtain an accurate fundamental matrix of rank two with a stable linear computation experiment with simulated and real image validate our method and clearly show the improvement over existing method 
in order to deal with unexpected or illegal behavior in multi agent system underlying causal model connecting the target system s behavior and each agent s behavior are indispensable in this paper we present a method for generating causal network which consist of arithmetic and differential relation for explicitly defined parameter and implicitly existing parameter embedded in the target system the task consists of three component a macro behavior rule generator which prepares implicit parameter and generates the rule about system s behavior at macro level a causal network constructor an explanation generator in the course of this process spatial extent are represented and reasoned with qualitative region we took a an example for this method the foraging behavior of ant colony which are typical mobile multi agent system with a local communication method by mean of the chemical pheromone 
we consider the effect of combining several least square estimator on the expected performanceof a regression problem computing the exact bias and variance curve a a function of the samplesize we are able to quantitatively compare the effect of the combination on the bias and varianceseparately and thus on the expected error which is the sum of the two first we show that bysplitting the data set into several independent part and training each estimator on a differentsubset the 
information retrieval system that use conceptual indexing to describe the information content perform better than syntactic indexing method based on word from a text however since conceptual index represent the semantics of a piece of information it is difficult to extract them automatically from a document and it is tedious to build them manually we implemented an information retrieval system that acquires conceptual index of text graphic and videotaped document our approach is to use an underlying model of the domain covered by the document to constrain the usds query this facilitates question based acquisition of conceptual index converting user query into index which accurately model the content of the document and can be reused we discus dedal a system that facilitates the indexing and retrieval of design document in the mechanical engineering domain a user formulates a query to the system and if there is no corresponding index dedal us the underlying domain model and a set of retrieval heuristic to approximate the retrieval and ask for confirmation from the user if the user find the retrieved information relevant dedal acquires a new index based on the query we demonstrate the relevance and coverage of the acquired index through experimentation 
we describe a method for computing visual correspondence which employ a formal model of the probability of a false match this model estimate the chance that the best match for each point could have occurred at random the model is effective at identifying point in one image for which there is no corresponding point in the other image a occurs at depth boundary in stereo and at motion boundary in optical flow more generally the model can be used to identify point where the best match is of poor quality a occurs in region of uniform texture we describe the similarity measure used in the method and present the formal model of a false match we also show example of using the method to compute stereo disparity 
abstract we give an analysis of the generalization error of cross validation in term of two natural measure of the difficulty of the problem under consideration the approximation rate the accuracy to which the target function can be ideally approximated a a function of the number of hypothesis parameter and the estimation rate the deviation between the training and generalization error a a function of the number of hypothesis parameter the approximation rate capture the complexity of the target function with respect to the hypothesis model and the estimation rate capture the extent to which the hypothesis model suffers from overfitting using these two measure we give a rigorous and general bound on the error of cross validation the bound clearly show the tradeoff involved with making the fraction of data saved for testing too large or too small by optimizing the bound with respect to we then argue through a combination of formal analysis plotting and controlled experimentation that the following qualitative property of cross validation behavior should be quite robust to significant change in the underlying model selection problem 
in this project we study the effect of a user s high level expository goal upon the detail of how case based reasoning cbr is performed and vice versa the effect of feedback from cbr on them our thesis is that case retrieval should reflect the user s ultimate goal in appealing to case and that these goal can be affected by the case actually available in a case base to examine this thesis we have designed and built frank flexible report and analysis system which is a hybrid blackboard system that integrates case based rule based and planning component to generate a medical diagnostic report that reflects a user s viewpoint and specification frank s control module relies on a set of generic hierarchy that provide taxonomy of standard report type and problemsolving strategy in a mixed paradigm environment our second focus in frank is on it response to a failure to retrieve an adequate set of supporting case we describe frank s planning mechanism that dynamically re specify the memory probe or the parameter for case retrieval when an inadequate set of case is retrieved and give an extended example of how the system responds to retrieval failure 
this paper is concerned with an inferential approach to information extraction reporting in particular on the result of an empirical study that wa performed to validate the approach the study brings together two line of research the rho framework for tractable terminological knowledge representation and the alembic message understanding system there are correspondingly two principal aspect of interest to this work from the knowledge representation perspective the present study serf to validate experimentally a normal form hypothesis that guarantee tractability of inference in the rho framework from the message processing perspective this study substantiates the utility of limited inference to information extraction 
technique for learning from data typically require data to be in standard form measurement must be encoded in a numerical format such a binary true or false feature numerical feature or possibly numerical code in addition for classification a clear goal for learning must be specified while some database may readily be arranged in standard form many others may be combination of numerical field or text with thousand of possibility for each data field and multiple instance of the same field specification a significant portion of the effort in real world data mining application involves defining identifying and encoding the data into suitable feature in this paper we describe an automatic feature extraction procedure adapted from modern text categorization technique that map very large database into manageable datasets in standard form we describe a commercial application of this procedure to mining a collection of very large database of home appliance service record for a major international retailer 
this paper present a physic based algorithm for hierarchical shape representation using deformable model with locally adaptive finite element our new adaptive finite element algorithm ensures that during subdivision the desirable finite element mesh generation property of conformity non degeneracy and smoothness are maintained through our algorithm we locally subdivide the triangular finite element based on the distance between the given datapoints and the model in this way we can very efficiently and accurately represent the shape of an object with a resulting small number of model node furthermore using our locally adaptive subdivision algorithm in conjunction with our model s global deformation we construct a hierarchical representation of the given d data 
the computational lexicalization of a grammar is the optimization of the link between lexicalized rule and lexical item in order to improve the quality of the bottom up filtering during parsing this problem is np complete and untractable on large grammar an approximation algorithm is presented the quality of the suboptimal solution is evaluated on real world grammar a well a on randomly generated one 
we deal with the moving target search problem where the location of the goal may change during the search process the trailblazer search tb chimura and tokoro achieves a systematic and effective search by maintaining a map the map store path information about the region where the algorithm ha already searched through however because of the growth of the map there is a problem that the time to make decision of search step increase rapidly we propose an algorithm the trailblazer search with an abstract map tbsa that reduces the cost of map maintenance and hence improves the reactiveness of the problem solver we partition the information about the problem space into local map and build an abstract map that control maintenance of the local map in this way the problem solver can systematically manage information about the problem space and it can utilize the map with le cost we evaluate the efficiency of our method and show how significant cost reduction in map maintenance can be achieved by using a two layered map 
we consider the problem of on line gradient descent learning forgeneral two layer neural network an analytic solution is presentedand used to investigate the role of the learning rate in controllingthe evolution and convergence of the learning process learning in layered neural network refers to the modification of internal parametersfjg which specify the strength of the interneuron coupling so a to bring the mapfjimplemented by the network a close a possible to a desired 
in this paper a probabilistic relational model is presented which combine relational algebra with probabilistic retrieval based on certain independence assumption the operator of the relational algebra are redefined such that the probabilistic algebra is a generalization of the standard relational algebra furthermore a special join operator implementing probabilistic retrieval is proposed when applied to typical document database query can not only ask for document but for any kind of object in the database in addition an implicit ranking of these object is provided in case the query relates to probabilistic indexing or us the probabilistic join operator the proposed algebra is intended a a standard interface to combined database and ir system a a basis for implementing user friendly interface 
we address the problem of estimating the position and motion of a human arm in d without any constraint on it behavior and without the use of special marker we model the arm a two truncated right circular cone connected with spherical joint we propose to use a recursive estimator for arm position and to provide the estimator with error signal obtained by comparing the projected estimated arm position with that of the actual arm in the image the system is demonstrated and tested on a real image sequence 
we exhibit a theoretically founded algorithm t for agnostic pac learning of decision tree of at most level whose computation time is almost linear in the size of the training set we evaluate the performance of this learning algorithm t on common real world datasets and show that for most of these datasets t provides simple decision tree with little or no loss in predictive power compared with c in fact for datasets with continuous attribute it error rate tends to be lower than that of c to the best of our knowledge this is the first time that a pac learning algorithm is shown to be applicable to real world classification problem since one can prove that t is an agnostic paclearning algorithm t is guaranteed to produce close to optimal level decision tree from sufficiently large training set for any distribution of data in this regard t differs strongly from all other learning algorithm that are considered in applied machine learning for which no guarantee can be given about their performance on new datasets we also demonstrate that this algorithm t can be used a a diagnostic tool for the investigation of the expressive limit of level decision tree finally t in combination with new bound on the vc dimension of decision tree of bounded depth that we derive provides u now for the first time with the tool necessary for comparing learning curve of decision tree for real world datasets with the theoretical estimate of paclearning theory valiant val had introduced the model for probably approximately correct learning in in applied machine learning an even larger literature exists about the performance of various other learning algorithm on realworld classification task however curiously enough this article apparently mark the first time that the performance of a pac learning algorithm for a model powerful enough to cover real world datasets a a special case is evaluated on real world classification task the paclearning algorithm t that we have developed for this purpose is described in section of this article and result about it performance on real world classification problem are discussed in the subsequent section in this introduction we will define some basic notion from theoretical and applied machine learning and also address some obstacle which one ha to overcome in order to combine both approach it should be mentioned in this context that although t is apparently the first pac learning algorithm that is tested on real world classification problem there ha previously been already a fruitful migration of various idea from pac learning theory into application see e g ds 
the paper unifies most of the current literature on d geometric invariant from point correspondence across multiple d view by using the tool of elimination from algebraic geometry the technique allows one to predict result by counting parameter and reduces many complicated result obtained in the past reconstructuon from two and three view epipolar geometry from seven point trilinearity of three view the use of a priori d information such a bilateral symmetry shading and color constancy and more into a few line of reasoning each the tool of grobner base computation is used in the elimination process in the process we obtain several result on n view geometry and obtain a general result on invariant function of view and it corresponding quadlinear tensor view admit minimal set of invariant function of quadlinear form with distinct coefficient that can be solved linearly from corresponding point across view this result ha non trivial implication to the understanding of n view geometry we show a new result on single view invariant based on point and show that certain relationship are impossible one of the appealing feature of the elimination approach is that it is simple to apply and doe not require any understanding of the underlying d from d geometry and algebra 
this paper deal with the automatic translation of route description into graphic sketch we discus some general problem implied by such inter mode transcription we propose a model for an automatic text to image translator with a two stage intermediate representation in which the linguistic representation of a route description precedes the creation of it conceptual representation 
it is widely accepted that the use of more compact representationsthan lookup table is crucial to scaling reinforcement learning rl algorithm to real world problem unfortunately almost all of thetheory of reinforcement learning assumes lookup table representation in this paper we address the pressing issue of combiningfunction approximation and rl and present a function approximatorbased on a simple extension to state aggregation a commonlyused form of compact 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
covering and divide and conquer are two well established search technique for top down induction of propositional theory however for top down induction of logic program only covering ha been formalized and used extensively in this work the divide and conquer technique is formalized a well and compared to the covering technique in a logic programming framework covering work by repeatedly specializing an overly general hypothesis on each iteration focusing on finding a clause with a high coverage of positive example divide and conquer work by specializing an overly general hypothesis once focusing on discriminating positive from negative example experimental result are presented demonstrating that there are case when more accurate hypothesis can be found by divide and conquer than by covering moreover since covering considers the same alternative repeatedly it tends to be le efficient than divide and conquer which never considers the same alternative twice on the other hand covering search a larger hypothesis space which may result in that more compact hypothesis are found by this technique than by divide and conquer furthermore divide and conquer is in contrast to covering not applicable to learning recursive definition 
in this article a new characterization of the set of diagnosis in the domain of consistency based diagnosis is developed the need for such a characterization arise from the inability of the current characterization e g minimal diagnosis and kernel diagnosis to deal with abstraction like component hierarchy we propose a characterization based on theory which take advantage of hierarchy and therefore allows a concise description of various set of diagnosis e g minimal diagnosis and kernel diagnosis we investigate a concise description of the set of all minimal diagnosis in detail and focus on hierarchical theory finally we show that theory based diagnosis and hierarchical theory provide efficient generator for computing minimal diagnosis 
in this paper we describe the theoretical formulation of remap an approach for the training and estimation of posterior probability using a recursive algorithm that is reminiscent of the em expectation maximization algorithm dempster et al for the estimation of data likelihood although very general the method is developed in the context of a statistical model for transition based speech recognition using artificial neural network ann to generate probability for hidden markov model hmms in the new approach we use local conditional posterior probability of transition to estimate global posterior probability of word sequence given acoustic speech data although we still use anns to estimate posterior probability the network is trained with target that are themselves estimate of local posterior probability these target are iteratively re estimated by the remap equivalent of the forward and backward recursion of the baum welch algorithm baum et al baum to guarantee regular increase up to a local maximum of the global posterior probability convergence of the whole scheme is proven unlike most previous hybrid hmm ann system that we and others have developed the new formulation determines the most probable word sequence rather than the utterance corresponding to the most probable state sequence also in addition to using all possible state sequence the proposed training algorithm us posterior probability at both local and global level and is discriminant in nature 
this paper describes the design and implementation ofa single instruction multiple data simd depth frommotionalgorithm on the image understanding architecturesimulator correspondence are established in parallelfor two temporally separated image through correlation the correspondence are used to determine thetranslational and rotational motion parameter of the camerathrough a parallel motion algorithm this is done byfirst determining the approximate translational parametersand 
automatic recognition of promoter sequence is an important open problem in molecular biology unfortunately the usual machine learning version of this problem is critically flawed in particular the dataset available from the irvine repository wa drawn from a compilation of promoter sequence that were preprocessed to conform to the biologist related notion of the corrserzsus sequence a first order approximation with a number of shortcoming that are well known in molecular biology although concept description learned from the irvine data may represent the consensus sequence they do not represent promoter more generally imperfection in preprocessed data and statistical variation in the location of biologically meaningful feature within the raw data invalidate standard attribute based approach i suggest a dataset a concept description language and a model of uncertainty in the promoter data that are all biologically justified then address the learning problem with incremental probabilistic evidence combination this knowledge based approach yield a more accurate and more credible solution than other more conventional machine learning system 
we present an efficient and geometrically intuitive algorithm to reliably interpret the image velocity of moving object in d it is well known that under weak perspective the image motion of point on a plane can be characterised by an affinetransformation we show that the relative image motion of a nearby noncoplanar point and it projection on the plane is equivalent to motion parallax and because it is independent of viewer rotation it is a reliable geometric cue to d shape and viewer object motion in particular we show how to interpret the motion parallax vector of non coplanar point and contour and the curl divergence and deformation component of the affine transformation defined by the three point or a closed contour of the plane in order to recover the projection of the axis of rotation of a moving object the change in relative position of the object the rotation about the ray the tilt of the surface and a one parameter family of solution for the slant a a function of the magnitude of the rotation of the object the latter is a manifestation of the ba relief ambiguity these measurement although representing an incomplete solution to structure from motion are the only subset of structure and motion parameter which can be reliably extracted from two view when perspective effect are small we present a real time example in which the d visual interpretation of hand gesture or a hand held object is used a part of a man machine interface this is an alternative to the polhemus coil instrumented dataglove commonly used in sensing manual gesture 
this paper study the convergence property of the well knownk mean clustering algorithm the k mean algorithm can be describedeither a a gradient descent algorithm or by slightly extendingthe mathematics of the em algorithm to this hard thresholdcase we show that the k mean algorithm actually minimizes thequantization error using the very fast newton algorithm introductionk mean is a popular clustering algorithm used in many application including theinitialization of more 
we have been developing a methodology system called gls global learning scheme for knowledge discovery in database the development of gls ha two main aspect the first is to develop a m i strategy system that is many kind of discovery learning method are cooperatively used in multiple learning phase for performing multi aspect intelligent data analysis a well a multi level conceptual abstraction and learning a a multi strategy system gls is implemented a a toolkit composed of several sub system and optional part with a multi level structure we have finished main part belonging to this aspect and have undertaken another aspect i e extending gls into a multi agent distributed and cooperative discovery system we try to increase versatility and autonomy of gls by multi strategy and distributed cooperation this paper briefly discus these two aspect of gls 
a new inductive learning system lab learning for abduction is presented which acquires abductive rule from a set of training example the goal is to find a small knowledge base which when used abductively diagnosis the training example correctly and generalizes well to unseen example this contrast with past system that inductively learn rule that are used deductively each training example is associated with potentially multiple category disorder instead of one a with typical learning system lab us a simple hill climbing algorithm to efficiently build a rule base for a set covering abductive system lab ha been experimentally evaluated and compared to other learning system and an expert knowledge base in the domain of diagnosing brain damage due to stroke 
the recently introduced quot parameterized self organizing map quot quot psom quot show excellent function mapping capability after learning of a remarkablesmall set of training data this is a very important feature in field wherethe acquisition of training data is costly for example in robotics a a firstdemonstration we compare result for the task of kinematic mapping of a dof robot finger obtained by a psom and a standard backprop network a new way of structuring learning becomes 
this paper is about some of the social aspect of knowledge and action relevant to thinking in ai and in particular the basic experience of multiple perspective and integrating different kind of local knowledge it discus way of rethinking a number of familiar concept including fact interaction knowledge and organization raising question about how well we can currently capture their social dimension conceptually representationally and computationally it suggests several approach to developing more complete computational model of these phenomenon 
the most commonly used neural network model are not well suitedto direct digital implementation because each node need to performa large number of operation between floating point value fortunately the ability to learn from example and to generalize isnot restricted to network of this type indeed network where eachnode implement a simple boolean function boolean network canbe designed in such a way a to exhibit similar property twoalgorithms that generate boolean 
in delayed reinforcement learning an agent is concerned with theproblem of discovering an optimal policy a function mapping statesto action the most popular delayed reinforcement learning technique q learning ha been proven to produce an optimal policyunder certain condition however often the agent doe not followthe optimal policy faithfully the agent must also explore theworld the optimal policy produced by q learning is no longeroptimal if it prescription are only 
the retrieval model based on the extended boolean retrieval framework e g the fuzzy set model and the extended boolean model have been proposed in the past to provide the conventional boolean retrieval system with the document ranking facility however due to undesirable property of evaluation formula for the and and or operation the former generates incorrect ranked output in certain case and the latter suffers from the complexity of computation there have been a variety of fuzzy operator to replace the evaluation formula in this paper we first investigate the behavioral aspect of the fuzzy operator and address important issue to affect retrieval effectiveness we then define an operator class called positively compensatory operator giving high retrieval effectiveness and present a pair of positively compensatory operator providing high retrieval efficiency a well a high retrieval effectiveness all the claim are justified through experiment 
trospective reasoning we can reuse expectation that apply to theplanner a a whole for it case based part during the planning process the introspective reasonercompares the planner s reasoning to it assertionsabout ideal behavior when a failure is detected forinstance if the system judge that the retrieved case isnot the quot best quot case in memory the introspective reasonerconsiders related assertion to pinpoint the sourceof the failure and to suggest a solution in this case our 
terminological knowledge representation sys tems tkrss are tool for designing and u ing knowledge base that make use of termino logical language or concept language the tkrs we consider in this paper is of practi cal interest since it go beyond the capabil ities of presently available tkrss first our tkrs is equipped with a highly expressive con cept language called alcnr including gen eral complement of concept number restriction and role conjunction second it allows one to express inclusion statement between general concept in particular to express ter minological cycle we provide a sound com plete and terminating calculus for reasoning in alcnr knowledge base based on the general technique of constraint system 
the goal of information extraction task is to identify categorize classify relate and normalize specific information of interest found in free text and to make that information available to a back end data base data fusion or other application a data structure referred to a a that have emerged these desideratum feed into the discussion of design element and a procedural review of the design process design iteration use of those linguistic analysis tool etc 
the utility problem occurs when the cost associated with searching for relevant knowledge outweighs the benefit of applying this knowledge one common machine learning strategy for coping with this problem ensures that stored knowledge is genuinely useful deleting any structure that do not contribute to performance in a positive sense and essentially limiting the size of the knowledge base we will examine this deletion strategy in the context of case based reasoning cbr system in cbr the impact of the utility problem is very much dependant on the size and growth of the case base larger case base mean more expensive retrieval stage an expensive overhead in cbr system traditional deletion strategy will keep performance in check and thereby control the classical utility problem but they may cause problem for cbr system competence this effect is demonstrated experimentally and in reply two new deletion strategy are proposed that can take both competence and performance into consideration during deletion 
this paper present a new model of anaphoric processing that utilizes the establishment of coherence relation between clause in a discourse we survey data that comprises a currently stalemated argument over whether vp ellipsis is an inherently syntactic or inherently semantic phenomenon and show that the data can be handled within a uniform discourse processing architecture this architecture which revise the dichotomy between ellipsis v model interpretive anaphora given by sag and hankamer is also able to accommodate divergent theory and data for pronominal reference resolution the resulting architecture serf a a baseline system for modeling the role of cohesive device in natural language 
in this work we present result from a new formulation fordetermining image velocity from a time sequence of x ray projectionimages of flowing fluid starting with the conservation of mass principle and physic of x ray projection we derive a motion constraint equationfor projection imaging a practical special case of which is shown to bethe horn and schunck s optical flow constraint we are interested in thestudy of non rigid motion of blood which is an incompressible fluid and 
my system us an interactive genetic algorithm to learn a user s criterion for the task of generating musical rhythm interactive genetic algorithm smith are well suited to solving this problem because they allow for a user to simply execute fitness function that is to choose which rhythm or feature of rhythm he like without necessarily understanding the detail or parameter of these function a the system learns develops an increasingly accurate model of the function which represents the user s criterion the quality of the rhythm it produce improves to suit the user s taste this approach is largely motivated by richard dawkins who succinctly summarizes the attraction of iga for artistic endeavor in stating effective searching procedure become when the search space is sufficiently large indistinguishable from true creativity dawkins in the context of this project rhythm are one measure long sequence of note and rest occurring on natural pulse subdivision of a beat i only deal with a specific subset of the enormous class of rhythm in order to provide a well defined domain for the application of the learning algorithm the benefit of this reduction of the domain is that a rhythm phenotype can now be viewed a a simple vector thus the set of rhythm satisfying the user s criterion could be represented by a boolean formula i actually use a slightly more complex representation for the rhythm genotype motivated by the benefit of using a diploid genetic structure consisting of several short array template the order of the layering of these template in creating the phenotype effectively determines the dominance hierarchy between the gene the simplest mode of interaction is for the user to playback each of the rhythm in a randomly generated population and then subjectively assign them fitness value based upon their satisfaction of his criterion the system then us standard ga selection with fitness scaling reproduction with crossover monitor and mutation operator in order to deal with the difficulty resultant from the subjectivity and variability of the user s criterion there are also several objective function with which the system can automatically evolve generation of rhythm syncopation density downbeat beat repetition cross rhythm and cluster function are currently included each of these function represents an axis in a feature space which is useful for distinguishing rhythm while these are only a few of the many possible objective function that could be implemented they provide a richset of possibility with which to begin exploring the user can specify the ideal target value for each of these fitness function and also their relative importance weighting of coefficient in determining the overall fitness of a rhythm the system then automatically evolves the indicated number of successive generation using the objective fitness value to determine selection the system also make use of a meta level genetic algorithm designed to evolve population of parameter target value and weight to the objective fitness function defined above this is motivated by the research done in the application of genetic algorithm to the k nearest neighbor technique of classification punch et al each meta level individual represents a warping of k nn space such that the fitness of each individual is determined by how well it warping of the feature space help to discriminate useful feature and thus correctly perform classification evolving population of metaindividuals allows a user to quickly reduce the search space by subjective evaluation of the rhythm generated by the meta individual without having to directly specify value for the objective function this combination of method prof to be a powerful hybrid approach to the subjectivity problem one which allows for greater coverage of the search space than would have been possible ordinarily using a small population which is necessitated by most iga s and is particularly important when dealing with sequential acoustic data and more efficient convergence on a satisficing solution the system is able to converge on near optimal solution acceptable to test user after about fifty user evaluation of rhythm while the ga itself is mechanically quite simple it is important to note that the implementation of appropriate fitness function is difficult and largely determines the musicality of the output the major future improvement will involve adding the capacity for the system to learn to design it own fitness function to represent feature characteristic of rhythm selected by user in past session 
the puzzle is the largest puzzle of it type that can be completely solved it is simple and yet obeys a combinatorially large problem space of state the n n extension of the puzzle is np hard in the first part of this paper we present complete statistical data based on an exhaustive evaluation of all possible tile configuration our result include data on the expected solution length the easiest and worst configuration and the density and distribution of solution node in the search tree in our second set of experiment we used the puzzle a a workbench model to evaluate the benefit of node ordering scheme in iterative deepening a ida one highlight of our result is that almost all ida implementation perform worse than would be possible with a simple random ordering of the operator 
we describe a new algorithm for table driven parsing with context free grammar designed to support efficient syntactic analysis of natural language the algorithm provides a general framework in which a variety of parser control strategy can be freely specified bottom up strategy top down strategy and strategy that strike a balance between the two the framework permit better sharing of parse forest substructure than other table driven approach and facilitates the early termination of semantically ill formed partial par the algorithm should thus find ready application to large scale natural language processing 
the turing test wa proposed by alan turing in he called it the imitation game in hu loebner prize competition offering a f h loebner started the prize to the author of the first computer program to pas an unrestricted turing test annual competition are held each year with smaller prize for the best program on a restricted turing test this paper describes the development of one such turing system including the technical design of the program and it performance on the first three loebner prize competition we also discus the program s four year development effort which ha depended heavily on constant interaction with people on the internet via tinymuds multiuser network communication server finally we discus the design of the loebner competition itself and address it usefulness in furthering the development of artificial intelligence 
the rete and treat algorithm are considered the most efficient implementation technique for forward chaining rule system these algorithm support a language of limited expressive power assertion are not allowed to contain variable making universal quantification impossible to express except a a rule in this paper we show how to support full unification in these algorithm we also show that supporting full unification is costly full unification is not used frequently a combination of compile time and run time check can determine when full unification is not needed we present data to show that the cost of supporting full unification can be reduced in proportion to the degree that it isn t employed and that for many practical system this cost is negligible 
winslett proposed a method for reasoning about action called the possible model approach pma the pma successfully removed the major difficulty manifested by ginsberg and smith s possible world approach pwa in this paper we show that winslett s pma fails to solve the frame and ramification problem for some action a doe the pwa from this observation we classify action a definite and indefinite and find that in general the pma is not appropriate for both definite and indefinite action we propose a new approach to formalize action based on persistence we compare our approach with the pma in detail and show that our new formalization can avoid the problem in the pma and pwa in most case and give more intuitive result for reasoning about action regardless of whether the action is definite or indefinite 
this paper proposes a new algorithm which when provided the relative cost of computation v probing minimizes the total cost of diagnosis during the diagnosis process the decision of whether to probe or to compute is dependent on the expected cost and benefit of each alternative it is unlikely that we will be able to find general analytic and simple to compute model for the cost and benefit therefore we base our algorithm on simple empirically derived model of cost and benefit with these model our algorithm operates by continuously choosing the optimum action to make next this algorithm will not blow up on the rare pathological case and will always on average find diagnosis at equal to or better cost than a conventional gde sherlock when the cost of probing is high then our algorithm behaves exactly the same a gde sherlock when the cost of computation is high the algorithm performs the diagnosis at far lower cost than gde sherlock 
face image are difficult to interpret because they are highly variable source of variability include individual appearance d pose facial expression and lighting we describe a compact parametrised model of facial appearance which take into account all these source of variability the model represents both shape and grey level appearance and is created by performing a statistical analysis over a training set of face image a robust multi resolution search algorithm is used to fit the model to face in new image this allows the main facial feature to be located and a set of shape and grey level appearance parameter to be recovered a good approximation to a given face can be reconstructed using le than of these parameter this representation can be used for task such a image coding person identification pose recovery gender recognition and expression recognition the system performs well on all the task listed above 
we present result using a markov random field color texture model for the unsupervised segmentation of image of outdoor scene the color random field model describes textured region in term of spatial interaction within color band and between different color band the model is used by a segmentation algorithm based on agglomerative hierarchical clustering at the heart of the clustering is a step wise optimal merging process that at each iteration maximizes a global performance functional the test for stopping the clustering is based on change in the likelihood of the image we provide experimental result that demonstrate the performance of the segmentation algorithm on color image of natural scene most of the processing during segmentation is local making the algorithm amenable to high performance parallel implementation 
we present a simple circumscriptive method for formalizing action with indirect effect ramification and show that in several example all second order quantifier can be eliminated from these formalization using existing technique for computing circumscription one of the two symbolic computation method employed here is a generalization of predicate completion and the other is based on the scan algorithm the simplicity of our new approach to representing action is due to the use of the formalism of nested abnormality theory 
most practical work on ai planning system during the last fifteen year ha been based on hierarchical task network htn d ecomposition but until now there ha been very little analytical work on the property of htn planner this paper describes how the complexity of htn planning varies with various condition on the task network network are required to be totally ordered and whether variable are allowed from this table we can draw the following conclusion 
this paper present a precise market model for awell defined class of distributed configuration designproblems given a design problem the model definesa computational economy to allocate basic resourcesto agent participating in the design the result ofrunning these quot design economy quot constitutes themarket solution to the original problem after definingthe configuration design framework i describe themapping to computational economy and our resultsto date for some simple 
this paper describes a hierarchical estimation framework for the computation of diverse representation of motion information the key feature of the resulting framework or family of algorithm are a global model that constrains the overall structure of the motion estimated a local model that is used in the estimation process and a coarse fine refinement strategy four specific motion model affine flow planar surface flow rigid body motion and general optical flow are described along with their application to specific example 
this paper present a fast probabilistic method for coordination based on markov process provided the agent goal and preference are sufficiently compatible by using markov chain a the agent inference mechanism we are able to analyze convergence property of agent interaction and to determine bound on the expected time of convergence should the agent goal or preference not be compatible they can detect this situation since coordination ha not been achieved within a probabilistic time bound and the agent can then resort to a higher level protocol the application used for motivating the discussion is the scheduling of task though the methodology may be applied to other domain using this domain we develop a model for coordinating the agent and demonstrate it use in two example 
unlike existing global shape from shading algorithm which involve the brightness constraint in their formulation we propose a new algorithm which replaces the brightness constraint by an intensity gradient constraint this is a global approach which obtains the solution by the minimization of an error function over the entire image through the linearization of the reflectance map and the discretization of the surface gradient the intensity gradient can be expressed a a linear function of the surface height a quadratic error function which involves the intensity gradient constraint and the traditional smoothness constraint is minimized efficiently by solving a sparse linear system using the multigrid technique neither the information at singular point nor the information at occluding boundary is needed for the initialization 
abstract two standard scheme for learning in classifier system have been proposed in the literature the bucket brigade algorithm bba and the profit sharing plan psp the bba is a local learning scheme which requires le memory and lower peak computation than the psp whereas the psp is a global learning scheme which typically achieves a clearly better performance than the bba this requirement versus achievement difference known a the locality globality dilemma is addressed in this paper a new algorithm called hierarchical chunking algorithm hca is presented which aim at synthesizing the local and the global learning scheme this algorithm offer a solution to the locality globality dilemma for the important class of reactive classifier system the content is a follows section describes the locality globality dilemma and motivates the necessity of it solution section briefly introduces basic aspect of reactive classifier system that are relevant to this paper section present the hca section give an experimental comparison of the hca the bba and the psp section concludes the paper with a discussion and an outlook on future work motivation 
we derive global h optimal training algorithm for neural network these algorithm guarantee the smallest possible predictionerror energy over all possible disturbance of fixed energy and aretherefore robust with respect to model uncertainty and lack ofstatistical information on the exogenous signal the ensuing estimatorsare infinite dimensional in the sense that updating theweight vector estimate requires knowledge of all previous weightesimates a certain finite dimensional 
in this paper we propose a propositional temporal language based on fuzzy temporal constraint which turn out to be expressive enough for domain like many coming from medicinewhere knowledge is of propositional nature and an explicit handling of time imprecision and uncertainty are required the language is provided with a natural possibilistic semantics to account for the uncertainty issued by the fuzziness of temporal constraint we also present an inference system based on specific rule dealing with the temporal constraint and a general fuzzy modus ponens rule whereby behaviour is shown to be sound the analysis of the different choice a fuzzy operator lead u to identify the well known lukasiewicz implication a very appropriate to define the notion of possibilistic entailment an essential element of our inference system 
a main obstacle for building large diagnosis system is the problem of deciding when to use which inference pattern or representation if diagnostic action such a changing representation or applying specific inference pattern are understood in term of change of working hypothesis the control problem becomes a belief revision problem when to adopt or drop belief our approach proceeds in two step first we adopt the principle of informational economy a kind of a law of inertia for diagnostic process it proposes candidate for revised belief state in a second step we employ specific diagnostic knowledge to actually choose the next belief state 
the computation of optical flow relies on merging information available over an image patch to form an estimate of d image velocity at a point this merging process raise a host of issue which include the treatment of outlier in component velocity measurement and the modeling of multiple motion within a patch which arise from occlusion boundary or transparency we present a new approach for dealing with these issuesm which s based on the use of a probabilistic mixture model to 
previous effort at facial expression recognition have been based on the facial action coding system facs a representation developed in order to allow human psychologist to code expression from static facial mugshot we develop new more accurate representation for facial expression by building a video database of facial expression and then probabilistically characterizing the facial muscle activation associated with each expression using a detailed physical model of the skin and muscle this produce a muscle based representation of facial motion which is then used to recognize facial expression in two different way the first method us the physic based model directly by recognizing expression through comparison of estimated muscle activation the second method us the physic based model to generate spatio temporal motion energy template of the whole face for each different expression these simple biologically plausible motion energy template are then used for recognition both method show substantially greater accuracy at expression recognition than ha been previously achieved 
generating language that reflects the temporal organization of represented knowledge requires a language generation model chat integrates contemporary theory of tense and aspect temporal representation and method to plan text this paper present a model that produce complex sentence that reflect temporal relation present in underlying temporal concept the mam result of this work is the successful application of constrained linguistic theory of tense and aspect to a generator which produce meaningful event combination and selects appropriate connecting word that relate them 
we present an active object recognition strategy which combine the use of an attentionmechanism for focusing the search for a d object in a d image with a viewpoint controlstrategy for disambiguating recovered object feature the attention mechanism consists of aprobabilistic search through a hierarchy of predicted feature observation taking object into aset of region classified according to the shape of their bounding contour we motivate the useof image region a a 
a model based diagnosis procedure trace connection between component only where these are provided explicitly in the system description consequently structure fault fall between the mesh this problem ha been known since research started in this field davis but no general solution ha been presented so far we present a procedure to diagnose structure fault based on a scheme to detect hidden interaction guided by the observation that structure fault lead to discrepancy in apparently unrelated area and which in contrast to preist welham modifies the system description dynamically like davis approach the one presented in the paper is based on the principle that an interaction can occur only where component are adjacent in some way davis unlike davis approach we introduce an explicit representation scheme for hidden interaction a hidden interaction model link a required contextual behaviour independent constellation to the impact of the interaction on the overall system behaviour in order to control hidden interaction hypothesis we exploit the structure of diagnosis based on behavioural mode assignment 
in this paper a framework is developed for measuring the complexity of deduction in an abstract and computationally perspicuous manner a a notion of central importance appears the so called polynomial transparency of a calculus if a logic calculus posse this property then the complexity of any deduction can be correctly measured in term of it inference step the resolution calculus lack this property it is proven that the number of inference step of a resolution proof doe not give a representative measure of the actual complexity of the proof even if only shortest proof are considered we use a class of formula which have proof with a polynomial number of inference step but for which the size of any proof is exponential the polynomial intransparency of resolution is due to the renaming of derived clause which is a fundamental deduction mechanism this result motivates the development of new data structure for the representation of logical formula 
we consider algorithm for learning functionsf x y where x and y are finite andthere is assumed to be no noise in the data learning algorithm alg are connected with gamma alg the set of prior probability distributionsfor which they are optimal a methodfor constructing gamma alg from alg is given andthe relationship between the various gamma alg is discussed improper algorithm are identifiedas those for which gamma alg ha zero volume improper algorithm are 
probabilistic network which provide compact description of complex stochastic relationship among several random variable are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence we show that network with fixed structure containing hidden variable can be learned automatically from data using a gradient descent mechanism similar to that used in neural network we also extend the method to network with intensionally represented distribution including network with continuous variable and dynamic probabilistic network because probabilistic network provide explicit representation of causal structure human expert can easily contribute pnor knowledge to the training process thereby significantly improving the learning rate adaptive probabilistic network apns may soon compete directly with neural network a model in computational neuroscience a well a in industrial and financial application 
our aim is to clarify which nonmonotonic consequence relation j is given by a set of supernormal default i e default of the form true there are in fact a number of proposal forj e g the skeptical and the credulous semantics in this paper we look at the space of all possible default semantics and try to characterize the known one by their property especially the valid deduction rule for instance it seems reasonable to require that any useful semantics should coincide with the original cwa if this is consistent we might also want to allow proof by case analysis then we get the skeptical semantics assuming some other very natural deduction rule our result are in fact completeness proof for natural deduction system based on dieren t default semantics 
in this paper a bayesian inference network model for automatic indexing with index term descriptor from a prescribed vocabulary is presented it requires an indexing dictionary with rule mapping term of the respective subject field onto descriptor and inverted list for term occuring in a set of document of the subject field and descriptor manually assigned to these document the indexing dictionary can be derived automatically from a set of manually indexed document an application of the network model is described followed by an indexing example and some experimental result about the indexing performance of the network model 
device representation and reasoning with affective relation occupies a middle ground between classical model based diagnosis and heuristic expert system a device is modeled by specifying a set of diagnostically motivated affective relation among it component reasoning is then performed by a set of inference rule that reason with the model to propagate symptom through the component representation and reasoning with affective relation extends several benefit of classical model based diagnosis the model a a unifying framework for knowledge methodical coverage of the domain and diagnostic reasoning based on equipment design and causality to a class of problem where classical model based diagnosis cannot be applied because the required model cannot be reasonably obtained or represented our work evolved from our redesign of a heuristic expert system for monitoring long distance telephone switching system and is applicable to highly complex self checking system 
in this paper we propose the language of state specification to uniformly specify effect of action executability condition of action and dynamic and static constraint this language allow u to be able to express effect of action and constraint with same first order representation but different intuitive behavior to be specified differently we then discus how we can use state specification to extend the action description language a and lo 
this paper describes an experimental investigationinto the map building and exploration capability ofa mobile robot two type of map are used a setof line and point feature and a grid based free spacemap potential feature are extracted from sonar rangereadings and classed a confirmed if detected repeatedly the free space map is derived from the set ofconfirmed feature a distance transform algorithm isthen used to plan path on this map the confirmedfeatures are used by a 
a novel method for combining decision treesand kernel density estimator is proposed standard classification tree or class probabilitytrees provide piecewise constant estimatesof class posterior probability kerneldensity estimator can provide smoothnon parametric estimate of class probability but scale poorly a the dimensionalityof the problem increase this paperdiscusses a hybrid scheme which us decisiontrees to find the relevant structure in 
we investigated a set of query message taken from an usenet newsgroup and analyzed relation between the nature of problem solving activity and their natural language description based on the corpus investigation this paper proposes an efficient computational mechanism for recovering problem solving activity from query message written in japanese the main claim of the paper is that the underlying problem solving activity described in a natural language message can be efficiently recovered if provided with general knowledge on human problem solving and the associated linguistic pattern in the description 
we describe clip r a theory revision system for the revision of clip rule base clip r differs from previous theory revision system in that it operates on forward chaining production system revision of production system rulebases is important because production system can perform a variety of task such a monitoring and design in addition to classification task that have been addressed by previous research we show that clip r can take advantage of a variety of user specified constraint on the correct processing of instance such a ordering constraint on the displaying of information and the content of the final fact list in addition we show that clip r can operate a well a existing system when the only constraint on processing an instance is the correct classification of the instance 
the problem of d shape description particularly with contour partitioning grouping and classification in term of straight and curved based on the minimum description length mdl criterion and shape fitting technique is discussed the mdl criterion is used to detect outlier in connection with shape fitting using the mdl criterion it is possible to derive for a given data set and a class of model a description which best explains the data a new algorithm for fitting d point to an ellipse is presented 
one of the original motivation for research in qualitative physic wa the development of intelligent tutoring system and learning environment for physical domain and complex system this paper demonstrates how a synergistic combination of qualitative physic and other ai technique can be used to create an intelligent learning environment for student learning to analyze and design thermodynamic cycle pedagogically this problem is important because thermodynamic cycle express the key property of system which interconvert work and heat such a power plant propulsion system refrigerator and heat pump and the study of thermodynamic cycle occupies a major portion of an engineering student s training in thermodynamics this paper describes cyclepad a fully implemented learning environment which capture a substantial fraction of a thermodynamics textbook s knowledge and is designed to scaffold student who are learning the principle of such cycle we analyze the combination of idea that made cyclepad possible comment on some lesson learned about the utility of various technique and describe our plan for classroom experimentation 
unsupervised learning procedure have been successful at low levelfeature extraction and preprocessing of raw sensor data so far however they have had limited success in learning higher orderrepresentations e g of object in visual image a promising approachis to maximize some measure of agreement between theoutputs of two group of unit which receive input physically separatedin space time or modality a in becker and hinton becker de sa using the same 
in data oriented parsing dop an annotated language corpus is used a a virtual stochastic grammar an input string is parsed by combining subtrees from the corpus a a consequence one parse tree can usually be generated by several derivation that involve different subtrees this lead to a statistic where the probability of a parse is equal to the sum of the probability of all it derivation in scha an informal introduction to dop is given while bod provides a formalization of the theory in this paper we show that the maximum probability parse can be estimated in polynomial time by applying monte carlo technique the model wa tested on a set of hand parsed string from the air travel information system atis corpus preliminary experiment yield test set parsing accuracy 
although artificial neural network have been applied in a variety of real world scenario with remarkable success they have often been criticized for exhibiting a low degree of human comprehensibility technique that compile compact set of symbolic rule out of artificial neural network offer a promising perspective to overcome this obvious deficiency of neural network representation this paper present an approach to the extraction of if then rule from artificial neural network it key mechanism is validity interval analysis which is a generic tool for extracting symbolic knowledge by propagating rule like knowledge through backpropagation style neural network empirical study in a robot arm domain illustrate the appropriateness of the proposed method for extracting rule from network with real valued and distributed representation 
this paper investigates learning in a lifelong context lifelong learning address situation in which a learner face a whole stream of learn ing task such scenario provide the opportunity to transfer knowledge across multiple learning task in order to generalize more accurately from le training data in this paper several different approach to lifelong learning are described and applied in an object recognition domain it is shown that across the board lifelong learning approach generalize consistently more accurately from le training data by their ability to transfer knowledge across learning task 
for a number of computational purpose including visualization of scientific data and registration of multimodal medical data smooth curve must be approximated by polygonal curve and surface by polyhedral surface an inherent problem of these approximation algorithm is that the resulting curve and surface appear faceted boundary following and iso surface construction algorithm are typical example to reduce the apparent faceting smoothing method are used in this paper we introduce a new method for smoothing piecewise linear shape of arbitrary dimension and topology this new method is in fact a linear low pas filter that remove high curvature variation and doe not produce shrinkage it computational complexity is linear in the number of edge or face of the shape and the required storage is linear in the number of vertex 
this paper present bidirectional chart generation bcg algorithm a an uniform control mechanism for sentence generation and text planning it is an extension of semantic head driven generation algorithm shieber et al in that recomputation of partial structure and backtracking are avoided by using a chart table these property enable to handle a large scale grammar including text planning and t o implement the algorithm in parallel programming language other merit of the algorithm are to deal with multiple context and to keep every partial structure in the chart it becomes easier for the generator to find a recovery strategy when user cannot understand the generated text 
the purpose of this paper is to study the fundamental mechanism human use in argumentation and it role in different major approach to commonsense reasoning in ai and logic programming we present three novel result we develop a theory for argumentation in which the acceptability of argument is precisely defined we show that logic programming and nonmonotonic reasoning in ai are different form of argumentation we show that argumentation can be viewed a a special form of logic programming with negation a failure this result introduces a general method for generating metainterpreters for argumentation system 
we present a new approach to disambiguating syntactically ambiguous word in context based on variable memory markov vmm model in contrast to fixed length markov model which predict based on fixed lenth history variable memory markov model dynamically adapt their history length based on the training data and hence may use fewer parameter in a test of a vmm based tagger on the brown corpus of token are correctly classified 
to alleviate the problem of overwhelming complexity in grasp synthesis and path planning associated with robot task planning we adopt the approach of teaching the robot by demonstrating in front of it the system ha four component the observation system the grasping task recognition module the task translator and the robot system the observation system comprises an active multibaseline stereo system and a dataglove the data stream recorded is then used to track object motion this paper illustrates how complimentary sensory data can be used for this purpose the data stream is also interpreted by the grasping task recognition module which produce higher level of abstraction to describe both the motion and action taken in the task the resulting information are provided to the task translator which creates command for the robot system to replicate the observed task in this paper we describe how these component work with special emphasis on the observation system the robot system that we use to perform the grasping task comprises the puma arm and the utah mit hand 
this paper describes an efficient control mechanism for incorporating picture specific context in the task of image interpretation although other knowledge based vision system use general domain context in reducing the computational burden of image interpretation to our knowledge this is the first effort in exploring picture specific collateral information we assume that constraint on the picture are generated from a natural language understanding module which process descriptive text accompanying the picture we have developed a unified framework for exploiting these constraint both in the object location and identification labeling stage in particular we describe a technique for incorporating constrained search in context based vision finally we demonstrate the effectiveness of this approach in piction a system that us caption to label human face in newspaper photograph 
the paper describes a novel approach to relational matching problem in machine vision rather than matching static scene description the approach adopts an active representation of the data to be matched this representation is iteratively reconfigured to increase it degree of topological congruency with the model relational structure in a reconstructive matching process the active reconfiguration of relational structure is controlled by a map update process the final restored graph representation is optimal in the sense that it ha maximum a posteriori probability with respect to the available attribute for the object under match the benefit of the technique are demonstrated experimentally on the matching of cluttered synthetic aperture radar data to a model in the form of a digital map the operational limit of the method are established in a simulation study 
much of the research in inductive learning concentrate on problem with relatively small amount of data with the coming age of very large network computing it is likely that order of magnitude more data in database will be available for various learning problem of real world importance some learning algorithm assume that the entire data set fit into main memory which is not feasible for massive amount of data one approach to handling a large data set is to partition the data set into subset run the learning algorithm on each of the subset and combine the result in this paper we evaluate different technique for learning from partitioned data our meta learningapproach is empirically compared with technique in the literature that aim to combine multiple evidence to arrive at one prediction 
this paper describes research that characterizes the development of routine behavior based on a model of the historic relation of the agent to his environment the view developed is that the agent form a habitat outside of which his performance degrades routine behavior emerges from the history of the relation between the agent and his habitat in the service of recurring goal routine are customized to the agent s environment but so constructed a to support future related activity and the adaptation to new circumstance that extend the agent s range of activity in this paper we focus on examining quantitatively how this customization reduces the agent s workload 
a novel combination of idea from cognitive linguistics and spatial occupancy model in robotics ha led to the wip word into picture system wip automatically generates depiction of natural language description of indoor scene a qualitative layer in the conceptual representation of object underlies a mechanism by which alternative depiction arise for qualitatively distinct interpretation a often occurs a a result of deictic intrinsic reference frame ambiguity at the same time a quantitative layer in conjunction with a potential field model of the semantics of projective preposition is used in the process of capturing the inherently fuzzy character of the meaning of natural language spatial predication 
this paper present a framework for implicit deformable model and a pair of new algorithm for solving the nonlinear partial differential equation that result from this framework implicit model offer a useful alternative to parametric model particularly when dealing with the deformation of higher dimensional object the basic expression for the evolution of implicit model are relatively straightforward they follow a a direct consequence of the chain rule for differentiation more challenging however is the development of algorithm that are stable and efficient the first algorithm is a viscosity approximation which give solution over a dense set in the range providing a mean of calculating the solution of embedded family of contour simultaneously the second algorithm incorporates sparse solution for a discrete set of contour this sparse field method requires a fraction of the computation compared to the first but offer solution only for a finite number of contour result from d medical data a well a video image are shown 
what skill is more important to teach than reading unfortunately million of american cannot read although a large body of educational software exists to help teach reading it inability to hear the student limit what it can do this paper report a significant step toward using automatic speech recognition to help child learn to read an implemented system that display a text follows a a student read it aloud and automatically identifies which word he or she missed we describe how the system work and evaluate it performance on a corpus of second grader oral reading that we have recorded and transcribed 
in this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it a the basis for an algorithm to track discourse context and bind pronoun a described in gjw the process of centering attention on entity in the discourse give rise to the intersentential transitional state of we propose an extension to these state which handle some additional case of multiple ambiguous pronoun the algorithm ha been implemented in an hpsg natural language system which serf a the interface to a database query application 
the paper describes an algorithm that employ english and french text tagger to associate noun phrase in an aligned bilingual corpus the tagger provide part of speech category which are used by finite state recognizers to extract simple noun phrase for both language noun phrase are then mapped to each other using an iterative re estimation algorithm that bear similarity to the baum welch algorithm which is used for training the tagger the algorithm provides an alternative to other approach for finding word correspondence with the advantage that linguistic structure is incorporated improvement to the basic algorithm are described which enable context to be accounted for when constructing the noun phrase mapping 
when looked at from a multilingual perspective grapheme to phoneme conversion is a challenging task fraught with most of the classical nlp quot vexed question quot bottle neck problem of data acquisition pervasiveness of exception difficulty to state range and order of rule application proper treatment of context sensitive phenomenon and long distance dependency and so on the hand crafting of transcription rule by a human expert is onerous and time consuming and yet for some european 
we study a simple general framework for search called bootstrap search which is defined a global search using only a local search procedure along with some memory for learning intermediate subgoals we present a simple algorithm for bootstrap search and provide some initial theory on it performance in our theoretical analysis we develop a random digraph problem model and use it to make some performance prediction and comparison we also use it to provide some technique for approximating the optimal resource bound on the local search to achieve the best global search we validate our theoretical result with empirical demonstration on the puzzle we show how to reduce the cost of a global search by order of magnitude using bootstrap search we also demonstrate a natural but not widely recognized connection between search cost and the lognormal distribution 
this paper address the alignment issue in the framework of exploitation of large bimultilingual corpus for translation purpose a generic alignment scheme is proposed that can meet varying requirement of different application depending on the level at which alignment is sought appropriate surface linguistic information is invoked coupled with information about possible unit delimiters each text unit sentence clause or phrase is represented by the sum of it content tag the result are then fed into a dynamic programming framework that computes the optimum alignment of unit the proposed scheme ha been tested at sentence level on parallel corpus of the celex database the success rate exceeded the next step of the work concern the testing of the scheme s efficiency at lower level endowed with necessary bilingual information about potential delimiters 
inductive logic programming ilp is often situated a a research area emerging at the intersection of machine learning and logic programming lp this paper make the link more clear between ilp and lp in particular between ilp and abductive logic programming alp i e lp extended with abductive reasoning we formulate a generic framework for handling incomplete knowledge this framework can be instantiated both to alp and ilp approach by doing so more light is shed on the relationship between abduction and induction a an example we consider the abductive procedure sldnfa and modify it into an inductive procedure which we call sldnfai 
an application of mathematical lattice theory called relationship lattice is utilized to attack problem of operational bibliographic information retrieval the proposed solution offer an interface to the information searcher enabling operation in a world of concept author and document record and their relationship this hide the complexity of query language and database structure and it allows to use a personally preferred terminology and to browse query and download document record in a convenient way the main component of the proposed solution is a personal thesaurus built up a a relationship lattice 
this paper describes a computer accompaniment system capable of providing musical accompaniment for an ensemble of performer the system track the performance of each musician in the ensemble to determine current score location and tempo of the ensemble missing part in the composition i e the accompaniment are synthesized and synchronized to the ensemble the paper present an overview of the component problem of automated musical accompaniment and discus solution and their implementation the system ha been tested with solo performer a well a ensemble having a many a three performer ducing an accompaniment in synchrony with the detected performance a solution for each subproblem and a method for it implementation is also provided 
several early game playing computer program used forward pruning i e the practice of deliberately ignoring node that are believed unlikely to affect a game tree s minimax value but this technique did not seem to result in good decision making the poor performance of forward pruning present a major puzzle for ai research on game playing because some version of forward pruning seems to be what people do and the best chess playing program still do not play a well a the best human a a step toward deeper understanding of forward pruning we have set up model of forward pruning on two different kind of game tree and used these model to investigate how forward pruning affect the probability of choosing the correct move in our study forward pruning did better than minimaxing when there wa a high correlation among the minimax value of sibling node in a game tree this result suggests that forward pruning may possibly be a useful decision making technique in certain kind of game in particular we believe that bridge may be such a game 
we have built a high speed physically robust stereo ranging system we describe our experience with this system on several autonomous robot vehicle we use a custom built trinocular stereo jig and three specially modified ccd camera stereo matching is per ormed using the sum ofsum of squared difference technique 
although creativity ha largely been studied in problem solving context creativity consists of both a generative component and a comprehension component in particular creativity is an essential part of reading and understanding of natural language story we have formalized the understanding process and have developed an algorithm capable of producing creative understanding behavior we have also created a novel knowledge organization scheme to assist the process our model of creativity is implemented a a portion of the isaac integrated story analysis and creativity reading system a system which model the creative reading of science fiction story 
this thesis present a real time tracking and saccade system that is designed to be thebasis for a more complete vision system for a humanoid robot it consists of severalsemi independent functional unit including a tracking system a saccade system anda calibration system these three unit are designed to be simple and run at videorate on a single digital signal processor chip each system recognizes algorithmicfailures in the other system and corrects for these failure thesis 
constructing an appropriate model is crucialin reasoning successfully about the behaviorof a physical situation to answer a query incompositional modeling a system is providedwith a library of composible piece of knowledgeabout the physical world called modelfragments it task is to select appropriatemodel fragment to describe the situation eitherfor static analysis of a single state orfor the more complicated case simulation ofdynamic behavior over a sequence of state in 
in the minimalist program chomsky it is assumed that there are different type of projection lexical and functional and therefore different type of head this paper explains why functional head are not treated a head corner by the minimalist head corner parser described here 
for many type of learner one can compute the statistically optimal way to select data we review how these technique have been used with feedforward neural network we then show how the same principle may be used to select data for two alternative statistically based learning architecture mixture of gaussians and locally weighted regression while the technique for neural network are expensive and approximate the technique for mixture of gaussians and locally weighted regression are both efficient and accurate 
a new model for chemosensory reception is presented it model reaction between odor molecule and receptor protein and the activation of second messenger by receptor protein the mathematical formulation of the reaction kinetics is transformed into an artificial neural network ann the resulting feed forward network provides a powerful mean for parameter fitting by applying learning algorithm the weight of the network corresponding to chemical parameter can be trained by presenting experimental data we demonstrate the simulation capability of the model with experimental data from honey bee chemosensory neuron it can be shown that our model is sufficient to rebuild the observed data and that simpler model are not able to do this task 
we develop a principled strategy to sample a function optimally forfunction approximation task within a bayesian framework usingideas from optimal experiment design we introduce an objectivefunction incorporating both bias and variance to measure the degreeof approximation and the potential utility of the data pointstowards optimizing this objective we show how the general strategycan be used to derive precise algorithm to select data for twocases learning unit step function 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
we introduce a recurrent architecture having a modular structure and we formulate a training procedure based on the em algorithm the resulting model ha similarity to hidden markov model but support recurrent network processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation 
this paper describes a comparative study of reconstruction algorithm from sequence of image comparing algorithm which make the weak perspective assumption also called scaled orthography or para perspective to algorithm assuming perspective projection the weak perspective assumption is usually employed to simplify the computation using three sequence of real image taken under condition corresponding to small medium and large field of view we compare two algorithm that compute invariant shape from motion one assumes scaled orthography and one assumes perspective projection 
we analyse the geometry of eye rotation and in particularsaccades using basic lie group theory and differential geometry various parameterizations of rotation are related througha unifying mathematical treatment and transformation betweenco ordinate system are computed using the campbell baker hausdorff formula next we describe listing s law by mean ofthe lie algebra so this enables u to demonstrate a directconnection to donders law by showing that eye orientation 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
we present a mathematical formulation for curve and surface reconstruction algorithm by introduction of auxiliary variable for deformable model and template two step iterative algorithm have been often used where at each iteration the model is first locally deformed according to the potential data attraction and then globally smoothed we show how these approach can be interpreted a the introduction of auxiliary variable and the minimization of a two variable energy this permit u to transform an implicit data constraint defined by a non convex potential into an explicit convex reconstruction problem we show some mathematical property and result on this new auxiliary problem in particular when the potential is a function of the distance to the closest feature point we then illustrate our approach for some deformable model and template and image restoration 
we consider the question of whether or not a successful attempt to simulate human rational thought on a computer can contribute to our understanding of the mind including perhaps consciousness the now fashionable concept of emergence may turn out to be more appropri ate but still doe not seem to provide a final answer 
this work develops a computational model for representing and reasoning about dialogue in term of the mutuality of belief of the conversants we simulated cooperative dialogue at the speech act level and compared the simulation with actual dialogue between pilot and air traffic controller engaged in real task in the simulation addressee and overhearers formed belief and took action appropriate to their individual role and context the result is a computational model capable of representing the evolving context of complete real world multiparty task oriented conversation in the air traffic control domain 
an application of data mining technique to heterogeneous database schema integration is introduced we use attribute oriented induction to mine for characteristic and classification rule about individual attribute from heterogeneous database each mining request is conditioned on a subset of attribute identified a common between the multiple database we develop a method to compare the rule for two or more attribute from different database and use the similarity between the rule a a basis to suggest similarity between attribute a a result we use relationship between and among entire set of attribute from multiple database to drive the schema integration process our initial effort and prototype applying data mining to assist schema integration prove promising and we feel identify a fruitful application area for data mining research 
we propose a shape representation scheme which allows two shape to be combined into a single model the desired region of the two shape are selected and then merged together forming a blended shape for reconstruction blending is incorporated into a deformable model framework the model automatically adapts to the data blending when necessary hierarchical blending allows multiple blend of a shape to occur forming an evolution from the initial shape of a sphere to the final shape blending also allows the insertion of a hole between arbitrary location the model used are globally defined making the recovered shape a natural symbolic description we present reconstruction experiment involving shape of various topology 
in this paper we present a semantic theory of abstraction based on viewing abstraction a model level mapping this theory capture important aspect of abstraction not captured in the syntactic theory of abstraction presented by giunchiglia and walsh instead of viewing abstraction a syntactic mapping we view abstraction a a two step process first the intended domain model is abstracted and then a set of abstract formula is constructed to capture the abstracted domain model viewing and justifying abstraction a model level mapping is both natural and insightful this basic theory yield abstraction that are weaker than the base theory we show that abstraction that are stronger than the base theory are model level mapping under certain simplifying assumption we provide a precise characterization of the abstract theory that exactly implement an intended abstraction and show that this theory while being axiomatizable is not always finitely axiomatizable we present an algorithm that automatically construct the strongest abstract theory that implement the intended abstraction 
any belief function can be decomposed into a confidence and a diffidence component each component is uniquely decomposable into simple support function that represent the impact of the simplest form of evidence the one that only partially support a given subset of the frame of discernment the nature of the inverse of dempster s rule of combination is detailed the confidence component translates the impact of good reason to believe it is the component classically considered when constructing a belief the diffidence component translates the impact of good reason not to believe 
fraud and uncollectible debt are multi billion dollar problem in the telecommunication industry because it is difficult to know which account will go bad we are faced with the difficult knowledge discovery task of characterizing a rare binary outcome using large amount of noisy high dimensional data binary characterizatioos may be of interest but will not be especially useful in this domain instead proposing an action requires an estimate of the probability that a customer or a call is uncollectible this paper address the discovery of predictive knowledge bearing on fraud and uncollectible debt using a supervised machine leamiog method that construct bayesiao network model the new method is able to predict rare event outcome and cope with the quirk and copious amount of input data the bay an network model it produce serve a ao input module to a normative decision support system and suggest way to reinforce or redirect existing effort in the problem area we compare the performance of several conditionally independent model with the conditionally dependent model discovered by the new learning system using real world datasets of million record and million byte 
this paperpresentsa causalsimulationmethod forincompletelyknown dynamic system inprocessengineering the causalmodel of a processisrepresentedas both a causal network ofinteractingelementarydynamic system calledqualitative automaton influencing one another and a setofqualitative constraint linkingpossiblyseveralofsuchautomata associated with each influenceisa weightwhich expressesitssensitivity a procedureispresentedwhich allowsus togeneratefuzzyweightsfrom therelativeorderofmagnitude relationsbetweenthem formula preservingnon linearity and fulfilling some relevantrequirementsareprovidedtocompute net influenceson extensivevariablesas wellason intensiveones finallyan algorithmis givenforsimulatingthe causalmodel and an example applicationto a fed batchfermentation processispresented 
a model based approach to on line cursive handwriting analysis and recognition is presented and evaluated in this model on line handwriting is considered a a modulation of a simple cycloidal pen motion described by two coupled oscillation with a constant linear drift along the line of the writing by slow modulation of the amplitude and phase lag of the two oscillator a general pen trajectory can be efficiently encoded these parameter are then quantized into a small number of value without altering the writing intelligibility a general procedure for the estimation and quantization of these cycloidal motion parameter for arbitrary handwriting is presented the result is a discrete motor control representation of the continuous pen motion via the quantized level of the model parameter this motor control representation enables successful word spotting and matching of cursive script our experiment clearly indicate the potential of this dynamic representation for complete cursive handwriting recognition 
sequence of event describing the behavior and action of user or system can be collected in several domain in this paper we consider the problem of recognizing frequent episode in such sequence of event an episode is defined to be a collection of event that occur within time interval of a given size in a given partial order once such episode are known one can produce rule for describing or predicting the behavior of the sequence we describe an efficient algorithm for the discovery of all frequent episode from a given class of episode and present experimental result 
this paper study the problem of obtaining depth information from focusing anddefocusing which have long been noticed a important source of depth informationfor human and machine vision in depth from focusing we try to eliminate the localmaxima problem which is the main source of inaccuracy in focusing in depth fromdefocusing a new computational model is proposed to achieve higher accuracy the major contribution of this paper are in depth from focusing instead ofthe popular 
a gradient descent algorithm for parameter estimation which issimilar to those used for continuous time recurrent neural networkswas derived for hodgkin huxley type neuron model using membranepotential trajectory a target the parameter maximalconductances threshold and slope of activation curve time constant were successfully estimated the algorithm wa applied tomodeling slow non spike oscillation of an identified neuron in thelobster stomatogastric ganglion a 
we present a directed improvisation paradigm in which computer character improvise a joint course of behavior that follows user direction but also engages and entertains user with the novelty life like quality and performance property of their improvisation we present requirement for improvisational character that differ from the usual requirement for conventional computer agent and present an architecture that is designed to meet the new requirement two implemented character exploit some of these architectural feature to meet simple version of the requirement finally we illustrate the utility of improvisational character for a variety of application related to the art and entertainment including a suite of interaction mode in our testbed environment a virtual theater for child 
image displacement field optical flow field stereo disparity field normal flow field due to rigid motion posse a global geometric structure which is independent of the scene in view motion vector of certain length and direction are constrained to lie on the imaging surface at particular locus whose location and form depends solely on the d motion parameter if optical flow field or stereo disparity field are considered then equal vector are shown to lie on conic section similarly for normal motion field equal vector lie within region whose boundary also constitute conic by studying various property of these curve and region and their relationship a characterization of the structure of rigid motion field is given the goal of this paper is to introduce a concept underlying the global structure of image displacement field this concept give rise to various constraint that could form the basis of algorithm for the recovery of visual information from multiple view 
many physical phenomenon are sufficiently complexthat the corresponding equation afford littleinsight or no analytical method provides anexact solution decompositional modeling dm capture a modeler s tacit skill at solving nonlinearalgebraic system dm divide statespaceinto a patchwork of simpler subregimes calledcaricatures each of which preserve only thedominant characteristic of that regime it thensolves the simpler nonlinear system and identifiesits domain of 
this paper present the ipus integrated processing and understanding of signal architecture to address the traditional perceptual paradigm s shortcoming in complex environment it ha two premise the search for correct interpretation of signal processing algorithm spa output requires concurrent search for spa and control parameter appropriate for the environment and interaction between these search process must be structured by a formal theory of how inappropriate spa usage can distort spa output we describe ipus s key component discrepancy detection diagnosis reprocessing and differential diagnosis and their instantiation in an acoustic interpretation system this application along with another in the radar domain support our claim that the ipus paradigm is feasible and generic 
in this paper we study the problem of achieving efficient interaction in a distributed scheduling system whose scheduling agent may borrow resource from one another specifically we expand on sycara s use of resource texture measure in a distributed scheduling system with a central resource monitor for each resource type and apply it to the decentralized case we show how analysis of the abstracted resource requirement of remote agent can guide an agent s choice of local scheduling activity not only in determining local constraint tightness but also in identifying activity that reduce global uncertainty we also exploit meta level information to allow the scheduling agent to make reasoned decision about when to attempt to solve impasse locally through backtracking and constraint relaxation and when to request resource from remote agent finally we describe the current state of negotiation in our system and discus plan for integrating a more sophisticated cost model into the negotiation protocol this work is presented in the context of the distributed airport resource management system a multi agent system for solving airport ground service scheduling problem 
the spatial distribution and time course of electrical signal in neuron have important theoretical and practical consequence because it is difficult to infer how neuronal form affect electrical signaling we have developed a quantitative yet intuitive approach to the analysis of electrotonus this approach transforms the architecture of the cell from anatomical to electrotonic space using the logarithm of voltage attenuation a the distance metric we describe the theory behind this approach and illustrate it use 
we analyse the basic of eleven measure for estimating the quality of the multivalued attribute the value of information gain j measure gini index and relevance tend to lin early increase with the number of value of an attribute the value of gam ratio dis tance measure relief and the weight of evidence decrease for informative attribute and increase for irrelevant attribute the bias of the statistic test based on the chi square distribution is similar but these function are not able to discriminate among the attribute of different quality we also introduce a new func tion based on the mdl principle whose value slightly decrease with the increasing number of attribute value 
evans and gazdar evans and gazdar a evans and gazdar b introduced datr a a simple non monotonic language for representing natural language lexicon although a number of implementation of datr exist the full language ha until now lacked an explicit declarative semantics this paper rectifies the situation by providing a mathematical semantics for datr we present a view of datr a a language for defining certain kin of partial function by case the formal model provides a transparent treatment of datr s notion of global context it is shown that datr s default mechanism can be accounted for by interpreting value descriptor a family of value indexed by path 
a new on line learning algorithm which minimizes a statistical dependency among output is derived for blind separation of mixed signal the dependency is measured by the average mutual information mi of the output the source signal and the mixing matrix are unknown except for the number of the source thegram charlier expansion instead of the edge worth expansion isused in evaluating the mi the natural gradient approach is usedto minimize the mi a novel activation function is 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
this paper give a practical algorithm for the selfcalibration of a camera from several view the method involves non iterative method for finding an initial calibration for the camera followed by leastsquares iteration to an optimum solution at the same time a scaled euclidean reconstruction of the scene appearing in the image is computed 
ion hierarchy for constraintsatisfaction by clustering approximatelyequivalent objectsthomas ellmandepartment of computer sciencehill center for mathematical sciencesrutgers university new brunswick nj ellman c rutgers edulcsr tr abstractabstraction technique are important for solving constraint satisfaction problemswith global constraint and low solution density in the presence of global constraint backtracking search is unable to prune partial solution it 
wet surface are ubiquitous in our visual experience autonomous machine with vision system will need to identify wet surface from dry wet surface especially rough absorbent one appear darker when wet this paper present the lekner and dorf model for describing the darkening caused by wetting we explain how to use this optic model to transform intensity value of a region of an image to make that region appear wet we also show how the model can be reversed in order to make a wet part of an image appear dry it is also shown that this technique can be used to identify wet region this identification is contrasted with darkening caused by shadow comparison of the gray level histogram of these real image show the validity of this approach for distinguishing wet surface from dry 
the paper describes an approach to detect face whose size and position are unknown in an image with a complex background the candidate of face are detected by finding out face like region in the input image using the fuzzy pattern matching method the perceptually uniform color space is used in our research in order to obtain reliable result the skin color that is used to detect face like region is represented by a model developed by u called skin color distribution function the skin color region are then extracted by estimating a measure that describes how well the color of a pixel look like the skin color for each pixel in the input image the face which appear in image are modeled a several dimensional pattern the face like region are extracted by a fuzzy pattern matching approach using these face model the face candidate are then verified by estimating how well the extracted facial feature fit a face model which describes the geometrical relation among facial feature 
it is well known that knowledge based system would be more robust and smarter if they can deal with the inconsistent incomplete or imprecise knowledge which ha been referred to a common sense knowledge in this paper we discus fuzzy implication in the sense of common sense reasoning firstly we analyse the rationality of some existing fuzzy implication based on the discussion of implicational paradox secondly we present a new fuzzy preferential implication that is nonmonotonic paraconsistent and without the general implicational paradox finally we propose sound and complete decision tableau of such implication which can be used a the inference engine of adaptive expert system or framework for the fuzzy prolog 
we present utile suffix memory a reinforcement learning algorithm that us short term memory to overcome the state aliasing that result from hidden state by combining the advantage of previous work in instance based or memorybased learning and previous work with statistical test for separating noise from task structure the method learns quickly creates only a much memory a needed for the task at hand and handle noise well utile suffix memory us a tree structured representation and is related to work on prediction suffix tree ron et al parti game moore g algorithm chapman and kaelbling and variable resolution dynamic programming moore 
in this paper we describe an information retrieval problem called collection fusion the collection fusion problem is to maximize the number of relevant natural language document retrieved given a natural language query multiple collection of document and a fixed total number of document to retrieve we describe two algorithm that use past query to learn collection fusion strategy test of these algorithm on a corpus of document indicate that they can learn good fusion strategy moreover the strategy learned by our method are consistently superior to those learned by a standard learning algorithm 
it is possible to recover the three dimensional structure of a scene using image taken with uncalibrated camera and pixel correspondence betweeen these image but such reconstruction can only be performed up to a projective transformation of the d space therefore constraint have to be put on the reconstructed data to get the reconstruction in the euclidean space such constraint arise from knowledge of the scene such a the location of point geometrical constraint on line etc the kind of constraint that have to be added are discussed and it is shown how they can be fed in a general framework experimental result on real data prove the feasibility and experiment on simulated data address the accuracy of the result 
recurrent perceptron classifies generalize the classical perceptron model they take into account those correlation and dependence among input coordinate which arise from linear digital filtering this paper provides tight bound on sample complexity associated to the fitting of such model to experimental data this research wa supported in part by u air force grant afosr 
according to the utilitarian paradigm an autonomous intelligent agent s interaction with the environment should be guided by the principle of expected utility maximization we apply this paradigm to reasoning about an agent s physical action and exploratory behavior in urgent time constrained situation we model an agent s knowledge with a temporalized version of kripke structure a a set of branching time line described by fluents with accessibility relation holding among the state comprising the time line we describe how to compute utility based on this model which reflects the urgency that the environment imposes on time since the physical and exploratory action that an agent could undertake transform the model of branching time line in specific way the expected utility of these action can be computed dictating rational tradeoff among them depending on the agent s state of knowledge and the urgency of the situation 
the purpose of conditional causal logic ccl is to constitute a formal theory of the process by which the representation of the world emerges in a cognitive system ccl is presented a a two level language this article concern the first called the e language this e language is a formal theory for the determination process by which a cognitive system construct it objective knowledge the internal dynamic of this construction do not belong to the world of the a e language but to the e language which constitutes the second level of ccl this e language is still being developed and it will only be referred to briefly in this article the t language in itself offer some original feature such a the notion of identity and distinction by determination and also a type of negation functional negation which ha no equivalent in other model of conventional logic or nonstandard logic in conclusion some word will be said about design of a connexionist system founded on this theory of e language 
cat is an instruction environment for practicing basic skill of legal research to use case in argument about a problem situation and to test a theory about a legal domain using the cat tool law student analyze a legal problem frame query of cato s database of legal case and judge how relevant the retrieved case are to their developing argument or theory cat aid hming by making explicit an abstract model of the process of argument it allows student to focus on the high level argumentation issue by assisting the student in various way by providing an abstract representation of the text of case it help student to reason about the text and help guide their critical analysis of the text cat make available opportunity for practice that are hard to set up with traditional instructional method cat differs from other instructional environment in the following respect few instructional environment focus on argumentation skill although there are other instructional environment in which student work with an abstract representation of the task domain abstracting from text is unusual cato demonstrates a contribution that case based reasoning technique can make to instructional environment 
the joint invariant of a pair of coplanar conic ha been widely used in recent vision literature in this paper the algebraic invariant of a pair of non coplanar conic in space is concerned the algebraic invariant of a pair of non coplanar conic is first derived from the invariant algebra of a pair of quaternary quadratic form by using the dual representation of space conic then this algebraic invariant is geometrically interpreted in term of cross ratio finally an analytical procedure for projective reconstruction of a space conic from two uncalibrated image is developed and the correspondence condition of the conic between two view are also explicited 
case based reasoning cbr ha a great deal tooffer in supporting creative design particularlyprocesses that rely heavily on previous design experience such a framing the problem and evaluatingdesign alternative however most existingcbr system are not living up to their potential they tend to adapt and reuse old solution inroutine way producing robust but uninspiredresults little research effort ha been directedtowards the kind of situation assessment evaluation and 
irrelevant and redundant feature may reduce both predictive accuracyand comprehensibility of induced concept most common machinelearning approach for selecting a good subset of relevant feature rely oncross validation a an alternative we present the application of a particularminimum description length mdl measure to the task of featuresubset selection using the mdl principle allows taking into account all ofthe available data at once the new measure is 
the ability to control egomotion using low resolutionperipheral vision is crucial for enablinga small high resolution fovea to attend to featuresthat require detailed examination the beebotdemonstrates the ability to use low resolutionmotion vision over large field of view to steer betweenobstacles the system us the maximumflow observed in left and right peripheral visualfields to indicate obstacle proximity each peripheralfield constitutes one third of a wide anglelens the 
from sphere image we have developed a new method for camera calibration in order to calculate with accuracy it intrinsic parameter we prove an interesting geometric propriety about ellipsis extracted from sphere image taking into account the lens geometrical distortion introduced by the optical system and searching a precise point detection for sphere image permit to obtain satisfactory result 
we provide a general method which can be used in an algorithmic manner to reduce certain class of nd order circumscription axiom to logically equivalent st order formula the algorithm take a input an arbitrary nd order formula and either return a output an equivalent st order formula or terminates with failure in addition to demonstrating the algorithm by applying it to various circumscriptive theory we analyze it strength and provide formal subsumption result based on compan son with existing approach 
illumination is rarely constant in intensity or color throughout a scene multiple light source with different spectrum sun and sky direct and interreflected light are the norm nonetheless almost all color constancy algorithm assume that the spectrum of the incident illumination remains constant across the scene we assume the converse that illumination doe vary in developing a new algorithm for color constancy rather than creating difficulty varying illumination is in fact a very powerful constraint indeed test of our algorithm using real image of an office scene show excellent result 
a statistical approach to decision tree modeling is described in this approach each decision in the tree is modeled parametrically a is the process by which an output is generated from an input and a sequence of decision the resulting model yield a likelihood measure of goodness of fit allowing ml and map estimation technique to be utilized an efficient algorithm is presented to estimate the parameter in the tree the model selection problem is presented and several alternative proposal are considered a hidden markov version of the tree is described for data sequence that have temporal dependency 
any attempt to introduce automation into the monitoring of complex physical system must start from a robust anomaly detection capability this task is far from straightforward for a single definition of what constitutes an anomaly is difficult to come by in addition to make the monitoring process efficient and to avoid the potential for information overload on human operator attention focusing must also be addressed when an anomaly occurs more often than not several sensor are affected and the partially redundant information they provide can be confusing particularly in a crisis situation where a response is needed quickly the focus of this paper is a new technique for attention focusing the technique involves reasoning about the distance between two frequency distribution and is used to detect both anomalous system parameter and broken causal dependency these two form of information together isolate the locus of anomalous behavior in the system being monitored 
a popular saying claim that innovation is inspiration and perspiration in this paper we present technique for automating most of the perspiration involved in creative design we assume that creative design consists of three step discovery of a new technique understanding it and generalizing it to useful application our program us first principle to automate the understanding and generalization phase which involve most of the perspiration and extends it knowledge accordingly so that it can construct practical design based on the new technique the technique could be used to create new device automatically but in practice user interaction is necessary to control the search we present a system which implement the method in the domain of elementary mechanism also called kinematic pair it main novelty are a formalism for modeling qualitative mechanical function and a technique similar to explanation based learning which generalizes a qualitative analysis of a novel device to extend domain knowledge the result are generalizable to other domain with similar characteristic 
identifying node of information that are highly related ha many application in any information system and in particular in hypertext system in this paper we present a technique to identify natural cluster in a hypertext a natural cluster is a cluster that is not arbitrary but depends only on intrinsic property of the hypertext in our case the property we will use to identify the cluster is the number of independent path between node using the graph theoretic definition of k edge component we present an aggregation technique to cluster the node we then use this technique to cluster three medium sized hypertext that were developed by different author for different user using different methodology we also show how to use clustering to improve data display browsing and retrieval 
recurrent neural network solving the task of short term traffic forecasting arepresented in this report they turned out to be very well suited to this task theyeven outperformed the best result obtained with conventional statistical method the outcome of a comparative study show that multiple combination of feedbackcan greatly enhance the network performance best result were obtained with thenewly developed multi recurrent network combining output hidden and input layermemories 
reasoning with multiple level of abstraction is a powerful method of controlling problem solving in complex domain we consider the problem of simplifying a knowledge base by creating an abstraction that is tailored for a given set of query our approach is based on associating formally an abstraction with some irrelevant detail that is removed from the knowledge base we show how creating an abstraction and determining it utility amount to automatically deciding which aspect of a representation are irrelevant to a query a a result we derive a general algorithm schema for automatically generating abstraction for a query a an instance of the schema we describe a novel algorithm for automatically abstracting a kb by projecting out relation argument 
in this paper we consider constrained and rational default logic we provide two characterization of constrained extension one of them is used to derive complexity result for decision problem involving constrained extension in particular we show that the problem of membership of a formula in at least one in all constrained extension s of a default theory is ef complete ilf complete we establish the relationship between constrained and rational default logic we prove that rational extension determine constrained extension and that for seminormal default theory there is a one to one correspondence between these object we also show that the definition of a constrained extension can be extended to cover the case of default theory which may contain justification free default 
the fundamental matrix is a key concept when working withuncalibrated image and multiple viewpoint it contains all the availablegeometric information and enables to recover the epipolar geometry fromuncalibrated perspective view this paper is about a stability analysisfor the fundamental matrix we first present a probabilistic approachwhich work well this approch however doe not give insight into thecauses of unstability two complementary explanation for unstabilityare the 
this paper introduces new approach to the conceptual design of electro mechanical system from qualitative specification of behaviour and function the power of these method stem from the integration of technique in qualitative physic symbolic mathematics computational geometry and constraint programming this is illustrated with an effective kinematic synthesis method that integrates reasoning with configuration space and constraint programming technique 
the study of belief change ha been an active area in philosophy and ai in recent year two special case of belief change belief revision and belief update have been studied in detail belief revision and update are clearly not the only possible notion of belief change in this paper we investigate property of a range of possible belief change operation we start with an abstract notion of a belief change system and provide a logical language that describes belief change in such system we then consider several reasonable property one can impose on such system and characterize them axiomatically we show that both belief revision and update fit into our classification a a consequence we get both a semantic and an axiomatic proof theoretic characterization of belief revision and update a well a some belief change operation that generalize them in one natural framework 
many learning system must confront theproblem of run time after learning beinggreater than run time before learning thisutility problem ha been a particular focusof research in explanation based learning inpast work we have examined an approach tothe utility problem that is based on restrictingthe expressiveness of the rule languageso a to guarantee polynomial bound on thecost of using learned rule in this articlewe propose a new approach that limit thecost of learned rule 
we introduce a new approach for on line recognition of handwrittenwords written in unconstrained mixed style the preprocessorperforms a word level normalization by fitting a model of the wordstructure using the em algorithm word are then coded into lowresolution quot annotated image quot where each pixel contains informationabout trajectory direction and curvature the recognizer is aconvolution network which can be spatially replicated from thenetwork output a hidden markov model 
one of the central knowledge source of an information extraction ie system is a dictionary of linguistic pattern that can be used to identify reference to relevant information in a text automatic creation of conceptual dictionary is important for portability and scalability of an ie system this paper describes crystal a system which automatically induces a dictionary of concept node definition sufficient to identify relevant information from a training corpus each of these concept node definition is generalized a far a possible without producing error so that a minimum number of dictionary entry cover the positive training instance because it test the accuracy of each proposed definition crystal can often surpass human intuition in creating reliable extraction rule 
the combination of induction axiom is investiga ted it is shown how a pair of competing induction axiom which e g are suggested by a heuristic of an induction theorem prover on a specific verification problem are combined yielding a new induction axiom the relation implicitly defined by the new axiom is the set theoretic union of the well founded relation implicitly defined by the induction axiom initially given the proposed approach is non heu ristic but safe in the sense that an induction proof with the new axiom can be obtained whenever an induction proof with one of the given axiom would have been successful based on a result of bachmair and dershowitz for proving term rewriting system noctherian a commutation test is developed a a de ductive requirement to verify the soundness of the combined axiom it is shown how so called commu tation formula can be derived by machine from the given axiom such that a verification of these for mulas e g by an induction theorem prover guaran tee the well foundcdness of the relation defined by the combined axiom example are presented to demonstrate the usefulness and strength of the pro posed technique 
the situation calculus is a popular technique for reasoning about action and change however it restriction to a firstorder syntax and pure deductive reasoning make it unsuitable in many context in particular we often face uncertainty due either to lack of knowledge or to some probabilistic aspect of the world while attempt have been made to address aspect of this problem most notably using nonmonotonic reasoning formalism the general problem of uncertainty in reasoning about action ha not been fully dealt with in a logical framework in this paper we present a theory of action that extends the situation calculus to deal with uncertainty our framework is based on applying the random world approach of bghk to a situation calculus ontology enriched to allow the expression of probabilistic action effect our approach is able to solve many of the problem imposed by incomplete and probabilistic knowledge within a unified framework in particular we obtain a default markov property for chain of action a derivation of conditional independence from irrelevance and a simple solution to the frame problem 
in this paper we present modeling analysis and synthesisof visual behavior of agent engaged in navigationaltasks we consider situation in which twoagents can navigate independently or in cooperation for the purpose of modeling the behavior we haveadopted a formalism from the discrete event system de theory suitable for investigating controltheoreticissues of a system the focus of this paperis on the identification of elementary behavior andtheir composition leading to more 
we present an implemented compilation algorithm that translates hpsg into lexicalized feature based tag relating concept of the two theory while hpsg ha a more elaborated principle based theory of possible phrase structure tag provides the mean to represent lexicalized structure more explicitly our objective are met by giving clear definition that determine the projection of structure from the lexicon and identify maximal projection auxiliary tree and foot node 
a proper characterization of a rational agent s action involves much more than simply recounting the change in the world affected by the agent it should also include an explanatory account connecting the upshot of an agent s action with the reason behind those action where those upshot might represent actual change either intentional or unintentional or merely counterfactual possibility the conventional view of action make it difficult to distinguish inter alia case of 
a system wolfie that acquires a mapping of word to their semantic representation is presented and a preliminary evaluation is performed tree least general generalization tlggs of the representation of input sentence are performed to assist in determining the representation of individual word in the sentence the best guess for a meaning of a word is the tlgg which overlap with the highest percentage of sentence representation in which that word appears some promising experimental result on a non artificial data set are presented 
the original formulation of sharedplans grosz and sidner wa developed to provide a model of collaborative planning in which it wa not necessary for one agent to have intention toward an act of a different agent this formulation provided for two agent to coordinate their activity without introducing any notion of jointly held intention or weintentions however it only treated activity that directly decomposed into single agent action in this paper we provide a revised and expanded version of sharedplans that accommodates action involving group of agent a well a complex action that decompose into multi agent action the new definition also allow for contracting out certain action and provide a model with the feature required in bratman s account of shared cooperative activity bratman a reformulation of the model of individual plan that mesh with the definition of sharedplans is also provided 
a dempster shafer belief structure provides a mechanism for representing uncertain knowledge about a variable a compatibility relation provides a structure for obtaining information about one variable based upon our knowledge about a second variable an inference scheme in the theory of evidence involves the use of a belief structure on one variable called the antecedent and a compatibility relationship to infer a belief structure on the second variable called the consequent the concept of monotonicity in this situation is related to change in the specificity of the consequent belief structure a the antecedent belief structure becomes more specific we show that the usual compatibility relation type are always monotonic we introduce type ii compatibility relation and show that a special class of these which we call irregular are needed to represent nonmonotonic relation between variable we discus a special class of nonmonotonic relation called default relation 
using revision to produce extended natural language text through a series of draft provides three significant advantage over a traditional natural language generation system first it reduces complexity through task decomposition second it promotes text polishing technique that benefit from the ability to examine generated text in the context of the underlying knowledge from which it wa generated third it provides a mechanism for the interaction of conceptual and stylistic decision kalos is a natural language generation system that produce advanced draft quality text for a microprocessor user guide from a knowledge base describing the microprocessor it us revision iteratively to polish it initial generation the system performs both conceptual and stylistic revision example output of the system showing both type of revision is presented and discussed 
many problem can be expressed in term of a numeric constraint satisfaction problem over finite or continuous domain numeric csp the purpose of this paper is to show that the consistency technique that have been developed for csps can be adapted to numeric csps since the numeric domain are ordered the underlying idea is to handle domain only by their bound the semantics that have been elaborated plus the complexity analysis and good experimental result confirm that these technique can be used in real application 
the optical flow constitutes one of the most widely adopted representation to define and characterize the evolution of image feature over time in order to compute the velocity field it is necessary to define a set of constraint on the temporal change of image feature we consider the implication in using multiple constraint arising from multiple data point the first step is the analysis of differential constraint and how they can be applied locally to compute the image velocity this analysis allows to relate each constraint to a particular gray level pattern this approach is extended to multiple image point allowing also the characterization of the temporal behaviour of the image feature and to detect erroneous measurement due to occlusion depth discontinuity or shadow several experiment are presented from real image sequence 
abstract a solution to the ramiflcation problem caused by underlying domain constraint in stripslike approach is presented we introduce the notion of causal relationship which are used in a post processing step after having applied an action description moreover we show how the information needed for these post computation can be automatically extracted from the domain constraint plus general knowledge of which uents can possibly affect each other we illustrate the necessity of causal relationship by an example that show the limitedness of a common method to avoid unintended ramiflcations namely the distinction between so called frame and non frame uents finally we integrate our solution into a recently developed strip like yet purely deductive approach to reasoning about action based on equational logic programming 
in a recent paper we have proposed terminological default logic a a formalism which combine both mean for structured representation of class and object and for default inheritance of property the major drawback which terminological default logic inherits from general default logic is that it doe not take precedence of more specific default over more general one into account the present paper address the problem of modifying terminological default logic such that more specific default are preferred it turn out that the existing approach for expressing priority between default do not seem to be appropriate for this purpose therefore we shall consider an alternative approach for dealing with prioritization in the framework of heifer s default logic the formalism is presented in the general setting of default logic where priority are given by an arbitrary partial ordering on the default we shall exhibit some interesting property of the new formalism compare it with existing approach and describe an algorithm for computing extension 
to coordinate with other agent in it environment an agent need model of what the other agent are trying to do when communication is impossible or expensive this information must be acquired indirectly via plan recognition typical approach to plan recognition start with a specification of the possible plan the other agent may be following and develop special technique for discriminating among the possibility perhaps more desirable would be a uniform procedure for mapping plan to general structure supporting inference based on uncertain and incomplete observation in this paper we describe a set of method for converting plan represented in a flexible procedural language to observation model represented a probabilistic belief network and we outline issue in applying the resulting probabilistic model of agent when coordinating activity in physical domain 
this paper present an approach to a new leanring problem the problem of learning from an approximate theory and a set of noisy example this problem requires a new learning approach since it cannot be satisfactorily solved by either indictive or analytic learning algorithm or their existing combination our approach can be viewed a an extension of the minimum description length mdl principle and is unique in that it is based on the encoding of the refinement required to transform the given theory into a better theory rather than on the encoding of the resultant theory a in traditional mdl experimental result show that based on our approach the theory learned from an approximate theory and a set of noisy example is more accnrate than either the approximate theory itself or a theory learned from the example alone this suggests that our approach can combine useful iuformation from both the theory and the training set even though both of them are only partially correct 
we present an algorithm based on mrf modelling for motion detection in image sequence and give a modified version for implementation on analog resistive network energy minimization is realized by a network relaxing to it state of minimal power dissipation it take a few nanosecond and replaces advantageously time consuming stochastic or suboptimal deterministic relaxation algorithm the elementary cell of the network is presented along with the environment needed to feed it with the required input two network architecture are proposed derived from ccd camera principle software simulation of a network demonstrate the good behaviour of the modified algorithm on real sequence electrical simulation of a network with ideal component give promising result implementation of the cmos circuit with vlsi technology is under study at our laboratory 
we describe and evaluate experimentally a method for clustering word according to their distribution in particular syntactic context word are represented by the relative frequency distribution of context in which they appear and relative entropy between those distribution is used a the similarity measure for clustering cluster are represented by average context distribution derived from the given word according to their probability of cluster membership in many case the cluster can be thought of a encoding coarse sense distinction deterministic annealing is used to find lowest distortion set of cluster a the annealing parameter increase existing cluster become unstable and subdivide yielding a hierarchical soft clustering of the data cluster are used a the basis for class model of word coocurrence and the model evaluated with respect to held out test data 
we describe and evaluate experimentally a method for clustering word according to their distribution in particular syntactic context word are represented by the relative frequency distribution of context in which they appear and relative entropy between those distribution is used a the similarity measure for clustering cluster are represented by average context distribution derived from the given word according to their probability of cluster membership in many case the cluster can be thought of a encoding coarse sense distinction deterministic annealing is used to find lowest distortion set of cluster a the annealing parameter increase existing cluster become unstable and subdivide yielding a hierarchical soft clustering of the data cluster are used a the basis for class model of word coocurrence and the model evaluated with respect to held out test data 
this paper considers the problem of modeling and extracting arbitrary deformable contour from noisy image we propose a global contour model based on a stable and regenerative shape matrix which is invariant and unique under rigid motion combined with markov random field to model local deformation this yield prior distribution that exerts influence over a global model while allowing for deformation we then cast the problem of extraction into posterior estimation and show it equivalence to energy minimization of a generalized active contour model we discus pertinent issue in shape training energy minimization line search strategy minimax regularization and initialization by generalized hough transform finally we present experimental result and compare it performance to rigid template matching 
we describe a density adaptive reinforcementlearning and a density adaptive forgetting algorithm this learning algorithm us hybridk d k tree to allow for a variable resolutionpartitioning and labelling of the inputspace the density adaptive forgetting algorithmdeletes observation from the learningset depending on whether subsequent evidenceis available in a local region of the parameterspace the algorithm are demonstratedin a simulation for learning feasiblerobotic grasp 
nearest neighbor classification expects the class conditional probability to be locally constant and suffers from bias in high dimension we propose a locally adaptive form of nearest neighbor classification to try to ameliorate this curse of dimensionality we use a local linear discriminant analysis to estimate an effective metric for computing neighborhood we determine the local decision boundary from centroid information and then shrink neighborhood in direction orthogonal to these local decision boundary and elongate them parallel to the boundary thereafter any neighborhood based classifier can be employed using the modified neighborhood the posterior probability tend to be more homogeneous in the modified neighborhood we also propose a method for global dimension reduction that combine local dimension information in a number of example the method demonstrate the potential for substantial improvement over nearest neighbor classification 
interactive spoken dialog provides many new challenge for spoken language system one of the most critical is the prevalence of speech repair this paper present an algorithm that detects and corrects speech repair based on finding the repair pattern the repair pattern is built by finding word match and word replacement and identifying fragment and editing term rather than using a set of prebuilt template we build the pattern on the fly in a the fair test our method when combined with a statistical model to filter possible repair wa successful at detecting and correcting of the repair without using prosodic information or a parser 
a unified differential geometric framework for estimation of local surface shape and orientation from projective texture distortion is proposed based on a differential version of the texture stationarity assumption introduced by malik and rosenholtz this framework allows the information content of the gradient of any texture descriptor defined in a local coordinate frame to be characterized in a very compact form the analysis encompasses both full affine texture descriptor and the classical texture gradient for estimation of local surface orientation and curvature from uncertain observation of affine texture distortion the proposed framework allows the dimensionality of the search space to be reduced from five to one 
multi armed bandit may be viewed a decompositionally structured markov decision process mdp s with potentially very large state set a particularly elegant methodology for computing optimal policy wa developed over twenty ago by gittins gittins jones gittins approach reduces the problem of finding optimal policy for the original mdp to a sequence of low dimensional stopping problem whose solution determine the optimal policy through the so called gittins index katehakis and veinott katehakis veinott have shown that the gittins index for a task in state i may be interpreted a a particular component of the maximum value function associated with the restart in i process a simple mdp to which standard solution method for computing optimal policy such a successive approximation apply this paper explores the problem of learning the gittins index on line without the aid of a process model it suggests utilizing task state specific q learning agent to solve their respective restart in state i subproblems and includes an example in which the online reinforcement learning approach is applied to a simple problem of stochastic scheduling one instance drawn from a wide class of problem that may be formulated a bandit problem 
motion based recognition deal with the recognition of object or motion directly from the motion information extracted from a sequence of image there are two main step in this approach the first consists of finding an appropriate representation for the object or motion from the motion cue of the sequence and then organize them into useful representation the second step consists of the matching of some unknown input with a model this paper provides a review of recent development in motion based recognition 
the idea of ordering play a basic role in commonsense reasoning for addressing three interrelated task inconsistency handling belief revision and plausible inference we study the behavior of non monotonic inference induced by various method for priority based handling of inconsistent set of classical formula one of them is based on a lexicographic ordering of maximal consistent subset and refines brewka s preferred sub theory this new approach lead to a nonmonotonic inference which satisfies the rationality property while solving the problem of blocking of property inheritance it differs from and improves previous equivalent approach such a gardenfors and makinson s expectation based inference pearl s system z and possibilistic logic 
this paper examines the current performance of the stochastic tagger part church in handling phrasal verb describes a problem that arises from the statistical model used and suggests a way to improve the tagger s performance the solution involves a change in the definition of what count a a word for the purpose of tagging phrasal verb 
although they are applicable to a wide array of problem and have demonstrated good performance on anumber of difficult real world task neural network are not usually applied to problem in which comprehensibilityof the acquired concept is important the concept representation formed by neural networksare hard to understand because they typically involve distributed nonlinear relationship encoded by alarge number of real valued parameter to address this limitation we have been 
we propose a method to build thesaurus on the basis of grammatical relation the proposed method construct thesaun by using a hierarchical clustering algorithm an important point in this paper is the claim that thesaurus in order to be efficient need to take surface case information into account we refer to the thesaurus a relation based thesaurus rbt in the experiment four rbts of japanese noun were constructed from verb noun cooccurrences and each rbt wa evaluated fry objective criterion the experiment ha shown that the rbts have better property for selectional restriction of case frame than conventional one 
in this paper we analyze the geometric active contour model discussed previously from a curve evolution point of view and propose some modification based on gradient flow relative to certain new feature based riemannian metric this lead to a novel snake paradigm in which the feature of interest may be considered to lie at the bottom of a potential well thus the snake is attracted very naturally and efficiently to the desired feature moreover we consider some d active surface model based on these idea 
the standard method for extracting range data from optical triangulation scanner are accurate only for planar object of uniform reflectance illuminated by an incoherent source using these method curved surface discontinuous surface and surface of varying reflectance cause systematic distortion of the range data coherent light source such a laser introduce speckle artifact that further degrade the data we present a new ranging method based on analyzing the time evolution of the structured light reflection using our spacetime analysis we can correct for each of these artifact thereby attaining significantly higher accuracy using existing technology we present result that demonstrate the validity of our method using a commercial laser stripe triangulation scanner 
this paper is concerned with modeling planning problem involving uncertainty a discrete time finite state stochastic automaton solving planning problem is reduced to computing policy for markov decision process classical method for solving markov decision process cannot cope with the size of the state space for typical problem encountered in practice a an alternative we investigate method that decompose global planning problem into a number of local problem solve the local problem separately and then combine the local solution to generate a global solution we present algorithm that decompose planning problem into smaller problem given an arbitrary partition of the state space the local problem are interpreted a markov decision process and solution to the local problem are interpreted a policy restricted to the subset of the state space defined by the partition one algorithm relies on constructing and solving an abstract version of the original decision problem a second algorithm iteratively approximates parameter of the local problem to converge to an optimal solution we show how property of a specified partition affect the time and storage required for these algorithm 
this paper describes the application of reinforcement learning rl to the difficult real world problem of elevator dispatching the elevatordomain pose a combination of challenge not seen in mostrl research to date elevator system operate in continuous statespaces and in continuous time a discrete event dynamic system their state are not fully observable and they are nonstationarydue to changing passenger arrival rate in addition we use a teamof rl agent each of which is 
in this paper we present an average case analysis of the nearest neighbor algorithm a simple induction method that ha been studied by many researcher our analysis assumes a conjunctive target concept noise free boolean attribute and a uniform distribution over the instance space we calculate the probability that the algorithm will encounter a test instance that is distance d from the prototype of the concept along with the probability that the nearest stored training case is distance e from this test instance from this we compute the probability of correct classification a a function of the number of observed training case the number of relevant attribute and the number of irrelevant attribute we also explore the behavioral implication of the analysis by presenting predicted learning curve for artificial domain and give experimental result on these domain a a check on our reasoning 
we discus the implication of holte s recentlypublished article which demonstrated that on the most commonly used data very simple classification rule are almost a accurate a decision tree produced by quinlan s c we consider in particular what is the significance of holte s result for the future of top down induction of decision tree to an extent holte questioned the sense of further research on multilevel decision tree learning we go in detail through all the part of holte s study we try to put the result into perspective we argue that the in absolute term small difference in accuracy between r and c that wa witnessed by holte is still significant we claim that c posse additional accuracy related advantage over r in addition we discus the representativeness of the database used by holte we compare empirically the optimal accuracy of multilevel and one level decision tree and observe some significant difference we point out several deficiency of limited complexity classifier 
shadow are a frequent occurrence but they cannot be infallibly recognized until a scene s geometry and lighting are known we present a number of cue which together strongly suggest the identification of a shadow and which can be examined with low cost the technique are a color image segmentation method that recovers single material surface a single image region irregardless of the surface partially in shadow a method to recover the penumbra and umbra of shadow a method for determining whether some object could be obstructing a light source the last cue requires the examination of well understood shadow in the scene our observer is equipped with an extendable probe for casting it own shadow actively obtained shadow allow the observer to experimentally determine the location of the light source in the scene the system ha been tested both indoors and out 
present a new approach to rendering arbitrary view of real world d object of complex shape we propose to represent an object by a sparse set of corresponding d view and to construct any other view a a combination of these reference view we show that this combination can be linear assuming proximity of the view and we suggest how the visibility of constructed point can be determined our approach make it possible to avoid difficult d reconstruction assuming only rendering is required moreover almost no calibration of view is needed we present preliminary result on real object indicating that the approach is feasible 
what is needed for an analysis of the existing approach to qualitative spatial reasoning and for a deeper understanding of the domain of space is a unifying theory that explains all of the concept used for the representation of the different aspect of space by some primitive but well understood relation in order to provide such primitive relation it will be shown that the concept used in the existing approach can be explained by simple order relation between point on some low dimensional structure one of the property of an order relation is transitivity it will be shown that this property alone is sufficient to explain all the inference described in the various approach to qualitative spatial reasoning 
various theory of event or action have been proposed to account for commonsence conclusion the typical way in which this proceeds is for a scenario to be invented and a logic tried against it then someone else may perturb the scenario a bit and the old logic fails while a new one may succeed the basic difficulty with this methodology is it ad hoc character to overcome this recent work l ha tried to delineate class of monotonic theory that the non monotonic one are to be measured against we believe a better methodology is to go the whole hog and admit model class a the arbiter to this end we outline some postulate about a possible world semantics that is suitable for evaluating non monotonic theory of event 
when planning system deal with realistic domain they must cope with a large variety of constraint imposed by the environment such a temporal or resource constraint the robustness of the generated plan is a direct consequence of a correct handling of these constraint we argue that increasing the expressiveness of a representation can be achieved without fundamentally affecting the global efficiency of the search this paper present a temporal planner lxtet which integrates sharable resource management into the process of plan generation in lxtet planning operator are described a temporal structure of condition effect and sharable resource us during the search pending subgoals protection threat and resource conflict are detected by three flaw analysis module the detection of sharable resource conflict is performed thanks to an efficient clique search algorithm on a possible intersection graph the control of the search is based on a least commitment opportunistic strategy our approach ha been implemented tested and shown to be satisfactory in various application domain 
a variety of reactive plan execution system have been developed in recent year each attempting to solve the problem of taking reasonable course of action fast enough in a dynamically changing world comparing these competing approach and collecting the best feature of each ha been problematic because of the diverse representation and sometimes implicit control structure that they have employed to rectify this problem we have extended the circuit semantics notion of teleo reactive program into richer yet compact semantics called structured circuit semantics sc that can be used to explicitly represent the control behavior of various reactive execution system by transforming existing system into sc we can identify underlying control assumption and begin to identify more rigorously the strength and limitation of these system moreover sc provides a basis for constructing new reactive execution system with more understandable semantics that can be tailored to particular domain need 
we describe a method for learning formula in firstorder logic using a brute force smallest first search the method is exceedingly simple it generates all irreducible well formed formula up to a fixed size and test them against a set of example although the method ha some obvious limitation due to it computational complexity it performs surprisingly well on some task this paper describes experiment with two application of the method in the multi tac system a program synthesizer for constraint satisfaction problem in the first application axiom are learned and in the second application search control rule are learned we describe these experiment and consider why searching the space of small formula make sense in our application 
this paper present a method for localization of modeled object that is general enough to cover articulated and other type of constrained model the flexibility between component of the model are expressed a spatial constraint which are fused into the pose estimation process the constraint fusion assist in obtaining a precise and stable pose of each object s component and in finding the correct interpretation the proposed method can handle any constraint including inequality between any number of different component of the model the framework is based on kalman filtering 
this paper explores the use of local parametrized model of image motion for recovering and recognizing the non rigid and articulated motion of human face parametric flow model for example affine are popular for estimating motion in rigid scene we observe that within local region in space and time such model not only accurately model non rigid facial motion but also provide a concise description of the motion in term of a small number of parameter these parameter are intuitively related to the motion of facial feature during facial expression and we show how expression such a anger happiness surprise fear disgust and sadness can be recognized from the local parametric motion in the presence of significant head motion the motion tracking and expression recognition approach performs with high accuracy in extensive laboratory experiment involving subject a well a in television and movie sequence 
this paper describes projective visualization which us previous observation of a process or activity to project the result of an agent s action into the future action which seem likely to sueceed are selected and applied action which seem likely to fail are rejected and other action can be generated and evaluated this paper present a description of the architecture for projective visualization preliminary result on learning to act from observation of a reactive system and a comparison of two type of case projection how situation are projected into the future 
we present a self organizing framework called the shoslif m for learning and recognizing spatiotemporal event or pattern from intensity image sequence the proposed framework consists of a multiclass multivariate discriminant analysis to automatically select the most discriminating feature mdf a space partition tree to achieve a logarithmic retrieval time complexity for a database of n item and a general interpolation scheme to do view inference and generalization in the mdf space based on a small number of training sample the system is tested to recognize different hand sign the experimental result show that the learned system can achieve a recognition rate for test sequence that have not been used in the training phase 
edge corner and vertex are strong and useful feature in computervision this paper deal with the development of an efficient modelbased approach in order to detect and characterize precisely these importantfeatures the key of our approach is first to propose some efficient modelsassociated to each of these feature and second to efficiently extract and characterizethese feature directly from the image the model associated to eachfeature include a large number of intrinsic 
although recurrent neural net have been moderately successfulin learning to emulate finite state machine fsms the continuousinternal state dynamic of a neural net are not well matchedto the discrete behavior of an fsm we describe an architecture called dolce that allows discrete state to evolve in a net a learningprogresses dolce consists of a standard recurrent neural nettrained by gradient descent and an adaptive clustering techniquethat quantizes the state space dolce is 
the aim of this work is to combine advantageously the two existing approach for theorem proving in non classical logic proving in the considered non classical logic called here the direct approach and proving in classical logic by way of translation called here the translation approach some result in propositional s show evidence of the relevance of this approach we assume a translation from s into first order logic and then we define a partial inverse formula translation from first order classical logic into s semantic relation are proved to hold between the backward translated formula we answer positively for s to one conjecture stated in a previous work by the author an interpolation theorem stating a property stronger than refutational completeness is also proved a plausible conjecture stronger than the interpolation theorem is proposed these result are interpreted in the framework of a slight variant of an existing resolution calculus for s we illustrate our method on a simple example future work includes application of the approach to other modal logic 
many knowledge based system need to represent vague concept although the practical approach of representing vague concept a precise interval over number is well accepted in ai there is no systematic method to delimit the boundary of interval only ad hoc method we present a framework to reason precisely with vague concept based on the observation that the vague concept and their interval boundary are constrained by the underlying domain knowledge the framework is comprised of a constraint language to represent logical constraint on vague concept a well a numerical constraint on the intervalboundaries a query language to request information about the interval boundary and a computational mechanism to answer the query a key step in answering query is preprocessing the constraint by extracting the numerical constraint from the logical constraint and combining them with the given numerical constraint 
the search space in partial order planning grows quickly with the number of subgoals and initial condition a well a le countable factor such a operator ordering and subgoal in teractions for partial order planner to solve more than simple problem the expansion of the search space will need to be controlled this paper present four new approach to controlling search space expansion by exploiting commonality in emerging plan these approach are described in term of their algorithm their effect on the completeness and correctness of the underlying planner and their expected performance the four new and two existing approach are compared on several metric of search space and planning overhead 
in our research we explore the role of negotiation for conflict resolution in distributed search among heterogeneous and reusable agent we present negotiated search an algorithm that explicitly recognizes and exploit conflict to direct search activity across a set of agent in negotiated search loosely coupled agent interleave the task of local search for a solution to some subproblem integration of local subproblem solution into a shared solution information exchange to define and refine the shared search space of the agent and assessment and reassessment of emerging solution negotiated search is applicable to diverse application area and problem solving environment it requires only basic search operator and allows maximum flexibility in the distribution of those operator these quality make the algorithm particularly appropriate for the integration of heterogeneous agent into application system the algorithm is implemented in a multi agent framework team that provides the infrastructure required for communication and cooperation 
we propose a new version of rippling called relational rippling rippling is a heuristic for guiding proof search especially in the step case of inductive proof relational rippling is designed for representation in which value passing is by shared existential variable a opposed to function nesting thus relational rippling can be used to guide reasoning about logic program or circuit represented a relation we give an informal motivation and introduction to relational rippling more detail including formal definition and termination proof can be found in the longer version of this paper bundy and lombart 
radial basis function rbf network also known a networksof locally tuned processing unit see are well known for theirease of use most algorithm used to train these type of network however require a fixed architecture in which the numberof unit in the hidden layer must be determined before trainingstarts the rce training algorithm introduced by reilly cooperand elbaum see and it probabilistic extension the p rcealgorithm take advantage of a growing 
this paper present a new algorithm for structure from motion from an arbitrary number of tracked feature over an arbitrary number of image which posse several advantage over previous formulation first it is recursive so the time complexity is independent of the number of image the complexity is linear with the number of tracked feature the algorithm allows newly appeared feature to be included stale feature to be discarded and missing data to be handled naturally dynamic outlier elimination is achieved without recourse to heuristic segmentation strategy lastly the algorithm can employ different kind of tracked feature e g edge and corner in the same framework the actual structure from motion recovered is affine which assumes limited depth variation within the field of view but the recovery is based on a more general recursive estimation algorithm known a the variable state dimension filter vsdf which we devised and applied earlier to active camera calibration result are presented for real image sequence and timing for the algorithm demonstrate the feasibility for real time implementation 
a method for matching d curve under euclideanmotions is presented our approach us a semidifferentialinvariant description requiring only firstderivatives and one reference point thus avoidingthe computation of high order derivative a novelcurve similarity measure building on the notion offfl reciprocal correspondence is proposed it is shownthat by combining ffl reciprocal correspondence with therobust least median of square motion estimation theregistration of partially 
we investigate the improvement of therom provers by reusing previously computed proof a proof of a conjecture is generalized by replacing function symbol with function variable this yield a schematic proof of a schematic conjecture which is instantiated subsequently for obtaining proof of new similar conjecture our reuse method requires solving so called free function variable i e variable which cannot be instantiated by matching the schematic conjecture with a new conjecture we develop an algorithm for solving free function variable by combining the technique of symbolic evaluation and second order matching heuristic for controlling the algorithm are presented and several example demonstrate their usefulness we also show how our reuse proposal support the discovery of useful lemma 
our overall goal is to produce a automatic aspossible facial expression with wrinkle fromspoken input we focus on two aspect of thisproblem integration of the expressive wrinklesand generation of synchronized speechanimation our facial model integrates facialmuscles deformation and bulge we have produceda high level programming language toautomatically drive d animation of facial expressionsfrom speech our system embodiesrule governed translation from speech and utterance 
the truth condition for conditional sentence have been well studied but few compelling attempt have been made to define mean of evaluating iterated or nested conditionals in particular most approach impose very few constraint on the set of conditionals an agent can hold after revision of it belief set in this paper we describe the method of natural revision that ensures the preservation of conditional belief after revision by an objective belief our model based on a simple modal logic for belief and conditionals extends the agm theory of belief revision to account for sentence of objective revision of a belief set this model of revision ensures that an agent make a few change a possible to the conditional component of it belief set adopting the ramsey test natural revision provides truth condition for arbitrary right nested conditionals we show that the problem of determining acceptance of any such nested conditional can be reduced to acceptance test for unnested conditionals indicating that iterated revision can be simulated by virtual update we also briefly describe certain reduction to sometimes tractable propositional inference and other informational property 
this paper present a new method of recognizing facial expression using a two dimensional physical model named potential net the advantage of the method is not to need extracting facial feature from an image so that it is robust for variation of illumination and facial individuality potential net is a physical model which consists of node connected by spring in two dimensional grid configuration this net is set on a facial image and is deformed by image force which move the node to position near to facial feature recognition is executed by analyzing the similarity between model net prepared previously and a net deformed by an input image 
the paper describes an approach to the tracking of complex shape through image sequence that combine deformable region model and deformable contour a deformable region model is presented it optimisation is based on texture correlation and is constrained by the use of a motion model such a rigid affine or homographic the use of texture information versus edge information noticeably improves the tracking performance of deformable model in the presence of texture then the region contour is refined using an edge based deformable model in order to better deal with specularities non planar object and occlusion the method is illustrated and validated by experimental result on real image 
we present pika an implemented self explanatory simulator that is more than time faster than simgen mk forbus and falkenhainer the previous state of the art like simgen pika automatically prepares and run a numeric simulation of a physical device specified a a particular instantiation of a general domain theory and it is capable of explaining it reasoning and the simulated behavior unlike simgen pika s modeling language allows arbitrary algebraic and differential equation with no prespecified causal direction pika infers the appropriate causality and solves the equation a necessary to prepare for numeric integration 
diagnosis of human disease or machine fault is a missing data problem since many variable are initially unknown additional information need to be obtained the joint probability distribution of the da ta can be used to solve this problem we model this with mixture model whose parameter are estimated by the em algorithm this give the benefit that missing data in the database itself can also be handled correctly th e request for new information to refine the diagnosis is performed using the maximum utility principle since the system is based on learning it i s domain independent and le labor intensive than expert system or probabilistic network an example using a heart disease database is presented 
this paper discus the problem of predicting image feature in an image from image feature in two other image and the epipolar geometry between the three image we adopt the most general camera model of perpective projection and show that a point can be predicted in the third image a a bilinear function of it image in the first two camera that the tangent to three corresponding curve are related by a trilinear function and that the curvature of a curve in the third image is a linear function of the curvature at the corresponding point in the other two image we thus answer completely the following question given two view of an object what would a third view look like we show that in the special case of orthographic projection our result for point reduce to those of ullman and basri we demonstrate on synthetic a well a on real data the applicability of our theory 
we present two addition to the hierarchical mixture of expert hme architecture we view the hme a a tree structured classifier firstly by applying a likelihood splitting criterion to eachexpert in the hme we quot grow quot the tree adaptively during training secondly by considering only the most probable path through thetree we may quot prune quot branch away either temporarily or permanentlyif they become redundant we demonstrate result forthe growing and pruning algorithm which show 
focus on approximating object part shape by distinctive type of volumetric primitive shape approximation is accomplished by fitting volumetric model called parametric geons to multiview range data of single part object and classifying the fitting residual parametric geons are seven qualitative shape type defined by parameterized equation which control the size and degree of tapering and bending model fitting is performed by minimizing an objective function which measure the similarity in both size and shape between model and object multiple view data global shape constraint and global optimization are employed to obtain unique model and to compensate for noise and minor variation in object shape this approach ha been studied in experiment with both synthetic d data and actual rangefinder data of perfect and imperfect geon like object 
the immune system offer to be a rich source of metaphor to guide the exploration of the notion of an adaptive system we might define a class of system which are inspired by but diverge from description of the immune system and refer to them a immune bused system the research reported here is motivated by a desire to explore the possibility of such system specifically we attempt to construct an associative memory using immune system modelling a a starting point 
the ability to argue to support a conclusion or to encourage some course of action is fundamental to communication guided by examination of naturally occurring argument this paper classifies the communicative structure and function of several different kind of argument and indicates how these can be formalized a plan based model of communication the paper describes the use of these communication plan in the context of a prototype which cooperatively interacts with a user to allocate scarce resource this plan based approach to argument help improve the cohesion and coherence of the resulting communication 
we suggest an approach to describing and tracking the deformation of facial feature we concentrate on the mouth since it shape is important in detecting emotion however we believe that our system could be extended to deal with other facial feature in our system the mouth is described by a valley contour which is based between the lip this contour is shown to exist independently of illumination viewpoint identity and expression we present a real time mouth tracking system that follows this valley it is shown to be robust to change in identity illumination and viewpoint a simple classification algorithm wa found to be sufficient to discriminate between different mouth shape with a recognition rate 
this paper discus why traditional reinforcement learning method and algorithm applied to those model result in poor performance in situated domain characterized by multiple goal noisy state and inconsistent reinforcement we propose a methodology for designing reinforcement function that take advantage of implicit domain knowledge in order to accelerate learning in such domain the methodology involves the use of heterogeneous reinforcement function and progress estimator and applies to learning in domain with a single agent or with multiple agent the methodology is experimentally validated on a group of mobile robot learning a foraging task 
relevance feedback which modifies query using judgement of the relevance of a few highly ranked document ha historically been an important method for increasing the performance of information retrieval system in this paper we extend the inference network model introduced by turtle and croft to include relevance feedback technique the difference between relevance feedback on text abstract and full text collection is studied preliminary result for relevance feedback on the structured query supported by the inference net model are also reported 
the paper present genuinely interdisciplinary research in the intersection of ai machine learning and art music we describe an implemented system that learnsexpressive interpretation of music piece from performance by human musician this problem shown to be very difficult in the introduction is solved by combininginsights from music theory with a new machine learning algorithm theoreticallyfounded knowledge about music perception is used to transform the original learning 
tr paul r cohen adam carlson lisa ballesteros robert st amant automating path analysis for building causal model from data path analysis is a generalization of multiple linear regression that build model with causal interpretation it is an exploratory or discovery procedure for finding causal structure in correlational data recently we have applied statistical method such a path analysis to the problem of building model of ai program which are generally complex and poorly understood for example we built by hand a path analytic causal model of the behavior of the phoenix planner path analysis ha a huge search space however if one measure see hardcopy parameter of a system then one can build see hardcopy causal model relating these parameter for this reason we have developed an algorithm that heuristically search the space of causal model this paper describes path analysis and the algorithm and present preliminary empirical result including what we believe is the first example of a causal model of an ai system induced from performance data by another ai system 
non deductive reasoning system are often representation dependent representing the same situation in two different way may cause such a system to return two different answer some have viewed this a a significant problem for example the principle of maximum entropy ha been subjected to much criticism due to it representation dependence there ha however been almost no work investigating representation dependence in this paper we formalize this notion and show that it is not a problem specific to maximum entropy in fact we show that any representation independent probabilistic inference procedure that ignores irrelevant information is essentially entailment in a precise sense moreover we show that representation independence is incompatible with even a weak default assumption of independence we then show that invariance under a restricted class of representation change can form a reasonable compromise between representation independence and other desideratum and provide a construction of a family of inference procedure that provides such restricted representation independence using relative entropy 
from the publisher markov random field mrf theory provides a basis for modeling contextual constraint in visual processing and interpretation it enables u to develop optimal vision algorithm systematically when used with optimization principle this book present a comprehensive study on the use of mrfs for solving computer vision problem the book cover the following part essential to the subject introduction to fundamental theory formulation of mrf vision model mrf parameter estimation and optimization algorithm various vision model are presented in a unified framework including image restoration and reconstruction edge and region segmentation texture stereo and motion object matching and recognition and pose estimation this book is an excellent reference for researcher working in computer vision image processing statistical pattern recognition and application of mrfs it is also suitable a a text for advanced course in these area 
the problem of accurate depth estimation using stereo in the presence of specular reflection is addressed specular reflection a fundamental and ubiquitous reflection mechanism is viewpoint dependent and can cause large intensity difference at corresponding point resulting in significant depth error we analyze the physic of specular reflection and the geometry of stereopsis which led u to a relationship between stereo vergence surface roughness and the likelihood of a correct match given a lower bound on surface roughness an optimal binocular stereo configuration can be determined which maximizes precision in depth estimation despite specular reflection however surface roughness is difficult to estimate in unstructured environment therefore trinocular configuration independent of surface roughness are determined such that at each scene point visible to all sensor at least one stereo pair can compute produce depth we have developed a simple algorithm to reconstruct depth from the multiple stereo pair 
nonmonotonic formalism and belief revision operator have been introduced a useful tool to describe and reason about evolving scenario both approach have been proven effective in a number of different situation however little is known about their relationship previous work by winslett ha shown some correlation between a specific operator and circumscription in this paper we greatly extend winslett s work by establishing new relation between circumscription and a large number of belief revision operator this highlight similarity and difference between these formalism furthermore these connection provide u with the possibility of importing result in one field into the other one 
agent plan in order to improve their performance however planning take time and consumes resource that may in fact degrade an agent performance ideally an agent should only plan when the expected improvement outweighs the expected cost and no resource should be expended on making this decision to do this an agent would have to be omniscient the problem of how to approximate this ideal without consuming too many resource in the process is the meta level control problem for a resource bounded rational agent there are two central question that have to be addressed for meta level control where to focus planning effort and when to start executing the current best plan these question are interrelated to start execution the beginning o f the plan must be elaborated to a level where it is operational even then execution should only begin when the expected improvement due to further planning is outweighed by the cost of delaying execution once the agent ha committed to executing some action the planner can then disregard any plan inconsistent with this action and can concentrate on elaborating and optimizing the rest of the plan in my thesis research i am exploring the use of sensitivity analysis based meta level control for focusing computational effort the object level problem of deciding which action to perform is modeled a a standard decision problem and an approximate sensitivity analysis is performed to facilitate the sensitivity analysis action both abst ract and operational are augmented with method for estimating their resource and time requirement method are also needed to estimate the likelihood of event and action outcome all estimate include both the expected value and the expected range or variance information about the preci sion of estimate is critical when deciding whether to commit to a particular plan or whether to refine estimate through further computation or sensing when presented with a new task the planner generates abstract plan for accomplishing the new and existing task a sensitivity analysis identifies which of these plan are potentially optimal and non dominated dominated and never optimal plan are discarded the sensitivity analys is also identifies which estimate the choice between plan is most sensitive to estimate that affect all plan more or le s equally need not be refined for instance the occurrence of an earthquake may adversely affect all plan equally determining the probability of an earthquake more exactly would not help in selecting between plan other factor may have differing affect for instance the likelihood of rain would help to choose between a plan to walk and a plan to drive somewhere the sensitivity of a plan to particular estimate can also suggest way of making the plan more robust for instance carrying an umbrella help to reduce sensitivity to the likelihood of rain for the plan to walk when there are a number of plan that are potentially optimal and non dominated and when the potential opportunity cost of selecting the wrong plan is significant the meta level controller directs the effort of the planner to refine critical estimate estimate of resource use and action time can be improved by elaborating abstract operator into more operational operator or by simulated execution other objec t level estimate can be refined by adding more sensing to the plan or by additional computation using technique such a temporal projection hank estimating computation time for complex planner is problematic further research is needed to determine how to best estimate and characterize expected plan improvement a a function of computation time information from the sensitivity analysis and estimate of the cost of improving the current plan are used to make the tradeoff between the cost of delaying execution and the expected improvement in the plan for doing additional planning often system that make this tradeoff ignore the fact that execution and planning can be overlapped in many situation the dta algorithm is one example russell and wefald in related work i show how taking into account overlapping of planning and execution can improve performance goodwin 
learning from reinforcement is a promising approach for creating intelligent agent however reinforcement learning usually requires a large number of training episode we present an approach that address this shortcoming by allowing a connectionist q learner to accept advice given at any time and in a natural manner by an external observer in our approach the advice giver watch the learner and occasionally make suggestion expressed a instruction in a simple programming language based on technique from knowledge based neural network these program are inserted directly into the agent s utility function subsequent reinforcement learning further integrates and refines the advice we present empirical evidence that show our approach lead to statistically significant gain in expected reward importantly the advice improves the expected reward regardless of the stage of training at which it is given 
cf loadingtexthtml cf contextpath cf ajaxscriptsrc cfide script ajax cf jsonprefix cf clientid b d b cf c f aa a a a model for hormonal modulation of learning function settab var mytabs coldfusion layout gettablayout citationdetails mytabs on tabchange function tabpanel activetab document cookie picked activetab id function letemknow coldfusion window show letemknow function testthis alert test function loadalert alert i am in the load alert function loadalert alert i am in the load alert google load visualization package orgchart google setonloadcallback drawchart function drawchart var data new google visualization datatable data addcolumn string name data addcolumn string manager data addcolumn string tooltip data addrows v f cc for this article 
we show that for a single neuron with the logistic function a the transfer function the number of local minimum of the error function based on the square loss can grow exponentially in the dimension 
this paper deal with the recovery of d information using a single mobile camera in the context of active vision we propose a general revisited formulation of the structure from motion issue and we determine adequate camera configuration and motion which lead to a robust and accurate estimation of the d structure parameter we apply the visual servoing approach to perform these camera motion real time experiment dealing with the d structure estimation of point and cylinder are reported and demonstrate that this active vision strategy can very significantly improve the estimation accuracy 
the author describe a framework for establishing correspondence computing canonical description and recognizing object that is based on the idea of describing object by their generalized symmetry a defined by the object s free vibration mode a technique given by a pentland and s scarloff described object in term of the mode of some prototype shape in contrast this new method computes the object s mode directly from available image information this result in greater generality and accuracy and is applicable to data of any dimensionality for the purpose of illustration a detailed mathematical formulation of the method is given for d problem and it is demonstrated on gray scale image and contour data 
semi markov decision problem are continuous time generalizationsof discrete time markov decision problem a number ofreinforcement learning algorithm have been developed recentlyfor the solution of markov decision problem based on the ideasof asynchronous dynamic programming and stochastic approximation among these are td q learning and real time dynamicprogramming after reviewing semi markov decision problemsand bellman s optimality equation in that context we propose 
when a plane undergoes a deformation that can be represented by a planar linear vector field the projected vector field on the image plane of an optical device is at most quadratic this d motion field ha one singular point with eigenvalue identical to those of the singular point describing the deformation a a consequence the nature of the singular point of the deformation is a projective invariant when the plane move and experience a linear deformation at the same time the associated d motion field is still quadratic with at most singular point in the case of a normal rototranslation i e when the angular velocity is normal to the plane and of a linear deformation the d motion field ha at most one singular point and substantial information on the rigid motion and on the deformation can be recovered from it experiment with simulated deformation and real deformable object show that the proposed analysis can provide accurate result and information on more general d deformation 
individual cue from visual module are fallible and oftenambiguous a a result only integrated vision systemscan be expected to give a reliable performance in practice the design of such system is challenging since eachvision module work under different and possibly conflictingsets of assumption we have proposed and implementeda multiresolution system which integrates perceptualgrouping segmentation stereo shape from shading and line labelling module the output of the 
this paper present a new algorithm for locatingthe boundary of textured region both step changesand outlier using a robust estimator previous robustimage filter perform poorly on binary image blur edge round corner and run slowly i avoidartifacts on binary image by modelling them a continuousand interpolating value information is combineddirectly between non adjacent location to preventblurring corner are sharpened by relabellingmis classified pixel the 
the performance of on line algorithm for learning dichotomy is studied in on line learning the number of example p is equivalent to the learning time since each example ispresented only once the learning curve or generalization error a a function of p dependson the schedule at which the learning rate is lowered for a target that is a perceptron rule the learning curve of the perceptron algorithm can decrease a fast a p gamma if the scheduleis optimized if the target is 
a rational agent in a multi agent world must decide on it action based on the decision it expects others to make but it might believe that they in turn might be basing decision on what they believe the initial agent will decide such reciprocal rationality lead to a nesting of model that can potentially become intractable to solve such problem game theory ha developed technique for discovering rational equilibrium solution and ai ha developed computational recursive method these different approach can involve different solution concept for example the recursive modeling method rmm find different solution than game theoretic method when solving problem that require mixed strategy equilibrium solution in this paper we show that a crucial difference between the approach is that rmm employ a solution concept that is overeager this eagerness can be reduced by introducing into rmm second order knowledge about what it know in the form of a flexible function for mapping relative expected utility of an option into the probability that the agent will pursue that option this modified solution concept can allow rmm to derive the same mixed equilibrium solution a game theory and thus help u delineate the type of knowledge that lead to alternative solution concept 
we have written a prototype computer program called trendx for automated trend detection during process monitoring the program us a representation called trend template that define disorder a typical pattern of relevant variable these pattern consist of a partially ordered set of temporal interval with uncertain endpoint bound to each temporal interval arc value constraint on real valued function of measurable parameter trendx ha been used to diagnose trend in growth pattern from examining height weight and other parameter of pediatric patient a trendx analyzes successive data point the program update it hypothesis about which stage of the growth process each data point belongs to we present an example of trendx reaching temporally plausible diagnosis for an actual patient with delayed growth currently being seen at boston child s hospital 
chinese sentence are written with no special delimiters such a space to indicate word boundary existing chinese nlp system therefore employ preprocessors to segment sentence into word contrary to the conventional wisdom of separating this issue from the task of sentence understanding we propose an integrated model that performs word boundary identification in lockstep with sentence understanding in this approach there is no distinction between rule for word boundary identification and rule for sentence understanding these two function are combined word boundary ambiguity are detected especially the fallacious one when they block the primary task of discovering the inter relationship among the various constituent of a sentence which essentially is the essence of the understanding process in this approach statistical information is also incorporated providing the system a quick and fairly reliable starting ground to carry out the primary task of relationship building 
structured text for example dictionary and user manual typically have a heirarchical tree like structure we describe a query language for retrieving information from collection of hierarchical text the language is based on a tree pattern matching notion called tree inclusion tree inclusion allows easy expression of query that use the structure and the content of the document in using it a user need not be aware of the whole structure of the database thus a language based on tree inclusion is data independent a property made necessary because of the great variance in the structure of the text 
in conventional knowledge acquisition a domain expert interacts with a knowledge engineer who interview the expert and code knowledge about the domain object and procedure in a rule based language or other textual representation language this indirect methodology can be tedious and errorprone since the domain expert s verbal description can be inaccurate or incomplete and the knowledge engineer may not correctly interpret the expert s intent we describe a user interface that allows a domain expert who is not a programmer to construct representation of object and procedure directly from a video of a human performing an example procedure the domain expert need not be fluent in the underlying representation language since all interaction is through direct manipulation starting from digitized video the user selects significant frame that illustrate beforeand afterstates of important operation then the user graphically annotates the content of each selected frame selecting portion of the image to represent each part labeling the part and indicating part whole relationship finally programming by demonstration technique describe the action that represent the transition between frame the result is object description for each object in the domain generalized procedural description and visual and natural language documentation of the procedure we illustrate the system in the domain of documentation of operational and maintenance procedure for electrical device 
we introduce a novel stochastic inversion transduction grammar formalism for bilingual language modeling of sentence pair and the concept of bilingual parsing with potential application to a variety of parallel corpus analysis problem the formalism combine three tactic against the constraint that render finite state transducer le useful it skip directly to a context free rather than finite state base it permit a minimal extra degree of ordering flexibility and it probabilistic formulation admits an efficient maximum likelihood bilingual parsing algorithm a convenient normal form is shown to exist and we discus a number of example ot how stochastic inversion transduction grammar bring bilingual constraint to bear upon problematic corpus analysis task 
we present a new approach to theory revision that us a linguistically based semantics to help detect and correct error in classification rule the idea is that preferring linguistically cohesive revision will enhance the comprehensibility and ultimately the accuracy of rule we explain how to associate term in the rule with element in a lexical class hierarchy and use distance within the hierarchy to estimate linguistic cohesiveness we evaluate the utility of this approach empirically using two relational domain 
with the increasing role of high performance computing in attacking complex physical problem there is an urgent need for the development of advanced computational technology to provide scientist with high level assistance in the analysis interpretation and modeling of a massive amount of quantitative data a critical area where this need is quite evident is the problem of turbulence the overall research goal is to develop a computational environment to help scientist efficiently make observation and conceptual model of turbulence data set this paper present the progress of this project my approach is based on two key idea local interaction and evolution of coherent object like vortex enable high level qualitative interpretation of turbulence data and abstracting from the particular feature of fluid dynamical reasoning propose five core operation aggregation classification re description spatial inference and configuration change a part of a general theory of imagistic reasoning a new vortex finding algorithm is also presented 
we consider the problem of robustly estimating optical flow from a pair of image using a new framework based on robust estimation which address violation of the brightness constancy and spatial smoothness assumption we also show the relationship between the robust estimation framework and line process approach for coping with spatial discontinuity in doing so we generalize the notion of a line process to that of an outlier process that can account for violation in both the 
researcher in computer vision have primarily studied the problem of visual reconstruction of environmental structure that is plainly visible in this thesis the conventional goal of visual reconstruction are generalized to include both visible and occluded forward facing surface this larger fraction of the environment is termed the it anterior surface because multiple anterior surface neighborhood project onto a single image neighborhood wherever surface overlap surface neighborhood and image neighborhood are not guaranteed to be in one to one correspondence a conventional shape from method assume the result is that the topology of three dimensional scene structure can no longer be taken for granted but must be inferred from evidence provided by image contour where boundary are not occluded and where surface reflectance is distinct from that of the background boundary will be marked by image contour however where boundary are occluded or where surface reflectance match background reflectance there will be no detectable luminance change in the image deducing the complete image trace of the boundary of the anterior surface under these circumstance is called the it figural completion problem in this thesis we show that the boundary of the anterior surface can be represented in viewer centered coordinate a a it labeled knot diagram the interior neighborhood of the anterior surface are explicitly represented by a combinatorial model called a it paneling which is produced from a labeled knot diagram by mean of a straightforward construction conventional shape from method formulated a variational problem and defined over image neighborhood can be applied to the neighborhood of the paneling equally well the labeling scheme and paneling construction provide a solid theoretical foundation for a working experimental system which computes surface representation from illusory contour display including well known figure from the visual psychology literature the experimental system employ a two stage process of completion hypothesis and combinatorial optimization the labeling scheme is enforced by a system of integer linear inequality so that the final organization is the optimal feasible solution of an integer linear program 
overgeneration is the main source of computational complexity in previous principle based parser this paper present a message passing algorithm for principle based parsing that avoids the overgeneration problem this algorithm ha been implemented in c and successfully tested with example sentence from van riemsdijk and williams 
we present a statistical method that pac learns the class of stochastic perceptrons with arbitrary monotonic activation function and weightswi f g when the probability distribution that generates the input example is member of a family that we call k blocking distribution such distribution represent an important step beyond the case where each input variable is statistically independent since the k blocking family contains all the markov distribution of order k by stochastic perceptron we mean a perceptron which upon presentation of input vector x output with probability f p iwixi because the same algorithm work for any monotonic nondecreasing or nonincreasing activation function f on boolean domain it handle the well studied case of sigmo d and the usual radial basis function 
for semantic query optimization one need detailedknowledge about the content of the database traditionaltechniques use static knowledge about all possiblestates of the database which is already given new technique use knowledge only about the currentstate of the database which can be found by methodsof knowledge discovery in database database areoften very large and permanently in use therefore method of knowledge discovery are only allowed totake a small amount of the 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
the scenario based engineering process sep is a novel approach to developing complex system haddock z harbison sep build new application system through a selection process that group primitive component into application specific component the selection of primitive component and the construction of interface among component in an application system is currently a tedious manual undertaking the automation of this process will require a configuration system that can support the complex interaction of the component the dynamic requirement of user and the capability of providing multiple viewpoint and managing extensive domain the university of michigan procedural reasoning system um pr lee et al is a reactive reasoning and planning system based on pr georgeff lansky um pr is currently being used in the autonomous vehicle domain it ability to continually consider the real time dynamic environment and access plan accordingly fit well into military application where plan have already been generated in the form of standing operating procedure and reaction to the quickly changing environment are paramount our sep domain doe not require the hard real time speed of a reactionary system however much of the um pr architecture map readily to the configuration problem in the sep domain the scenario based engineering procedural reasoning system seprs will use the architecture of um pr to implement a configuration system for sep primitive component will take the place of plan and will be selected according to the application requirement and the application architecture in progress the interpreter will use the application requirement a goal to satisfy by accessing the primitive component component previously selected for an application architecture will be in the in process area they are accessed by the interpreter to determine which goal are not yet satisfied the interpreter will activate relevant primitive component that are maintained by the intention structure the intention structure will release the chosen primitive component to the component integrator the 
the paper describes a new approach to image segmentation it accepts the inherent deficiency occuring when extracting low level feature and when dealing with the complexity of real scene image segmentation therefore is understood a deriving a rich symbolic description useful for task such a stereo or object recognition in outdoor scene the approach is based on a polymorphic scheme for simultaneously extracting point line and segment in a topologically consistent manner together with their mutual relation derived from the feature adjacency graph fag thereby performing several grouping step which gradually use more and more specific domain knowledge to achieve an optimal image description the heart of the approach is a detailed analysis of the fag and a robust estimation for validating the found geometric hypothesis the analysis of the fag derived from the exoskeleton of the feature allows to detect inconsistency of the extracted feature with the ideal image model a cell complex the fag is used for finding hypothesis about incidence relation and geometric hypothesis such a collinearity or parallelity also between non neighbored point and line the m type robust estimation is used for simultaneously eliminating wrong hypothesis on geometric relationship it us a new argument for the weighting function 
we find that dissemination of collection wide information cwi in a distributed collection of document is needed to achieve retrieval effectiveness comparable to a centralized collection complete dissemination is unnecessary the required dissemination level depends upon how document are allocated among site low dissemination is needed for random document allocation but higher level are needed when document are allocated based on content we define parameter to control dissemination and document allocation and present result from four test collection we define the notion of iso knowledge line with respect to the number of site and level of dissemination in the distributed archive and show empirically that iso knowledge line are also iso effectiveness line when document are randomly allocated 
proposing measurement in diagnosis system for static system is a well understood task usually entropy based approach are used sometimes extended by cost and other consid erations how to do the same task in dynamic system is le clear and so far measurement proposal algorithm have been ignored in the recent approach advanced for dynamic sys tems in this paper we will describe a set of technique and algorithm suitable for mea surement proposal in a temporal diagnosis for malism discussed in our previous work this formalism is based on qualitative allen con straints the current paper introduces a mea surement proposal algorithm and improves it in several way finally an entropy based com putation method is described for this temporal setting 
we describe two parallel analog vlsi architecture that integrate optical ow data obtained from array of elementary velocity sen sors to estimate heading direction and time to contact for heading direction computation we performed simulation to evaluate the most important qualitative property of the optical ow eld and determine the best functional operator for the implementation of the architecture for time to contact we exploited the divergence theorem to integrate data from all velocity sensor present in the architecture and average out possible error 
this paper introduces oc a new algorithm forgenerating multivariate decision tree multivariatetrees classify example by testing linear combinationsof the feature at each non leaf node ofthe tree each test is equivalent to a hyperplane atan oblique orientation to the ax because of thecomputational intractability of finding an optimalorientation for these hyperplanes heuristic methodsmust be used to produce good tree thispaper explores a new method that combine 
the goal of the pangloss project are to investigate and develop a new generation knowledge based interlingual machine translation system combining symbolic and statistical technique the system is to translate newspaper text in arbitrary domain though a specific financial domain is given preference to a high quality a possible using a little human intervention a possible 
gr is a hybrid knowledge based system consisting of a multilayer perceptron mlp and a rule based system for hybrid knowledge representation and reasoning knowledge embedded in the trained mlp is extracted in the form of general production rule a natural format of abstract knowledge representation the rule extraction method integrates black box and open box technique obtaining feature salient and statistical property of the training pattern set the extracted general rule are quantified and selected in a rule validation process multiple inference facility such a categorical reasoning probabilistic reasoning and exceptional reasoning are performed in gr 
the paper give a practical rapid algorithm for doing projective reconstruction of a scene consisting of a set of line seen in three or more image with uncalibrated camera the algorithm is evaluated on real and ideal data to determine it performance in the presence of varying degree of noise by carefully consideration of source of error it is possible to get accurate reconstruction with realistic level of noise the algorithm can be applied to image from different camera or the same camera for image with the same camera with unknown calibration it is possible to do a complete euclidean reconstruction of the image this extends to the case of uncalibrated camera previous result of spetsakis and aloimonos on scene reconstruction from line 
visual motion estimation can be regarded a estimation of the state of a system of differenceequations with unknown input defined on a manifold such a system happens to be quot linear quot but it is defined on a space the so called quot essential manifold quot which is not a linear vector space in this paper we will introduce a novel perspective for viewing the motion estimation problemwhich result in three original scheme for solving it the first consists in quot flattening the space quot and 
our goal is to identify the feature that predict cue selection and placement in order to devise strategy for automatic text generation much previous work in this area ha relied on ad hoc method our coding scheme for the exhaustive analysis of discourse allows a systematic evaluation and refinement of hypothesis concerning cue we report two result based on this analysis a comparison of the distribution of since and because in our corpus and the impact of embeddedness on cue selection 
we study the problem of when to stop learning a class of feedforward network network with linear output neuron and fixed input weight when they aretrained with a gradient descent algorithm on a finite number of example undergeneral regularity condition it is shown that there are in general three distinctphases in the generalization performance in the learning process and in particular the network ha better generalization performance when learning is stopped at acertain time 
intelligent human agent exist in a cooperativesocial environment that facilitateslearning they learn not only by trialand error but also through cooperation bysharing instantaneous information episodicexperience and learned knowledge thekey investigation of this paper are quot giventhe same number of reinforcement learningagents will cooperative agent outperformindependent agent who do not communicateduring learning quot and quot what is the pricefor such cooperation quot using 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
the applicability of lexicographic comparison in nonmonotonic reasoning with specificity is investigated a priority mechanism based on lexicographic comparison is defined for rciter s default logic the following principle earlier used by geffner and pearl in conditional entailment is used a the basis for specificity based priority for normal default theory for each rule there is a context where it may not be defeated by any other rule a method for computing priority according to the principle is given connection to earlier work is discussed 
self explanatory simulator have many potential application including supporting engineering activity intelligent tutoring system and computer based training system to fully realize this potential requires improving the technology to efficiently generate highly optimized simulator this paper describes an algorithm for compiling selfexplanatory simulator that operates in polynomial time it is capable of constructing self explanatory simulator with thousand of parameter which is an order of magnitude more complex than any previous technique the algorithm is fully implemented and we show evidence that suggests it performance is quadratic in the size of the system being simulated we also analyze the tradeoff between compiler and interpreter for self explanatory simulation in term of application imposed constraint and discus plan for application 
we propose and demonstrate a new paradigm for active vision research that draw upon recent advance in the field of artificial life and computer graphic a software alternative to the prevailing hardware vision mindset animat vision prescribes artificial animal or animats situated in physic based virtual world a autonomous virtual robot possessing active perception system to be operative in it world an animat must autonomously control it eye and muscle actuated body applying computer vision algorithm to continuously analyze the retinal image stream acquired by it eye in order to locomote purposefully through it world we describe an initial animat vision implementation within lifelike artificial fish inhabiting a physic based virtual marine world emulating the appearance motion and behavior of real fish in their natural habitat these animats are capable of spatially nonuniform retinal imaging foveation retinal image stabilization color object recognition and perceptually guided navigation these capability allow them to pursue moving target such a fellow artificial fish animat vision offer a fertile approach to the development implementation and evaluation of computational theory that profess sensorimotor competence for animal or robotic situated agent 
pinta is a system for segmentation and visualization ofanatomical structure obtained from serial section reconstructedfrom magnetic resonance imaging the systemapproaches the segmentation problem by assigning eachvolumetric region to an anatomical structure this is accomplishedby satisfying constraint at the pixel level slicelevel and volumetric level each slice is represented by anattributed graph where node correspond to region andlinks correspond to the relation between 
we consider the problem of image segmentation and describe an algorithm that is based on the minimum description length mdl principle is fast is applicable to multi band image and guarantee closed region we construct an objective unction that when minimized yield a partitioning of the image into region where the pixel value in each band of each region are described by a polynomial surface plus noise the polynomial order and their coefficient are determined by the algorithm the minimization is difficult because it involves a search over a very large space and there is extensive computation required at each stage of the search to address the first of these problem we use a region merging minimization algorithm to address the second we use an incremental polynomial regression that us computation from the previous stage to compute result in the current stage resulting in a significant speed up over the non incremental technique the segmentation result obtained is suboptimal in general but of high quality result on real image are shown 
abstract we present a method for partitioning a set of surfaceestimates obtained with a laser range finding systeminto subset corresponding to part of an object ourstrategy us two complementary representation forsurfaces one that describes local structure in termsof differential property e g edge line contour and the other that represents the surface a a collectionof smooth patch at different scale by enforcing aconsistent interpretation between these two representation 
the computation of the optical flow field from an image sequence requires the definition of constraint on the temporal change of image feature in general these constraint limit the motion of the body in space and or of the feature on the image plane 
this paper explores the application of the minimum description length principle for pruning decision tree we present a new algorithm that intuitively capture the primary goal of reducing the misclassification error an experimental comparison is presented with three other pruning algorithm the result show that the mdl pruning algorithm achieves good accuracy small tree and fast execution time 
this paper present an action scheme for dialogue management for natural language interface the scheme guide a dialogue manager which directs the interface s dialogue with the user communicates with the background system and assist the interpretation and generation module the dialogue manager wa designed on the basis of an investigation of empirical material collected in wizard of oz experiment the empirical investigation revealed that in dialogue with database system user specify an object or a set of object and ask for domain concept information e g the value of a property of that object or set of object the interface responds by performing the appropriate action e g providing the requested information or initating a clarification subdialogue the action to bt carried out by the interface can be determined based on how object and property are specified from information in the user utterance the dialogue context and the response from the background system and it domain model 
this paper present an action scheme for dialoguemanagement for natural language interface the scheme guide a dialogue managerwhich directs the interface s dialogue with theuser communicates with the background system and assist the interpretation and generationmodules the dialogue manager wasdesigned on the basis of an investigation ofempirical material collected in wizard of ozexperiments the empirical investigation revealedthat in dialogue with database systemsusers specify 
we present a corpus based study of method that have been proposed in the linguistics literature for selecting the semantically unmarked term out of a pair of antonymous adjective solution to this problem are applicable to the more general task of selecting the positive term from the pair using automatically collected data the accuracy and applicability of each method is quantified and a statistical analysis of the significance of the result is performed we show that some simple method are indeed good indicator for the answer to the problem while other proposed method fail to perform better than would be attributable to chance in addition one of the simplest method text frequency dominates all others we also apply two generic statistical learning method for combining the indication of the individual method and compare their performance to the simple method the most sophisticated complex learning method offer a small but statistically significant improvement over the original test 
the problem of temporal credit assignment in reinforcement learning is typically solved using algorithm based on the method of temporal difference td lambda of those q learning is currently best understood and most widely used using td based algorithm with often allows one to speed up the propagation of credit significantly but it involves certain implementational problem the traditional implementation of td based on eligibility trace suffers from lack of generality and 
this paper continues work reported at ml on the use of the minimum descriptionlength principle with non probabilistic theory a new encoding scheme is developedthat ha similar benefit to the adhocpenalty function used previously thescheme ha been implemented in c rulesand empirical trial on real world datasetsreveal a small but useful improvement in classificationaccuracy introductionwhen classifier are induced from data the resultingtheories are commonly 
evaluation oriented information provision is a function performed by many system that serve a personal assistant advisor or sale assistant five general task are distinguished which need to be addressed by such system for each task technique employed in a sample of system are discussed and it is shown how the lesson learned from these system can be taken into account with a set of unified technique that make use of well understood concept and principle from multi attribute utility theory and bayesian network these technique are illustrated a realized in the dialog system pracma 
this paper describes a logic for reasoning in a multi source environment and a theorem prover for this logic we assume the existence of several source of information data knowledge base each of them providing information the main problem dealt with here is the problem of the consistency of the information even if each separate source is consistent the global set of information may be inconsistent in our approach we assume that the different source are totally ordered according to their reliability this order is then used in order to avoid inconsistency the logic we define for reasoning in this case is based on a classical logic augmented with pseudo modality it semantic is first detailed then a sound and complete axiomatic is given finally a theorem prover is specified at the meta level we prove that it is correct with regard to the logic we then implement it in a prolog like language 
a subpart of the following problem is considered it is assumed that a set of known three dimensional line with an unknown pose and orientation are observed with a camera the problem is to recover the position and orientation of the camera from the observed image line assuming that a correspondence ha been established between the d and the d line it is shown that there exist infinite set of three dimensional line such that no matter how many line are observed in these set the solution to the orientation or pose determination problem is not unique the maximum number of possible solution is given these result clearly define the domain of validity of algorithm which solve the orientation or pose determination problem 
an approach to improve accuracy of incorrectdomain theory is presented that learnsconcept description from positive and negativeexamples of the concept the methoduses the available domain theory that mightbe both overly general and overly specific to group training example before attemptingconcept induction gentre is a systemthat ha been implemented to test theperformance of the method gentre is notlimited to variable free function free or nonrecursivedomains a 
this paper proposes logic program a a specification for robot control these provide a formal specification of what an agent should do depending on what it sens and it previous sensory input and action we show how to axiomatise reactive agent event a an interface between continuous and discrete time and persistence a well a axiomatising integration and differentiation over time in term of the limit of sum and difference this specification need not be evaluated a a prolog program we use can the fact that it will be evaluated in time to get a more efficient agent we give a detailed example of a nonholonomic maze travelling robot where we use the same language to model both the agent and the environment one of the main motivation for this work is that there is a clean interface between the logic program here and the model of uncertainty embedded in probabilistic horn abduction this is one step towards building a decision theoretic planning system where the output of the planner is a plan suitable for actually controlling a robot 
we address the problem of automatically learning object model for recognition and pose estimation in contrast to the traditional approach we formulate the recognition problem a one of matching visual appearance than shape the appearance of an object in a two dimensional image depends on it shape reflectance property pose in the scene and the illummation condition while shape and reflectance are intrinsic property of an object and are constant pose and illumination vary from scene to scene we present a new compact representation of object appearance that is parametrized by pose and illumination for each object of interest a large set of image is obtained by automatically varying pose and illumination this large image set is compressed to obtain a low dimensional subspace called the eigenspace in which the object is represented a a hypersurface given an unknown input image the recognition system project the image onto the eigenspace the object is recognized based on the hypersurface it lie on the exact position of the projection on the hypersurface determines the object s pose in the image we have conducted experiment usmg several object with complex appearance characteristic these result suggest the proposed appearance representation to be a valuable tool for a variety of machine vision application 
generating production quality plan is an essential element in transforming planner from research tool into real world application however most research on planning so far ha concentrated on method for constructing sound and complete planner that find a satisficing solution and on how to find such solution in an efficient way similarly most of the work to date on automated control knowledge acquisition ha been aimed at improving the eficiency of planning this work ha been termed speed up learning our work focus on how control knowledge may guide a planner towards better plan and how such control knowledge can be learned better may be defined in 
standard approach to motion analysis assumethat the optic flow is smooth such technique havetrouble dealing with occlusion boundary the mostpopular solution is to allow discontinuity in the flowfield imposing the smoothness constraint in a piecewisefashion but there is a sense in which the discontinuitiesin flow are artifactual resulting from theattempt to capture the motion of multiple overlappingobjects in a single flow field instead we can decomposethe image sequence 
we introduce a new approach to planning in strip like domain based on constructing and analyzing a compact structure we call a planning graph we describe a new planner graphplan that us this paradigm graphplan always return a shortest possible partial order plan or state that no valid plan exists we provide empirical evidence in favor of this approach showing that graphplan outperforms the total order planner prodigy and the partial order planner ucpop on a variety of interesting natural and artificial planning problem we also give empirical evidence that the plan produced by graphplan are quite sensible since search made by this approach are fundamentally different from the search of other common planning method they provide a new perspective on the planning problem 
many ai problem can be modeled a constraint satisfaction problem csp but many of them are actually dynamic the set of constraint to consider evolves because of the environment the user or other agent in the framework of a distributed system in this context computing a new solution from scratch after each problem change is possible but ha two important drawback inefficiency and instability of the successive solution in this paper we propose a method for reusing any previous solution and producing a new one by local change on the previous one first we give the key idea and the corresponding algorithm then we establish it property termination correctness and completeness we show how it can be used to produce a solution either from an empty assignment or from any previous assignment and how it can be improved using filtering or learning method such a forward checking or nogoodrecording experimental result related to efficiency and stability are given with comparison with well known algorithm such a backtrack heuristic repair or dynamic backtracking 
in this work we address the problem of occlusion in tracking multiple d object in a known environment and propose a new approach for tracking vehicle in road traffic scene using an explicit occlusion reasoning step we employ a contour tracker based on intensity and motion boundary the motion of the contour of the vehicle in the image is assumed to be well describable by an affine motion model with a translation and a change in scale a vehicle contour is represented by closed cubic spline the position and motion of which is estimated along the image sequence in order to employ linear kalman filter we decompose the estimation process into two filter one for estimating the affine motion parameter and one for estimating the shape of the contour of the vehicle occlusion detection is performed by intersecting the depth ordered region associated to the object the intersection part is then excluded in the motion and shape estimation this procedure also improves the shape estimation in case of adjacent object since occlusion detection is performed on slightly enlarged region in this way we obtain robust motion estimate and trajectory for vehicle even in the case of occlusion a we show in some experiment with real world traffic scene 
robust linguistic method are applied to the task of answering closed class question using a corpus of natural language the method are illustrated in a broad domain answering general knowledge question using an on line encyclopedia a closed class question is a question stated in natural language which assumes some definite answer typified by a noun phrase rather than a procedural answer the method hypothesize noun phrase that are likely to be the answer and present the user with relevant text in which they are marked focussing the user s attention appropriately furthermore the sentence of matching text that are shown to the user are selected to confirm phrase relation implied by the question rather than being selected solely on the basis of word frequency the corpus is accessed via an information retrieval ir system that support boolean search with proximity constraint query are automatically constructed from the phrasal content of the question and passed to the ir system to find relevant text then the relevant text is itself analyzed noun phrase hypothesis are extracted and new query are independently made to confirm phrase relation for the various hypothesis the method are currently being implemented in a system called murax and although this process is not complete it is sufficiently advanced for an interim evaluation to be presented 
we describe a bottom up approach to the design of software agent we built and tested an agent system that address the real world problem of handling the activity involved in scheduling a visitor to our laboratory the system employ both task specific and user centered agent and communicates with user using both email and a graphical interface this experiment ha helped u to identify crucial requirement in the successful deployment of software agent including issue of reliability security and ease of use the architecture we developed to meet these requirement is fle xible and extensible and is guiding our current research on principle of agent design 
one of the advantage of supervised learning is that the final error metricis available during training for classifier the algorithm can directly reducethe number of misclassifications on the training set unfortunately when modeling human learning or constructing classifier for autonomousrobots supervisory label are often not available or too expensive in thispaper we show that we can substitute for the label by making use ofstructure between the pattern distribution to 
in this paper we present a technique for automatically generating constraint on parameter derivative that reduce ambiguity in the behaviour prediction starting with a behaviour prediction using an initial library containing general domain knowledge the technique employ feedback about valid and spurious state of behaviour and knowledge about the causal dependency between the parameter in the model in order to determine the constraint that remove the undesired state of behaviour that result from spurious ambiguity in addition the technique point out the assembly of physical object to which the generated constraint apply 
we show that feature logic extended by functional uncertainty is decidable even if one admits cyclic description we present an algorithm which solves feature description containing functional uncertainty in two phase both phase using a set of deterministic and non deterministic rewrite rule we then compare our algorithm with the one of kaplan and maxwell that doe not cover cyclic feature description 
we present an approach for recognizing and classifying moving vehicle in monocularimages sequence of traffic scene recorded by a stationary camera a generic vehicle model represented by a d polyhedral model described by length parameter is used to coverthe different shape of road vehicle the object recognition process is initialized by formulatinga model hypothesis using a reference model and initial value provided by a motionsegmentation step from a model based tracking 
automatic abstracting typically based on extraction of important sentence from a text ha been treated a a largely separate task from automatic indexing this paper describes an approach in which the indexing and abstracting task are effectively combined it is applicable to highly structured empirical research paper whose content can be organised using a semantic frame during a scan of a source text stylistic clue and construct are used for extracting candidate filler for the various slot in the frame subsequently an actual concept name is chosen for each slot by comparing the various candidate and their weight 
bounded response time is an important requirement when rule based expert system are used in real time application in the case the rule based system cannot terminate in bounded time we should detect the culprit condition causing the non termination to assist programmer in debugging this paper describes a novel tool which analyzes ops program to achieve this goal the first step is to verify that an ops program can terminate in bounded time a graphical representation of an ops program is defined and evaluated once the termination of the ops program is not expected the culprit condition are detected these condition are then used to correct the problem by adding extra rule to the original program 
abstract information filtering system have potential power that may provide an efficient mean of navigating through large and diverse data space however current information filtering technology heavily depends on a user s active participation for describing the user s interest to information item forcing the user to accept extra load to overcome thealready loaded situation fumhemo because theuser s interest weoften expressed indiscrete fomat such a a set of keywords sometimes augmented with if then rule it is difficult to express ambiguous interest which user often want to do we propose a technique that us user behavior monitonng to transparently capture the user sinterest in information andatechnique to use this interest to fikerincoming information in avery efficient way the proposed technique are verified to perform very well by having conducted a field experiment and a series of simulation 
a distinct advantage of symbolic learningalgorithms over artificial neural network isthat typically the concept representationsthey form are more easily understood by human one approach to understanding therepresentations formed by neural network isto extract symbolic rule from trained network in this paper we describe and investigatean approach for extracting rule fromnetworks that us the nofm extractionalgorithm and the network trainingmethod of soft 
researcher in distributed tificial intelligence have suggested that it would be worthwhile to isolate aspect of cooperative behavior general rule that cause agent to act in way conducive to cooperation one kind of cooperative behavior is when agent independently alter the environment to make it easier for everyone to function effectively cooperative behavior of this kind might be to put away a hammer that one find lying on the floor knowing that another agent will be able to find it more easily later on we examine the effect a specific cooperation rule ha on agent in the multi agent tileworld domain agent are encouraged to increase tile degree of freedom even when the tile is not involved in an agent s own primary plan the amount of extra work an agent is willing to do is captured in the agent s cooperation level result from simulation are presented we present a way of characterizing domain a multi agent deterministic finite automaton and characterizing cooperative rule a transformation of these automaton we also discus general characteristic of cooperative state changing rule it is shown that a relatively simple easily calculated rule can sometimes improve global system performance in the tileworld coordination emerges from agent who use this rule of cooperation without any explicit coordination or negotiation 
in the design of system of multiple agent we must deal with the potential for conflict that is inherent in the interaction among agent to ensure efficient operation these interaction must be coordinated we extend in two related way an existing framework that allows behavioral convention to emerge in agent society we first consider localizing agent thus limiting their interaction we then consider giving some agent authority over others by implementing asymmetric interaction our primary interest is to explore how locality and authority affect the emergence of convention through computer simulation of agent society of various configuration we begin to develop an intuition about what feature of a society promote or inhibit the spontaneous generation of coordinating convention 
the recognition of object from their projected two dimensional shape is a challengingproblem owing to the spectrum of possible variation reflected in the image domain e g those causedby movement of part change in viewing geometry occlusion etc this motivates a need for quantitativeas well a qualitative description of shape in term of structural relation between component the latter remain largely invariant under the above change in this paper we confront the theoretical 
pose clustering is a method of object recognition that determines the transformation that align hypothesized match of group of image and model feature an object that appears in an image corresponds to a large cluster of transformation in pose space close to the correct pose of the object if there are m model feature and n image feature then there are o m n transformation to consider for the case of recognition of three dimensional object from feature point in two dimensional image i show that clustering can be equally accurate when examining only o mn match due to correlation between the transformation if we are given two correct match this property is used with randomization to decompose the pose clustering problem into o n problem each of which cluster o mn match for a total complexity of o mn besides reducing the time necessary to perform pose clustering this method requires much le memory and make the use of accurate clustering algorithm le costly further time reduction can be gained by using grouping or indexing to determine the initial match 
digital video is rapidly becoming an important source for information entertainment and a host of multimedia application with the size of these collection growing to thousand of hour technology is needed to effectively browse segment in a short time without losing the content of the video we propose a method to extract the significant audio and video information and create a skim video which represents a short synopsis of the original the extraction of significant information such a specific object audio keywords and relevant video structure is made possible through the integration of technique in image and language understanding the resulting skim is much smaller and retains the essential content of the original segment 
abstract in this paper it is shown how false operator response due to missing or uncertain data can be signiflcantly reduced or eliminated perhaps the most well knownofsuchefiectsarethevarious edgeefiects which invariably occur at the edge of the input data set further itisshownhowoperatorshavingahigher degreeofselectivityandhighertoleranceagainstnoise can be constructed using simple combination of appropriately chosen convolution the theory is based on linear operation and is general in that it allows for both data and operator to be scalar vector or tensor of higher order threenewmethodsarepresented normalized convolution difierential convolutionand normalized differential convolution all three method are example of the power of the signal certainty philosophy i e the separation of both data and operator into a signal part and a certainty part missing data is simply handled by setting the certainty to zero in the case of uncertain data an estimate of the certainty must accompany the data localization or windowing of operator is done using an applicability function the operator equivalent to certainty not by changing the actual operator coe cients spatially or temporally limited operator are handled by setting the applicability function to zero outside the window consistentwiththephilosophyofthispaperallalgorithms produce a certainty estimate to be used if fur 
we investigate two realization of parallel abductive reasoning system using the model generation theorem prover mgtp the first one called the mgtp mgtp method is a co operative problem solving architecture in which model generation and consistency checking communicate with each other there parallelism is exploited by checking consistency in parallel however since this system consists of two different component the possibility for parallelization are limited in contrast the other method called the skip method doe not separate the inference engine from consistency checking but realizes both function in only one mgtp that is used a a generate and test mechanism in this method multiple model can be kept in distributed memory thus a great amount of parallelism can be obtained we also attempt the upside down meta interpretation approach for abduction in which top down reasoning is simulated by a bottom up reasoner 
classical planner have traditionally made the closed world assumption fact absent from the planner s world model are false incompleteinformation planner make the open world assumption the truth value of a fact absent from the planner s model is unknown and must be sensed the open world assumption lead to two difficulty h ow can the planner determine the scope of a universally quantified goal when is a sensory action redundant yielding information already known to the planner this paper describes the fully implemented xii planner which solves both problem by representing and reasoning about zocal closed world information lcw we report on experiment utilizing our unix softbot software robot which demonstrate that lcw can substantially improve the softbot s performance by eliminating redundant information gathering 
we introduce curbing a new nonmonotonic technique of commonsense reasoning that is based on model minimality but unlike circumscription treat disjunction inclusively a finitely axiomatized first order theory t is transformed to a formula curb t whose set of model is defined a the smallest collection of model which contains all minimal model of t and which is closed under formation of minimal upper bound with respect to inclusion we first give an intuitive definition of curb in third order logic and then show how curb can be equivalently expressed in second order logic we study the complexity of inferencing from a curbed propositional theory and present a pspace algorithm for this problem finally we address different possibility to approximate the curb of a theory 
visualizing and structuring pairwise dissimilarity data a re difficult combinatorial optimization problem known a multidimensional scaling or pairwise data clustering algorithm for embedding dissimilarity data set in a euclidian space for clustering these data and for actively selecting data to support the clu stering process are discussed in the maximum entropy framework active data selection provides a strategy to discover structure in a data set efficiently with partially unknown data grouping experimental data into compact cluster arises a a data analysis problem in psychology linguistics genetics and other experimental sci ences the data which are supposed to be clustered are either given by an explicit coordinate re presentation central clustering or in the non metric case they are characterized by dissimilarity value for pa irs of data point pairwise clustering in this paper we study algorithm i for embedding non metric data in a dimensional euclidian space ii for simultaneous clustering and embedding of non metric data and iii for active data selection to determine a particular cluste r structure with minimal number of data query all algorithm are derived from the maximum entropy principle hertz et al which guarantee robust statistic tikochinsky et al the data are given by a real valued symmetric proximity matrix ir being the pairwise dissimilarity between the data point apart from the symmetry constraint we make no further assumption about the dissimilarity i e we do not require being a metric the number quite often violate the triangular inequality and the dissi milarity of a datum to itself could be finite 
i describe an exploration criterion that attempt to minimize the error of a learner by minimizing it estimated squared bias i describe experiment with locally weighted regression on two simple kinematics problem and observe that this bias only approach outperforms the more common variance only exploration approach even in the presence of noise 
we analyze the amount of information needed to carry out various model based recognition task in the context of a probabilistic data collection model we focus on object that may be described a semi algebraic subset of a euclidean space and on a wide class of object transformation including perspective and affine transformation of d object and perspective projection of d object our approach borrows from computational learning theory we draw close relation between recognition task and a certain learnability framework we then apply basic technique of learnability theory to derive upper bound on the number of data feature that provably suffice for drawing reliable conclusion the bound are based on a quantitative analysis of the complexity of the hypothesis class that one ha to choose from our central tool is the vc dimension which is a well studied parameter measuring the combinatorial complexity of family of set it turn out that these bound grow linearly with the task complexity measured via the vc dimension of the class of object one deal with 
in order to recover camera motion and d structurefrom a sequence of image we must first relate pointsin the image plane to direction in space this paperdescribes a least square algorithm for computingcamera calibration from a series of motion sequencesfor which the translational direction of the camera isknown the method doe not require special calibrationobjects or scene structure it only requires theability to move the camera in a given direction andto track feature in the 
we present a framework for d surface reconstruction that can be used to model fully dimensional scene from an arbitrary number of stereo view taken from vastly different viewpoint this is a key step toward producing d world description of complex scene using stereo and is a very challenging problem real world scene tend to contain many d object they do not usually conform to the d assumption made by traditional algorithm and one cannot take it for granted that the computed d point can easily be clustered into separate group by combining a particle based representation robust fitting and optimization of an image based objective function we have been able to reconstruct surface without any a priori knowledge of their topology and in spite of the noisiness of the stereo data our current implementation go through three step initializing a set of particle from the input d data optimizing their location and finally grouping them into global surface using several complex scene containing multiple object we demonstrate it competence and ability to merge information and thus to go beyond what can be done with conventional stereo alone 
this paper address a model of analogy driven theorem proving that is more general and cognitively more adequate than previous approach the model work at the level of proof plan more precisely we consider analogy a a control strategy in proof planning that employ a source proof plan to guide the construction of a proof plan for the target problem our approach includes a reformulation of the source proof plan this is in accordance with the well known fact that constructing an analogy in math often amount to first finding the appropriate representation which brings out the similarity of two problem i e finding the right concept and the right level of abstraction several well known theorem were processed by our analogy driven proof plan construction that could not be proven analogically by previous approach 
we describe the least cost flaw repair lcfr strategyfor performing flaw selection during partial ordercausal link pocl planning lcfr can be seenas a generalization of peot and smith s quot delay unforcedthreats quot dunf strategy peot amp smith where dunf treat threat differently from open condition lcfr ha a uniform mechanism for handling allflaws we provide experimental result that demonstratethat the power of dunf doe not come from delayingthreat repair per se but 
we investigate the use of oblivious read once decision graphsas structure for representing concept over discrete domain and presenta bottom up hill climbing algorithm for inferring these structure fromlabelled instance the algorithm is robust with respect to irrelevant attribute and experimental result show that it performs well on problemsconsidered difficult for symbolic induction method such a the monk sproblems and parity introductiontop down induction of 
this report discus the well known problem of structure from motionfor the special case of rigid curve it is already known that it is theoretically possibleto recover the motion and then the structure of a moving d rigid curve observed cina monocular image sequence a soon a some set of derivative that are defined onthe so called spatio temporal surface can be computed this is true under the mostgeneral camera model of perspective projection we give here a new simplificationof 
the theory of concept or galois lattice provides a natural and formal setting in which to discover and represent concept hierarchy in this paper we present a system galois which is able to determine the concept lattice corresponding to a given set of object galois is incremental and relatively efficient the time complexity of each update ranging from o n to o n where n is the number of concept in the lattice unlike most approach to conceptual clustering galois represents and update all possible class in a restricted concept space therefore the concept hierarchy it find are always justified and are not sensitive to object ordering we experimentally demonstrate using several machine learning data set that galois can be successfully used for class discovery and class prediction we also point out application of galois in field related to machine learning i e information retrieval and database 
we concentrate on nonrigid motion analysis in d imagesusing modal dynamic borrowing the solid state physic formulation we develop the equation of motion and the analytical expression of vibrationmodes of a multidimensional elastically deformable model thus nonrigid motion of a d deformable object can be recovered in closedformin real time the power of the approach is demonstrated by a setof experimental result on d medical data backgroundfollowing the theory of 
chapman s paper planning for conjunctive goal ha been widely acknowledged for it contribution toward understanding the nature of nonlinear partial order plann ing and it ha been one of the base of later work by others but it is not free of problem this paper address some problem involving modal truth and the modal truth criterion mtc our result are a follows even though modal duality is a fundamental axiom of classical modal logic it doe not hold for modal truth in chapman s plan i e necessarily p is not equivalent to not possibly p although the mtc for necessary truth is correct the mtc for possible truth is incorrect it provides necessary but insufficient condition for ensuring possible truth furthermore even though necessary truth can be determined in polynomial time possible truth is np hard if we rewrite the mtc to talk about modal conditional truth i e modal truth conditional on executability rat her than modal truth then both the mtc for necessary conditional truth and the mtc for possible conditional truth are correct and both can be computed in polynomial time 
we analyze the amount of information needed tocarry out model based recognition task in thecontext of a probabilistic data collection model and independently of the recognition methodemployed we consider the very rich class ofsemi algebraic d object and derive an upperbound on the number of data feature that provably suffice for localizing the object with somepre specified precision our bound is based onanalysing the combinatorial complexity of the hypothesesclass that one 
the parietal cortex is thought to represent the egocentric positionsof object in particular coordinate system we propose analternative approach to spatial perception of object in the parietalcortex from the perspective of sensorimotor transformation the response of single parietal neuron can be modeled a a gaussianfunction of retinal position multiplied by a sigmoid functionof eye position which form a set of basis function we show herehow these basis function can be used to 
condition satisfaction in planning ha received a great deal of experimental and formal attention a truth criterion lie at the heart of many planner and is critical to their capability and performance however there ha been little study of way in which the search space of a planner incorporating such a truth criterion can be guided the aim of this document is to give a description of the use of condition type information to inform the search of an ai planner and to guide the production of answer by a planner s truth criterion algorithm the author aim to promote discussion on the merit or otherwise of using such domain dependent condition type restriction a a mean to communicate valuable information from the domain writer to a general purpose domain independent planner 
many induction program use decision treesboth a the basis for their search and a arepresentation of their classifier solution inthis paper we propose a new structure calledse tree a a more general alternative introductionmany learning algorithm use decision tree a an underlyingframework for search and a a representationof their classifier solution e g id quinlan cart breiman et al this framework however is known to mix search bias introduced when 
cale at which navigationoccurs ffl navigation is much simpler than arbitrarygraph search because street are topologically sensible it is impossible to drive along a street and suddenly endup on the other side of town cul de sac are relativelyrare so hill climbing tends to work one way streetsnever completely isolate region ffl navigation occursin a topographically translucent environment some informationis available by virtue of the d nature ofthe environment although not 
the paper is concerned with the design of a module system for logic programming so a to satisfy many of the requirement of software engineering the design is based on the language godel which is a logic programming language which already ha a simple type and module system the module system described here extends the godel module system so a to include parameterised module in particular this extended system allows general purpose predicate that depend on fact and rule for specific application to be defined in module that are independent of their application 
interpreting spectral image requires comparing known pattern with input data image to identify which pattern are contained in the input data in practice however it is hard to identify any pattern when the inaccuracy of input data is not slight in this paper we present a method for interpreting spectral image by using qualitative reasoning first we put forward a new concept called support coefficient function scf which can be used to extract represent and calculate qualitative correlation among data then we introduce an approach to determining dynamic shift interval of inaccurate data on the basis of qualitative correlation finally we discus how to use qualitative correlation a evidence of enhancing or depressing hypothesis for in accurate data the method ha been applied to a practical system for interpreting infrared spectral image we have fully tested the system against several hundred real spectral image the rate of identification ri and the rate of correctness rc are near and respectively and the latter is the highest among known system 
this paper present a new method for extracting the d shape and texture of an object undergoing translational motion from image sequence captured through a monocular extra wide picture viewing angle the feature of this work is extracting this information from image sequence without requiring rigid environmental condition in this method the relative position between target and view position are estimated based on spatio temporal image analysis and shape is reconstructed from the multiple silhouette information after reconstructing the d shape the voxel value of a surface point is determined by statistically analyzing those image that contain the surface point the proposed method can extract d shape and surface texture at a stroke from outdoor scene an experiment using real outdoor scene confirms the effectiveness of the method 
we describe a computer vision system for observing the action unit of a face using video sequence a input the visual observation sensing is achieved by using an optimal estimation optical flow method coupled with a geometric and a physical muscle model describing the facial structure this modeling result in a time varying spatial patterning of facial shape and a parametric representation of the independent muscle action group responsible for the observed facial motion these muscle action pattern may then be used for analysis interpretation and synthesi s thus by interpreting facial motion within a physic base d optimal estimation framework a new control model of facial movement is developed the newly extracted action unit which we name facs are both physic and geometry based and extend the well known facs parameter for facial expression by adding temporal informatio n and non local spatial patterning of facial motion 
learning can often be viewed a the problem of mapping from an input space to an output space example of these mapping are used to construct a continuous function that approximates given data and generalizes for intermediate instance generalized radial basis function grbf network are used to formulate this approximating function a novel method is introduced to construct an optimal grbf network for a given mapping and error bound using the integral wavelet transform simple one dimensional example are used to demonstrate how the optimal network is superior to one constructed using standard ad hoc optimization technique the paper concludes with an application of optimal grbf network to object recognition and pose estimation the result of this application are favorable 
in the single rent to buy decision problem without a priori knowledge of the amount oftime a resource will be used we need to decidewhen to buy the resource given that wecan rent the resource for per unit time orbuy it once and for all for c in this paper westudy algorithm that make a sequence of singlerent to buy decision using the assumptionthat the resource use time are independentlydrawn from an unknown probabilitydistribution our study of this rent to buyproblem is 
using low order global motion hypothesis and the assumption that there are no more than two motion at a single point it is possible to successfully decompose motion stimulus that contain additively combined transparent layer it is assumed that the space of flow field is sufficiently smooth that a relatively coarse sampling of the flow parameter s will produce a set of vector field that can be combined to reasonably approximate the actual motion in the scene the definition of support from an exclusively spatial notion is extended to include the spatio temporal energy domain the key insight is that when processing transparent motion display the support of a motion hypothesis should exist over both a region of space and velocity so that it can be isolated both spatially and in term of local velocity 
reasoning about time often involves incomplete information about period and their relationship variety of incompleteness include uncertainty about the number of object involved the distribution of a set of temporal relation among these object and what can be called the participation of a set of object in a temporal relation a solution to the problem of representing and reasoning about incomplete temporal information of these kind is forthcoming if a restricted class of non convex interval called n tntervals is added to the temporal domain of discourse an n interval corresponds to the common sense notion of a recurring period of time with a possibly unspecified number of occurrence in this paper we formalize a representation for temporal reasoning problem using n interval the language of the framework is restricted in such a way that tractable technique from constraint satisfaction can be applied specifically it is demonstrated how the problem of determining path consistency in a network of binary n interval relation can be solved 
four version of a k nearest neighbor algorithm with locally adaptive k are introduced and compared to the basic k nearest neighboralgorithm knn locally adaptive knn algorithm choose thevalue of k that should be used to classify a query by consulting theresults of cross validation computation in the local neighborhoodof the query local knn method are shown to perform similar toknn in experiment with twelve commonly used data set encouragingresults in three constructed task 
this paper describes a flexible framework and an efficient algorithm for constraint based spatio temporal configuration problem binary constraint between spatio temporal object are first converted to constraint region which are then decomposed into hierarchical data structure based on this constraint decomposition an improved backtracking algorithm called hbt can compute a solution quite efficiently in contrast to other approach the proposed method is characterized by the efficient handling of arbitrarily shaped object and the flexible integration of quantitative and qualitative constraint it allows a wide range of object and constraint to be utilized for specifying a spatio temporal configuration the method is intended primarily for configuration problem in user interface but can effectively be applied to similar problem in other area a well 
this paper describes a new indexing method for japanese text database using the simple keyword string in which a compound word is treated a a string of simple word which are the smallest unit in japanese grammar which still maintain their meaning this method allows retrieved text to be ranked according to the similarity of their meaning to the query without using a control vocabulary or thesaurus this paper also introduces the keyword feature which describes the syntactic and semantic characteristic of a word and result in more precise keyword extraction and text retrieval a well a simple dictionary maintenance 
we study a bimodal nonmonotonic logic mbnf suggested in lifschitz a a generalization of a number of nonmonotonic formalism we show first that it is equivalent to a certain nonmodal system involving rule of a special kind next it is shown that the latter admits a modal representation that us only one modal opera tor the operator of belief moreover under this translation the model of mbnf correspond to expansion of the associated modal nonmonotonic logic finally we show that a far a such model are concerned mbnf is redunhle to nonmodal default consequence relation from bochman these result have general consequence concerning relationship between different formalization of nonmonotonic reasoning 
a distributed neural network model called spec for processing sentence with recursive relative clause is described the model is based on separating the task of segmenting the input word sequence into clause forming the case role representation and keeping track of the recursive embeddings into different module the system need to be trained only with the basic sentence construct and it generalizes not only to new instance of familiar relative clause structure but to novel structure a well spec exhibit plausible memory degradation a the depth of the center embeddings increase it memory is primed by earlier constituent and it performance is aided by semantic constraint between the constituent the ability to process structure is largely due to a central executive network that monitor and control the execution of the entire system this way in contrast to earlier subsymbolic system parsing is modeled a a controlled high level process rather than one based on automatic reflex response 
pruning a decision tree is considered by some researcher to be the most important part of tree building in noisy domain while there are many approach to pruning an alternative approach of averaging over decision tree ha not received a much attention we perform an empirical comparison of pruning with the approach of averaging over decision tree for this comparison we use a computationally efficient method of averaging namely averaging over the extended fanned set of a tree since 
vision should provide an explanation of the scenein term of a causal semantics what affect what andwhy an important part of the causal explanation ofstatic scene is what support what or counterfactually why aren t thing moving we use simple naivephysical knowledge a the basis of a vertically integratedvision system that explains arbitrarily complexstacked block structure the semantics provides a basis for controlling theapplication of visual attention and form a 
vital information for corporate activity is generally stored in large database while conventional data base management system offer limited query flexibility system capable of generating similarity based query such a those seen in case based reasoning research would certainly enhance the utility of data resource this paper describes a method for building case based system using a conventional relational data base rdb the core of the algorithm is a novel approach to similarity computing in which database query form similarity rather than similarity of individual case are computed the method us standard query language sql to achieve nearest neighbor matching thus allowing similarity based database retrieval it ha been implemented a a part of the caret case retrieval tool and evaluated through the use of a newly developed corporate wide case based system for a software quality control domain experiment have shown the proposed method to provide retrieval result equivalent to those of non rdb implementation at a sufficiently fast response time 
this paper examines several system which learn a large number of rule production including one which learns rule the largest number ever learned by an ai system and the largest number in any production system in existence it is important to match these rule dficiently in order to avoid the machine learning utility problem moreover examination of such large system reveals new phenomenon and call into question some common assumption based on previous observation of smalkr system we first show that the rete and treat match algorithm do not scale well with the number of rule in our system in part because the number of rule affected by a change to working memory increase with the total number of rule in these system we also show that the sharing of node in the beta part of the rete network becomes more and more important a the number of rule increase finally we describe and evaluate a new optimization for rete which improves it scalability and allows two of our system to learn over rule without significant performance degradation 
a the first step in an automated text summarization algorithm this work present a new method for automatically identifying the central idea in a text based on a knowledge based concept counting paradigm to represent and generalize concept we use the hierarchical concept taxonomy wordnet by setting appropriate cutoff value for such parameter a concept generality and child to parent frequency ratio we control the amount and level of generality of concept extracted from the text 
decision made in setting up and running search program bias the search that they perform search bias refers to the definition of a search space and the definition of the program that navigates the space this paper address the problem of using knowledge regarding the complexity of various syntactic search bias to form a policy for selecting bias in particular this paper show that a simple policy iterative weakening is optimal or nearly optimal in case where the bias can be ordered by computational complexity and certain relationship hold between the complexity of the various bias the result are obtained by viewing bias selection a a higher level search problem iterative weakening evaluates the state in order of increasing complexity an offshoot of this work is the formation of a near optimal policy for selecting both breadth and depth bound for depth first search with very large possibly unbounded breadth and depth 
ai need many idea that have hitherto been studied only by philosopher this is because a robot if it is to have human level intelligence and ability to learn from it experience need a general world view in which to organize fact it turn out that many philosophical problem take new form when thought about in term of how to design a robot some approach to philosophy are helpful and others are not 
based on flat ground assumption a correhtion method is veloprd to estimate orientation of vehicle relative to it run ning environment a weight finclion u introduced to account for figure varirrtion caused by d dynamic scene and perspective efect caused by the camera system the estimated parameter can be qualitatively wed in visual navigation promising result indicate that robust esthation can be achieved in real time by wing powerfil image processing system pipe pipelined image processing engine 
we propose a natural model of abduction based on the revision of the epistemic state of an agent we require that explanation be sufficient to induce belief in an observation in a manner that adequately account for factual and hypothetical observation our model will generate explanation that nonmonotonically predict an observation thus generalizing most current account which require some deductive relationship between explanation and observation it also provides a natural preference ordering on explanation defined in term of normality or plausibility we reconstruct the theorist system in our framework and show how it can be extended to accommodate our predictive explanation and semantic preference on explanation 
knowledge discovery system for database are employed to provide valuable insight into characteristic and relationshi that may exist in the data but are unknown to the user s paper describes a methodology and system for perfonrung knowledge discovery across multiple database these en hancements have been integrated into the prototype knowledge discovery system called inlen the enhancement include the incorporation of primary and foreign key a well a the development and processing of knowledge segment 
rotationally symmetric operation in the image domain may give rise to shape distortion this article describes a way of reducing this effect for a general class of method for deriving d shape cue from d image data which are based on the estimation of locally linearized distortion of brightness pattern by extending the linear scale space concept into an affine scale space representation and performing affine shape adaption of the smoothing kernel the accuracy of surface 
a neural network model for the self organization of ocular dominance and lateral connection from binocular input is presented the self organizing process result in a network where afferent weight of each neuron organize into smooth hill shaped receptive field primarily on one of the retina neuron with common eye preference form connected intertwined patch and lateral connection primarily link region of the same eye preference similar self organization of cortical structure ha been observed experimentally in strabismic kitten the model show how patterned lateral connection in the cortex may develop based on correlated activity and explains why lateral connection pattern follow receptive field property such a ocular dominance 
compositionality at aaai elkan ha claimed to have a result trivializing fuzzy logic this trivialization is based on too strong a view of equivalence in fuzzy logic and relates to a fully compositional treatment of uncertainty such a treatment is shown to be impossible in this paper we emphasize the distinction between i degree of partial truth which are allowed to be truth functional and which pertain to gradual or fuzzy proposition and ii degree of uncertainty which cannot be compositional with respect to all the connective when attached to classical proposition this distinction is exemplified by the difference between fuzzy logic and possibilistic logic we also investigate an almost compositional uncertainty calculus but it is shown to lack expressiveness 
one of the most important problem in rule induction method is how to estimate which method is the best to use in an applied domain while some method are useful in some domain they ate not useful in other domain therefore it is very dificult to choose one of these method fot this purpose we introduce multiple testing based on recursive iteration of resampling method for rule induction mult recite r this method consists of four procedure which includes the inner loop and the outer loop procedure first orkginal training sample are randomly split into new training sample and teat sample t using a tesampiing scheme second are again spiii inio training sample and training sample li using the same resampling scheme rule induction method ave applied and predefined metric ate calculated this second procedure a the inner loop is repeated for time then third rule induction method are applied to and the met s calculated by tl are cornpaved with those by tz if the metric derived by tz predicts those by tl then we count it a a success the second and third procedure a the outet loop are iterated fot time finally fourth the overall result are interpreted and the best method is selected if the resampling scheme performs well in otdet to evaluate this system we apply this mult reciter method to three uci database the result show that this method give the best selection of estimation method statistically 
the structure from motion problem ha been extensively studied in the field of computer vision yet the bulk of the existing work assumes that the scene contains only a single moving object the more realistic case where an unknown number of object move in the scene ha received little attention especially for it theoretical treatment we present a new method for separating and recovering the motion and shape of multiple independently moving object in a sequence of image the method doe not require prior knowledge of the number of object nor is dependent on any grouping of feature into an object at the image level for this purpose we introduce a mathematical construct of object shape called the shape interaction matrix which is invariant to both the object motion and the selection of coordinate system this invariant structure is computable solely from the observed trajectory of image feature without grouping them into individual object once the structure is computed it allows for segmenting feature into object by the process of transforming it into a canonical form a well a recovering the shape and motion of each object 
this paper make the following two contribution toformal theory of action showing that a causal minimizationframework can be used effectively to specifythe effect of indeterminate action and showing thatfor certain class of such action regression an effectivecomputational mechanism can be used to reasonabout them introductionmuch recent work on theory of action ha concentratedon primitive determinate action in this paper we pose ourselves the problem of 
we describe a grammarless method for simultaneously bracketing both half of a parallel text and giving word alignment assuming only a translation lexicon for the language pair we introduce inversion invariant transduction grammar which serve a generative model for parallel bilingual sentence with weak order constraint focusing on transduction grammar for bracketing we formulate a normal form and a stochastic version amenable to a maximum likelihood bracketing algorithm several extension and experiment are discussed 
when a case based planner is retrieving a previous case in preparation for solving a new similar problem it is often not aware of all of the implicit feature of the new problem situation which determine if a particular case may be successfully applied this mean that some case may fail to improve the planner s performance by detecting and explaining these case failure a they occur retrieval may be improved incrementally in this paper we provide a definition of case failure for the case based planner derivation replay in which solves new problem by replaying it previous plan derivation we provide explanationbased learning ebl technique for detecting and constructing the reason for the case failure we also describe how the case library is organized so a to incorporate this failure information a it is produced finally we present an empirical study which demonstrates the effectiveness of this approach in improving the performance of 
knowledge about the imaging geometry and acquisition parameter provides useful geometric constraint for the analysis and extraction of man made feature in aerial imagery particularly in oblique view in this paper we discus the identification of horizontal and vertical line in the scene using image orientation information and vanishing point calculation and the calculation of their dimension the vertical and horizontal attribution are used to constrain the set of possible building hypothesis vertical line are extracted at corner to estimate structure height and permit the generation of three dimensional building model from monocular view result of these technique are presented for nadir and oblique imagery and evaluated against manually generated d ground truth building model 
we present a very simple selective search algorithm for two player game it always expands next the frontier node that determines the minimax value of the root the algorithm requires no information other than a static evaluation function and it time overhead per node is similar to that of alpha beta minimax we also present an implementation of the algorithm that reduces it space complexity from exponential to linear in the search depth at the cost of increased time complexity in the game of othello using the evaluation function from biil lee mahajan bestfirst minimax outplays alpha beta at moderate depth a hybrid best first extension algorithm which combine alpha beta and best first minimax performs significantly better than either pure algorithm even at greater depth similar result were also obtained for a class of random game tree 
in this paper we investigate algorithm for evaluating surface orientation from pair of stereo image using limited calibration information and without reconstructing an explicit metric representation of the observed scene 
this paper study the problem of diffusion in markovian model such a hidden markov model hmms and how it make verydifficult the task of learning of long term dependency in sequence using result from markov chain theory we show that the problemof diffusion is reduced if the transition probability approach or under this condition standard hmms have very limited modelingcapabilities but input output hmms can still perform interestingcomputations introduction 
the paper show how technique from computational vision can be deployed to support interactive sketch editing while conventional computer supported drawing tool give user access to visible mark or image object at a single level of abstraction a human user s visual system rapidly construct complex grouping and association among image element according to his or her immediate purpose we have been exploring perceptually supported sketch editor in which computer vision algorithm run continuously behind the scene to afford user efficient access to emergent visual object in a drawing we employ a flexible image interpretation architecture based on token grouping in a multiscale blackboard data structure this organization support multiple perceptual interpretation of line drawing data domain specific knowledge base for interpreting visual structure and natural gesture based selection of visual object 
autonomous mobile robot need very reliable navigation capability in order to operate unattended for long period of time this paper report on first result of a research program that us par tially observable markov model to robustly track a robot location in office environment and to direct it goal oriented action the approach explicitly maintains a probability distribution over the possible location of the robot taking into account various source of uncertainly including approximate knowledge of the environment and actuator and sensor uncertainty a novel feature of our approach is it integration of topological map information with approximate metric information we demonstrate the robustness of this approach in controlling an actual indoor mobile robot navigating corridor 
in previous work we presented an algorithm for tense interpretation which employ a temporal focus to determine the intended temporal relation between the state and event mentioned in a narrative in this paper we propose a new two phased classification scheme for aspect each situation described in an utterance is first classified a static state or dynamic event and if dynamic a telic event with a culmination point or atelic event without a culmination point then independent of the class the view of the situation is identified either a a point or a an interval we then demonstrate how the determination of aspect can be integrated into our tense interpretation algorithm to produce a richer analysis of temporal relation our classification for aspect is more detailed than most of the existing scheme allowing u to extract the interval relation between situation and cover a wide range of english narrative 
this contribution investigates local differential technique for estimating optical flow and it derivative based on the brightness change constraint by using the tensor calculus representation we build the taylor expansion of the gray value derivative a well a of the optical flow in a spatiotemporal neighborhood such a formulation simplifies a unifying framework for all existing local differential approach and allows to derive new system of equation to estimate the optical flow and it derivative we also tested various optical flow estimation approach on real image sequence recorded by a calibrated camera fixed on the arm of a robot by moving the arm of the robot along a precisely defined trajectory we can determine the true displacement rate of scene surface element projected into the image plane and compare it quantitatively with the result of different optical flow estimator 
present a vision system for autonomous navigation based on stereo perception without d reconstruction this approach us weakly calibrated stereo image i e image for which only the epipolar geometry is known the vision system first rectifies the image match selected point between the two image and then computes the relative elevation of the point relative to a reference plane a well a the image of their projection on this plane we have integrated this vision module into a complete navigation system in this system the relative elevation is used a a shape indicator in order to compute appropriate steering direction everytime a new stereo pair is processed we have conducted initial experiment in unstructured outdoor environment with an wheeled rover 
an approach to reduce number of spurious symptom in aircraft engine fault monitoring is investigated two strategy were utilized a set of rule designed to filter spurious symptom wa created then a neural network wa designed to generate expectation value for each of the sensor monitored the neural net wa trained for a specific engine during normal operation after capturing pattern for normal engine behavior in the neural net an expectation value for the sensor is predicted the success of this approach relies on generating better expectation value which in turn produce smaller variation from actual operating behavior and hence generate fewer spurious symptom resulting hybrid system of neural network and rule based model demonstrates a drastic reduction of overall spurious symptom 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
this paper describes an integrated method for processing grammatically ill formed input we use partial par of the input for recovering from parsing failure in order to select partial par appropriate for error recovery cost and reward are assigned to them cost and reward represent the badness and goodness of a partial parse respectively the most appropriate partial parse is selected on the basis of cost and reward trade off the system contains three module module a handle local ill formedness such a constraint violation module b handle non local ill formedness such a word order violation and module c handle non local ill formedness such a contextual ellipsis these three module work in a uniform framework based on the notion of cost and reward 
a key issue in managing today s large amount of genetic data is the availability of efficient accurate and selective technique for detecting homology similarity between newly discovered and already stored sequence a common characteristic of today s most advanced algorithm such a fasta blast and blaze is the need to scan the content of the entire database in order to find one or more match this design decision result in either excessively long search time or a is the case of blast in a sharp trade off between the achieved accuracy and the required amount of computation the homology detection algorithm presented in this paper on the other hand is based on a probabilistic indexing framework the algorithm requires minimal access to the database in order to determine match this minimal requirement is achieved by using the sequence of interest to generate a highly redundant number of very descriptive tuples these tuples are subsequently used a index in a table look up paradigm in addition to the description of the algorithm theoretical and experimental result on the sensitivity and accuracy of the suggested approach are provided the storage and computational requirement are described and the probability of correct match and false alarm is derived sensitivity and accuracy are shown to be close to those of dynamic programming technique a prototype system ha been implemented using the described idea it contains the full swiss prot database rel mr and the genome of e coli mr the system is currently being expanded to include the complete genbank database abstract truncated at word 
stratified case based reasoning is a technique in which abstract solution produced during hierarchical problem solving are used to assist case based retrieval matching and adaptation we describe the motivation for the integration of case based reasoning with hierarchical problem solving exemplify it benefit detail a set of algorithm that implement our approach and present their comparative empirical evaluation on a path planning task our result show that stratified case based reasoning significantly decrease the computational expense required to retrieve match and adapt case leading to performance superior both to simple case based reasoning and to hierarchical problem solving ab initio 
proposes a new approach for vision based longitudinal and lateral vehicle control the novel feature of this approach is the use of binocular vision we integrate two module consisting of a new domain specific efficient binocular stereo algorithm and a lane marker detection algorithm and show that the integration result in a improved performance for each of the module longitudinal control is supported by detecting and measuring the distance to leading vehicle using binocular stereo the knowledge of the camera geometry with respect to the locally planar road is used to map the image of the road plane in the two camera view into alignment this allows u to separate image feature into those lying in the road plane e g lane marker and those due to other object which are dynamically integrated into an obstacle map therefore in contrast with the previous work we can cope with the difficulty arising from occlusion of lane marker by other vehicle the detection and measurement of the lane marker provides u with the positional parameter and the road curvature which are needed for lateral vehicle control moreover this information is also used to update the camera geometry with respect to the road therefore allowing u to cope with the problem of vibration and road inclination to obtain consistent result from binocular stereo 
contract algorithm offer a tradeoff between output quality and computation time provided that the amount of computation time is determined prior to their activation originally they were introduced a an intermediate step in the composition of interruptible anytime algorithm however for many real time task such a information gathering game playing and a large class of planning problem contract algorithm offer an ideal mechanism to optimize decision quality this paper extends previous result regarding the meta level control of contract algorithm by handling a more general type of performance description the output quality of each contract algorithm is described by a probabilistic rather than deterministic conditional performance profile such profile map input quality and computation time to a probability distribution of output quality the composition problem is solved by an efficient off line compilation technique that simplifies the run time monitoring task 
we previously presented sarkar and boyer the perceptual inference network pin a formalism based on bayesian network to reason among a set of object or feature hypothesis and to integrate multiple source of information in the context of perceptual organization the design of a pin requires knowledge of the dependency structure among the organization of interest and the specification of the conditional probability this design wa done manually with large dos of tedium and guesswork in this paper we present an algorithm based on structural entropic measure and random parametric structural description rpsds to design a pin automatically and in a more theoretically sound fashion experimental result present evidence of the robustness of the algorithm and make performance comparison on real image data with a manually structured pin since pin are a form of bayesian network we hope that this work will also prove useful towards structuring bayesian network in other computer vision context 
the dynamic of complex neural network modelling the self organization process in cortical map must include the aspect oflong and short term memory the behaviour of the network is suchcharacterized by an equation of neural activity a a fast phenomenonand an equation of synaptic modification a a slow part of theneural system we present a quadratic type lyapunov function forthe flow of a competitive neural system with fast and slow dynamicvariables we also show the consequence 
termination of logic program with negated body atom here called general logic program is an important topic this is also due to the fact that the computational mechanism used to process negated atom like clark s negation a failure and chan s constructive negation are based on termination condition this paper introduces a methodology for proving termination of general logic program when the prolog selection rule is considered this methodology is based on the notion of low up acceptable program we prove that low up acceptable program characterize a class of general logic program which terminate for a large class of query which contains the set of all ground query we consider a operational model sld resolution augmented with a procedure to deal with negative literal known a chan s constructive negation general logic program can be used to express concept and problem in non monotonic reasoning we show here that interesting problem in non monotonic reasoning can be formalized and implemented by mean of up low general logic program 
given a set of number the two way partitioning problem is to divide them into two subset so that the sum of the number in each subset are a nearly equal a possible the problem is np complete and is contained in many scheduling application based on a polynomial time heuristic due to karmarkar and karp we present a new algorithm called complete karmarkar karp ckk that optimally solves the general number partitioning problem ckk significantly outperforms the best previously known algorithm for this problem by restricting the number to twelve significant digit we can optimally solve two way partitioning problem of arbitrary size in practice ckk first return the karmarkar karp solution then continues to find better solution a time allows almost five order of magnitude improvement in solution quality is obtained within a minute of running time rather than building a single solution one element at a time ckk construct subsolutions and combine them in all possible way ckk is directly applicable to the knapsack problem since it can be reduced to number partitioning this general approach may also be applicable to other np hard problem a well 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
an algorithm for camera calibration is presented it is a significant improvement in mathematical simplicity accuracy and computational efficiency in the solution of all extrinsic external camera geometric and intrinsic internal camera geometric and camera optic parameter the method involves a direct transformation from the three dimensional d object world to the two dimensional d image or sensor plane in term of homogeneous vector form for both coplanar and noncoplanar distribution of object point a strong robust property of the proposed algorithm is demonstrated by proving that if the camera is calibrated with image data not compensated for image center displacement and scale factor the proposed algorithm yield parameter that cause no error in the computation of both image and world coordinate 
this paper deal with learning in reactive multi agent system the central problem addressed is how several agent can collectively learn to coordinate their action such that they solve a given environmental task together in approaching this problem two important constraint have to be taken into consideration the incompatibility constraint that is the fact that different action may be mutually exclusive and the local information constraint that is the fact that each agent typically know only a fraction of it environment the content of the paper is a follows first the topic of learning in multi agent system is motivated section then two algorithm called ace and age standing for action estimation and action group estimation respectively for the reinforcement learning of appropriate sequence of action set in multi agent system are described section next experimental result illustrating the learning ability of these algorithm are presented section finally the algorithm are discussed and an outlook on future research is provided section 
abstract diagrammatic reasoning is a type of reason ing in which the primary mean of inference is direct manipulation and inspection of a di agram diagrammatic reasoning is prevalent in human problem solving behavior especially for problem involving spatial relation among physical object our research examines the relationship between diagrammatic and sym bolic reasoning in a computational framework we have built a system called redraw that emulates the human capability for reasoning with picture in civil engineering the class of structural analysis problem chosen provides a realistic domain whose solution process re quire domain specific knowledge a well a pic torial reasoning skill we hypothesize that dia grammatic representation provide an environ ment where inference about the physical re sults of proposed structural configuration can take place in a more intuitive manner than that possible through purely symbolic representa tions 
we review accuracy estimation method and compare the two most common method crossvalidation and bootstrap recent experimental result on artificial data and theoretical re cult in restricted setting have shown that for selecting a good classifier from a set of classifier model selection ten fold cross validation may be better than the more expensive leaveone out cross validation we report on a largescale experiment over half a million run of c and a naive bayes algorithm to estimate the effect of different parameter on these algrithms on real world datasets for crossvalidation we vary the number of fold and whether the fold are stratified or not for bootstrap we vary the number of bootstrap sample our result indicate that for real word datasets similar to ours the best method to use for model selection is ten fold stratified cross validation even if computation power allows using more fold 
last summer aaai sponsored a mobile robot competition in conjunction with the aaai conference in san jose california ten robot from across the country competed in the competition with carmel from the university of michigan finishing first carmel is a cybermotion k a mobile platform with a ring of sonar sensor and a single black and white ccd camera for computing carmel ha three processor one for motor control one for sonar ring firing and one executing high level routine such a obstacle avoidance and object recognition all computation and power is contained entirely on board 
there have been a number of recent paper on aligning parallel text at the sentence level e g brown et al gale and church to appear isabelle kay and r senschein to appear simard et al warwick armstrong and russell on clean input such a the canadian hansard these method have been very successful at least correct by sentence unfortunately if the input is noisy due to ocr and or unknown markup convention then these method tend to break down because the noise can make it difficult to find paragraph boundary let alone sentence this paper describes a new program char align that aligns text at the character level rather than at the sentence paragraph level based on the cognate approach proposed by simard et al 
problem of liveness and fairness are considered in multi agent system by mean of abstract language different approach to define such property for the agent and for a multi agent system a a whole are discussed it turn out that the property of a multi agent system need not correspond to separately definable property of the agent e g a community of fair agent need not constitute a fair multi agent system in general analysis and verification need the consideration of the whole system and the agent have to be considered in the context of the system too the result are not unique there are different result for deadlock freedom liveness and fairness respectively 
example form an integral and very important part of many description especially in context such a tutoring and documentation generation the ability to tailor a description for a particular situation is particularly important when different situation can result in widely varying description this paper considers the generation of description with example for two different situation introductory text and advanced reference manual style text previous study have focused on any the example or the language component of the explanation in isolation however there is a strong interaction between the example and the accompanying description and it is therefore important to study how both these component are affected by change in the situation in this paper we characterize example in the context of their description along three orthogonal ax the information content the knowledge type of the example and the text type in which the explanation is being generated while variation along either of the three ax can result in different description this paper address variation along the text type axis we illustrate our discussion with a description of a list from our domain of lisp documentation and present a trace of the system a it generates these description 
current specialized planner for query processing are designed to work in local reliable and predictable environment however a number of problem arise in gathering information from large network of distributed information in this environment the same information may reside in multiple place action can be executed in parallel to exploit distributed resource new goal come into the system during execution action may fail due to problem with remote database or network and sensing may need to be interleaved with planning in order to formulate efficient query we have developed a planner called sage that address the issue that arise in this environment this system integrates previous work on planning execution replanning and sensing and extends this work to support simultaneous and interleaved planning and execution sage ha been applied to the problem of information gathering to provide a flexible and efficient system for integrating heterogeneous and distributed data 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
analytical result from ai planning research provide the motivation for this experimental study of ordering relationship in human planning we examine timing of human performing specific task from the ai planning literature and present evidence that normal human planner like state of the art ai planning system use partial order plan representation we also describe ongoing experiment that are designed to shed light on the plan representation used by child and by adult with planning deficit due to brain damage several point of interest for collaboration between ai scientist and neuropsychologists are noted a are impact that we feel this research may have on future work in ai planning 
a nonlinear clustering filter is derived using the maximum entropy principle this filter is governed by a single scale parameter and us local characteristic in the data to determine the scale parameter in the output space it provides a mechanism for removing impulsive noise preserving edge and improving smoothing of nonimpulsive noise it also present a scheme for nonlinear scale space filtering comparison with gaussian scale space filtering are made using real image it is demonstrated that the clustering filter give much better result 
this paper present a method by which areinforcement learning agent can solve theincomplete perception problem using memory the agent us a hidden markov model hmm to represent it internal state spaceand creates memory capacity by splittingstates of the hmm the key idea is a test todetermine when and how a state should besplit the agent only split a state when doingso will help the agent predict utility thusthe agent can create only a much memory asneeded to perform the task 
abstract this paper analyzes the complexity of on line reinforcement learning algorithm namely asynchronous realtime version of q learning and value iteration applied to the problem of reaching a goal state in deterministic domain previous work had concluded that in many case tabula rasa reinforcement learning wa exponential for such problem or wa tractable only if the learning algorithm wa augmented we show that to the contrary the algorithm are tractable with only a simple change in the task representation or initialization we provide tight bound on the worst case complexity and show how the complexity is even smaller if the reinforcement learning algorithm have initial knowledge of the topology of the state space or the domain ha certain special property we also present a novel bidirectional q learning algorithm to nd optimal path from all state to a goal state and show that it is no more complex than the other algorithm 
to create a pose invariant face recognizer one strategy is the view based approach which us a set of real example view at different pose but what if we only have one real view available such a a scanned passport photo can we still recognize face under different pose given one real view at a known pose it is still possible to use the view based approach by exploiting prior knowledge of face to generate virtual view or view of the face a seen from different pose to represent prior knowledge we use d example view of prototype face under different rotation we develop example based technique for applying the rotation seen in the prototype to essentially rotate the single real view which is available next the combined set of one real and multiple virtual view is used a example view for a view based pose invariant face recognizer oar experiment suggest that among the technique for expressing prior knowledge of face d example based approach should be considered alongside the more standard d modeling technique 
an attribute oriented rough set method for knowledge discovery in database is described the method is based on information generalization which examines the data at various level of abstraction followed by the discovery analysis and simplification of significant data relationship first an attribute oriented concept tree ascension technique is applied to generalize the information this step substantially reduces the overall computational cost then rough set technique are applied to the generalized information system to derive rule the rule represent data dependency occurring in the database we focus on discovering hidden pattern in the database rather than statistical summary 
it is well known that state abstraction can speed up planning exponentially under ideal condi tions we add to the knowledge showing that state abstraction may likewise slow down planning exponentially and even result in generat ing an exponentially longer solution than necessary this phenomenon can occur for abstraction hierarchy which are generated automatically by the alpine and highpoint algorithm we further show that there is little hope of any drastic improvement upon these algorithm it is computationally difficult to generate abstraction hierarchy which allow finding good approximation of optimal plan 
in multiagent planning an agent sometimes need to collaborate with others to construct complex plan or to accomplish large organizational task which it cannot do alone since each agent in a group may have incorrect belief about the world and incomplete knowledge and because agent s ability differ constructing a coordinated collaborative plan among agent is a difficult proposition in previous work osawa and tokoro we developed a scheme for constructing collaborative plan from the possibly incomplete individual plan of agent this scheme wa designed to provide availability based assignment of goal to agent and opportunistic collaboration to distributed planning in open multiagent environment based on the contract net in this paper we formalize incomplete individual plan and collaborative planning among rational agent using the multi world model and provide a utility based model for rational choice of action agent can effectively balance workload based on the utility theory a condition for incomplete collaborative plan is also presented 
each year people spend a huge amount of time typing the text people type typically contains a tremendous amount of redundancy due to predictable word usage pattern and the text s structure this paper describes a neural network system call auto qpist that monitor a person s typing and predicts what will be entered next auto qpist display the most likely subsequent word to the typist who can accept it with a single keystroke instead of typing it in it entirety the multi layer perceptron at the heart of auto qpist adapts it prediction of likely subsequent text to the user s word usage pattern and to the characteristic of the text currently being typed increase in typing speed of when typing english prose and when typing c code have been demonstrated using the system suggesting a potential time saving of more than hour per user per year in addition to increasing typing speed autotypist reduces the number of keystroke a user must type by a similar amount for english for computer program this keystroke saving ha the potential to significantly reduce the frequency and seventy of repeated stress injury caused by typing which are the most common injury suffered in today s office environment 
the relevance of context in disambiguating natural language input ha been widely acknowledged in the literature however most attempt at formalising the intuitive notion of context tend to treat the word and it context symmetrically we demonstrate here that traditional measure such a mutual information score are likely to overlook a significant fraction of all co occurrence phenomenon in natural language we also propose metric for measuring directed lexical influence and compare performance 
in previous application bilateral symmetry of object wa used either a a descriptivefeature in domain such a recognition and grasping or a a way to reduce the complexity ofstructure from motion in this paper we propose a novel application using the symmetry propertyto quot symmetrize quot data before and after reconstruction we first show how to compute theclosest symmetric d and d configuration given noisy data this give u a symmetrizationprocedure which we apply to image 
this paper present a statistical analysis of the davis putnam procedure and propositional satisfiability problem sat sat ha been researched in ai because of it strong relationship to automated reasoning and recently it is used a a benchmark problem of constraint satisfaction algorithm the davis putnam procedure is a well known satisfiability checking algorithm based on tree search technique in this paper i analyze two average case complexity for the davis putnam procedure the complexity for satisfiability checking and the complexity for finding all solution i also discus the probability of satisfiability the complexity and the probability strongly depend on the distribution of formula to be tested and i use the fixed clause length model a the distribution model the result of the analysis coincides with the experimental result well 
this paper outline the linguistic semantic commitment underlying an application which automatically construct depiction of verbal spatial description our approach draw on the ideational view of linguistic semantics developed by ronald langacker in his theory of cognitive grammar and the conceptual representation of physical object from the two level semantics of bierwisch and lang in particular the dimension of the process of conventional imagery are used a a metric for the design of our own conceptual representation 
the image segmentation problem may be considered a the search for a way to subdivide an image domain into region which represent the projection of visible part of object in a real scene the author analyze the problem of image segmentation in the framework of the approximation theory a defined by d mumford and j shah they show that for real image the problem of the choice of the energy functional is dictated by the model of the world and they propose a method to optimize it based on a deterministic algorithm processed at multiple level of resolution problem encountered in dealing with real scene lead to several modification of the algorithm and the energy functional image are shown on which the algorithm wa tested 
this paper show that decision tree can beused to improve the performance of casebasedlearning cbl system we introducea performance task for machine learning systemscalled semi flexible prediction that liesbetween the classification task performed bydecision tree algorithm and the flexible predictiontask performed by conceptual clusteringsystems in semi flexible prediction learning should improve prediction of a specificset of feature known a priori ratherthan a single known 
technique that traditionally have been useful for retrieving same domainanalogies from small single use knowledge base such a spreading activationand indexing on selected feature are inadequate for retrieving cross domainanalogies from large multi use knowledge base in this paper we describeknowledge directed spreading activation kdsa a new method for retrievinganalogies in a large semantic network kdsa us task specific knowledgeto guide a spreading activation search 
representing and modeling the motion and spatial support of multiple object and surface from motion video sequence is an important intermediate step towards dynamic image understanding one such representation called layered representation ha recently been proposed although a number of algorithm have been developed for computing these representation there ha not been a consolidated effort into developing a precise mathematical formulation of the problem this paper present one such formulation based on maximum likelihood estimation mle of mixture model and the minimum description length mdl encoding principle the three major issue in layered motion representation are i how many motion model adequately describe image motion ii what are the motion model parameter and iii what is the spatial support layer for each motion model 
real time algorithm need to address the time constraint e g deadline imposed by application like process control and robot navigation furthermore dependable real time algorithm need to be predictable about their ability to meet the time constraint of given task a real time algorithm is predictable if it can decide the feasibility of meeting time constraint of a given task or an arbitrary task from a task set well ahead of the deadline lastly a real time algorithm should exhibit progressively optimizing behavior i e the quality of the solution produced should improve a time constraint are relaxed we propose a new algorithm sarts that is based on a novel on line technique to choose the proper value of parameter which control the time allocated to planning based on the time constraint sarts also provides criterion to predict it ability to meet the time constraint of a given task the paper provides theoretical and experimental characterization of sarts a a dependable real time algorithm 
we propose a propositional language for temporal reasoning that is computationally effective yet expressive enough to describe information about fluents event and temporal constraint although the complete inference algorithm is exponential we characterize a tractable core with limited expressibility and inferential power our result render a variety of constraint propagation technique applicable for reasoning with constraint on auents 
the standard strategy for evaluation based on precision and recall are examined and their relative advantage and disadvantage are discussed in particular it is suggested that relevance feedback be evaluated from the perspective of the user a number of different statistical test are described for determining if difference in performance between retrieval method are significant these test have often been ignored in the past because most are based on an assumption of normality which is not strictly valid for the standard performance measure however one can test this assumption using simple diagnostic plot and if it is a poor approximation there are a number of non parametric alternative 
this paper report on an implementation of kanerva s sparse distributed memory for the connection machine in order to accomplish a modular and adaptive software library we applied a plain object oriented programming style to the common lisp extension ltsp some variation of the original model the selected coordinate design the hyperplane design and a new general design a well a the folded sdm due to kanerva are realized it ha been necessary to elaborate a uniform presentation of the theoretical foundation the different design are based on we demonstrate the simulator s functionality with some simple application runtime comparison are given we encourage the use of our simulation tool when outlining research topic of special interest to sdm 
we present teleassistance a two tiered control structure for robotic manipulation that combine the advantage of autonomy and teleoperation at the top level a teleoperator provides global deictic reference via a natural sign language each sign indicates the next action to perform and a relative and hand centered coordinate frame in which to perform it for example the teleoperator may point to an object for reaching or preshape the hand for grasping at the lower level autonomous servo routine run within the reference frame provided teleassistance offer two benefit first the servo routine can position the robot in relative coordinate and interpret feedback within a constrained context this significantly simplifies the computational load of the autonomous routine and requires only a sparse model of the task second the operator s action are symbolic conveying intent without requiring the person to literally control the robot this help to alleviate many of the problem inherent to teleoperation including poor mapping between operator and robot physiology reliance on a broad communication bandwidth and the potential for robot damage when solely under remote control to demonstrate the concept a utah mit hand mounted on a puma arm open a door 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
the bayesian classifier is a simple approach to classification that produce result that are easy for people to interpret in many case the bayesian classifier is at least a accurate a much more sophisticated learning algorithm that produce result that are more difficult for people to interpret to use numeric attribute with bnyesian classifier often requires the attribute value to be discretized into a number of interval we show that the discretization of numeric attribute is critical to successful application of the bayesinn classifier and propose a new method based on iterative improvement search we compare this method to previous approach and show that it result in signiticrmt reduction in misclassification error and cost on an industrial problem of troubleshooting the local loop in a telephone network the approach can take prior knowledge into account by improving upon a user provided set of boundxy point or can operate autonomously 
although the idea of generating plan through nonlinear or partially ordered partially instantiated popi planningi ha been around for almost twenty year it is only recently that the search space characteristic of popi planner have received particular attention a big thrust in this work ha been on reducing the redundancy in the search space of popi planner this wa largely motivated by the belief that redundancy reduction will lead to improvement in planning efficiency one approach towards redundancy elimination that turned out to be particularly influential a evidencedby several closely related extension wa that of mcallester s mcallester showed that it is possible to design a popi planner that is systematic in the strong sense that it never visit two equivalent plan or plan having overlapping linearizations such systematic planner were claimed to be more efficient than planner that admit redundancy in their search space while search space redundancy is an important factor affecting the efficiency of a p o pi planner another perhaps equally important one is the level of commitment in the planner after all avoiding premature commitment wa one of the primary motivation for popi planning there is a tradeoff between the redundancy elimination and least commitment in that often the redundancy is eliminated at the expense of increased commitment in the planner for example mcallester s planner achieves systematicity by keeping track of the causal structure of the plan generated during search and ensuring that each branch of the search space commits to and protects mutually exclusive causal structure for the partial plan we will see that such protection amount to a strong form of premature commitment which increase the amount of backtracking a well a the solution depth and can have an adverse effect on the performance of the planner in this paper we shall argue that the performance of a popi planner depends more closely on the way it deal with the tradeoff between redundancy and commitment than with the systematicity of it search we will start with a rational reconstruction of the motivation behind systematicity in popi planning and show that systematicity is just one extreme solution for the tradeoff between redundancy and commitment we will show that there are a spectrum of solution to this tradeoff and identify the dimension along which they vary we will explore the relative utility of the different solution through a comparative study of seven planner that fall at different point on the spectrum our study show that 
we present an integrated analog processor for real time wavelet decompositi on and reconstruction of continuous temporal signal covering the audio frequen cy range the processor performs complex harmonic modulation and gaussian lowpass filtering i n parallel identical channel each channel clocked at a different rate to produce a multiresolution m apping on a logarithmic frequency scale our implementation us mixed mode analog and digital circuit oversampling technique and switched capacitor filter to achieve a wide linear d ynamic range while maintaining compact circuit size and low power consumption we include experimental result on the channel processor and characterize it component separately from measurement on a reduced scale single channel test chip 
color histogram matching ha been shown to be apromising way of quickly indexing into a large imagedatabase yet few experiment have been done to testthe method on truly large database and even if theywere performed they would give little guidance to auser wondering if the technique would be useful withhis or her database in this paper we define and analyzea measure relevant to extending color histogramindexing to large database capacity how many distinguishablehistograms can 
this is a report of the application of the model generation theorem prover developed at icot to problem in the theory of finite quasigroups several of the problem were previously open in this paper we discus our theorem proving method related to those of the existing provers satchmo manthey bry and otter mccune and note how parallel processing on the icot parallel inference machine wa used to obtain high speed we then present and discus our machine aided investigation of seven problem concerning the existence of type of quasigroup 
a statistical theory for overtraining is proposed the analysistreats realizable stochastic neural network trained with kullbackleiblerloss in the asymptotic case it is shown that the asymptoticgain in the generalization error is small if we perform early stopping even if we have access to the optimal stopping time consideringcross validation stopping we answer the question in what ratiothe example should be divided into training and testing set in orderto obtain the optimum 
we propose an algorithm for the determination of three dimensional shape and perspective based on the response of the human visual system to change in visual texture current computer vision algorithm are computationally intensive and show inherent difficulty in integrating additional cue for the determination of shape such a shading contour or motion in order to develop a fast and simple mechanism le constrained for integrating other cue we incorporated aspect of the physiological property of cortical cell in vi into a network model we provide psychophysical evidence that the local spatial frequency spectrum is represented by the spatially averaged peak frequency apf after normalization this apf measure texture compression and lead to estimate of d shape and depth simulation of the model show good agreement with human response to a range of textured image 
this paper introduces a new concept a decisiontree or list over tree pattern which is anatural extension of a decision tree or decisionlist for dealing with tree structured object the learnability of this class is studied withinthe framework of the probably approximatelycorrect learning model and the identificationin the limit model it is found that the classk node dlrtp a subclass of decision list overregular tree pattern is not polynomial timepac learnable if np rp 
we present an abstraction of the genetic algorithm ga termed population based incremental learning pbil that explicitly maintains the statistic contained in a ga s population but which abstract away the crossover operator and redefines the role of the population this result in pbil being simpler both computationally and theoretically than the ga empirical result reported elsewhere show that pbil is faster and more effective than the ga on a large set of commonly used benchmark problem here we present result on a problem custom designed to benefit both from the ga s crossover operator and from it use of a population the result show that pbil performs a well a or better than gas carefully tuned to do well on this problem this suggests that even on problem custom designed for gas much of the power of the ga may derive from the statistic maintained implicitly in it population and not from the population itself nor from the crossover operator 
during the planning of multimedia presentation at least two distinct processesare required planning the underlying discourse structure that is ordering andinterrelating the information to be presented and allocating the medium that is delimiting the portion to be displayed by each individual medium the formerprocess ha been the topic of several study in the area of text planning butnumerous question remain for the latter including what is the nature of theallocation 
this paper address the problem of accurately and automaticallyrecovering the epipolar geometry from an uncalibrated stereorig and it application to the image matching problem a robust correlationbased approach that eliminates outlier is developped to produce areliable set of corresponding high curvature point these point are usedto estimate the so called fundamental matrix which is closely related tothe epipolar geometry of the uncalibrated stereo rig we show that an 
selective suppression of transmission at feedback synapsis during learning is proposed a a mechanism for combining associative feedback with self organization of feedforward synapsis experimental data demonstrates cholinergic suppression of synaptic transmission in layer i feedback synapsis and a lack of suppression in layer iv feedforward synapsis a network with this feature us local rule to learn mapping which are not linearly separable during learning sensory stimulus and desired response are simultaneously presented a input feedforward connection form self organized representation of input while suppressed feedback connection learn the transpose of feedforward connectivity during recall suppression is removed sensory input activates the self organized representation and activity generates the learned response 
in many diagnosis and repair domain diagnostic reasoning cannot be abstracted from repair action nor from action necessary to obtain diagnostic information in general in exploratory corrective domain an agent ha to interleave exploratory activity with activity aimed at achieving it goal in traumaid a consultation system for multiple trauma management we implement a reasoning framework for such domain which integrates diagnostic reasoning with planning and action this paper present goal directed diagnosis gdd a formalization of traumaid s diagnostic reasoning taking the view that a diagnosis is only worthwhile to the extent that it can affect repair decision gdd us goal to focus on such goal are also useful a a mean of communicating with it accompanying planner 
text categorization can be viewed a a process of category search in which one or more category for a test document are searched for by using given training document with known category a cluster based search with a probabilistic clustering algorithm is proposed and evaluated on two data set the efficiency effectiveness and noise tolerance of this search strategy were confirmed to be better than those of a full search a category based search and a cluster based search with nonprobabilistic clustering 
the problem of representing and reasoning about two notion of time that are relevant in the context of knowledge base is addressed these are called historical time and belief time respectively historical time denotes the time for which information model reality belief time denotes the time lor which a belief is held by an agent or a knowledge base we formalize an appropriate theory of time using logic a a meta language we then present a metalogic program derived from this theory through fold unfold transformation the metalogic program enables the temporal reasoning required for knowledge base application to be carried out efficiently the metalogic program is directly implementable a a prolog program and hence the need for a more complex theorem prover is obviated the approach is applicable for such knowledge base application a legislation and legal reasoning and in the context of multi agent reasoning where an agent reason about the belief of another agent 
we present experimental result on supervised learning of dynamicalfeatures in an analog vlsi neural network chip the recurrentnetwork containing six continuous time analog neuron and free parameter connection strength and threshold is trained togenerate time varying output approximating given periodic signalspresented to the network the chip implement a stochastic perturbativealgorithm which observes the error gradient along randomdirections in the parameter space for 
demonstrates a method of using nonmetric visual information derived from an uncalibrated active vision system to navigate an autonomous vehicle through free space region detected in a cluttered environment the structure of space is recovered modulo an affine transformation using an uncalibrated active stereo head carried by the vehicle the plane at infinity necessary for recovering affine structure from projective structure is found in a novel manner by making controlled rotation of the head the structure is composed of d point obtained by detecting and matching image corner through the stereo image sequence considerable care ha been taken to ensure that the processing is reliable robust and automatic driveable region are determined from the projection of the affine structure onto a plane parallel to the ground determined using projective construct two method of negotiating the region are explored the first introduces metric information to allow control of a euclidean vehicle the second us visual servoing of the active head to navigate in the affinely described free space region 
there ha been a great deal of interest within the information retrieval community in evaluating the use of linguistic knowledge to improve the indexing and searching of textual database such system must often employ a lexicon to store information about the word and phrase comprising the application s domain unlike a static lexicon a dynamic lexicon raise practical concern about the coordination between the state of the lexicon and ir indexing scheme based on lexical knowledge additionally it introduces a host of database management issue many of which are similar to those found in the text database a well in this paper we explore a range of system design and performance issue that arise when integrating a dynamic lexicon with a dynamic full text information retrieval system we observe that the principle of functional isolation argues against the use of language dependent information in article index and favor the use of query time strategy for applying lexical knowledge we propose and evaluate a system architecture which embodies this principle we also show how a storage and retrieval infrastructure based on burkowski s burkowski containment model abstraction can be employed to implement both the text retrieval and lexicon facility required in an integrated system 
we ass the usefulness of monocular recursive motion estimation technique for vehicle navigation in the absence of a model for the environment for this purpose we extend a recently proposed recursive motion estimator the essential filter to handle scale estimation we examine experimentally the accuracy with which the motion and position of the vehicle may be computed on an frame indoors sequence the issue of sampling time frequency and number of necessary feature in the environment are addressed systematically 
array x fc c hierarchical or multi level planning and reinforcement learning in this paper we treat only the prediction problem that of learning a model and value function for the case of fixed agent behavior within this context we establish the theoretical foundation of multi scale model and derive td algorithm for learning them two small computational experiment are presented to test and illustrate the theory this work is an extension and generalization of the work of singh dayan and sutton and pinette 
temporal difference td learning can beused not just to predict reward a is commonlydone in reinforcement learning butalso to predict state i e to learn a modelof the world s dynamic we present theoryand algorithm for intermixing td modelsof the world at different level of temporalabstraction within a single structure such multi scale td model can be used inmodel based reinforcement learning architecturesand dynamic programming method inplace of conventional markov 
in many vision based task the ability to focus attention on the important portion of a scene is crucial for good performance on the task in this paper we present a simple method of achieving spatial selective attention through the use of a saliency map the saliency map indicates which region of the input retina are important for performing the task the saliency map is created through predictive autoencoding the performance of this method is demonstrated on two simple task which have multiple very strong distracting feature in the input retina architectural extension and application direction for this model are presented motivation many real world task have the property that only a small fraction of the available input is important at any particular time on some task this extra input can easily be ignored nonetheless often the similarity between the important input feature and the irrelevant feature is great enough to interfere with task performance ho example of this phenomenon are the famous cocktail party effect otherwise known a speech recognition in a noisy environment and image processing of a cluttered scene in both case the extraneous information in the input signal can be easily confused with the important feature making the task much more difficult the concrete real world task which motivates this work is vision based road following in this domain the goal is to control a robot vehicle by analyzing the scene ahead and choosing a direction to travel based on the location of important feature like lane marking and road edge this is a difficult task since the scene ahead is often cluttered with extraneous feature such a other vehicle pedestrian tree guardrail crosswalk road sign and many other object that can appear on or around a roadway while we have had significant success on the road following task using simple feed forward neural network to transform image of the road ahead into steering command for the vehicle pomerleau b these method fail when presented with cluttered environment like those encoun for the general task of autonomous navigation these extra feature are extremely important but for restricted task of road following which is the focus of this paper these feature are merely distraction although we are addressing the more general task using the technique described here in combination with other method a description of these effort is beyond the scope of this paper 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
case based reasoning refers to the class of memory based problem solving method which emphasize the adaptation of recalled solution explanation diagnosis plan over the generation of solution from first principle cbr ha become a popular methodology resulting in a proliferation of case organization and representation proposal the goal of this paper is to sort through some of these proposal using the formal model of procedure and case based reasoning introduced in zito wolf and alterman we compare three current proposal for the organization of procedural case base individual case microcases and multi case we give a worst case analysis that show the advantage of the multi case in term of case storage and retrieval cost the model predicts that multi case reduce case storage and retrieval cost a compared to the other two model we then provide some empirical evidence from an implemented system that suggests that the trend observed in the formal model are also observable in case base of practical size 
we have developed and implemented in the qportrait program a qualitative simulation based method to construct phase portrait for a significant class of system of two coupled first order autonomous differential equation even in the presence of incomplete qualitative knowledge differential equation model are important for reasoning about physical system the field of nonlinear dynamic ha introduced the powerful phase portrait representation for the global analysis of nonlinear differential equation qportrait us qualitative simulation to generate the set of all possible qualitative behavior of a system constraint on two dimensional phase portrait from nonlinear dynamic make it possible to identify and classify trajectory and their asymptotic limit and constrain possible combination by exhaustively forming all combination of feature and filtering out inconsistent combination qportrait is guaranteed to generate all possible qualitative phase portrait we have applied qportrait to obtain tractable result for a number of nontrivial dynamical system guaranteed coverage of all possible behavior of incompletely known system complement the more detailed but approximation based result of recently developed method for intelligentlyguided numeric simulation nishida et al sack yip zhao combining the strength of both approach would better facilitate automated understanding of dynamical system 
multilingual instruction generation ha been the object of many study recently motivated by the increased need to produce multilingual manual coupled with the cost of technical writing and translating these study concentrate on the automatic generation of instruction leaving technical writer out of the loop in many case however it is not possible to dispense with human intervention entirely for at least two reason first the system must be provided with a semantic knowledge base from which the instruction can be generated second it is the technical writer who have the expertise necessary for producing instruction appropriate for a specific product or company and it is not necessarily an easy task to make this expertise available to a system the result of a requirement analysis study confirm the view that the most useful tool is not a stand alone writing tool but rather one that support technical writer in their task in this paper we describe such a support tool which we developed based on the result of our user requirement analysis 
detecting interaction and resolving conflict is one of the key issue for generative planning system hierarchical task network htn planning sytems use critic for this purpose critic have provided extra efficiency and flexibility to htn planning system but their procedural and sometimes domain specific nature ha not been amenable to analytical study a a result little work is available on the correctness or efficiency of critic this paper describes a principled approach to handling conflict a implemented in umcp an htn planning system critic in umcp have desirable property such a systematicity and the preservation of soundness and completeness 
semantic hyper linking plaisted et al chu and plaisted chu and plaisted ha been proposed recently to use semantics with hyper linking lee and plaisted an instance based theorem proving technique ground instance are generated until an unsatisfiable ground set is obtained semantics is used to greatly reduce the search space one disadvantage of semantic hyper linking is that large ground literal if needed in the proof sometimes are hard to generate in this paper we propose rough resolution a refinement of resolution robinson to only resolve upon maximum literal that are potentially large in ground instance and obtain rough resolvent rough resolvent can be used by semantic hyper linking to avoid generating large ground literal since maximum literal have been deleted a an example we will show how rough resolution help to prove lim bledsoe which cannot be proved using semantic hyper linking only we will also show other result in which rough resolution help to find the proof faster though incomplete rough resolution can be used with other complete method that prefer small clause 
the agm paradigm is a formal approach to ideal and rational information change from a practical perspective it suffers from two shortcoming the first involves difficulty with respect to the finite representation of information and the second involves the lack of support for the iteration of change operator in this paper we show that these practical problem can be solved in theoretically satisfying way wholely with in the agm paradigm we introduce a partial entrenchment ranking which serf a a canonical representation for a theory base and a well ranked episterruc entrenchment and we provide a computational model for adjusting partial entrenchment ranking when they receive new information using a procedure based on the principle of minimal change the connection between the standard agm theory change operator and the theory base change operator developed herein suggest that the proposed computational model for iterated theory base change exhibit desirable behaviour 
this paper provides a search based algorithm for computing prior and posterior probability in discrete bayesian network this is an anytime algorithm that at any stage can estimate the probability and give an error bound whereas the most popular bayesian net algorithm exploit the structure of the network for efficiency we exploit probability distribution for efficiency the algorithm is most suited to the case where we have extreme close to zero or one probability a is the case in many diagnostic situation where we are diagnosing system that work most of the time and for commonsense reasoning task where normality assumption allegedly dominate we give a characterisation of those case where it work well and discus how well it can be expected to work on average 
this paper describes a case based approach to knowledge acquisition for natural language system that simultaneously learns part of speech word sense and concept activation knowledge for all open class word in a corpus the parser begin with a lexicon of function word and creates a case base of context sensitive word definition during a humansupervised training phase then given an unknown word and the context in which it occurs the parser retrieves definition from the case base to infer the word s syntactic and semantic feature by encoding context a part of a definition the meaning of a word can change dynamically in response to surrounding phrase without the need for explicit lexical disambiguation heuristic moreover the approach acquires all three class of knowledge using the same case representation and requires relatively little training and no hand coded knowledge acquisition heuristic we evaluate it in experiment that explore two of many practical application of the technique and conclude that the case based method provides a promising approach to automated dictionary construction and knowledge acquisition for sentence analysis in limited domain in addition we present a novel case retrieval algorithm that us decision tree to improve the performance of a k nearest neighbor similarity metric 
we introduce a logic based system which improvesthe performance of intelligent help systemsby supplying them with plan generationand plan recognition component bothcomponents work in close mutual cooperation there are two mode of cross talk betweenthem one where plan recognition is done on thebasis of abstract plan provided by the plannerand the other where optimal plan are generatedbased on recognition result the exampleswhich are presented are taken from an operatingsystem 
this paper discus the linearly weighted combination of estimator in which the weighting function are dependent on the input we show that the weighting function can be derived either by evaluating the input dependent variance of each estimator or by estimating how likely it is that a given estimator ha seen data in the region of the input space close to the input pattern the latter solution is closely related to the mixture of expert approach and we show how learning rule for the mixture of expert can be derived from the theory about learning with missing feature the presented approach are modular since the weighting function can easily be modifled no retraining if more estimator are added furthermore it is easy to incorporate estimator which were not derived from data such a expert system or algorithm 
in the general case a trilinear relationship between three perspective view is shown to exist the trilinearity result is shown to be of much practical use in visual recognition by alignment yielding a direct method superior to the conventional epipolar line intersection method the proof of the central result may be of further interest a it demonstrates certain regularity across homographies of the plane 
the issue of recognizing d elongated object from d intensity image is addressed a tube model locally similar to generalized cone is developed for the class of elongated object a recognition strategy that combine d contour property and surface shading information is used to exploit the power of the d model reliable contour provide constraint for localizing the object of interest the theory of optimal filter is adopted in verifying the shading of hypothesized object object recognition is achieved through optimizing the signal to noise response with respect to model parameter a sweeping operation is proposed a a further stage of identifying object so that the overall performance of the system doe not heavily rely on the quality of local feature detection 
both the number and the size of spatial database are rapidly growing because of the large amount of data obtained from satellite image x ray crystallography or other scientific equipment therefore automated knowledge discovery becomes more and more important in spatial database so far most of the method for knowledge discovery in database kdd have been based on relational database system in this paper we address the task of class identification in spatial database using clustering technique we present an interface to the database management system dbms which is crucial for the efficiency of kdd on large database this interface is based on a spatial access method the r tree it cluster the object according to their spatial neighborhood and support efficient processing of spatial query furthermore we propose a method for spatial data sampling a part of the focusing component significantly reducing the number of object to be clustered thus we achieve a considerable speed up for clustering in large database we have applied the proposed technique to real data from a lar ge protein database used for predicting protein protein docking a performance evaluation on this database indicates that clustering on lar ge spatial database can be performed both efficiently and effectively using our approach 
in multi agent environment where agent independently generate and execute plan to satisfy their goal the resulting plan may sometimes overlap in this paper we propose a collaboration mechanism using social law through which rational agent can smoothly delegate and receive the execution of the overlapping part of plan in order to reduce the cost of plan execution also we consider collaboration with agent that do not abide by social law that is self centered agent simulation result show that our mechanism also ha the property of balancing the cost of plan execution and show flexibility towards selfcentered agent 
the access structure the retrieval model and the system architecture of the spider information retrieval system are described the access structure provides efficient weighted retrieval on dynamic data collection it is based on signature and non inverted item description the signature provide upper bound for the exact retrieval status value such that only a small number of exact retrieval status value have to be computed spider s retrieval model is a probabilistic retrieval model that is capable to exploit the database scheme of semistructured data collection this model can be considred a a further development of the binary independence indexing bii model the system architecture wa derived systematically from a given set of requirement such a effective and efficient retrieval on dynamic data collection exploitation of the database scheme computed view and the integration of information retrieval functionality and database functionality 
during the conceptual analysis the coexistence of several kind of ambiguity tends to complicate the task of every kind of disambiguation module we tackle this problem here for two type of ambiguity anaphor and the pp attachment we show what kind of problem every conceptual analyser ha to face we present then our solution to resolve these problem we study the efficiency of the proposed solution and it adequacy regarding dependency between both ambiguity 
we present a method based on kalman filtering for image motion estimation within kalman formalism a motion boundary can be modelled a a jump in the evolution equation of the filter the detection of such a jump relies on a statistical test applied to the innovation signal the optimal estimation of the jump parameter and the compensation of the current estimate are performed using a general likelihood ratio glr algorithm to exploit the spatial redundancy inherent to a motion boundary the original glr algorithm is reformulated by integrating spatiotemporal motion information this result in a significant decrease of the compensation delay 
an automatic ego motion compensation based feature detection and correspondence algorithm is presented for image sequence taken from a moving camera feature displacement over consecutive frame can be approximately decomposed into two component the displacement due to camera motion which can be compensated for by image rotation scaling and translation and the displacement due to object motion and or perspective projection the author introduce a two step approach first the motion of the camera is compensated for by using a computational vision based image registration algorithm then consecutive frame are transformed to the same coordinate system and the feature correspondence problem is solved a though for a stationary camera feature point are detected using a gabor wavelet decomposition and a local interaction based algorithm method for subpixel accuracy feature matching and tracking are introduced experimental result on a real image sequence are presented 
a method for incorporating context dependent phone class ina connectionist hmm hybrid speech recognition system is introduced a modular approach is adopted where single layer networksdiscriminate between different context class given the phone classand the acoustic data the context network are combined with acontext independent ci network to generate context dependent cd phone probability estimate experiment show an averagereduction in word error rate of and from the 
we represent arbitrary smooth curved dshapes by a discrete set of hot curve where a surfaceadmits high order tangent these curve determinethe structure of the image contour and it catastrophicchanges and there is a natural correspondence betweensome of them and monocular contour feature such a inflectionsand bitangents we present a method for automaticallyconstructing the hot curve from continuoussequences of video image and describe an approach to objectrecognition 
an application of laterally interconnected self organizing map lissom to handwritten digit recognition is presented the lat eral connection learn the correlation of activity between unit on the map the resulting excitatory connection focus the activity into local patch and the inhibitory connection decorrelate redun dant activity on the map the map thus form internal representa tions that are easy to recognize with e g a perceptron network the recognition rate on a subset of nist database is higher with lissom than with a regular self organizing map som a the front end and higher than recognition of raw input bitmap directly these result form a promising starting point for building pattern recognition system with a lissom map a a front end 
we describe and evaluate hidden understanding model a statistical learning approach to natural language understanding given a string of word hidden understanding model determine the most likely meaning for the string we discus the problem of representing meaning in this framework the structure of the statistical model the process of training the model and the process of understanding using the model finally we give experimental result including result on an arpa evaluation 
we describe a new approach to default reasoning based on a principle on indifference among possible world we interpret default rule a extreme statistical statement thus obtaining a knowledge base kb comprised of statistical and first order statement we then assign equal probability to all world consistent with kb in order to assign a degree of belief to a statement the degree of belief can be used to decide whether to defeasibly conclude various natural pattern of reasoning such a a preference for more specific default indifference to irrelevant information and the ability to combine independent piece of evidence turn out to follow naturally from this technique furthermore our approach is not restricted to default reasoning it support a spectrum of reasoning from quantitative to qualitative it is also related to other system for default reasoning in particular we show that the work of goldszmidt et al which applies maximum entropy idea to semantics can be embedded in our framework 
this paper present a method for using qualitativemodels to guide inductive learning our objective are to induce rule whichare not only accurate but also explainablewith respect to the qualitative model andto reduce learning time by exploiting domainknowledge in the learning process such explainabilityis essential both for practical applicationof inductive technology and for integratingthe result of learning backintoan existing knowledge base we apply thismethod to 
we construct a mixture of locally linear generative model of a collection of pixel based image of digit and use them for recognition different model of a given digit are used to capture different style of writing and new image are classified by evaluating their log likelihood under each model we use an em based algorithm in which the m step is computationally straightforward principal component analysis pca incorporating tangent plane information about expected local deformation only requires adding tangent vector into the sample covariance matrix for the pca and it demonstrably improves performance 
this paper describes an adaptive method for the recovery of d shape model from sequence of image a d surface model initialised to be spherical is progressively deformed under the action of simulated external force arising from the profile of the target object in successive image obtained using a low level motion segmentation algorithm intrinsic constraint encourage the model to deform smoothly and to remain symmetrical about a vertical plane parallel to the direction of motion 
morphology is the area of linguistics concerned with the internal structure of word information retrieval ha generally not paid much attention to word structure other than to account for some of the variability in word form via the use of stemmer this paper will describe our experiment to determine the importance of morphology and the effect that it ha on performance we will also describe the role of morphological analysis in word sense disambiguation and in identifying lexical semantic relationship in a machine readable dictionary we will first provide a brief overview of morphological phenomenon and then describe the experiment themselves 
this paper describes a nethod for finding structuralmatching between parallel sentence of twolauguages such a japanese and english par allel sentence are analyzed based on unificationgrammars and structural matching is performedby making use of a similarity measure of word pairsin the two language syntactic ambiguity are resolvedsimultaneously in the matching process theresults serve a a useful source for extracting linguisticand lexical knowledge 
this paper describes a method for finding structural matching between parallel sentence of two language such a japanese and english parallel sentence are analyzed based on unification grammar and structural matching is performed by making use of a similarity measure of word pair in the two language syntactic ambiguity are resolved simultaneously in the matching process the result serve a a useful source for extracting linguistic and lexical knowledge 
this paper present a cooperative consultation system on a restricted domain the system build hypothesis on the user s plan and avoids misunderstanding with consequent repair dialogue through clarification dialogue in case of ambiguity the role played by constraint in the generation of the answer is characterized in order to limit the case of ambiguity requiring a clarification dialogue the answer of the system are generated at different level of detail according to the user s competence in the domain 
this paper introduces a problem solving task involving common sense reasoning that human are adept at but one which ha not received much attention within the area of cognitive modeling until recently this is the task of predicting the operation of simple mechanical device in term of behavior of their component from labeled schematic diagram showing the spatial configuration of component and a given initial condition we describe this task present a cognitive process model developed from task and protocol analysis and illustrate it using the example of a pressure gauge then the architecture of a corresponding computer model and a control algorithm embodying the cognitive strategy are proposed 
recognizing face is a difficult problem due to the generally similar shape of face combined with the considerable variability in image of the same face under different viewing condition in this report we consider image variation due to mainly illumination condition 
abstract uncertainty sampling method iteratively request class label for training instance whose class are uncertain despite the previous labeled instance these method can greatly reduce the number of instance that an expert need label one problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instance we test the use of one classifier a highly efficient probabilistic one to select example for training another the c rule induction program despite being chosen by this heterogeneous approach the uncertainty sample yielded classifier with lower error rate than random sample ten time larger 
in this paper we discus the result of experiment which use a context essentially an ordered set of lexical item a the seed from which to build a network representing statistically important relationship among lexical item in some corpus a metric is then applied to the node in the network in order to discover those pair of item related by high index of similarity the goal of this research is to instantiate a class of item corresponding to each item in the priming context we believe that this instantiation process is ultimately a special case of abstraction over the entire network in this abstraction similar node are collapsed into metanodes which may then function a if they were single lexical item 
this paper present the design and simulation result of a selforganizing neural network which induces a grammar from example sentence input sentence are generated from a simple phrase structure grammar including number agreement verb transitivity and recursive noun phrase construction rule the network induces a grammar explicitly in the form of symbol categorization rule and phrase structure rule 
in this paper we present a system for detection trackingand representation of tubular object in image theuniqueness of the proposed system is twofold at the macrolevel the novelty of the system lie in the integration ofobject localization and tracking using geometric property at the micro level in the use of high and low levelconstraints to model the detection and tracking subsystem the underlying philosophy for object detection isto extract perceptually significant feature 
information retrieval system are being challenged to managelarger and larger document collection in an effort to providebetter retrieval performance on large collection moresophisticated retrieval technique have been developed that supportrich structured query structured query are not amenable topreviously proposed optimization technique optimizing execution however is even more important in the context of large documentcollections we present a new structured query optimizationtechnique which we have implemented in an inference network basedinformation retrieval system experimental result show that queryevaluation time can be reduced by more than half with little impacton retrieval effectiveness 
this paper considers the problem of simulating creativity in the domain of jazz improvisation and accompaniment unlike most current approach we try to model the musician behavior by taking into account their experience and how they use it with respect to the evolving context of live performance to represent this experience we introduce the notion of musical memory which exploit the principle of case based reasoning slade to produce live music using this musical memory we propose a problem solving method based on the notion of pact potential action that are activated according to the context and then combined in order to produce note we show that our model support two of the main feature of creativity i e non determinism and the absence of well defined goal johnson laird 
this paper explores the relationship among reactivity heuristic reasoning and search it describes a hybrid hierarchical reasoner that first ha the opportunity to react correctly if no ready reaction is computed the reasoner activates a set of reactive trigger for time limited search procedure if they too fail to produce a response the reasoner resort to collaboration among a set of heuristic rationale in a series of experiment this hybrid reasoner is shown to be effective and efficient the data also show how each of the three process correct reaction time limited search with reactive trigger and heuristic rationale play an important role in problem solving reactivity is demonstrably enhanced by brief knowledge based intelligent search to generate solution fragment 
the paper present a novel method for measuring a discontinuityin range image the discontinuity basically the qualitativesurface characteristic is assigned a quantitative value called measureof the surface discontinuity strength theoretical emphasis is put on estimationof c crease discontinuity three principal advantage areachieved first the discontinuity strength is computed locally second the d problem of the surface discontinuity strength estimation is 
this paper present a method for doing motion segmentation for autonomous vehicle which drive on planar surface there are two distinct type of independent motion that may occur within an image sequence taken from a moving vehicle the first generic type of independent motion is when the projected motion of point on the independent object violate the epipolar constraint the second case is where the epipolar constraint is not violated this paper demonstrates that it is possible to detect this second type of independent motion by looking for progressive dis occlusion of the road a novel collision prediction method is also given the method predicts the projection of a corridor down which the agv will travel this prediction may be used for time to contact collision prediction and the corridor width embodies an estimate of the vehicle size 
this paper provides a systematic analysis of the relative utility of basing ebg based plan reuse technique in partial ordering v total ordering planning framework we separate the potential advantage into those related to storage compaction and those related to the ability to exploit stored plan we observe that the storage compaction provided by partially ordered partially instantiated plan can to a large ex ent be exploited regardless of the underlying planner we argue that it is in the ability to exploit stored plan during planning that partial ordering planner have some distinct advantage in particular to be able to flexibly reuse and extend the retrieved plan a planner need the ability to arbitrarily and efficiently splice in new step and sub plan into the retrieved plan this is where partial ordering planner with their least commitment strategy and flexible plan representation score significantly over state based planner a well a planner that search in the space of totally ordered plan we will clarify and supporll this hypothesis through an empirical study of three planner and two reuse strategy 
constraint satisfaction problem involve finding value for problem variable that satisfy constraint on what combination of value are permitted they have application in many area of artificial intelligence from planning to natural language understanding a new method is proposed for decomposing constraint satisfaction problem using inferred disjunctive constraint the decomposition reduces the size of the problem some solution may be lost in the process but not all the decomposition support an algorithm that exhibit superior performance analytical and experimental evidence suggests that the algorithm can take advantage of local weak spot in globally hard problem 
this paper report work in progress on a sentence generation model which attempt to emulate certain language output pattern of child between the age of one and one half and three year in particular the model address the issue of why missing or phonetically null subject appear a often a they do in the speech of young english speaking child it will also be used to examine why other pattern of output appear in the speech of child learning language such a italian and chinese initial finding are that an output generator successfully approximates the null subject output pattern found in english speaking child by using a processing overload metric alone however reference to several parameter related to discourse orientation and agreement morphology is necessary in order to account for the differing pattern of null argument appearing cross linguistically based on these finding it is argued that the null subject phenomenon is due to the combined effect of limited processing capacity and early accurate parameter setting 
a constraint satisfaction problem may not admit a complete solution in this case a good partial solution may be acceptable this paper present new technique for organizing search with branch and bound algorithm so that maximal partial solution those having the maximum possible number of satisfied constraint can be obtained in reasonable time for moderately sized problem the key feature is a type of variable ordering heuristic that combine width at a node of the constraint graph number of constraint shared with variable already chosen with factor such a small domain size that lead to inconsistency in value of adjacent variable ordering based on these heuristic lead to a rapid rise in branch and bound s cost function together with local estimate of future cost which greatly enhances lower bound calculation both retrospective and prospective algorithm based on these heuristic are dramatically superior to earlier branch and bound algorithm developed for this domain 
we propose an affine framework for perspective view captured by a single extremely simple equation basedon a viewer centered invariant we call relative affinestructure via a number of corollary of our mainresults we show that our framework unifies previouswork including euclidean projective and affine in a natural and simple way finally the main resultswere applied to a real image sequence for purpose of d reconstruction from d view introductionthe introduction of 
this paper present instance based state identification an approachto reinforcement learning and hidden state that build disambiguatingamounts of short term memory on line and also learns with anorder of magnitude fewer training step than several previous approach inspired by a key similarity between learning with hiddenstate and learning in continuous geometrical space this approachuses instance based or quot memory based quot learning a method thathas worked well in continuous 
this paper present a method for constructing deterministicprolog parser from corpus of parsedsentences our approach us recent machinelearning method for inducing prolog rule fromexamples inductive logic programming we discussseveral advantage of this method comparedto recent statistical method and present resultson learning complete parser from portion of theatis corpus introductionrecent approach to constructing robust parsersfrom corpus primarily use statistical 
the reconstruction of noise corrupted surface can be inferred by methodology such a bayesian estimation and minimum description length both of these imply a formulation where the reconstruction minimizes a functional often this functional is non convex and the minimum cannot be found by simple gradient method the paper concern functionals with quadratic data term criterion for such functionals to be convex and the variational approach of minimizing non convex functionals initial convexity of the approximating functional is considered to be a critical point two fully automatic method of generating convex functionals are presented they are based on gaussian convolution and are compared to the blake zisserman graduated non convexity gnc a blake a zisserman and g l bilbro et al and d geiger and f girosi s mean field annealing mfa of the weak membrane 
intellectics ie artificial intelligence and cognitive science is an interdisciplinaryfield whose goal are to understand and explain intelligence on the one hand and todevelop computational model which show intelligent behavior on the other hand the system presented in this paper is based on idea taken from automated reasoningand connectionism the connectionist inference system chcl is applied tosolve the question whether a given sentence correctly describes the spatial relationsof 
domain oriented knowledge acquisition tool provide efficient support for the design of knowledge based system however the cost of developing such tool is high especially when their restricted scope is taken into account developer can use metalevel tool to generate domain oriented knowledge acquisition tool that are custom tailored for a small group of expert with considerably le effort than is required for manual tool development an epistemic obstacle to creating such metatools is the specification model for target knowledge acquisition tool the metatool dot is based on an abstract architecture approach to the specification and generation of knowledge acquisition tool dot is domain and method independent because it is based on an architectural model of the target knowledge acquisition tool 
a consistent text contains rich resource of information such a collocation pattern that can be used to resolve ambiguity within it sentence for example attachment ambiguity in a sentence can be resolved by selecting a candidate attachment that match attachment found in other sentence in the same discourse thus discourse can be regarded a a valuable knowledge resource for sentence analysis in this paper we examine some feature of discourse a a knowledge resource and propose a framework for natural language processing that provides a simple algorithm for using information extracted from discourse together with information stored in knowledge base the experimental result of using our framework to disambiguate sentence in technical document offer good prospect for improving the accuracy of a broad coverage natural language processing system that handle various text without constructing knowledge base for each text in advance some noteworthy feature of discourse information are also deduced from the result of our experiment 
this panel explores issue of systematic and stochastic control in the context of constraint satisfaction 
a method for automatically evaluating the quality of document page segmentation algorithm is introduced many d ifferent zoning technique are now available but there exists no robust method to benchmark and evaluate them reliably our proposed strategy is a region based approach in which segmentation result are compared with manually generated ground truth file describing all possible correct segmentation a segmentation ground truthing scheme wa already proposed the evaluation of segmentation quality is achieved by testing the overlap between the two set of region in fact the region are defined a being the valued pixel contained in the extracted polygon an explicit specification of segmentation error and a numerical evaluation are derived the algorithm is simple and fast and provides a multi level output for each segmentation 
abstract thisarticlcshcsws howrational analysis canbcuscclto minimize learning cost for a general class of statistical learning problem wc discus the factor that inftuence learning cost and show that the problem of efficient learning can bccast a arescrurce optimization problem solution found inthiswaycan besignificantlymore efficient than the best solution that do not account for these factor we introduce a heuristic learning algorithm that approximately solves this optimization problem and document it performance in provemcntson synthetic and real world problem lntrodudion machine learning technique are valuable tool both in ac 
the situation recognition system to which this paper is devoted receives a input a stream of time stamped event it performs recognition of instance of occurring situation a they are developing and it generates a output deduced event and action to trigger it is mainly a temporal reasoning system it is predictive in the sense that it predicts forthcoming event relevant to it task it focus it attention on them and it maintains their temporal window of relevance it main functionality is to recognize efficiently complex temporal pattern on the fly while they are taking place this system ha been tested for the surveillance of an environment by a multisensory perception machine it is being applied to monitoring a complex dynamic system 
a formal theory of action and change in dynamic system is presented our formalism is based on the paradigm that state transition in a system naturally occur while time pass by one or more agent have the possibility to direct the development of the system by executing action our theory cover concurrency of action and event and includes a natural way to express delayed efiects and nondeterminism a uniform semantics for speciflcations of dynamic system is developed which enables u to express solution to problem like temporal projection planning and postdiction in term of logical entailment 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
current mt system whatever translation method they at present employ do not reach an optimum output on free text our hypothesis for the experiment reported in this paper is that if an mt environment can use the best result from a variety of mt system working simultaneously on the same text the overall quality will improve using this novel approach to mt in the latest version of the pangloss mt project we submit an input text to a battery of machine translation system engine collect their possibly incomplete result in a joint chart like data structure and select the overall best translation using a set of simple heuristic this paper describes the simple mechanism we use for combining the finding of the various translation engine 
combinatory categorial grammar ccgs steedman have been shown by weir and joshi to generate the same class of language a tree adjoining grammar tag head grammar hg and linear indexed grammar lig in this paper i will discus the effect of using variable in lexical category assignment in ccgs it will be shown that using variable in lexical category can increase the weak generative capacity of ccgs beyond the class of grammar listed above 
we present zeno a least commitment planner that handle action occurring over extended interval of time deadline goal metric precondition metric effect and continuous change are supported simultaneous action are aliowed when their effect do not interfere unlike most planner that deal with complex language the zeno planning algorithm is sound and complete the running code is a complete implementation of the formal algorithm capable of solving simple problem i e those involving le than a dozen step 
this article deal with the problem of estimating deformation of brightness pattern using visual front end operation estimating such deformation constitutes an important subtask in several computer vision problem relating to image correspondence and shape estimation the following subject are treated the problem of decomposing affine flow field into simpler component is analysed in detail a canonical parametrization is presented based on singular value decomposition which naturally separate the rotationally invariant component of the flow field from the rotationally variant one a novel mechanism is presented for automatic selection of scale level when estimating local affine deformation this mechanism is expressed within a multiscale framework where disparity estimate are computed in a hierarchical coarse to fine manner and corrected using iterative technique then deformation estimate are selected from the scale that minimize a certain normalized residual over scale finally the descriptor so obtained serve a initial data for computing refined estimate of the local deformation 
we present a binocular active vision system that can attend to and fixate a moving target our system ha an open and expandable design and it form the first step of a long term effort towards developing an active observer using vision to interact with the environment in particular capable of figure ground segmentation we also present partial real time implementation of this system and show their performance in real world situation together with motor control in pursuit we particularly focus on occlusion of other target both stationary and moving and integrate three cue ego motion target motion and target disparity to obtain an overall robust behavior an active vision system must be open expandable and operate with whatever data are available momentarily it must also be equipped with mean and method to direct and change it attention this system is therefore equipped with motion detection for changing attention and pursuit for maintaining attention both of which run concurrently 
programming language interpreter proving theorem of the form a abstract data type and program optimization can all be represented by a finite set of rule called a rewrite system in this paper we study two fundamental concept uniqueness of normal form and confluence for nonlinear system in the absence of termination this is a difficult topic with only a few result so far through a novel approach we show that every persistent system ha unique normal form this result is tight and a substantial generalization of previous work in the process we derive a necessary and sufficient condition for persistence for the first time and give new class of persistent system we also prove the confluence of the union function symbol can be shared of a nonlinear system with a left linear system under fairly general condition again persistence play a key role in this proof we are not aware of any confluence result that allows the same level of function symbol sharing 
explanation based learning ha typicallybeen considered a symbolic learning method an explanation based learning method thatutilizes purely neural network representation called ebnn ha recently been developed and ha been shown to have several desirableproperties including robustness to errorsin the domain theory this paper brieflysummarizes the ebnn algorithm then exploresthe correspondence between this neuralnetwork based ebl method and eblmethods based on symbolic 
a knowledge based system us it database a k a it theory to produce answer to the query it receives unfortunately these answer may be incorrect if the underlying theory is faulty standard theory revision system use a given set of labeled query each a query paired with it correct answer to transform the given theory by adding and or deleting either rule and or antecedent into a related theory that is a accurate a possible after formally defining the theory revision task and bounding it sample complexity this paper address the task s computational complexity it first prof that unless p np no polynomial time algorithm can identify the optimal theory even given the exact distribution of query except in the most trivial of situation it also show that except in such trivial situation no polynomial time algorithm can produce a theory whose inaccuracy is even close i e within a particular polynomial factor to optimal these result justify the standard practice of hill climbing to a locally optimal theory based on a given set of labeled sample 
we evaluate the first order learning system foil on a series of text categorization problem it is shown that foil usually form classifier with lower error rate and higher rate of precision and recall with a relational encoding than with a propositional encoding we show that foil s performance can be improved by relation selection a first order analog of feature selection relation selection improves foil s performance a measured by any of recall precision f measure or error rate with an appropriate level of relation selection foil appears to be competitive with or superior to existing propositional technique 
task decomposition planner make use of schematathat define task in term of partially ordered setsof task and primitive action most existing taskdecompositionplanners synthesize plan via a topdownapproach called task reduction which usesschemata to replace task with network of task andactions until only action remain in this paper we present a bottom up plan parsingapproach to task decomposition instead of reducingtasks into action we use an incremental parsing 
machine translation mt ha recently been formulated in term of constraint based knowledge representation and unification theory but it is becoming more and more evident that it is not possible to design a practical mt system without an adequate method of handling mismatch between semantic representation in the source and target language in this paper we introduce the idea of information based mt which is considerably more flexible than interlingual mt or the conventional transfer based mt 
an efficient graph matching approach is proposed for finding region correspondence between two image of the same scene but taken from different viewpoint region and their relation in an image are represented with region adjacency graph rag which is a kind of attributed planar graph the problem to find an optimal region correspondence which match the region in two image with maximal similarity in region feature and region relation is formulated into the problem to find the optimal inexact matching between two rag the property specific to planar graph and that of the region adjacency relation are utilized to invent an efficient algorithm to solve the problem experimental result on various kind of image show the effectiveness of the method 
there ha been some recent interest in intelligent backtracking procedure that can return to the source of a dif ulty without erasing the intermediate work in this paper we show that for some problem it can be counterproductive to do this and in fact that such inteiiigence can cause an exponentkd increase in the size of the ultimate search space we discus the reason for this phenomenon and we present one way to deal with it 
the problem of deciding which subset of value of a categorical valued attribute to branch on during decision tree generation is addressed algorithm such a id and c do not address the issue and simply branch on each value of the selected attribute the gid algorithm is presented and evaluated the gid algorithm is a generalized version of quinlan s id and c and is a non parametric version of the gid algorithm presented in an earlier paper it branch on a subset of individual value of an attribute while grouping the rest under a single default branch it is empirically demonstrated that gid outperforms id c and gid for any parameter setting of the latter the empirical test include both controlled synthetic randomized domain a well a real world data set the improvement in tree quality a measured by number of leaf and estimated error rate is significant 
range data offer a direct way to produce shape descriptionsof surface typically single range imageshave the form of a graph surface z g x y and thussuffer from occlusion one can reduce this problemby taking several image from different location andmerging them together the result is a real d descriptionof the object s surface in this paper we addressseveral problem that result from the d to dtransition we present an algorithm which is able tomerge depth image of 
in this paper we present a method for improving search efficiency in the area of constraint satisfaction problem in finite domain this method is based on the analysis of the micro structure of a csp we call micro structure of a csp the graph defined by the compatible relation between variablevalue pair vertex are these pair and edge are defined by pair of compatible vertex given the micro structure of a csp we can realize a preprocessing to simplify the problem with a decomposition of the domain of variable so we propose a new approach to problem decomposition in the field of csps well adjusted in case such a classical decomposition method are without interest i e when the constraint graph is complete the method is described in the paper and a complexity analysis is presented given theoretical justification of the approach furthermore two polynomial class of csps are induced by this approach the recognition of them being linear in the size of the instance of csp considered 
this paper describes the analysis of image sequence takenby a t v camera mounted on a car moving in usual outdoor scenery because of the presence of shock and vibration during the image acquisition the numerical computation of temporal derivative is very noisyand therefore differential technique to compute the optical flow do notprovide adequate result by using correlation based technique and bycorrecting the optical flow for shock and vibration it is possible to 
this paper present an approach to automatic discovery of function in genetic programming the approach is based on discovery of useful building block by analyzing the evolution trace generalizing block to define new function and finally adapting the problem representation onthe fly adaptating the representation determines a hierarchical organization of the extended function set which enables a restructuring of the search space so that solution can be found more easily measure of complexity of solution tree are defined for an adaptive representation framework the minimum description length principle is applied to justify the feasibility of approach based on a hierarchy of discovered function and to suggest alternative way of defining a problem s fitness function preliminary empirical result are presented next we introduce different notion of complexity useful in controlling the algorithm we view the problem of determining a gp program that satisfies the criterion imposed by the fitness function a one of hypothesis formation this enables u to apply the minimum description length principle to analyze the power of approach based on automatic discovery of function we conclude by analyzing an example mentioning related work and considering direction for future work 
we discus the use of case based reasoning cbr to drive an information retrieval ir system our hybrid cbr ir approach take a input a standard frame based representation of a problem case and output text of relevant case retrieved from a document corpus dramatically larger than the case base available to the cbr system while the smaller case base is accessible by the usual case based indexing and is amenable to knowledge intensive method the larger ir corpus is not our approach provides two benefit it extends the reach of cbr for retrieval purpose to much larger corpus and it enables the injection of knowledge based technique into traditional ir our system work by first performing a standard hypo style cbr analysis and then using text associated with certain important case found in this analysis to seed a modified version of inquery s relevance feedback mechanism in order to generate a query we describe our approach and report on experiment performed in two different legal domain 
this paper we take this argument one sma step further a nd suggest a way these two levelsmight be organized into a stratified strltcture our discussion here ha s a very ha trow tbcus a nd doesnot attempt to answer such iml orta nt question a s whether the list collection of l resenta tionalrelations is exhaustive and adequate lbr describing a possible intentiola l structure 
in planning task an agent may often find himself in a situation demanding that he choose an action that would prevent some unwanted event from occurring similarly in task involving the generation of description or explanation of sequence of event it is often useful to draw a many informative connection a possible between event in the sequence often this mean explaining why certain event are not possible in this paper i consider the semantics of event prevention and argue that a naive semantics which equates prevention with the elimination of all future possibility of the event in question is often difficult if not impossible to implement i argue for a more useful semantics which fall out of some reasonable assumption regarding restriction on the set of potential action available to an agent those action about which the agent ha formed intention those action consistent with the agent s attitude including it other intention and the set of action evoked by the type of situation in which the agent is embedded 
the information age is characterized by a rapid growth in the amount of information available in electronic medium traditional data handling method are not adequate to cope with this information flood knowledge discovery in database kdd is a new paradigm that focus on computerized exploration of large amount of data and on discovery of relevant and interesting pattern within them while most work on kdd is concerned with structured database it is clear that this paradigm is required for handling the huge amount of information that is available only in unstructured textual form to apply traditional kdd on text it is necessary to impose some structure on the data that would be rich enough to allow for interesting kdd operation on the other hand we have to consider the severe limitation of current text processing technology and define rather simple structure that can be extracted from text fairly automatically and in a reasonable cost we propose using a text categorization paradigm to annotate text article with meaningful concept that are organized in hierarchical structure we suggest that this relatively simple annotation is rich enough to provide the basis for a kdd framework enabling data summarization exploration of interesting pattern and trend analysis this research combine the kdd and text categorization paradigm and suggests advance to the state of the art in both area 
reinforcement learning rl algorithm providea sound theoretical basis for buildinglearning control architecture for embeddedagents unfortunately all of the theory andmuch of the practice see barto et al for an exception of rl is limited to markoviandecision process mdps many realworlddecision task however are inherentlynon markovian i e the state of the environmentis only incompletely known to the learningagent in this paper we consider only partially 
an autoencoder network us a set of recognition weight to convertan input vector into a code vector it then us a set of generative weight to convert the code vector into an approximate reconstructionof the input vector we derive an objective function fortraining autoencoders based on the minimum description length mdl principle the aim is to minimize the information requiredto describe both the code vector and the reconstruction error weshow that this information is 
describes a simple and accurate method for internal camera calibration based on tracking image feature through a sequence of image while the camera undergoes pure rotation a special calibration object is not required and the method can therefore be used both for laboratory calibration and for self calibration in autonomous robot experimental result with real image show that focal length and aspect ratio can be found to within percent and lens distortion error can be reduced to a fraction of a pixel the location of the principal point and the location of the center of radial distortion can each be found to within a few pixel we perform a simple analysis to show to what extent the various technical detail affect the accuracy of the result we show that having pure rotation is important if the feature are derived from object close to the camera in the basic method accurate angle measurement is important the need to accurately measure the angle can be eliminated by rotating the camera through a complete circle while taking an overlapping sequence of image and using the constraint that the sum of the angle must equal degree 
recent development in the area of reinforcement learning haveyielded a number of new algorithm for the prediction and control ofmarkovian environment these algorithm including the td algorithmof sutton and the q learning algorithm of watkins can be motivated heuristically a approximation to dynamic programming dp in this paper we provide a rigorous proof of convergence ofthese dp based learning algorithm by relating them to the powerfultechniques of stochastic 
we prove that the so called loading problem for recurrent neural network is unsolvable this extends several result which already demonstrated that training and related design problem for neura l network are at least np complete our result also implies that it is imp ossible to find or to formulate a universal training algorithm which for any neural network architecture could determine a correct set of we ights for the simple proof of this we will just show that the loading pr oblem is equivalent to hilbert s tenth problem which is known to be unsolvable 
in this paper we propose a new method for solving the handeye calibration problem and we show how this method can be used in conjunction with a reconstruction technique in order to estimate on line the relationship between the frame in which the scene ha been reconstructed or calibration frame and the frame attached to the robot hand the method is particularly well suited for calibrating stereo head with respect to the robot on which they are mounted we discus the advantage of on line self versus off line hand eye and camera calibration we develop two solution for solving for the hand eye calibration problem a closed form solution and a non linear least square solution finally we report on some experiment performed with a stereo head mounted onto a degree of freedom robot arm 
the circuit fix it shoppe is a voice interactive dialog system which ha been constructed in our laboratory the mission of the system is to help people repair electronic circuit the system contains a domain modeler a reasoning system a dialog controller a user modeling system an error correcting natural language parser and a natural language generator a commercial speech recognizer and speech synthesizer are used for voice input and output more detailed information about our dialog system can be found in and this videotape record two live dialog between the circuit fix it shoppe program and a user who ha no special knowledge of computer electronic repair or our system a brief description of the experimental setup and of the circuit fix it shoppe program precedes these dialog the circuit fix it shoppe program is capable of varying it level of initiative it can be highly directive in which case it control the conversation or it may be passive in which case the user control the dialog or it may take some level of initiative between these two extreme in the first videotape demonstration the system is running in directive mode in this second demonstration the system is set to operate in declarative mode in this mode the user is free to take the initiative and to control the conversation declarative mode is appropriate for user who are much more familiar with the circuit and require only minimal help from the computer duration minute second tape format vhs 
when learning classifier more extensive search for rule is shown to lead to lower predictive accuracy on many of the leal world domain investigated this counter intuitive re suit is particularly relevant to recent system the search method that use risk free pruning to achieve the same outcome a exhaustive search we propose an iterated search method that commences with greedy search extending it scope at each iteration until a stopping criterion is satisfied this layered search is often found to produce theory that are more accurate than those obtained with either greedy search or moderately extensive beam search 
testing the satisfiability of a boolean formula over linear constraint is not a simple matter existing ai system handle that kind of problem with a general proof method for their boolean part and a separate module for combining linear constraint on the contrary traditional operation research method need the problem to be transformed and solved with a mixed integer linear programming algorithm both approach appear to be improvable if no early separation is introduced between the logical and numerical part in this case combinatorial explosion can be dramatically reduced thanks to efficient looking ahead technique and learning method indeed propagating bound following the initial formula give precious information besides an especially tight linear relaxation can be driven from the formula and allows a simplex algorithm to make a good test for satisfiability finally these two looking ahead method can be easily coupled for more efficiency and completed by local enumeration moreover discovering a good failure explanation is relatively easy in the proposed framework by learning these explanation it is possible to prune important redundant part of the search tree 
much of the previous work on hand eye coordination ha emphasized the reconstructive aspect of vision recently technique that avoid explicit reconstruction by placing visual feedback into a control loop have been developed when properly defined these method lead to calibration insensitive hand eye coordination recent work on projective geometry a applied to vision is used to extend this paradigm in two way first it is shown how result from projective geometry can be used to perform online calibration second result on projective invariance are used to define setpoints for visual control that are independent of viewing location these idea are illustrated through a number of example and have been tested on an implemented system 
abstract critical to the success of any real world agent is the ability to detect and recover from unsuccessful action failing to detect these error may cause the execution of the remainder of a plan to have unex pected and dangerous effect in this paper we present ed the error detective which systematically exe cutis lesioned operator in order to generate a table of error and associated cause for a software agent we describe feature of software environment that allow u to efficiently build this table without searching for simultaneous error or making the single fault assumption we then report on experiment compar 
this paper describes a distributed adaptive first order logic engine with exceptional performance characteristic the system combine serial search reduction technique such a bounded overhead subgoal caching and intelligent backtracking with a novel parallelization strategy particularly well suited to coarse grained parallel execution on a network of workstation we present empirical result that demonstrate our system s performance using workstation on over first order logic problem drawn from the thousand of problem for theorem provers collection utroduction we have developed an distributed adaptive first order logic engine a the core of a planning system intended to solve large logistics and transportation scheduling problem calistri yeh segre this underlying inference engine called dali distributed adaptive logical inference is based on an extended version of the warren abstract machine wam architecture ait kaci which also serf a the basis for many modem prolog implementation dali take a first order specification of some application domain the domain theory and us it to satisfy a series of query via a model elimination inference procedure our approach is inspired by ptip stickel in that it is based on prolog technology i e the wam but circumvents the inherent limitation thereof to provide an inference procedure that is complete relative to first order logic unlike pttp however dali employ a number of serial search reduction technique such a bounded overhead subgoal caching segre scharstein and intelligent backtracking kumar lin to improve search efficiency dali also exploit a novel parallelization scheme called nagging sturgill segre that support the effective use of a large number of loosely coupled processing element the message of this paper is that efficient implementation technology serial search reduction technique and parallel nagging can be successfully combined to produce a highperformance first order logic engine we support this claim with an extensive empirical performance evaluation 
eficient and efiective discovery of resource and knowledge from the internet ha become an imminent research issue especially with the advent of the information super highway a multiple layered database mldb approach is proposed to handle the resource and knowledge discovery in global information base a preliminary experiment show the advantage of such an approach information retrieval data mining and data analysis technique can be used to extract and transform information from a lower layer database to a higher one resource can be found by controlled search through dinerent layer of the database and knowledge discovery can be performed eficiently in such a layered database 
when designing a device the final product of the design process is usually considered to be a physical specification of a device however the design of the causal mechanism underlying the physical specification i e how the device is intended to work to achieve it function is a product just a important a the physical specification if not more capturing this knowledge of causal mechanism is necessary in order to understand the physical specification of the device a well a to evaluate and refine the specification during the design process despite the importance of such knowledge existing cad tool do not support it explicit representation or manipulation we describe a design support system under development in which knowledge of both the causal mechanism and the physical structure of a device being designed is explicitly represented and manipulated the system allows the designer to provide functional specification at various level of abstraction in a language called cfrl causal functional representation language the cfrl specification acquired from the user enables the system to evaluate the physical specification a it is being developed in order to provide useful feedback to the designer furthermore functional specification provide an important basis for recording the engineer s design rationale 
this paper present a formalization of the bidding and awarding decision process that wa left undefined in the original contract net task allocation protocol this formalization is based on marginal cost calculation based on local agent criterion in this way agent having very different local criterion based on their selfinterest can interact to distribute task so that the network a a whole function more effectively in this model both competitive and cooperative agent can interact in addition the contract net protocol is extended to allow for clustering of task to deal with the possibility of a large number of announcement and bid message and to effectively handle situation in which new bidding and awarding is being done during the period when the result of previous bid are unknown the protocol is verified by the traconet transportation cooperation net system where dispatch center of different company cooperate automatically in vehicle routing the implementation is asynchronous and truly distributed and it provides the agent extensive autonomy the protocol is discussed in detail and test result with real data are presented 
we report a novel possibility for extracting asmall subset of a data base which contains allthe information necessary to solve a given classificationtask using the support vector algorithmto train three different type of handwrittendigit classifier we observed that these typesof classifier construct their decision surface fromstrongly overlapping small subset of thedata base this finding open up the possibilityof compressing data base significantly by disposingof the 
this paper present a theoretical framework for mapping from structure to function in engineering domain we argue that a generative approach grounded in qualitative process theory produce useful functional explanation these explanation are articulate in that they enable the user to explore their theoretical justification and perform counterfactual reasoning these explanation stem from a teleological representation based on goal plan role and view we show that an ontology based on aggregated process facilitates the recognition of recurring thermodynamic structure we describe an implementation of this theory a system called carnot that explains steady flow thermodynamic cycle ranging in complexity from four to component 
oz is an experimental higher order concurrent constraint programming system under development at dfki it combine idea from logic and concurrent programming in a simple yet expressive language from logic programming oz inherits logic variable and logic data structure which provide for a programming style where partial information about the value of variable is imposed concurrently and incrementally a novel feature of oz is that it accommodates higher order programming without sacrificing that denotation and equality of variable are captured by first order logic another new feature of oz is constraint communication a new form of asynchronous communication exploiting logic variable constraint communication avoids the problem of stream communication the conventional communication mechanism employed in concurrent logic programming constraint communication can be seen a providing a minimal form of state fully compatible with logic data structure based on constraint communication and higher order programming oz readily support a variety of object oriented programming style including multiple inheritance 
with the goal of reducing computational cost without sacrificing accuracy we describe two algorithm to find set of prototype for nearest neighbor classification here the term prototype refers to the reference instance used in a nearest neighbor computation the instance with respect to which similarity is assessed in order to assign a class to a new data item both algorithm rely on stochastic technique to search the space of set of prototype and are simple to implement the first is a monte carlo sampling algorithm the second applies random mutation hill climbing on four datasets we show that only three or four prototype sufficed to give predictive accuracy equal or superior to a basic nearest neighbor algorithm whose run time storage cost were approximately to time greater we briefly investigate how random mutation hill climbing may be applied to select feature and prototype simultaneously finally we explain the performance of the sampling algorithm on these datasets in term of a statistical measure of the extent of clustering displayed by the target class 
in this paper we describe a new technique for parsing free text a transformational grammar is automatically learned that is capable of accurately parsing text into binary branching syntactic tree with nonterminals unlabelled the algorithm work by beginning in a very naive state of knowledge about phrase structure by repeatedly comparing the result of bracketing in the current state to proper bracketing provided in the training corpus the system learns a set of simple structural transformation that can be applied to reduce error after describing the algorithm we present result and compare these result to other recent result in automatic grammar induction 
this paper show how to formally characterize language learning in a finite parameter space a a markov structure important new language learning result follow directly explicitly calculated sample complexity learning time under different input distribution assumption inclding childes database language input and learning regime we also briefly describe a new way to formally model rapid diachronic syntax change 
in this paper we introduce ant q a family of algorithm which present many similarity with q learning watkins and which we apply to the solution of symmetric and asymmetric instance of the traveling salesman problem tsp ant q algorithm were inspired by work on the ant system a a distributed algorithm for combinatorial optimization based on the metaphor of ant colony which wa recently proposed in dorigo dorigo maniezzo and colorni we show that a is a particular instance of the ant q family and that there are instance of this family which perform better than a we experimentally investigate the functioning of ant q and we show that the result obtained by ant q on symmetric tsp s are competitive with those obtained by other heuristic approach based on neural network or local search finally we apply ant q to some difficult asymmetric tsp s obtaining very good result ant q wa able to find solution of a quality which usually can be found only by very specialized algorithm 
an important result of visual understanding is an explanation of a scene s causal structure how action usually motion is originated constrained and prevented and how this determines what will happen in the immediate future to be useful for a purposeful agent these explanation must also capture the scene in term of the functional property of it object their purpose us and affordances for manipulation design knowledge describes how the world is organized to suit these function and causal knowledge describes how these arrangement work we have been exploring the hypothesis that vision is an explanatory process in which causal and functional reasoning play an intimate role in mediating the activity of low level visual process in particular we have explored two of the consequence of this view for the construction of purposeful vision system causal and design knowledge can be used to drive focus of attention and choose between ambiguous image interpretation both principle are at work in sprocket a system which visually explores simple machine integrating diverse visual clue into an explanation of a machine s design and function 
experiment were performed to reveal some of the computationalproperties of the human motor memory system we show thatas human practice reaching movement while interacting with anovel mechanical environment they learn an internal model of theinverse dynamic of that environment subject show recall of thismodel at testing session hour after the initial practice therepresentation of the internal model in memory is such that thereis interference when there is an attempt to learn 
recently markov decision process and optimal control policy have been applied to the problem of decision theoretic planning however the classical method for generating optimal policy are highly intractable requiring explicit enumeration of large state space we explore a method for generating abstraction that allow approximately optimal policy to be constructed computational gain are achieved through reduction of the state space ab traction are generated by identifying proposition that are r elevant either through their direct impact on utility or their influ ence on action this information is gleaned from the representati on of utility and action we prove bound on the loss in value due to abstraction and describe some preliminary experimental re sults reduced space our approach ha several advantage over the envelope method foremost among these is the fact that no state are ignored in abstract policy generation each state may have some influence on the constructed policy by membership in an abstract state this allows u to prove bound on the value of abstract policy with respect to an optimal policy furthermore finer grained abstraction are guaranteed to increase the value of policy finally abst ractions can be generated quickly these factor allow abstract policy of varying degree of accuracy to be constructed in response to time pressure the information obtained in abstract policy generation can then be used in a real time fashion to refine the abstract policy a we describe in the concluding section this is also well suited to circumstan ce where the goal or reward structure communicated to an agent change frequently thus problem specific abstraction can be generated a needed in the next section we describe the mdps howard s policy iteration algorithm for optimal policy construction and briefly the anytime approach of in section we discus a possible knowledge representation scheme for action and utility the information implicit in such a sp ecification will be crucial in generating useful abstraction in section we present an algorithm for generating an abstrac t state space and an appropriate decision model we show how policy iteration is used to generate abstract policy in this state space that are directly applicable to the origi nal concrete space and prove bound on the possible loss due to abstraction we also discus preliminary experimental result that suggest that abstraction of this form is qui te valuable in certain type of domain 
any phenomenon can be seen under a more or le precise granularity depending on the kind of detail which are perceivable this can be applied to time and space a characteristic of abstract space such a the one used for representing time is their granularity independence i e the fact that they have the same structure under different granularity so time place and their relationship can be seen under different granularity and they still behave like time place and relationship under each granularity however they do not remain exactly the same time place and relationship here is presented a pair of operator for converting upward and downward qualitative time relationship from one granularity to another these operator are the only one to satisfy a set of six constraint which characterize granularity change they are also shown to be useful for spatial relationship 
abstract random error and insufficiency in database limit the performance of any classifier trained from commadatabase in this paper we propose a method to estimate the limiting performance of classifier imposed by the database we demonstrate this technique on the task of predicting failure in telecommunication path 
this paper describes a unique example based mapping method for document retrieval we discovered that the knowledge about relevance among query and document can be used to obtain empirical connection between query term and the canonical concept which are used for indexing the content of document these connection do not depend on whether there are shared term among the query and document therefore they are especially effective for a mapping from query to the document where the concept are relevant but the term used by article author happen to be different from the term of database user we employ a linear least square fit llsf technique to compute such connection from a collection of query and document where the relevance is assigned by human and then use these connection in the retrieval of document where the relevance is unknown we tested this method on both retrieval and indexing with a set of medline document which ha been used by other information retrieval system for evaluation the effectiveness of the llsf mapping and the significant improvement over alternative approach wa evident in the test 
in this paper we preseni scalable data parallel algorithm for geometric hashing we perform implementation of the proposed algorithm on maspar mp l mp in earlier parallel implementation the number of processor employed a independent of the sire of the scene but depends on the size of the model database which is usually very large we destgn new parallel algorithm and map them onto mp i mp these technique significantly improve apon the nuii ber of processor employed while achieving superzor time performance our tmplementatzons run on a p processor machine such that p s where s is the number of feature point in the scene our result show that a probe of the recognztion phase for a scene consisting of feature point take le thaii msec on a k processor mp l mp background object recognition a high level vision task is a key step in an integrated vision system in object recognition using geometric hashing given a set of model and their feature point for each model all possible pair of the feature point are designated a a basis set the coordinate of the feature point of a model are computed relative to each member of it basis set these coordinate are then used a index into a hash table the record in the hash table comprise of model basis pair in the recognition phase an arbitrary pair of feature point in the scene is chosen a a basis and the coordinate of the feature point in the scene are computed the new coordinate are used to hash into the hash table and the corresponding entry of the hashed bin are accessed vote are accumulated tor the model basis pair stored in the hashed location the pair winning the maximumnumber of vote is chosen a a candidate for matching there have been two prior effort in paralleliziiig the geometric hashing algorithm l both implementation have been performed on simd liyper 
this paper proposes a solution to the frame problem for knowledge producing action an example of a knowledge producing action is a sense operation performed by a robot to determine whether or not there is an object of a particular shape within it grasp the work is an extension of reiter s solution to the frame problem for ordinary action and moore s work on knowledge and action the property of our specification are that knowledge producing action do not affect fluents other than the knowledge fluent and action that are not knowledge producing only affect the knowledge fluent a appropriate in addition memory emerges a a side effect if something is known in a certain situation it remains known at successor situation unless something relevant ha changed also it will be shown that a form of regression examined by reiter for reducing reasoning about future situation to reasoning about the initial situation now also applies to knowledge producing action 
d f dementhon and l s davis proposed a method for determining the pose of a d object with respect to a camera from d to d point correspondence the method consists of iteratively improving the pose computed with a weak perspective camera model to converge at the limit to a pose estimation computed with a perspective camera model we show that the method of dementhon and davis can be extended to paraperspective the iterative paraperspective pose algorithm that we describe in detail ha interesting property both in term of speed and rate of convergence moreover we introduce a simple way of taking into account the orthogonality constraint associated with the rotation matrix and we define the optimal experimental setup to be used in the presence of camera calibration error 
we argue that many ai planning problem should be viewed a process oriented where the aim is to produce a policy or behavior strategy with no termination condition in mind a opposed to goal onented the full power of markov decision model adopted recently for ai planning becomes apparent with process oriented problem the question of appropriate optimality criterion becomes more critical in this case we argue that average reward optimal is most suitable while construction of averageoptimal policy involves a number of subtlety and computational difficulty certain aspect of the problem can be solved using compact action representation such a bayes net in particular we provide an algorithm that identifies the structure of the markov process underlying a planning problem a crucial element of constructing average optimal policy without explicit enumeration of the problem state space 
a novel approach to integrating case based reasoning with model based diagnosis is presented the main idea is to use the model of the device and the result of diagnostic test to index and match case representing past diagnostic situation with the current one the initial diagnostic methodology is presented a well a the problem encountered while applying this methodology to two real world device the incorporation of a case based reasoning system is then motivated and described in detail experimental result show the effectiveness of both the indexing schema and the matching algorithm the paper also discus how and why these result can be generalized to a multiple fault situation to other type of device model and to other application in the field of artificial intelligence 
when autonomous agent attempt to coordinate action it is often necessary that they reach some kind of consensus reaching consensus ha traditionally been dealt with in the distributed artificial intelligence literature via negotiation another alternative is to have agent use a voting mechanism each agent express it preference and a group choice mechanism is used to select the result some choice mechanism are better than others and ideally we would like one that cannot be manipulated by untruthful agent coordination of action by a group of agent corresponds to a group planning process we here introduce a new multi agent planning technique that make use of a dynamic iterative search procedure through a process of group constraint aggregation agent incrementally construct a plan that brings the group to a state maximizing social welfare at each step agent vote about the next joint action in the group plan i e what the next transition state will be in the emerging plan using this technique agent need not fully reveal their preference and the set of alternative final state need not be generated in advance of a vote with a minor variation the entire procedure can be made resistant to untruthful agent 
we have developed an artificial neural network based gaze tracking system which can be customized to individual user a three layer feed forward network trained with standard error back propagation is used to determine the position of a user s gaze from the appearance of the user s eye unlike other gaze tracker which normally require the user to wear cumbersome headgear or to use a chin rest to ensure head immobility our system is entirely non intrusive currently the best intrusive gaze tracking system are accurate to approximately degree in our experiment we have been able to achieve an accuracy of degree while allowing head mobility in it current implementation our system work at hz in this paper we present an empirical analysis of the performance of a large number of artificial neural network architecture for this task suggestion for further exploration for neurally based gaze tracker are presented and are related to other similar artificial neural network application such a autonomous road following 
this paper present the semantics of trio an object oriented language devoted to specify realtime system referring to different time granularity time granularity allows to describe the behavior and the property of a system and it environment with respect to different time scale trio semantics is expressed by translation into a logical framework supporting the notion of time granularity such a semantics provides the executability of object oriented specification 
generating explanation of device behavior is a long standmg goal of ai research in reasoning about physical system much of the relevant work ha concentrated on new method for modeling and simulation such a qualitative physic or on sophisticated natural language generation in which the device model are specially crafted for explanatory purpose we show how two technique from the modeling research compositional modeling and causal ordering can be effectively combined to generate natural language explanation of device behavior from engineering model the explanation offer three advance over the data display produced by conventional simulation software causal interpretation of the data summary at appropriate level of abstraction physical mechanism and component operating mode and query driven natural language summary furthermore combining the compositional modeling and causal ordering technique allows model that are more scalable and le brittle than model designed solely for explanation however these technique produce model with detail that can be distracting in explanation and would be removed in hand crafted model e g intermediate variable we present domain independent filtering and aggregation technique that overcome these problem 
we present a new context based approach to default logic called contextual default logic the approach extends the notion of a default rule and supply each extension with a context contextual default logic allows for embedding all existing variant of default logic along with more traditional approach like the closed world assumption a key advantage of contextual default logic is that it provides a syntactical instlmment for comparing existing default logic in a unified setting in particular it reveals that existing default logic mainly differ in the way they deal with an explicit or implicit underlyiug context 
information measure with respect to spatial location and scale of object in an image are important to image processing and interpretation it allows u to focus attention on relevant data saving effort and reducing false positive in particular the information content of a man made scene is typically confined to a small set of scale we devise a scale space based measure of image information kullback contrast between successive resolution length give the differential information gain experiment show that this measure give a clear indication of characteristic length in a variety of real world image and is superior to power spectrum based measurement decomposing the expected information gain into spatial coordinate give u a saliency map for use by an attention selector we combine the scale and spatial decomposition into a single information measure giving both the spatial extent and scale range of interest the information measure ha an efficient implementation and thus can be used routinely in early vision processing 
an abstraction scheme is developed to simplify bayesian belief network structure for future inference session the concept of abstract network and abstract junction tree are proposed based on the inference time efficiency good abstraction are characterized furthermore an approach for automatic discovery of good abstraction from the past inference session is presented the learned abstract network is guaranteed to have a better average inference time efficiency if the characteristic of the future session remains moreorless the same a preliminary experiment is conducted to demonstrate the feasibility of this abstraction scheme 
a common technique for bounding the runtime required to solve a constraint satisfaction problem is to exploit the structure of the problem s constraint graph dechter we show that a simple structure based technique with a minimal space requirement pseudo tree search freuder quinn is capable of bounding runtime almost a effectively a the best exponential space consuming scheme specifically if we let n denote the number of variable in the problem w denote the exponent in the complexity function of the best structure based technique and h denote the exponent from pseudotree search we show h w lg n the result should allow reduction in the amount of real time accessible memory required for predicting runtime when solving csp equivalent problem 
the fundamental matrix is a key concept when working with uncalibrated imagesand multiple viewpoint it contains all the available geometric information and enablesto recover the epipolar geometry from uncalibrated perspective view this paper isabout the problem of it determination from point which lie in several plane in thatcase there is an homography between coordinate of point in the two image we firstinvestigate the use of different criterion to compute the homography a 
automatic symbolic traffic scene analysis is essential to many area of ivhs intelligent vehicle highway system traffic scene information can be used to optimize traffic flow during busy period identify stalled vehicle and accident and aid the decision making of an autonomous vehicle controller improvement in technology for machine vision based surveillance and high level symbolic reasoning have enabled u to develop a system for detailed reliable traffic scene analysis the machine vision component of our system employ a contour tracker and an affine motion model based on kalman filter to extract vehicle trajectory over a sequence of traffic scene image the symbolic reasoning component us a dynamic belief network to make inference about traffic event such a vehicle lane change and stall in this paper we discus the key task of the vision and reasoning component a well a their integration into a working prototype 
we present a bottom up generalization whichbuilds the maximally general term coveringa positive example and rejecting negativeexamples in first order logic fol i e interms of version space the set g this algorithm is based on rewriting negativeexamples a constraint upon the generalizationof the positive example at hand theconstraints space is partially ordered inducinga partial order on negative example thenear miss a defined by winston can thenbe formalized in fol 
we describe a computational framework for a grammar architecture in which different linguistic domain such a morphology syntax and semantics are treated not a separate component but compositional domain the framework is based on combinatory categorial grammar and it us the morpheme a the basic building block of the categorial lexicon 
the standard approach to decision tree induction is a top down greedy agorithm that make locally optimal irrevocable decision at each node of a tree in this paper we empircally study an alternative approach in which the algorithm use one level lookahead to decide what test to use at a node we systematically compare using a very large number of artificial data set the quality of dimension tree induced by the greedy approach to that of tree induced using lookahead the main observation from our experiment are the greedy approach consistently produced tree that were just a at accurate a tree produced with the much more expensive lookahead step and n we observed many instance of pathology i e lookahead produced tree that were both larger and le accurate than tree produced without it 
estimating principal curvature and principal direction of a surface from a polyhedral approximation with a large number of small face such a those produced by iso surface construction algorithm ha become a basic step in many computer vision algorithm particularly in those targeted at medical application we describe a method to estimate the tensor of curvature of a surface at the vertex of a polyhedral approximation principal curvature and principal direction are obtained by computing in closed form the eigenvalue and eigenvectors of certain spl time symmetric matrix defined by integral formula and closely related to the matrix representation of the tensor of curvature the resulting algorithm is linear both in time and in space a a function of the number of vertex and face of the polyhedral surface 
in this paper we describe a content planningmechanism which take into consideration auser s possible inference in order to generatethe most concise discourse that achieves agiven communicative goal the considerationof a user s inference result in the additionof information that address erroneous inference and the omission of easily inferred information given a communicative goal ourmechanism applies inference rule in backwardreasoning mode to plan rhetorical 
machine assisted language translation system for technical document guide human through a process of selecting and composing variant partial translation the constrained nature of technical sublanguages make language processing aid cost effective to build and use analogously we have developed kit a knowledge based translation system for converting informal english scenario of the desired behavior of complex reactive system into formal executable test script a trainable parser and reference resolver capture domain specific linguistic knowledge a logic analyzer establishes coherence in the translation process in a role comparable to a story understander it check the consistency of each step of a translated test script using a theorem prover a planner and logic encoded background knowledge about the system under test this help correct common but serious specification error including underspecificity omitted step and even some outright mi statement to evaluate how well such technology can scale we have exercised our technology progressively on a graduated corpus of behavior scenario spanning advanced calling feature for a private telephone switch pbx successfully translating into test script without any manual post hoc editing our experience with kit ha enabled u to identify many of the tradeoff in accommodating informality in specification versus demanding formality from a human agent 
this report present a new framework for the computation of shape and motion from a sequence of image taken under perspective projection the framework is based on two abstraction the picture and trail locus that represent respectively the set of all picture of the same scene and the set of all trail that a point in the world can leave on the image for a given camera trajectory these abstraction lead to a remarkably clean relation between perspective and orthography furthermore image motion is described in term of angle between projection ray thereby eliminating the need to model camera rotation and leading to more stable result a numerically sound global minimization method is developed based on this framework for the case of a two dimensional world but all concept also hold in three dimension experiment show that the method is rather immune to noise but critically dependent on camera calibration 
in this paper we show how both geometry driven diffusion and optimization of the mumford shah functional can be used to develop a type of curve evolution that is able to preserve salient feature of closed curve such a corner and straight line segment while simultaneously suppressing noise and irrelevant detail the idea is to characterize the curve by mean of it angle function i e the angle between the tangent and a fixed axis and to apply the appropriate dynamic to this one dimensional representation we show how constrained evolution equation can be used to keep the corresponding curve closed at all time 
many machine learning algorithm aim atfinding quot simple quot rule to explain trainingdata the expectation is the quot simpler quot the rule the better the generalization ontest data occam s razor most practicalimplementations however use measuresfor quot simplicity quot that lack the power universalityand elegance of those based on kolmogorovcomplexity and solomonoff s algorithmicprobability likewise most previousapproaches especially those of the quot bayesian quot kind suffer from the problem 
this paper describes a set of experiment with a system that synthesizes constraint satisfaction program the system multi tac is a csp expert that can specialize a library of generic algorithm and method for a particular application multi tac not only proposes domain specific version of it generic heuristic but also search for the best combination of these heuristic and integrates them into a complete problem specific program we demonstrate multi tac s capability on a combinatorial problem minimum maximal matching and show that multi tac can synthesize program for this problem that are on par with hand coded program in synthesizing a program multi tac base it choice of heuristic on the instance distribution and we show that this capability ha a significant impact on the result 
this paper present an algorithm that combine traditional ebl technique and recent development in inductive logic programming to learn effective clause selection rule for prolog program when these control rule are incorporated into the original program significant speed up may be achieved the algorithm is shown to be an improvement over competing ebl approach in several domain additionally the algorithm is capable of automatically transforming some intractable algorithm into one that run in polynomial time 
the success of reinforcement learning in practical problem depends on the ability to combine function approximation with temporal difference method such a value iteration experiment in this area have produced mixed result there have been both notable success and notable disappointment theory ha been scarce mostly due to the difficulty of reasoning about function approximators that generalize beyond the observed data we provide a proof of convergence for a wide class of temporal difference method involving function approximators such a k nearest neighbor and show experimentally that these method can be useful the proof is based on a view of function approximators a expansion or contraction mapping in addition we present a novel view of approximate value iteration an approximate algorithm for one environment turn out to be an exact algorithm for a different environment 
this paper describes a combination of compression method which may be used to reduce the size of inverted index for very large text database these method are prefix omission run length encoding and a novel family of numeric representation called n s coding using these compression method on two different text source the king james version of the bible and a sample of wall street journal story the compressed index occupies le than of the size of the original text even when both stopwords and number are included in the index the decreased time required for i o can almost fully compensate for the time needed to uncompress the posting this research is part of an effort to handle very large text database on the cm a massively parallel mimd supercomputer 
we describe recent extension to our frameworkfor the automatic generation of music makingprograms we have previously used geneticprogramming technique to produce musicmakingprograms that satisfy user providedcritical criterion in this paper we describe newwork on the use of connectionist technique toautomatically induce musical structure from acorpus we show how the resulting neural networkscan be used a critic that drive ourgenetic programming system we argue thatthis 
we present algorithm for the discovery and useof topological map of an environment by an activeagent such a a person or a mobile robot we discus several issue dealing with the useof pre existing topological map of graph likeworlds by an autonomous robot and present algorithm worst case complexity and experimentalresults for representative real world example for two key problem the first of theseproblems is to verify that a given input map isa correct description of the 
a system ha been developed to acquire extend and refine d geometric site model from aerial imagery this system hypothesizes potential building roof in an image automatically locates supporting geometric evidence in other image and determines the precise shape and position of the new building via multiimage triangulation model to image registration technique are applied to align new incoming image against the site model model extension and refinement procedure are then performed to add previously unseen building and to improve the geometric accuracy of the existing d building model 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
the ability of an inductive learning system tofind a good solution to a given problem is dependentupon the representation used for thefeatures of the problem system that performconstructive induction are able to change theirrepresentation by constructing new feature we describe an important real world problem finding gene in dna that we believe offersan interesting challenge to constructiveinductionresearchers we report experimentsthat demonstrate that two different 
a fundamental problem of determining the position and orientation of a d object using a single perspective image view is defined and investigated the technique is based on the interpretation of trihedral angle constraint information a new closed form solution to the problem is proposed the method also provides a general analytic technique for dealing with a class of problem of shape from inverse perspective projection by using angle to angle correspondence information simulation experiment show that the author method is effective and robust for real application 
inspired by the success of the distributed computing community in applying logic of knowledge and time to reasoning about distributed protocol we aim for a similarly powerful and high level abstraction when reasoning about control problem involving uncertainty here we concentrate on robot motion planning with uncertainty in both control and sensing this problem ha already been well studied within the robotics community our contribution include the following we define a new natural problem in this domain obtaining a sound and complete termination condition given initial and goal location we consider a specific class of simple motion plan in rn from the literature and provide necessary and sufficient condition for the existence of sound and complete termination condition for plan in that class we define a high level language a logic of time and knowledge to reason about motion plan in the presence of uncertainty and use them to provide general condition for the existence of sound and complete termination condition for a broader class of motion plan 
incorporating declarative bias or priorknowledge into learning is an active researchtopic in machine learning treestructuredbias specifies the prior knowledgeas a tree of quot relevance quot relationshipsbetween attribute this paper present alearning algorithm that implement treestructuredbias i e learns any target functionprobably approximately correctly fromrandom example and membership queriesif it obeys a given tree structured bias thetheoretical prediction of the paper are 
vision algorithm are often developed in a bayesian framework two estimator are commonly used maximum a posteriori map and minimum mean squared error mmse we argue that neither is appropriate for perception problem the map estimator make insufficient use of structure in the posterior probability the squared error penalty of the mmse estimator doe not reflect typical penalty we describe a new estimator which we call maximum local mass mlm which integrates the local probability density the mlm method is sensitive to local structure of the posterior probability which map is not the new method us an optimality criterion that is appropriate for perception task it find the most probable approximately correct answer for the case of low observation noise we provide an efficient approximation we apply this new estimator to color constancy an unknown illuminant fall on surface of unknown color we seek to estimate both the illuminant spectrum and the surface spectrum from photosensor response which depend on the product of the unknown spectrum in simulation we show that the mlm method performs better than the map estimator and better than two standard color constancy algorithm the mlm method may prove useful in other vision problem a well 
we show how a special decomposition of general projection matrix called canonic enables u to build geometric description for a system of camera which are invariant with respect to a given group of transformation these representation are minimal and capture completely the property of each level of description considered euclidean in the context of calibration and in the context of structure from motion which we distinguish clearly affine and projective that we also relate to each other in the last case a new decomposition of the well known fundamental matrix is obtained dependency which appear when three or more view are available are studied in the context of the canonic decomposition and new composition formula are established a well a the link between local ie for pair of view representation and global ie for a sequence of image representation 
though arrangement knowledge is well suited for qualitative representation of spatial situation if we only use this kind of knowledge we cannot do interesting inference about relative position of point in the plane for example if we know the orientation of two triangle over four point we cannot say anything about the orientation of the other two triangle in this paper we show that the augmentation of arrangement knowledge by qualitative angle lead to interesting and useful inference 
we describe our implementation of a parallel depth recovery scheme for a four camera multibaseline stereo in a convergent configuration our system is capable of image capture at video rate this is critical in application that require three dimensional tracking we obtain dense stereo depth data by projecting a light pattern of frequency modulated sinusoidally varying intensity onto the scene thus increasing the local discriminability at each pixel and facilitating match in addition we make most of the camera view area by converging them at a volume of interest result show that we are able to extract stereo depth data that are on the average le than mm in error at distance between to m away from the camera 
the tipster collection is unusual because of both it size and detail in particular it describes a set of information need a opposed to traditional query these detailed representation of information need are an opportunity for research on different method of formulating query this paper describes several method of constructing query for the inquery information retrieval system and then evaluates those method on the tipster document collection both adhoc and routing query processing method are evaluated 
the use of a multistage diffusion process in the early processing of range data is examined the input range data are interpreted a occupying a volume in d space each diffusion stage simulates the process of diffusing part of the boundary of the volume into the volume the outcome of the process can be used for both discontinuity detection and segmentation into shape homogeneous region the process is applied to synthetic noise free and noisy step roof and valley edge a well a to real range image 
this article describes a connectionist method for refining algorithm represented a generalized finite state automaton the method translates the rule like knowledge in an automaton into a corresponding artificial neural network and then refines the reformulated automaton by applying backpropagation to a set of example this technique for translating an automaton into a network extends the kbann algorithm a system that translates a set of prepositional rule into a corresponding neural network the extended system fskbann allows one to refine the large class of algorithm that can be represented a state based process a a test fskbann is used to improve the chou fasman algorithm a method for predicting how globular protein fold empirical evidence show that the multistrategy approach of fskbann lead to a statistically significantly more accurate solution than both the original chou fasman algorithm and a neural network trained using the standard approach extensive statistic report the type of error made by the chou fasman algorithm the standard neural network and the fskbann network 
an approach is presented to learning high dimensional function in the case where the learning algorithm can affect the generation of new data a local modeling algorithm locally weighted regression is used to represent the learned function architectural parameter of the approach such a distance metric are also localized and become a function of the query point instead of being global statistical test are given for when a local model is good enough and sampling should be moved to a new area our method explicitly deal with the case where prediction accuracy requirement exist during exploration by gradually shifting a center of exploration and controlling the speed of the shift with local prediction accuracy a goal directed exploration of state space take place along the fringe of the current data support until the task goal is achieved we illustrate this approach with simulation result and result from a real robot learning a complex juggling task 
this paper present an algorithm for learning the probability of optional phonological rule from corpus the algorithm is based on using a speech recognition system to discover the surface pronunciation of word in speech corpus using an automatic system obviates expensive phonetic labeling by hand we describe the detail of our algorithm and show the probability the system ha learned for ten common phonological rule which model reduction and coarticulation effect these probability were derived from a corpus of sentence of read speech from the wall street journal and are shown to be a reasonably close match to probability from phonetically hand transcribed data timit finally we analyze the probability difference between rule use in male versus female speech and suggest that the difference are caused by differing average rate of speech 
the feature correspondence problem is a classic hurdle in visualobject recognition concerned with determining the correct mappingbetween the feature measured from the image and the feature expectedby the model in this paper we show that determining goodcorrespondences requires information about the joint probabilitydensity over the image feature we propose quot likelihood basedcorrespondence matching quot a a general principle for selecting optimalcorrespondences the approach is 
we consider the following problem how should an observerchange viewpoint in order to generate a dense imagesequence of an arbitrary smooth surface so that it can be incrementallyreconstructedusing the occluding contour and theepipolar parameterization we present a collection of qualitativebehaviors that when integrated appropriately purposefullycontrol viewpoint based on the appearance of the surfacein order to provably solve this problem 
this paper investigates several method for coping with inconsistency caused by multiple source information by introducing suitable consequence relation capable of inferring non trivial conclusion from an inconsistent stratified knowledge base some of these method presuppose a revision step namely a selection of one or several consistent subset of formula and then classical inference is used for inferring from these subset two alternative method that do not require any revision step are studied inference based on argument and a new approach called safely supported inference where inconsistency is kept local these two last method look suitable when the inconsistency is due to the presence of several source of information the paper offer a comparative study of the various inference mode under inconsistency 
a qualitative approach to visually guided navigationbased on the computation of optical flow field is presented the approach is based on the use of two camerasmounted on a mobile robot and with the optical axisdirected in opposite direction such that the two visualfields do not overlap divergent stereo range computationis based on the computation of the apparentimage speed on image acquired during robot s motion an example of reflex type control of motion drivenby differential 
we introduce a spatial representation s map for an indoor navigation robot the s map represents the location of obstacle in a planar domain where obstacle are defined a any object that can block movement of the robot in building the s map the viewing triangle constraint and the stability constraint are introduced for efficient verification of vertical surface these verified vertical surface and d segment of obstacle smaller than a robot are mapped to the s map by simply dropping height information thus the s map is made directly from d segment with simple verification and represents obstacle in a planar domain so that it becomes a navigable map for the robot without further processing in addition to efficient map building the s map represents the environment more realistically and completely furthermore the s map convert many navigation problem in d such a map fusion and path planning into d one we present the analysis of the s map in term of complexity and reliability and discus it pro and con moreover we show the result of the s map for indoor environment 
in extracting a polynomial surface patch near an intensity or range discontinuity a robust estimator must tolerate not only the truly random bad data random outlier but also the coherently structured point pseudo outlier that belong to a different surface to characterize the performance of least median of square m estimator hough transforms ransac and minpran on data containing both random and pseudo outlier we develop two analytical measure pseudo outlier bias and pseudo outlier breakdown using these measure we find that each robust estimator ha surprisingly poor performance even under the best possible circumstance implying that present estimator should be used with care and new estimator should be developed 
so far tractable planning problem reported in the literature have beendefined by syntactical restriction to better exploit the inherent structurein problem however it is probably necessary to study also structuralrestrictions on the state transition graph such restriction aretypically computationally hard to test though since this graph is ofexponential size we take an intermediate approach using a statevariablemodel for planning and restricting the state transition graph 
this paper present a new approach to inductive learning that combine aspect of instancebased learning and rule induction in a single simple algorithm the rise system search for rule in a specific to general fashion starting with one rule per training example and avoids some of the difficulty of separate and eonquer approach by evaluating each proposed induction step globally i e through an efficient procedure that is equivalent to checking the accuracy of the rule set a a whole on every training example classification is performed using a best match strategy and reduces to nearest neighbor if all generalization of instance were rejected an extensive empirical study show that rise consistently achieves higher accuracy than state of the art representative of it parent paradigm pebls and cn and also outperforms a decision tree learner c in out of test domain in with confidence 
we propose a statistical mechanical framework for the modelingof discrete time series maximum likelihood estimation is done viaboltzmann learning in one dimensional network with tied weight we call these network boltzmann chain and show that theycontain hidden markov model hmms a a special case ourframework also motivates new architecture that address particularshortcomings of hmms we look at two such architecture parallel chain that model feature set with disparate time 
simplex mesh are simply connected mesh thatare topologically dual of triangulation in a previouswork we have introduced the simplex meshrepresentation for performing recognition of partiallyoccluded smooth object in this paper wepresent a physically based approach for recoveringthree dimensional object based on the geometry ofsimplex mesh elastic behavior is modelled by localstabilizing functionals controlling the mean curvaturethrough the simplex angle extracted at 
this paper describes an approach to automatically learn planning operator by observing expert solution trace and to further refine the operator through practice in a learning by doing paradigm this approach us the knowledge naturally observable when expert solve problem without need of explicit instruction or interrogation the input to our learning system are the description language for the domain expert problem solving trace and practice problem to allow learning by doing operator refinement given these input our system automatically acquires the precondition and effect including conditional effect and precondition of the operator we present empirical result to demonstrate the validity of our approach in the process planning domain these result show that the system learns operator in this domain well enough to solve problem a effectively a human expert coded operator our approach differs from knowledge acquisition toolsin that it doe not require a considerable amount of direct interaction with domain expert it differs from other work on automaticallylearning operator in that it doe not require initial approximate planning operator or strong background knowledge 
although ai planning technique can potentially be useful in several manufacturing domain this potential remains largely unrealized in order to adapt ai planning technique to manufacturing it is important to develop more realistic and robust way to address issue important to manufacturing engineer furthermore by investigating such issue ai researcher may he able to discover principle that are relevant for ai planning in general a an example in this paper we describe the technique for manufacturing operation planning used in imacs interactive manufacturability analysis and critiquing system and compare and contrast them with the technique used in classical ai planning system we describe how one of imacs s planning technique may be useful for ai planning in general and a an example we describe how it help to explain a puzzling complexity result in ai planning 
this paper s main result is to show that under the condition imposed by the maloneywandell color constancy algorithm color constancy can in fact he expressed in term of a simple independent adjustment of the sensor response in other word a a uon kries adaptation type of coeficient rule algorithm so long a the sensor space is first transformed to a new hasis our overall goal is to present a theoretical analysis connecting many established theory of color constancy for the case where surface refiectances are dimensional and illuminant are dimensional we prove that perfect color constancy can always be solved for by an independent adjustment of sensor response which mean that the color constancy transform can he expressed a a diagonal matrix this result requires a prior transformation of the sensor basis and to support it we show in particular that there exists n transformation of the original sensor basis under which the non diagonal meth od of maloneywandell forsyth s mwext and funt and drew s lightness algorithm all reduce to simpler diagonal matrix theones of color constancy our result are strong in the sense that no constraint is placed on the initial sensor spectral sensitzuities in addition to purely theoretical argument the paper contains result from simulation of diagonal matrix based color constancy in which the spectrum of real illuminant and refiectances along with the human cone sensitivity function are used the simulation demonstrate that when the cone sensor space is transformed to it new basis in the appropriate manner a diagonal matrix support close to optimal color constancy 
we present a method for navigating a robot from an initial position to a specified landmark in it visual field using a sequence of monocular image the location of the landmark with respect to the robot is determined using the change in size and location of the landmark in the image a a function of the motion of the robot the landmark location is estimated after the first three image are taken and this estimate is refined a the robot move the method can correct for error in the robot motion a well a navigate around obstacle the obstacle avoidance is done using bump sensor sonar and dead reckoning rather than visual servoing the method doe not require prior calibration of the camera we show some example of the operation of the system 
the scientific study of biological system offer an approach to the development of sensor based robot that is complementary to the more formal analytic method currently favoured by roboticists i initially propose several general lesson from the biological field next i consider a specific example selected from the work of lederman klatzky which focus on human haptic object processing an empirical base and recent theoretical development from our research program on this topic are described the human haptic system is an information processing system that combine input from sensor in skin muscle tendon and joint with motor capability to extract different object property a general model of human haptic object identification which emphasis how object exploration is controlled is presented the model describes major architectural element including representation of haptically accessible object property and exploratory procedure eps which are dedicated movement pattern specialized to extract particular property these architectural unit are related in processing specific way the resulting architecture is treated a a system of constraint which guide the exploration of an object during the course of identification empirical support for the model is also examined to conclude i show how this scientifically based approach might be applied to developing strategy for active manual robotic exploration of unstructured environment 
a sensory system consisting of a camera and several laser beam is described it is designed for estimating the parameter of a planar surface with respect to the camera estimation is possible when the beam position and direction are known a well a the image of the beam spot on a planar surface the system is readily applicable in an industrial environment for automation if it is attached to a robot arm and is connected to a computer 
a fast simulated annealing algorithm is developed for automatic object recognition the object recognition problem is addressed a the problem of best describing a match between a hypothesized object and an image the normalized correlation coefficient is used a a measure of the match template are generated on line during the search by transforming model image simulated annealing reduces the search time by order of magnitude with respect to an exhaustive search the algorithm is applied to the problem of how landmark e g traffic sign can be recognized by a navigating robot we illustrate the performance of our algorithm with real world image of complicated scene with traffic sign false positive match occur only for template with very small information content to avoid false positive match we propose a method to select model image for robust object recognition by measuring the information content of the model image the algorithm work well in noisy image for model image with high information content 
gelfond and lifschitz introduce a declarative language a for describing effect of action and define a translation of theory in this language into extended logic program elp s the purpose of this paper is to extend the language and the translation to allow reasoning about the effect of concurrent action logic programming formalization of situation calculus with concurrent action presented in the paper can be of independent interest and may serve a a test bed for the investigation of various transformation and logic programming inference mechanism 
the author investigate the computational time complexity of the labeling problem for line drawing of polyhedral scene it is found that line drawing can be labeled in time proportional to the number of segment once the vanishing point associated to the possible direction for the edge are known the vanishing point can be given a priori otherwise they can in many case be detected by standard technique from the line drawing itself the np completeness of the labeling problem for line drawing of trihedral scene kirousis and papadimitriou is then due to the lack of knowledge about the vanishing point which is equivalent to the knowledge of the possible direction for the edge these result help draw a more accurate boundary between the problem in the interpretation of line drawing that are polynomially solvable and those that are np complete 
figure ground segmentation is a fundamental problem in computer vision the main difficulty is the integration of low level pixel based local image feature to obtain global object based description active contour in the form of snake balloon and level set modeling technique have been proposed that satisfactorily address this question for certain application however these method require manual initialization do not always perform well near sharp protrusion or indentation or often cross gap we propose an approach inspired by these method and a shock based representation of shape in term of part protrusion and bend since initially it is not clear where the object or their part are part are hypothesized in the form of fourth order shock randomly initialized in homogeneous area of image these shock then form evolving contour or bubble which grow shrink merge split and disappear to capture the object in the image in the homogeneous area of the image bubble deform by a reaction diffusion process in the inhomogeneous area indicated by differential property computed from low level process such a edge detection texture optical flow and stereo etc bubble do not deform a such the randomly initialized bubble integrate low level information and in the process segment the figure from the ground 
this paper concern learning task that require the prediction of a continuous value rather than a discrete class a general method is presented that allows prediction to use both instance based and model based learning result with three approach to constructing model and with eight datasets demonstrate improvement due to the composite method 
a decision list is an ordered list of conjunctiverules rivest inductive algorithm such a aqand cn learn decision list incrementally one ruleat a time such algorithm face the rule overlap problem the classification accuracy of the decision listdepends on the overlap between the learned rule thus even though the rule are learned in isolation they can only be evaluated in concert existing algorithmssolve this problem by adopting a greedy iterativestructure once a 
over the last few year constraint based grammar formalism have become the predominant paradigm in natural language processing and computational linguistics from the viewpoint of computer science typed feature structure can be seen a data structure that allow the representation of linguistic knowledge in a uniform fashion type expansion is an operation that make constraint of a typed feature structure explicit and determines it satisfiability we describe an efficient expansion algorithm that take care of recursive type definition and permit the exploration of different expansion strategy through the use of control knowledge this knowledge is specified on a separate layer independent of grammatical information the algorithm a presented in the paper ha been full implemented in common lisp and is an integrated part of the typed feature formalism tdc that is employed in several large nl project 
we consider the question how should one act when the only goal is to learn a much a possible building on the theoretical result of fedorov and mackay we apply technique from optimal experiment design oed to guide the query action selection of a neural network learner we demonstrate that these technique allow the learner to minimize it generalization error by exploring it domain efficiently and completely we conclude that while not a panacea oed based query action ha much to offer especially in domain where it high computational cost can be tolerated 
several widely accepted modal nonmonotonic logic for reasoning about knowledge and belief of rational agent with introspection power are based on strong modal logic such a kd s s f and s in this paper we argue that weak modal logic without even the axiom k and therefore below the range of normal modal logic also give rise to useful non monotonic system we study two such logic the logic n containing propositional calculus and necessitation but no axiom schema for manipulating the modality and the logic nt the extension of n by the schema t for the nonmonotonic logic n and nt we develop minimal model semantics we use it to show that the nonmonotonic logic n and nt are at least a expressive a autoepistemic logic reflexive autoepistemic logic and default logic in fact each can be regarded a a common generalization of these classic nonmonotonic system we also show that the nonmonotonic logic n and nt have the property of being conservative with respect to adding new definition and prove that computationally they are equivalent to autoepistemic and default logic 
in recent time there ha been an increase in the number of natural language generation system that take into consideration a user s inference the statement generated by these system are typically connected by inferential link which are opportunistic in nature in this paper we describe a discourse structuring mechanism which organizes inferentially linked statement a well a statement connected by certain prescriptive link our mechanism first extract relation and constraint from the output of a discourse planner it then us this information to build a directed graph whose node are rhetorical device and whose link are the relation between these device the mechanism then applies a search procedure to optimize the traversal through the graph this process generates an ordered set of linear discourse sequence where the element of each sequence are maximally connected our mechanism ha been implemented a the discourse organization component of a system called wishful which generates concept explanation 
a semantics for tense modality and aspect in natural language must capture causal and contingent relation between event and state a welt a merely temporal one the paper investigates a non reified dynamic logic based formulation of the situation calculus is a formalism for a computational semantics for a number of temporal category in english and suggests that some recent claim that dynamic logic are inherently unsuitable for this purpose have taken too narrow a view of the situation calculus 
we present an algorithm for automatic word sense disambiguation based on lexical knowledge contained in wordnet and on the result of surface syntactic analysis the algorithm is part of a system that analyzes text in order to acquire knowledge in the presence of a little pre coded semantic knowledge a possible on the other hand we want to make the besl use of public domain information source such a wordnet rather than depend on large amount of hand crafted knowledge or statistical data from large corpus we use syntactic information and information in wordnet and minimize the need for other knowledge source in the word sense disambiguation process we propose to guide disambiguation by semantic similarity between word and heuristic rule based on this similarity the algorithm ha been applied to the canadian income tax guide test result indicate that even on a relatively small text the proposed method produce correct noun meaning more than of the time 
we are developing special purpose low power analog to digitalconverters for speech and music application that feature analogcircuit model of biological audition to process the audio signalbefore conversion this paper describes our most recent converterdesign and a working system that us several copy of the chip tocompute multiple representation of sound from an analog input this multi representation system demonstrates the plausibility ofinexpensively implementing an auditory 
we describe a framework for real time tracking of facial exp ressions that us neurally inspired correlation and interpolatio n method a distributed view based representation is used to characte rize facial state and is computed using a replicated correlation network the ensemble response of the set of view correlation score is input to a ne twork based interpolation method which map perceptual state to motor control state for a simulated d face model activation level of the motor state correspond to muscle activation in an anatomically derived model by integrating fast and robust d processing with d model we obtain a system that is able to quickly track and interpret complex fa cial motion in real time 
human chess player exhibit a large variation in the amount of time they allocate for each move yet the problem of devising resource allocation strategy for game playing did not receive enough attention in this paper we present a framework for studying resource allocation strategy we define allocation strategy and identify three major type of strategy static semi dynamic and dynamic we then proceed to describe a method for learning semi dynamic strategy from self generated example the method assigns class to the example based on the utility of investing extra resource the method wa implemented in the domain of checker and experimental result show that it is able to learn strategy that improve game playing performance 
learning of continuous valued function using neural network ensemble committee can give improved accuracy reliable estimationof the generalization error and active learning the ambiguityis defined a the variation of the output of ensemble member averagedover unlabeled data so it quantifies the disagreement amongthe network it is discussed how to use the ambiguity in combinationwith cross validation to give a reliable estimate of the ensemblegeneralization error and how this 
we propose a new procedure for proof by induction in conditional theory where case analysis is simulated by term rewriting this technique reduces considerably the number of variable of a conjecture to be considered for applying induction scheme inductive position our procedure is presented a a set of inference rule whose correctness ha been formally proved moreover when the axiom are ground convergent and the defined function are completely defined over free constructor it is possible to apply the system for refuting conjecture the procedure is even refutationally complete for conditional equation with boolean precondition under the same hypothesis the method is entirely implemented in the prover spike this system ha proved interesting example in a completely automatic way that is without interaction with the user and without ad hoc heuristic it ha also proved the challenging gilbreath card trick with only easy lemma 
most connectionist research ha focused on learning mapping from one space to another eg classification and regression this paper introduces the more general task of learning constraint surface it describes a simple but powerful architecture for learning and manipulating nonlinear surface from data we demonstrate the technique on low dimensional synthetic surface and compare it to nearest neighbor approach we then show it utility in learning the space of lip image in a system for improving speech recognition by lip reading this learned surface is used to improve the visual tracking performance during recognition 
we present a new incremental radial basis function network suitablefor classification and regression problem center positionsare continuously updated through soft competitive learning thewidth of the radial basis function is derived from the distanceto topological neighbor during the training the observed erroris accumulated locally and used to determine where to insert thenext unit this lead in case of classification problem to theplacement of unit near class border 
we present a new approach to relative stereo and motion reconstruction from a discrete set of point correspondence in completely uncalibrated pair of image this approach also yield new projective invariant and we present some application to object recognition finally we introduce a new approach to camera self calibration from two image which allows full metric reconstruction up to some unknown scale factor we have implemented the proposed method and present example using real image 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
agent interacting with an incompletely known dynamic world need to be able to reason about the effect of their action and to gain further information about that world using sensor of some sort unfortunately sensor information is inherently noisy and in general serf only to increase the agent s degree of confidence in various proposition build ing on a general logical theory of action formalized in the sit uation calculus developed by reiter and others we propose a simple axiomatization of the effect on an agent s state of belief of taking a reading from a noisy sensor by exploiting reiter s solution to the frame problem we automatically obtain that these sensor action leave the rest of the world unaffected and further that non sensor action change the state of belief of the agent in appropriate way 
reactive deliberation is a novel robot architecture that ha been designed to overcome some of the problem posed by dynamic robot environment it is argued that the problem of action selection in nontrivial domain cannot be intelligently resolved without attention to detailed planning experimental evidence is provided that the goal and action of a robot must be evaluated at a rate commensurate with change in the environment the goal oriented behaviour of reactive deliberation are a useful abstraction that allow sharing of scarce computational resource and effective goal arbitration through inter behaviour bidding the effectiveness of reactive deliberation ha been demonstrated through a tournament of one on one soccer game between real world robot soccer is a dynamic environment the location of the ball and the robot are constantly changing the result suggest that the architectural element in reactive deliberation are sufficient for real time intelligent control in dynamic environment 
we introduce a constructive incremental learning system for regression problem that model data by mean of locally linear expert in contrast to other approach the expert are trained independently and do not compete for data during learning only when a prediction for a query is required do the expert cooperate by blending their individual prediction each expert is trained by minimizing a penalized local cross validation error using second order method in this way an expert is able to find a local distance metric by adjusting the size and shape of the receptive field in which it prediction are valid and also to detect relevant input feature by adjusting it bias on the importance of individual input dimension we derive asymptotic result for our method in a variety of simulation the property of the algorithm are demonstrated with respect to interference learning speed prediction accuracy feature detection and task oriented incremental learning 
abstract most stereo algorithm do not take into ac count discontinuity in disparity and the fact that there are half occlusion consisting of area seen by one eye but not the other at the same time very few of them are formulated using the framework of energy functionals which are so succesfully used in other area of computer vision such a image segmentation and surface represen tation in this paper a formulation is presented within such a framework taking into account the discontinuity and half occlusion the formulation follows directly from the assumption that when matching the left and right im age the order of point must be preserved a model is derived consisting of two coupled energy functionals cor responding to the two eye they are coupled in the sense that the discontinuity locus determined by one eye also determines the occluded area in the image seen by the other eye a nonlinear system of diffusion equation is de rived by simultaneously applying gradient descent to these functionals the diffusion equation are implemented by a straight forward finite difference scheme 
a distributed system of computer play an increasingly important role in society it will be necessary to consider way in which these machine can be made to interact effectively especially when the interacting machine have been independently designed it is essential that the interaction environment be conducive to the aim of their designer these designer might for example wish their machine to behave efficiently and with a minimum of overhead required by the coordination mechanism itself the rule of interaction should satisfy these need and others formal tool and analysis can help in the appropriate design of these rule we here consider how concept from field such a decision theory and game theory can provide standard to be used in the design of appropriate negotiation and interaction environment this design is highly sensitive to the domain in which the interaction is taking place different interaction mechanism are suitable for different domain if attribute like efficiency and stability are to be maintained 
it is known that rotationally symmetric surface canbe recognized from their outline alone using crossratio s of bitangent intersection this paper demonstratesa successful implementation of this technique using a novel bitangent finder that work on imagesof real scene we report on the stability of the crossratios and compare this to affine invariant therecognition technique is shown to extend to the caseof straight homogeneous generalised cylinder introductionthis paper 
the ai lab at chicago ha begun development of a new set of software agent designed to man age the flood of data colloquially called the in formation superhighway our approach take it lead from case based technology riesbeck and schank hammond kolodner in that we are building system that em phasize the use of example over explicit query or question for communicating with the user 
model of complex physical system often cannot be defined precisely either because of lack of knowledge or because the system parameter change over time according to unknown phenomenon such system can be represented by semi quantitative model that combine both qualitative and quantitative knowledge this paper present numerical interval simulation a method that can produce tight prediction of system involving nonmonotonic function we present a successful application of ni to predict the behavior of a complex process at a brazilian japanese steel company we claim that such capability of simulating nonmonotonic function is fundamental in order to handle real world complex industrial process 
the complex sentence of news wire report contain floating content unit that appear to be opportunistically placed where the form of the surrounding text allows we present a corpus analysis that identified precise semantic and syntactic constraint on where and how such information is realized the result is a set of revision tool that form the rule base for a report generation system allowing incremental generation of complex sentence 
the paper present a framework for a system that describes object in a qualitative fashion a subset of spatial preposition is chosen and an appropriate quantification is applied to each of them that capture their inherent qualitative property the quantification use such object attribute a area center and elongation property the familiar zeroth first and second order moment are used to characterize these attribute the paper detail how and why the particular quantification were chosen since spatial preposition are by their nature rather vague and dependent on context a technique for fuzzifying the definition of the spatial preposition is explained finally an example task is chosen to illustrate the appropriateness of the quantification technique 
we apply reinforcement learning method to learn domain specific heuristic for job shop scheduling a repair based scheduler start with a critical path schedule and incrementally repair constraint violation with the goal of finding a short conflict free schedule the temporal difference algorithm td is applied to tram a neural network to learn a heuristic evaluation function over state this evaluation function is used by a one step lookahead search procedure to find good solution to new scheduling problem we evaluate this approach on synthetic problem and on problem from a nasa space shuttle pay load processing task the evaluation function is trained on problem involving a small number of job and then tested on larger problem the td scheduler performs better than the best known existing algorithm for this task zwehen s iterative repair method based on simulated annealing the result suggest that reinforcement learning can provide a new method for constructing high performance scheduling system 
we discus two type of algorithm for selectingrelevant example that have been developed in thecontext of computation learning theory the examplesare selected out of a stream of examplesthat are generated independently at random thefirst two algorithm are the so called quot boosting quot algorithm of schapire schapire and freund freund and the query by committeealgorithm of seung seung et al we describethe algorithm and some of their provenproperties 
derivation replay wa first proposed by carbonell a a method of transferring guidance from a previous problemsolving episode to a new one subsequent implementation have used state space planning a the underlying methodology this paper is motivated by the acknowledged superiority of partial order po planner in plan generation and is an attempt to bring derivation replay into the realm of partial order planning here we develop dersnlp a framework for doing replay in snlp a partial order plan space planner and analyze it relative effectiveness we will argue that the decoupling of planning derivation order and the execution order of plan step provided by partial order planner enables dersnlp to exploit the guidance of previous case in a more efficient and straightforward fashion we validate our hypothesis through empirical comparison between dersnlp and two replay system based on state space planner 
this paper describes a method of pattern recognitiontargeted for recognizing complex annotationsfound in paper document our investigation is motivatedby the high reliability required for accomplishingautonomous interpretation of map and engineeringdrawings our approach includes a strategy basedon multiscale representation obtained by hexagonalwavelet analysis a feasibility study is described in which more than pattern were recognized with an error rate of by a neural 
ambiguity in the solution of inverse problem arises when data are insufficient to define a unique solution i e the problem is ill posed data fusion ha the potential to reduce this ambiguity by using other sensory data that complement the original data this paper examines the application of data fusion to limited angle computed tomography ct to resolve ambiguity while ct in it conventional form is ill posed with a small null space limited angle ct ha a much larger null space structure that lie primarily in the null space of the limited angle radon transform are particularly prone to ambiguity we describe a novel constraint based data fusion system that fuse spatial support and ultrasound measurement with x ray data the ensuing problem is le ambiguous ha a reduced null space and permit accurate reconstruction of a sandwich structure where otherwise impossible 
we present a novel statistical and variational approach to image segmentation based on a new algorithm named region competition this algorithm is derived by minimizing a generalized bayes mdl minimum description length criterion using the variational principle we show that existing technique in early vision such a snake balloon model region growing and bayes mdl are addressing different aspect of the same problem and they can be unified within a common statistical framework which combine their advantage we analyze how to optimize the precision of the resulting boundary location by studying the statistical property of the region competition algorithm and discus what are good initial condition for the algorithm our method is generalized to color and texture segmentation and is demonstrated on grey level image color image and texture image 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
parka a frame based knowledge representation system implemented on the connection machine provides a representation language consisting of concept description frame and binary relation on those description slot the system is designed explicitly to provide extremely fast property inheritance inference capability parka performs fast recognition query of the form find all frame satisfying p property constraint in o d p time proportional only to the depth d of the knowledge base kb and independent of it size for conjunctive query of this type parka s performance is measured in tenth of a second even for kb with frame with similar result for timing on the cyc kb because parka s run time performance is independent of kb size it promise to scale up to arbitrarily larger domain with such run time performance we believe parka is a contender for the title of fastest knowledge representation system in the world 
the output of handwritten word recognizers hwr tends to be very noisy due to variousfactors in order to compensate for thisbehaviour several choice of the hwr mustbe initially considered in the case of handwrittensentence phrase recognition linguisticconstraints may be applied in order to improvethe result of the hwr this paper discussestwo statistical method of applying syntacticconstraints to the output of an hwr on inputconsisting of sentence phrase both methodsare 
we consider the problem of matching perspective view of coplanar structure composed of line segment both model to image and image to image correspondence matching are given a consistent treatment these matching scenario generally require discovery of an eight parameter projective mapping however when the horizon line of the object plane can be found in the image done here using vanishing point analysis these problem reduce to a simpler six parameter affine matching problem when the intrinsic lens parameter of the camera are known the problem further reduces to four parameter affine similarity matching 
we develop a language model using probabilistic context free grammar pcfgs that is pseudo context sensitive in that the probability that a non terminal n expands using a rule r depends on n s parent we derive the equation for estimating the necessary probability using a variant of the inside outside algorithm we give experimental result showing that beginning with a high performance pcfg one can develop a pseudo pcsg that yield significant performance gain analysis show that the benefit from the context sensitive statistic are localized suggesting that we can use them to extend the original pcfg experimental result confirm that this is both feasible and the resulting grammar retains the performance gain this implies that our scheme may be useful a a novel method for pcfg induction 
the game of go ha a high branching factor that defeat the tree search approach used in computer chess and long range spatiotemporal interaction that make position evaluation extremely difficult development of conventional go program is hampered by their knowledge intensive nature we demonstrate a viable alternative by training network to evaluate go position via temporal difference td learning our approach is based on network architecture that reflect the spatial organization of both input and reinforcement signal on the go board and training protocol that provide exposure to competent though unlabelled play these technique yield far better performance than undifferentiated network trained by selfplay alone a network with le than weight learned within game of x go a position evaluation function that enables a primitive one ply search to defeat a commercial go program at a low playing level 
abstract flexible operation of a robotic agent in an uncalibratedenvironment requires the ability to recover unknown or partiallyknown parameter of the workspace through sensing ofthe sensor available to a robotic agent visual sensor provideinformation that is richer and more complete than othersensors in this paper we present robust technique for thederivation of depth from feature point on a target s surfaceand for the accurate and high speed tracking of moving target we use these 
in differential geometry curve are characterized a mapping from an interval to the plane in topology curve are characterized a a hausdorff space with certain countability property neither of these definition capture the role that curve play in vision however in which curve can denote simple object such a a straight line or complicated object such a a jumble of string the difference between these situation is in part a measure of their complexity and in part a measure of their dimensionality note that the map defining such curve is unknown a is the proper way to represent them we propose a formal complexity theory of curve appropriate for computational vision in general and for problem like separating straight line from jumble in particular the theory is applied to the problem of perceptual grouping 
in this paper we present some novel application of explanation based learning ebl technique to parsing lexicalized tree adjoining grammar the novel aspect are a immediate generalization of par in the training set b generalization over recursive structure and c representation of generalized par a finite state transducer a highly impoverished parser called a stapler ha also been introduced we present experimental result using ebl for different corpus and architecture to show the effectiveness of our approach 
many researcher have noted the importance of combining inductive and analytical learning yet we still lack combined learning method that are effective in practice we present here a learning method that combine explanation based learning from a previously learned approximate domain theory together with inductive learning from observation this method called explanation based neural network learning ebnn is based on a neural network representation of domain knowledge explanation are constructed by chaining together inference from multiple neural network in contrast with symbolic approach to explanation based learning which extract weakest precondition from the explanation ebnn extract the derivative of the target concept with respect to the training example feature these derivative summarize the dependency within the explanation and are used to bias the inductive learning of the target concept experimental result on a simulated robot control task show that ebnn requires significantly fewer training example than standard inductive learning furthermore the method is shown to be robust to error in the domain theory operating effectively over a broad spectrum from very strong to very weak domain theory 
we present a novel robust integrated approach to segmentationshape and motion estimation of articulated object initially we assume the object consists of a singlepart and we fit a deformable model to the given data usingour physic based framework a the object attains newpostures we decide based on certain criterion if and whento replace the initial model with two new model thesecriteria are based on the model s state and the given data we then fit the model to the data using 
we describe the use of smoothing spline analysis of variance ssanova in the penalized log likelihood context for learning estimating the probability p of a outcome given a trainingset with attribute vector and outcome p is of the formp t ef t ef t where if t is a vector of attribute fis learned a a sum of smooth function of one attribute plus asum of smooth function of two attribute etc the smoothingparameters governing f are obtained by an 
in this paper we present method and technique for calibrating camera of a head eye system which ha computer controlled focusing zooming and iris the idea is to build up look up table for intrinsic parameter so we can index them extensive experiment were carried out and result are reported here 
we have recently developed a theory of spatial representation inwhich the position of an object is not encoded in a particular frameof reference but instead involves neuron computing basis functionsof their sensory input this type of representation is ableto perform nonlinear sensorimotor transformation and is consistentwith the response property of parietal neuron we now askwhether the same theory could account for the behavior of humanpatients with parietal lesion these 
in this paper we examine a perceptron learning task the task isrealizable since it is provided by another perceptron with identicalarchitecture both perceptrons have nonlinear sigmoid outputfunctions the gain of the output function determines the level ofnonlinearity of the learning task it is observed that a high levelof nonlinearity lead to overfitting we give an explanation for thisrather surprising observation and develop a method to avoid theoverfitting this method ha two 
work on game playing in ai ha typically ignored game of imperfect information such a poker in this paper we present a framework for dealing with such game we point out several important issue that arise only in the context of imperfect information game particularly the insufficiency of a simple game tree model to represent the player information state and the need for randomization in the player optimal strategy we describe gala an implemented system that provides the user with a very natural and expressive language for describing game from a game description gala creates an augmented game tree with information set which can be used by various algorithm in order to find optimal strategy for that game in particular gala implement the first practical algorithm for finding optimal randomized strategy in two player imperfect information competitive game koller et al the running time of this algorithm is palinomial in the size of the game tree whereas previous algorithm were exponential we present experimental result showing that this algorithm is also efficient in practice and can therefore form the basis for a game playing system 
a multiscale extension to the medial axis transform mat or skeleton can be obtained by combining information derived from a scale space hierarchy of boundary representation w ith region information provided by the mat the skeleton space is constructed by attributing each skeleton component with a hierarchically ordered sequence of residual value each e xpressing the saliency of the component at a distinct resolut ion level since our method amount to a rather symbolic than iconic computation of a multiscale mat it doe not introduce the correspondence problem between distinct level of detail in contrast to other commonly proposed technique our multiscale mat is capable of describing complex shape characterized by significantly jagged boundary furthermore tracking the evolution of prominent locus of the mat such a node across scale permit to ass the most significant skeleton constituent and to automatically determine pruning parameter a salient subset of the mat first order skeleton can be extracted without the need of manual threshold adjustment 
a theory of early stopping a applied to linear model is presented the backpropagation learning algorithm is modeled a gradientdescent in continuous time given a training set and a validationset all weight vector found by early stopping must lie on a certainquadric surface usually an ellipsoid given a training set anda candidate early stopping weight vector all validation set haveleast square weight lying on a certain plane this latter fact canbe exploited to estimate the 
we draw a simple correspondence between kernelrules and prime implicants kernel minimal rule play an important role in many inductiontechniques prime implicants were previouslyused to formally model many other problemdomains including boolean circuit minimizationand such classical ai problem a diagnosis truthmaintenance and circumscription this correspondence allows computing kernelrules using any of a number of prime implicantgeneration algorithm it also lead u to an 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
abstract inthis report we discus the development of an integrated problem solving 
in this paper we define and empirically evaluate new heuristic for solving the job shop scheduling problem with non relaxable time window the hypothesis underlying our approach is that by approaching the problem a one of establishing sequencing constraint between pair of operation requiring the same resource a opposed to a problem of assigning start time to each operation and by exploiting previously developed analysis technique for limiting search through the space of possible sequencing decision simple localized look ahead technique can yield problem solving performance comparable to currently dominating technique that rely on more sophisticated analysis of resource contention we define a series of attention focusing heuristic based on simple analysis of the temporal flexibility associated with different sequencing decision and a similarly motivated heuristic for determining how to sequence a given operation pair performance result are reported on a suite of benchmark problem previously investigated by two advanced approach and our simplified look ahead analysis technique are shown to provide comparable problem solving leverage at reduced computational cost 
explores the reconstruction of object surface from viewed change in surface texture pattern our approach differs from those in the past in that instead of simply producing local estimate of the surface orientation our algorithm recovers complete surface past approach only found the surface orientation locally and therefore did not take advantage of the surface integrability constraint our algorithm doe not assume that the surface texture pattern is isotropic and it doe not assume that the viewed surface is at some point fronto parallel furthermore our algorithm ha mechanism for handling texture boundary and consequently doe not produce erratic result in the region abutting these boundary result on real image are presented demonstrating the potential of our approach 
efficient utilization of cell switched network supporting diverse application will require service discipline that are well designed for the particular quality of service constraint and traffic mix a difficult task in view of the paucity of information about the expected traffic we demonstrate the use of on line dynamic programming in an adaptive cell scheduling mechanism that can easily be engineered to meet arbitrary quality of service constraint when the objective is to minimize the total cell loss rate our algorithm urgency scheduling compare favorably with the optimal earliest deadline first algorithm for more complex quality of service constraint where optimal scheduling algorithm are unavailable the simulation show urgency scheduling can provide significant increase in the usable bandwidth of a link the learning technique we develop are quite general and should be readily applicable to other network control problem 
in this report we present method and technique for calibrating camera of thekth head eye system in particular intrinsic parameter the intention is to buildup look up table for them so that we can index them and no on the job calibrationis needed extension experiment were carried out and result are reported here m x li camera calibration of the kth head eye system introduction and backgroundcamera calibration is useful if not necessary in many computer vision 
it is known that the deformation of the apparent contour of a surface under perspective projection and viewer motion enable the recovery of the geometry of the surface for example by utilising the epipolar parametrization these method break down with apparent contour that are singular i e with cusp in this paper we study this situation in detail and show how nevertheless the surface geometry including the gauss curvature and mean curvature of the surface can be recovered by following the cusp indeed the formula are much simpler in this case and require lower spatio temporal derivative than in the general case of nonsingular apparent contour we give a simulated example and also show that following cusp doe not by itself provide u with information on ego motion 
the main objective of machine discovery is the determination of relation between data and of data model in the paper we describe a method for discovery of data model represented by concurrent system from experimental table the basic step consists in a determination of role which yield a decomposition of experimental data table the component are then used to define fragment of the global system corresponding to a table the method ha been applied for automatic data model discovery from experimental table with petri net a model for concurrency 
this paper proposes a new indicator of text structure called the lexical cohesion profile lcp which locates segment boundary in a text a text segment is a coherent scene the word in a segment are linked together via lexical cohesion relation lcp record mutual similarity of word in a sequence of text the similarity of word which represents their cohesiveness is computed using a semantic network comparison with the text segment marked by a number of subject show that lcp closely correlate with the human judgment lcp may provide valuable information for resolving anaphora and ellipsis 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion is unit refutable there is a polynomial time algorithm for finding minimal explanation a test for unit refutability of clausal theory is presented based on the topology of the connection graph of the theory 
this video first summarizes current research at the university of massachusetts on mobile vehicle navigation using landmark recognition and a partial d world model we then show how landmark and world model might be automatically acquired and updated over time a fundamental goal in robot navigation is to determine the pose of the robot that is the position and orientation of the robot with respect to a d world model such a a hallway in order to determine it pose the robot identifies modeled d landmark such a door and baseboard in a d image of the hallway identifying landmark involves determining correspondence of extracted image line segment with predicted landmark line projected into the image model matching is achieved by a combinatorial optimization technique local search which minimizes the error in the model to data fit from the modeldata feature correspondence thus obtained the d pose of the robot is computed via a non linear optimization procedure the best pose requires that line in the d image lie on the plane formed by the corresponding d landmark line and the camera center robust statistical method are employed to detect outlier extension of the initial partial model over time is achieved by determining the camera pose over a sequence of image while simultaneously tracking new unmodeled feature triangulation is then used to determine the depth of these new feature allowing them to be incorporated into the d model 
constraint satisfaction problem where value are sought for problem variable subject to restriction on which combination of value are acceptable have many application in artificial intelligence conventional learning method acquire individual tuples of inconsistent value these learning experience can be generalized we propose a model of generalized learning based on inconsistency preserving mapping which is sufficiently focused so a to be computationally cost effective rather than recording an individual inconsistency that led to a failure and looking for that specific inconsistency to recur we observe the context of a failure and then look for a related context in which to apply our experience opportunistically a a result we leverage our learning power this model is implemented extended and evaluated using two simple but important class of constraint problem 
thematic knowledge is a basis of semantic interpretation in this paper we propose an acquisition method to acquire thematic knowledge by exploiting syntactic clue from training sentence the syntactic clue which may be easily collected by most existing syntactic processor reduce the hypothesis space of the thematic role the ambiguity may be further resolved by the evidence either from a trainer or from a large corpus a set of heuristic based on linguistic constraint is employed to guide the ambiguity resolution process when a trainer is available the system generates new sentence whose thematic validity can be justified by the trainer when a large corpus is available the thematic validity may be justified by observing the sentence in the corpus using this way a syntactic processor may become a thematic recognizer by simply deriving it thematic knowledge from it own syntactic knowledge 
determining whether a propositional theory is satisfiable is a prototypical example of an np complete problem further a large number of problem that occur in knowledge representation learning planning and other area of ai are essentially satisfiability problem this paper report on a series of experiment to determine the location of the crossover point the point at which half the randomly generated propositional theory with a given number of variable and given number of clause are satisfiable and to ass the relationship of the crossover point to the difficulty of determining satisfiability we have found empirically that for sat the number of clause at the crossover point is a linear function of the number of variable this result is of theoretical interest since it is not clear why such a linear relationship should exist but it is also of practical interest since recent experiment mitchell et al cheeseman et al indicate that the most computationally difficult problem tend to be found near the crossover point we have also found that for random sat problem below the crossover point the average time complexity of satisfiability problem seems empirically to grow linearly with problem size at and above the crossover point the complexity seems to grow exponentially but the rate of growth seems to be greatest near the crossover point 
discovering repetitive interesting and functional substructure in a structural database improves the ability to interpret and compress the data however scientist working with a database in their area of expertise often search for predetermined type of structure or for structure exhibiting characteristic specific to the domain this paper present a method for guiding the discovery process with domain specific knowledge in this paper the jbdufi discovery system is used to evaluate the benefit of using domain knowledge to guide the discovery process the domain knowledge is incorporated into subdue following a single general methodology to guide the discovery process result show that domain specific knowledge improves the search for substructure which are useful to the domain and lead to greater compression of the data to illustrate these benefit example and experiment from the computer programming computer aided design circuit and artificially generated domain are presented 
in the present paper we address the problem of computing structure and motion given a set point correspondence in a monocular image sequence considering small motion when the camera is not calibrated we first set the equation defining the calibration rigid motion and scene structure we then review the motion equation the structure from equation and the depth evolution equation including the particular case of planar structure considering a discrete displacement between two frame a step further we develop the first order expansion of these equation and analyse the observability of the related infinitesimal quantity it is shown that we obtain a complete correspondence between these equation and the equation derived in the discrete case however in the case of infinitesimal displacement the projection of the translation focus of expansion or epipole is clearly separated from the rotational component of the motion this is an important advantage of the present approach using this last property we propose a mechanism of image stabilization in which the rotational disparity is iteratively canceled this allows a better estimation of the focus of expansion and simplifies different aspect of the analysis of the equation structure from motion equation analysis of ambiguity geometrical interpretation of the motion equation 
intelligent artificial agent need to be able to explain and justify their action they must therefore understand the rationale for their own action this paper describes a technique for acquiring this understanding implemented in a multimedia explanation system the system determines the motivation for a decision by recalling the situation in which the decision wa made and replaying the decision under variant of the original situation through experimentation the agent is able to discover what factor led to the decision and what alternative might have been chosen had the situation been slightly different the agent learns to recognize similar situation where the same decision would be made for the same reason this approach is implemented in an artificial fighter pilot that can explain the motivation for it action situation assessment and belief 
this research characterizes the spontaneous spoken disfluency typical of human computer interaction and present a predictive model accounting for their occurrence data were collected during three empirical study in which people spoke or wrote to a highly interactive simulated system the study involved within subject factorial design in which input modality and presentation format were varied spoken disfluency rate during human computer interaction were documented to be substantially lower than rate typically observed during comparable human human speech two separate factor both associated with increased planning demand were statistically related to increased speech disfluency rate length of utterance and lack of structure in the presentation format regression technique revealed that a linear model based simply on utterance length account for over of the variability in spoken disfluency therefore design technique capable of channeling user speech into briefer sentence potentially could eliminate most spoken disfluency in addition the degree of structure in the presentation format wa manipulated in a manner that successfully eliminated to of all disfluent speech the long term goal of this research is to provide empirical guidance for the design of robust spoken language technology 
we present a theory of a modeler s problem decomposition skill in the context of optimal reasonzng the use of qualitative modeling to strategically guide numerical exploration of objective space our technique called activity analysis applies to the pervasive family of linear and non linear constrained optimization problem and easily integrates with any existing numerical approach activity analysis draw from the power of two seemingly divergent perspective the global conflict based approach of combinatorial satisficing search and the local gradientbased approach of continuous optimization combined with the underlying insight of engineering moriotonicity analysis the result is an approach that strategically cut away subspace that it can quickly rule out a suboptimal and then guide the numerical method to the remaining subspace 
although plan space planner have been shown to be flexible and efficient in plan generation they do suffer from the problem of looping that is they may spend an inordinate amount of time doing locally seemingly useful but globally useless refinement in this paper i review the anatomy of looping and argue that looping is intimately tied to the production of non minimal solution i then propose two class of admissible pruning technique based on the notion of plan minimality i show that the first one is admissible for planner which do not protect their establishment but allow a precondition to be reestablished any number of time the second one is admissible for planner which protect their establishment through causal link i also discus the complexity of the proposed pruning strategy and then potential application 
neural network were evolved through genetic algorithm to focus minimax search in the game of othello at each level of the search tree the focus network decide which move are promising enough to be explored further the network effectively hide problem state from minimax based on the knowledge they have evolved about the limitation of minimax and the evaluation function focus network were encoded in marker based chromosome and were evolved against a full width minimax opponent that used the same evaluation function the network were able to guide the search away from poor information resulting in stronger play while examining fewer state when evolved with a highly sophisticated evaluation function of the bill program the system wa able to match bill s performance while only searching a subset of the move 
a new learning algorithm is derived which performs online stochasticgradient ascent in the mutual information between output andinputs of a network in the absence of a priori knowledge aboutthe signal and noise component of the input propagation ofinformation depends on calibrating network non linearity to thedetailed higher order moment of the input density function byincidentally minimising mutual information between output aswell a maximising their individual 
the purpose of this paper is to discus and compare two type of method for reasoning with inconsistent belief base coherence based approach to non monolonic entailment based on the selection and management of consistent subbasis and argumentation system based on the construction and selection of argument in favor of a conclusion we present several argumentation system then we show that most of the associated inference relation can be also defined using well known principle for selecting consistent subbase so we establish a formal correspondence between the so called argumentation paradigm and recent work on nonmonotonic entailment lastly we propose several direction for further research concerning the integration of preference relation into argumentation system 
this paper present a framework for analyzing salient pattern vortex structure in the velocity field of turbulent fluid flow vortex are modeled a rotational motion in the velocity field and concentration in the corresponding vorticity field a pointwise linear model is then built to approximate the kinematics of the flow field locally the vector field are analyzed in term of fluid motion and singular pattern the region of vortex structure are then extracted by identifying those dominated by rotational motion or those of focus type singularity in addition to this d vortex a a special case are detected by searching region of vorticity concentration the algorithm is applied to both d and d experimental and computational turbulent flow 
probabilistic expert system based on bayesian network bns require initial specification both a qualitative graphical structure and quantitative assessment of conditional probability table this paper considers statistical batch learning of the probability table on the basis of incomplete data and expert knowledge the em algorithm with a generalized conjugate gradient acceleration method ha been dedicated to quantification of bns by maximum posterior likelihood estimation for a super class of the recursive graphical model this new class of model allows a great variety of local functional restriction to be imposed on the statistical model which hereby extent the control and applicability of the constructed method for quantifying bns 
an overview is given with new result of mathematical model and algorithm for probabilistic logic probabilistic entailment and various extension analytical and numerical solution are considered the former leading to automated generation of theorem in the theory of probability way to restore consistency and relationship with bayesian network are also studied 
this paper present a markov random field mrf model for object recognition in high level vision thelabeling state of a scene in term of a model object isconsidered a an mrf or couple mrfs within thebayesian framework the optimal solution is definedas the maximum a posteriori map estimate of themrf the posterior distribution is derived based onsound mathematical principle from theory of mrfand probability which is in contrast to heuristic formulation an experimental 
the tracking of moving object in the d space for long term image sequence must be very robust with respect to noise and computational error thus for example autoregressive and newtonian model have been adopted mainly with least square kalman filter and other technique the parameter measured are predicted corrected on the basis of the model adopted which can be adaptive or not in this paper a new method for tracking object in the d space belonging to the class of matching based algorithm with an adaptive prediction correction mechanism is presented the prediction correction is based on d and d motion estimation and both these correction are used for measuring the displacement on the image plane the mechanism proposed is very robust with respect to the accumulation error and thus it is suitable for very long term object tracking 
it ha recently been shown that local search is surprisingly good at f inding satisfying assignment for certain computationally hard class of cnf formula the performance of basic local search method can be further enhanced by introducing mechanisme for escaping from local minimum in the search space we will compare three such mechanism simulated annealing random noise and a strategy called mixed random walk we show that mixed random walk is the superior strategy we also present result demonstrating the effectiveness of local search with walk for solving circuit bynthebib and circuit diagnosis problem finally we demonstrate that mixed random walk improves upon the best known method for solving max sat problem 
more and more low level vision algorithm are being carried out in the spatialfrequency domain using gabor filter there are two basic problem concerned withgabor filterings we will address in this paper one is the window size problem inwhich we will adopt a set of d variable window gabor filter and compare it performancewith those of fixed window filter we will show that the variable windowscheme is more adaptive to image content while fixed window scheme may suffereither 
the paper present a new idea for detecting an unknown human face in input imagery and recognizing his her facial expression represented in the deformation of the two dimensional net called potential net the method deal with the facial information faceness and expression a an overall pattern of the net activated by edge in a single input image of face rather than from change in the shape of the facial organ or their geometrical relationship we build model of facial expression from the deformation pattern in the potential net for face image in the training set of different expression and then project them into emotion space expression of an unknown subject can be recognized from the projection of the net for the image into the emotion space the potential net is further used to model the common human face the mosaic method representing energy in the net is used a a template for finding candidate for the face area and the candidate are verified their faceness by projecting them into emotion space in order to select the finalist precise location of the face is determined by the histogram analysis of vertical and horizontal projection of edge 
many abductive understanding system explain novel situation by a chaining process that is neutral to explainer need beyond generating some plausible explanation for the event being explained this paper examines the relationship of standard model of abductive understanding to the case based explanation model in case based explanation construction and selection of abductive hypothesis are focused by specific explanation of prior episode and by goal based criterion reflecting current information need the case based method is inspired by observation of human explanation of anomalous event during everyday understanding and this paper focus on the method s contribution to the problem of building good explanation in everyday domain we identify five central issue compare how those issue are addressed in traditional and case based explanation model and discus motivation for using the case based approach to facilitate generation of plausible and useful explanation in domain that are complex and imperfectly understood 
new type of test instance generator have been developed for generating random cnf conjunctive normal form formula with controlled attribute in this paper we use these generator to test the performance of local search based sat algorithm for this purpose the generator which produce formula having exactly one satisfying truth assignment is especially desirable it is shown that i among several different strategy of local search the weighting strategy is overwhelmingly faster than the others and that ii local search work significantly better for instance of larger clause variable ratio which allows u to come up with a new strategy for making local search even faster 
model selection is important in many area of supervised learning given a dataset and a set of model for predicting with that dataset we must choose the model which is expected to best predict future data in some situation such a online learning for control of robot or factory data is cheap and human expertise costly cross validation can then be a highly effective method for automatic model selection large scale cross validation search can however be computationally expensive this paper introduces new algorithm to reduce the computational burden of such search we show how experimental design method can achieve this using a technique similar to a bayesian version of kaelbling s interval estimation several improvement are then given including the use of blocking to quickly spot near identical model and schema search a new method for quickly finding family of relevant feature experiment are presented for robot data and noisy synthetic datasets the new algorithm speed up computation without sacrificing reliability and in some case are more reliable than conventional technique 
this paper present a method to incorporate knowledge from possibly imperfect model and domain theory into inductive learning of decision tree for classification the approach assumes that a model or domain theory reflects useful prior knowledge of the task thus the default bias should accept the model prediction a accurate even in the face of somewhat contradictory data which may be unrepresentative or noisy however our approach allows the system to abandon the model or domain theory or portion thereof in the fact of sufficiently contradictory data in particular we use c to induce decision tree from data that ha heen augmented by model or domaintheory derived feature we weakly bias the system to select model derived feature during decision tree induction but this preference is not dogmatically applied our experiment very imperfection in a model the representativeness of data and the veracitv with which model demed feature are preferred 
hinton proposed that generalization in artificial neural net should improve if net learn to represent the domain s underlying regularity abu mustafa s hint work show that the output of a backprop net can be used a input through which domain specific information can be given to the net we extend these idea by showing that a backprop net learning many related task at the same time can use these task a inductive bias for each other and thus learn better we identify five 
in this paper we propose preferential matrix semantics for nonmonotonic inference system and show how this algebraic framework can be used in methodological study of cumulative inference operation 
despite the promising result of numerous application the hitherto proposed snake technique share some common problem snake attraction by spurious edge point snake degeneration shrinking and flattening convergence and stability of the deformation process snake initialization and local determination of the parameter of elasticity we argue here that these problem can be solved only when all the snake aspect are considered the snake proposed here implement a new potential field and external force in order to provide a deformation convergence attraction by both near and far edge a well a snake behaviour selective according to the edge orientation furthermore we conclude that in the case of model based segmentation the internal force should include structural information about the expected snake shape experiment using this kind of snake for segmenting bone in complex hand radiograph show a significant improvement 
degree of belief are formed using observed evidence and statistical background information in this paper we examine the process of how prior degree of belief derived from the evidence are combined with statistical data to form more specific degree of belief a statistical model for this process then is shown to vindicate the cross entropy minimization principle a a rule for probabilistic default inference 
the bulk of previous work on goal and plan recognition may be crudely stereotyped in one of two way neat theory rigorous jus tified but not yet practical scruffy system heuristic domain specific but practical in contrast we describe a goal recognition mod ule that is provably sound and polynomial time and that performs well in a real domain our goal recognizer observes action executed by a human and repeatedly prune inconsistent ac tions and goal from a graph representation of the domain we report on experiment on hu man subject in the unix domain that demon strate our algorithm to be fast in practice the average time to process an observed action with an initial set of goal schema and action schema wa cpu second on a sparc 
this paper present a qualitative analysis that relates stable structure in visual motion field to property of corresponding three dimensional environment such an analysis is fundamental in the development of method for recovering useful information from dynamic visual data without the need for highly accurate and precise sensing methodologically the technique of singularity theory are used to describe the mapping from image space to velocity space and to relate this mapping to the three dimensional environment the specific result of this paper address situation where an optical sensor is undergoing pure rotational or pure translational motion through it environment for the case of pure rotational motion it is shown that the qualitative structure of visual motion provides information about the ax and relative magnitude of rotation for the case of pure translational motion it is shown that the qualitative structure of visual motion provides information about the shape and orientation of viewed surface a well a information about the translation itself further the temporal evolution of the visual motion field is described these result suggest that valuable information regarding three dimensional environmental structure and motion can be recovered from qualitative consideration of visual motion field 
two new method are presented for recovering the focused image of an object from only two blurred image recorded with different camera parameter setting the camera parameter include lens position focal length and aperture diameter first a blur parameter sigma is estimated using one of our proposed depth from defocus method then one of the two blurred image is deconvolved to recover the focused image the first method is based on a spatial domain convolution deconvolution transform this method requires only the knowledge of sigma of the camera s point spread function psf it doe not require information about the actual form of the camera s psf the second method in contrast to the first requires full knowledge of the form of the psf a part of the second method we present a calibration procedure for estimating the camera s psf for different value of the blur parameter sigma in the second method the focused image is obtained through deconvolution in the fourier domain using a wiener filter for both method the result of experiment on actual defocused image recorded by a ccd camera are given the first method requires much le computation than the second method the first method give satisfactory result for up to medium level of blur and the second method give good result for up to relatively high level of blur 
the paper deal with the problem of unsupervised classification of image modeled by markov random field mrf if the model parameter are known then we have various method to solve the segmentation problem simulated annealing icm etc however when they are not known the problem becomes more difficult one ha to estimate the hidden label field parameter from the only observable image our approach consists of extending a recent iterative method of estimation called iterative conditional estimation ice to a hierarchical markovian model the idea resembles the estimation maximization em algorithm a we recursively look at the maximum a posteriori map estimate of the label field given the estimated parameter then we look at the maximum likelihood ml estimate of the parameter given a tentative labeling obtained at the previous step we propose unsupervised image classification algorithm using a hierarchical model the only parameter supposed to be known is the number of region all the other parameter are estimated the presented algorithm have been implemented on a connection machine cm comparative test have been done on noisy synthetic and real image remote sensing 
by looping over a set of behavior reactive system use repetition and feedback to deal with error and environmental uncertainty their robust fault tolerant performance make reactive system desirable for executing plan however most planning system cannot reason about the loop that characterize reactive system in this paper we show how the structured application of abstraction and nondeterminism can map complex planning problem requiring loop plan into a simpler representation amenable to standard planning technology in the process we illustrate key recipe for automatically building predictable reactive system that are guaranteed to achieve their goal 
this paper investigates an algorithm for the construction of decision tree comprised of linear threshold umts and also present a novel algorithm for the learning of nonlinearly separable boolean function using madaline style network which are isomorphic to decision tree the construction of such network is discussed and their performance in learning is compared with standard back propagation on a sample problem in which many irrelevant attribute are introduced littlestone s winnow algorithm is also explored within this architecture a a mean of learning in the presence of many irrelevant attribute the learning ability of this madaline style architecture on non optimal larger than necessary network is also explored 
the best known algorithm for symbolic model matching incomputer vision is the interpretation tree search algorithm this algorithmhas a high computational complexity when applied to matchingproblems with large number of feature this paper examines ten variationsof this algorithm in a search for improved performance and concludesthat the non wildcard and hierarchical algorithm have reducedtheoretical complexity and run faster than the standard algorithm introductionthe most 
many different discrete time recurrent neural network architectureshave been proposed however there ha been virtually noeffort to compare these architecture experimentally in this paperwe review and categorize many of these architecture and comparehow they perform on various class of simple problem includinggrammatical inference and nonlinear system identification introductionin the past few year several recurrent neural network architecture have emerged in this paper we 
statistical modeling and evaluation of the performance of obstacle detection system for unmanned ground vehicle ugv s is essential for the desig n evaluation and comparison of sensor system in this report we address this issue for ima ging range sensor by dividing the evaluation problem into two level quality of the range dat a itself and quality of the obstacle detection algorithm applied to the range data we review existing model of the quality of range data from stereo vision and am cw ladar then use these to derive a new model for the quality of a simple obstacle detection algorithm this model predicts the probability of detecting obstacle and the probability of false alarm a a function of the size and distance of the obstacle the resolution of the sensor and the level of n oise in the range data we evaluate these model experimentally using range data from stereo image pair of a gravel road with known obstacle at several distance the result show that the approach is a promising tool for predicting and evaluating the performance of obstacle detection with imaging range sensor 
we are considering the problem of recovering the three dimensional geometry of a scene from binoculor stereo disparity once a dense disparity map ha been computed from a stereo pair of image one often need to calculate some local diferential property of the cowesponding surface such a orientation or curvature the wual approach is to build a reconstruction of the surface s from which all shape property will then be derived without ever going back to the original image in this paper we depart from this paradigm and propose to we the image directly to compute the shape property we thus propose a new method extending the classical cowelation method to estimate accurately both the disparity and it derivative directly from the image data we then relate those derivative to diferential property of the surface such a orientation and curvature 
in binocular visual system vergence is the process of directing the gaze so that the optical ax intersect at a surface point correlation based method of disparity analysis provide fast estimate of the vergence errol unfortunately most correlation technique do not provide mechanism to determine which image location contributed to q given correlation peak the result is that large correlation peak may have contribution from image area not relevant to the vergence task this paper present a vergence system that applies a cepstral filter to multiscale image obtained from a dominant eye binocular sensol a used by this system the cepstral filter ha two main advantage it enhances target through narrow band signal suppression and it support a back projection operation to determine the image location associated with particular correlation peak the use of multiscale image allows the system to have both high resolution for precision in the final vergence and a large field of view for a wide range of initial camera orientation without undue computational cost 
we present a mean field theory method for locating twodimensionalobjects that have undergone rigid transformation the resulting algorithm is a coarse to fine correlation matching we first consider problem of matching synthetic point data andderive a point matching objective function a tractable line segmentmatching objective function is derived by considering eachline segment a a dense collection of point and approximating itby a sum of gaussians the algorithm is tested on real 
plan recognition technique frequently make rigid assumption about the student s plan and invest substantial effort to infer unobservable property of the student the pedagogical benefit of plan recognition analysis are not always obvious we claim that these difficulty can be overcome if greater attention is paid to the situational context of the student s activity and the pedagogical task which plan recognition is intended to support this paper describes an approach to plan recognition called situated plan attribution that take these factor into account it devotes varying amount of effort to the interpretation process focusing the greatest effort on interpreting impasse point i e point where the student encounter some difficulty completing the task this approach ha been implemented and evaluated in the context of the react tutor a trainer for operator of deep space communication station appear to be based on an analysis of whether it is appropriate to intervene this paper describes an approach to plan recognition called situated plan attribution that take these factor into account situated plan attribution analyzes both the student s action and the environmental situation attention to the situation is important because it allows the plan recognizer to recognize when the student must deviate from the usual plan a well a alternative way of achieving the goal of the plan this flexibility avoids the rigidity problem of other technique such a kautz and allen s deductive approach kautz allen which assumes that all possible way of performing an action are known and every action is a step in a known plan 
this paper is devoted to an analytical study of extremum curvature evolution through scale space our analytical study allows to get result which show that from a qualitative point of view corner evolution in scale space ha the same behavior for planar curve or surface in particular this analysis performed with different corner shape model show that for a two corner shape two curvature maximum exist and merge at a certain scale depending on the shape for a two corner grey level surface the evolution of the determinant of hessian det show a merging point for a certain independently of contrast and the evolution of gaussian curvature present the same characteristic but this point evolves with contrast 
knowledge based natural language processing system have achieved good success with certain task but they are often criticized because they depend on a domain specific dictionary that requires a great deal of manual knowledge engineering this knowledge engineering bottleneck make knowledge based nlp system impractical for real world application because they cannot be easily scaled up or ported to new domain in response to this problem we developed a system called autoslog that automatically build a domain specificdictionary of concept for extracting information from text using autoslog we constructed a dictionary for the domain of terrorist event description in only person hour we then compared the autoslog dictionary with a hand crafted dictionary that wa built by two highly skilled graduate student and required approximately person hour of effort we evaluated the two dictionary using two blind test set of text each overall the autoslog dictionary achieved of the performance of the hand crafted dictionary on the first test set the autoslog dictionary obtained of the performance of the hand crafted dictionary on the second test set the overall score were virtually indistinguishable with the autoslog dictionary achieving of the performance of the handcrafted dictionary 
widespread access to the internet ha led to the formation of geographically dispersed scientific community collaborating through the network the tool supporting such collaboration currently are based primarily on electronic mail through mailing list server and access to archive of research report through ftp gopher and world wide web however electronic communication can support the knowledge process of scientific community more directly through overtly represented knowledge structure this paper describes some experiment in the use of knowledge acquisition ka and representation kr tool to define and analyze major policy and technical issue in an international research community responsible for one of the test case in the intelligent manufacturing system ims research program it is concluded that distributed knowledge support system in routine use by world class scientific community collaborating through the internet will provide a major impetus to artificial intelligence research 
most research in computer vision ha been directed towards minimalistic approach in which problem are addressed on how property of the environment can be computed from a little information a possible although such approach may be scientifically well motivated they have only resulted in limited progress towards our understanding of seeing system ballard bajcsy and others have pointed out the importance of vision being an active process which is tightly connected to behavior we support this thought and also propose that utilizing that the world is rich on information is essential we develop this idea to show how attention and figure ground segmentation by an active observer using multiple cue can be separated from analyzing and recognizing what is seen in a consistent way continuous operation over time and early use of three dimensional cue are important in this context we illustrate our proposed approach by some experiment on a real time active system 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
localization is a general purpose representational technique for partitioning a problem into subproblems a localized problem solver search several smaller search space one for each subproblem unlike most method of partitioning however localization allows for subproblems that overlap e multiple search space may be involved in constructing shared piece of the overall plan in this paper we focus on two criterion for forming localization scope and abstraction we describe a method for automatically generating such localization and provide empirical result that contrast their use in an office building construction domain 
on the basis of it optimal asymptotic time complexity ac is often considered the best algorithm for establishing arc consistency in constraint satisfaction problem csps in the present work ac wa found to be much more efficient than ac for csps with a variety of feature variable pair were in lexical order and in ac they were added to the end of the list of pair this is supported by argument for the superiority of ac over most of the range of constraint satisfiabilities and for the unlikelihood of condition leading to worst case performance the efficiency of ac is affected by the order of variable testing in phase setting up phase performance in this phase can thus be enhanced and this establishes initial condition for phase that improve it performance but since ac is improved by the same ordering it still outperforms ac in most case 
this paper present a novel approach to diagnosis which address the two problem computational complexity of abduction and device model that have prevented model based diagnostic technique from being widely used the experience aided diagnosis ead model is defined that combine deduction to rule out hypothesis abduction to generate hypothesis and induction to recall past experience and account for potential error in the device model a detailed analysis of the relationship between case based reasoning and induction is also provided the ead model yield a practical method for solving hard diagnostic problem and provides a theoretical basis for overcoming the problem of partially incorrect device model 
this paper introduces a new general purpose algorithm that allows the optimal geometric match between contour to be determined that is the transformation yielding a minimal deformation is obtained the algorithm relies only on the geometric property of the contour and doe not call for any other constraint so that it is particularly suitable when no parameterization of title deformation is available or desirable contour deformation is explicitly incorporated in the computation allowing for a thorough use of all geometric information available moreover no discretization is involved in the computation resulting in two main advantage first the algorithm is robust to difference in the segmentation of contour and allows the matching of polygonal approximation of contour with very little loss of precision second subpixel precision matching can be achieved 
a series of japanese full text retrieval experiment were conducted using an inference network document retrieval model the retrieval performance of two major indexing method character based and word based were evaluated using structured query the character based indexing performed retrieval a well a or slightly better than the word based system this result ha practical significance since the character based indexing speed is considerably faster than the traditional word based indexing all the query in this experiment were automatically formulated from natural language input 
one important issue of automated theorem proving is the complexity of the inference rule used in theorem provers if krishnamurty s general symmetry rule is to be used then one should provide some way of computing non trivial symmetry we show that this problem is np complete but this general rule can be simplified by restricting it to what we call symmetry yielding the well known symmetry rule we show that computing symmetry is in the same complexity class a the graph isomorphism problem which appears to be neither polynomial nor np complete however it is sufficient to compute the set of all symmetry at the beginning of the proof search and since it is a permutation group there exist some efficient technique from computational group theory to represent this set we also show how these technique can be used for applying the s symmetry rule in polynomial time 
one of the central problem in the field of knowledge discovery is the development of good measure of interestingness of discovered pattern such measure of interestingness are divided into objective measure those that depend only on the structure of a pattern and the underlying data used in the discovery process and the subjective measure those that also depend on the class of user who examine the pattern the purpose of this paper is to lay the groundwork for a comprehensive study of subjective measure of interestingness in the paper we classify these measure into actionable and unexpected and examine the relationship between them the unexpected measure of interestingness is defined in term of the belief system that the user ha interestingness of a pattern is expressed in term of how it affect the belief system 
this paper describes a new method for inducinglogic program from example which attemptsto integrate the best aspect of existingilp method into a single coherent framework in particular it combine a bottomupmethod similar to golem with a topdownmethod similar to foil it also includesa method for predicate invention similarto champ and an elegant solution tothe quot noisy oracle quot problem which allows thesystem to learn recursive program withoutrequiring a complete set of 
a new method for representing and recognizing human body movement is presented the basic idea is to identify set of constraint that are diagnostic of a movement expressed using body centered coordinate such a joint angle and in force only during a particular movement assuming the availability of cartesian tracking data we develop technique for a representation of movement defined by space curve in subspace of a phase space the phase space ha ax of joint angle and torso location and attitude and the ax of the subspace are subset of the ax of the phase space using this representation we develop a system for learning new movement from ground truth data by searching for constraint we then use the learned representation for recognizing movement in unsegmented data we train and test the system on nine fundamental step from classical ballet performed by two dancer the system accurately recognizes the movement in the unsegmented stream of motion 
the difference between bayesian conditioning and lewis imaging is somewhat similar to the one between gardenfors belief revision and katsuno and mendelzon updating in the logical framework counterpart in possibility theory of these two operation are presented including the case of conditioning upon an uncertain observation possibilistic conditioning satisfies all the postulate for belief revision and possibilistic imaging all the updating postulate lastly a third operation called focusing is naturally introduced in the setting of belief and plausibility function 
graphical presentation can he used to communicate information in relational data set suffciently and effectively however novel graphical presentation about numerous attribute and their relationship are often difficult to understand completely until explained automatically generated graphical presenlations must therefore either be limited to simple conventional one or risk incornprehensibility one way of alleviating this problem is to design graphical presentation system that can work in conjunction with a natural language generator to produce explanatory caption this paper present three strategy for generating explanatory caption to accompany information graphic based on a representation of the structure of the graphical presentation a framework for identifying the perceptual complexity of graphical element and the structure of the data expressed in the graphic we describe an implemented system and illustrate how it is used to generate explanatory caption for a range of graphic from a data set about real estate transaction in pittsburgh 
considerable progress ha been made in recent yearsin understanding and solving propositional satisfiabilityproblems much of this work ha been basedon experiment on randomly generated sat problem one generally accepted shortcoming of thiswork is that it is not clear how the result and algorithmsdeveloped will carry over to quot real quot constraintsatisfactionproblems this paper report on a seriesof experiment applying satisfiability algorithmsto scheduling problem we have found 
computer sensing of hand and limb motion is an important problem for application in human computer interaction and computer graphic we describe a framework for local trading of self occluding motion in which one part of an object obstructs the visibility of another our approach us a kinematic model to predict occlusion and windowed template to track partially occluded object we present offline d tracking result for hand motion with significant self occlusion 
we report a novel application of polarization based vision addressing the robustness of laser triangulation range sensor such sensor are based on the accurate detection of a pattern of laser light projected onto a scene usually a point or line typical problem arise with highly specularly reflective surface which can generate visible reflection of the light in various part of the image this can confuse the detection algorithm and lead to wrong range measurement this paper demonstrates experimentally the feasibility of polarization based vision for disambiguating multiple specular inter reflection of the laser light we concentrate on metal component a they have high interest for inspection in manufacturing and show positive result with situation of various complexity 
lifschitz introduced a logic of minimal belief and negation a failure called mbnf in order to provide a theory of epistemic query to non monotonic database we present a feasible subsystem of mbnf which can be translated into a logic built on first order logic and negation a failure called fonf we give a semantics for fonf along with an extended connection calculus in particular we demonstrate that the obtained system is still more expressive than other approach 
this paper present a system for large vocabularyrecognition of on line handwritten cursive word thesystem first us a filtering module based on simpleletter feature to quickly reduce a large reference dictionaryto a smaller number of candidate the reducedlexicon along with the original input is subsequently fedto a recognition module in order to exploit the sequentialnature of the temporal data we employ a tdnnstylenetwork architecture which ha been successfullyused in the 
large collection of full text document are now commonly used in automated information retrieval when the stored document text are long the retrieval of complete document may not be in the user best interest in such circumstance efficient and effective retrieval result may be obtained by using passage retrieval strategy designed to retrieve text excerpt of varying size in response to statement of user interest new approach are described in this study for implementing selective passage retrieval system and identifying text passage responsive to particular user need an automated encyclopedia search system is used to evaluate the usefulness of the proposed method 
short term memory is indispensable for the processing of timevarying information with artificial neural network in this paper amodel for linear memory is presented and way to includememories in connectionist topology are discussed a comparisonis drawn among different memory type with indication of what isthe salient characteristic of each memory model 
uncertainty processing method are analysed from the viewpoint of their sensitivity to small variation of certainty factor the analysis make use of the algebraic theory which defines the function for combining partial certainty factor by mean of a group operation of the ordered abelian group over the interval of uncertainty two approach are introduced a sensitivity analysis of the inference network and b calculation of second order probability sensitivity function are defined a partial derivative of the combining function with respect to their argument based on the sensitivity function we define the path sensitivity which measure the sensitivity of a larger part of the inference network if a set of sample of certainty factor is available instead of a single value the second order probability distribution can be approximated by the distribution of an average value it is shown that the parametric form of the distribution is completely determined by the combining function 
an important area of learning in autonomous agent is the ability to learn domain specific model of action to be used by planning system in this paper we present method by which an agent learns action model from it own experience and from it observation of a domain expert these method differ from previous work in the area in two way the use of an action model formalism which is better suited to the need of a reactive agent and successful implementation of noise handling 
understanding high dimensional real worlddata usually requires learning the structureof the data space the structure may containhigh dimensional cluster that are related incomplex way method such a merge clusteringand self organizing map are designedto aid the visualization and interpretation ofsuch data however these method often failto capture critical structural property of theinput although self organizing map capturehigh dimensional topology they do notrepresent 
we present an abductive semantics for general propositional logic program which defines the meaning of a logic program in term of it extension this approach extends the stable model semantics for normal logic program in a natural way the new semantics is equivalent to stable semantics for a logic program p whenever p is normal and ha a stable model the abductive semantics can also be applied to generalize default logic and autoepistemic logic in a like manner our approach is based on an idea recently proposed by konolige for causal reasoning instead of maximizing the set of hypothesis alone we maximize the union of the hypothesis along with possible hypothesis that are excused or refuted by the theory 
an extension of the concept description language acc used in kl one like terminological reasoning is presented the extension includes multi modal operator that can either stand for the usual role quantification or for modality such a belief time etc the modal operator can be used at all level of the concept term and they can be used to modify both concept and role this is an instance of a new kind of combination of modal logic where the modal operator of one logic may operate directly on the operator of the other logic 
present a method for calibrating intrinsic and extrinsic camera parameter this algorithm can easily be modified by other user to suit their particular calibration need without requiring a high precision calibration target or complicated linear algebra the algorithm us controlled motion and a single light source to simulate calibration target in convenient d location these convenient calibration target enable u to simplify the calibration algorithm and gather dense data for lens distortion dense data make the distortion correction more accurate than traditional low order polynomial fit and allows u to calibrate wide angle lens 
we present a new method for obtaining local error bar for nonlinear regression i e estimate of the confidence in predicted value that depend on the input we approach this problem by applying a maximumlikelihood framework to an assumed distribution of error we demonstrate our method first on computer generated data with locally varying normally distributed target noise we then apply it to laser data from the santa fe time series competitionwhere the underlying system noise is known quantization error and the error bar give local estim ate of model misspecification in both case the method also provides a weightedregression effect that improves generalization performan ce 
in this paper we will consider the problem of classifying electroencephalogram eeg signal of normal subject and subject suffering from psychiatric disorder e g obsessivecompulsive disorder schizophrenia using a class of artificial neural network viz multilayerperceptron it is shown that the multilayer perceptron is capable of classifyingunseen test eeg signal to a high degree of accuracy introductionthe spontaneous electrical activity of the brain wa first 
concept learned by neural network are difficultto understand because they are representedusing large assemblage of real valuedparameters one approach to understandingtrained neural network is to extractsymbolic rule that describe their classificationbehavior there are several existingrule extraction approach that operate bysearching for such rule we present a novelmethod that cast rule extraction not a asearch problem but instead a a learningproblem in addition to 
shape from texture is best analyzed in two stage analogous to stereopsis and structure from motion a computing the texture distortion and b interpreting the texture distortion to infer the orientation and shape of the surface we model the texture distortion for a given point and direction on the image plane a an affine transformation and derive the relationship between the parameter of this transformation and the shape parameter we use non linear minimization of a least square error criterion to estimate the shape parameter from the affine transformation using a simple linear algorithm to obtain an initial guess under the assumption that the measurement error in the affine parameter are independent and normally distributed we can find error bound on the shape parameter estimate we present result on image of planar and curved surface under perspective projection we find all five local shape and orientation parameter with no a priori assumption about the shape of the surface 
our long term goal is to develop a trainable tool for locating pattern of interest in large image database toward this goal we have developed a prototype system based on classical filteringand statistical pattern recognition technique for automatically locating volcano in the magellan sar database of venus training for the specificvolcano detectiontask is obtained by synthesizing feature template via normalizationand principal component analysis from a small number of example provided by expert candidateregions identifiedby a focus of attention foa algorithm are classifiedbased on correlation with the feature template preliminary test show performance comparableto trained human observer 
the bankxx system model the process of perusing and gathering information for argument a a heuristic best first search for relevant case theory and other domain specific information a bankxx search it heterogeneous and highly interconnected network of domain knowledge information is incrementally analyzed and amalgamated into a dozen desirable ingredient for argument called argument piece such a citation to case application of legal theory and reference to prototypical factual scenario at the conclusion of the search bankxx output the set of argument piece filled with harvested material relevant to the input problem situation this research explores the appropriateness of the search paradigm a a framework for harvesting and mining information needed to make legal argument we discus how we tackled the problem of evaluation of bankxx from both the case based reasoning cbr and task performance perspective in particular we discus how various system parametersstart node evaluation function resource limit affected bankxx from the cbr perspective and how well bankxx performs it assigned task of gathering information useful for legal argumentation by running bankxx on real legal case and comparing it output with the published court opinion for those case 
this paper address the problem of computing cue to thethree dimensional structure of surface in the world directly from thelocal structure of the brightness pattern of a binocular image pair thegeometric information content of the gradient of binocular disparity isanalyzed for the general case of a fixating system with symmetric orasymmetric vergence and with either known or unknown viewing geometry a computationally inexpensive technique which exploit thisanalysis is proposed 
this paper present a novel framework for shape modeling and shape recovery based on idea developed by osher sethian for interface motion in this framework shape are represented by propagating front whose motion is governed by a hamilton jacobi type equation this equation is written for a function in which the interface is a particular level set unknown shape are modeled by making the front adhere to the object boundary of interest under the influence of a synthesized halting criterion the resulting equation of motion is solved using a narrow band algorithm designed for rapid front advancement our technique can be applied to model arbitrarily complex shape which include shape with significant protrusion and to situation where no a priori assumption about the object s topology can be made we demonstrate the scheme via example of shape recovery in d and d from synthetic and low contrast medical image data 
two observation motivate our work a modelbased diagnosis program are powerful but do not learn from experience and b one of the long term trend in learning research ha been the increasing use of knowledge to guide and inform the process of induction we have developed a knowledge guided learning method based in ebl that allows a model based diagnosis program to selectively accumulate and generalize it experience our work is novel in part because it produce several different kind of generalization from a single example where previous work in learning ha for the most part intensively explored one or another specific kind of generalization our work ha focused on accumulating and using multiple different ground for generalization i e multiple domain theory a a result our system not only learns from a single example a in all ebl it can learn multiple thing from a single example simply saying there ought to be multiple ground for generalization only open up the possibility of exploring more than one domain theory we provide some guidance in determining which ground to explore by demonstrating that in the domain of physical device causal model are a ric source of useful domain theory we also caution that adding more knowledge can sometimes degrade performance hence we need to select the ground for generalization carefully and analyze the resulting rule to ensure that they improve performance we illustrate one such quantitative analysis in the context of a model based troubleshooting program measuring and analyzing the gain resulting from the generalization produced 
an image of a scene with occlusion can yield only partial knowledge about disconnected fragment of the scene if this were the only knowledge available program attempting to interpret the scene would have to conclude that the scene fragment would collapse in a jumble but they won t we describe a program that exploit commonsense knowledge of naive physic to make sense of scene with occlusion our causal analysis focus on the static stability of structure what support what occluded connection in a link and junction scene are inferred by determining the stability of each subassembly in the scene and connecting part when they are unstable the causal explanation that is generated reflects a deeper understanding of the scene than mere model matching it allows the seeing agent to predict what will happen next in the scene and determine how to interact with it 
human improve their performance by mean of a variety of learning strategy including both gradual statistical induction from experience and rapid incorporation of advice in many learning environment these strategy may interact in complementary way the focus of this work is on cognitively plausible model of multistrategy learning involving the integration of inductive generalization and learning by being told such model might be developed by starting with an architecture for which advice taking is relatively easy such a one based upon a sentential knowledge representation and subsequently adding some form of inductive learning mechanism alternatively such model might be grounded in a statistical learning framework appropriately extended to operationalize instruction this latter approach is taken here specifically connectionist back propagation network rumelhart mcclelland the pdp research group are made to instantaneously modify their behavior in response to quasi linguistic advice many of the previous approach to the instruction of connectionist network have involved the encoding of symbolic rule a initial connection weight which may be later refined by inductive learning giles omlin tresp hollatz ahmad a major drawback of this approach is that advice may only be given before inductive training begin this is an unreasonable constraint for a cognitive model of instructed learning instead a connectionist network is needed which may have it behavior altered by a stream of encoded instruction without a delay period for lengthy retraining the approach which is examined here focus on encoding the receipt of instruction a motion in a network s activation space in short advice is presented to such an instructable network a a temporal sequence of instruction token where each token is encoded a an input activation pattern the network is trained to appropriately modulate it behavior based on input of such advice sequence the correct interpretation and operationalization of input instruction sequence is learned inductively but once this initial learning is complete instruction following proceeds at the speed of activation propagation this focus on activation space dynamic allows instructional learning and standard connectionist inductive learning to function in tandem this strategy ha been successfully applied to a simple discrete mapping task and to the learning of natural number arithmetic in this latter domain the connectionist adder of cottrell and tsung cottrell tsung which is capable of systematically operating on arbitrarily large natural number wa augmented to receive instruction in various method of addition and subtraction the resulting network tackle arithmetic problem by examining one column of digit at a time and sequentially performing action such a writing a resultant digit for the column announcing a carry or borrow and shifting attention to the next digit column the network s behavior is determined by the most recently presented sequence of instruction token future experiment will extend these multistrategy learner to include auto associative memory containing articulated attractor in activation space which will facilitate systematic generalization to novel advice sequence these later experiment will abandon arithmetic and will focus instead on simple planning task in a block world environment 
an approach is described for supplying selectional restriction to parser in natural language interface nlis to database by extracting the selectional restriction from semantic description of those nlis automating the process of finding selectional restriction reduces nli development time and may avoid error introduced by hand coding selectional restriction 
a newpractical method is given for the self calibration of a camera in this method at least three image are taken from the same point in space with different orientation of the camera and calibration is computed from an analysis of point match between the image the method requires no knowledge of the orientation of the camera calibration is based on the image correspondence only this method differs fundamentally from previous result by maybank and faugeras on selfcalibration using the epipolar structure of image pair in the method of this paper there is no epipolar structure since all image are taken from the same point in space since the image are all taken from the same point in space determination of point match is considerably easier than for image taken with a moving camera since problem of occlusion or change of aspect or illumination do not occur the calibration method is evaluated on several set of synthetic and real image data 
we discus the indexing of case for use in precedent based argument our focus is on how multiple related index into a case base of legal precedent are exploited by an argument generation program called bankxx this system s architecture and control scheme are rooted in a conceptualization of legal argument a heuristic search although our framing argument a search is not discussed in detail in this paper we describe the main feature of this view to provide context for a discussion of an indexing scheme that facilitates argument creation we describe five inter related index type citation prototypical story factor family resemblance and legal theory index and show how they can be used to access view widen or filter a set of case the application domain is a u s federal statute that governs the approval of bankruptcy plan 
many representation in early vision can be constructed by performing orientation analysis along several sampling dimension texture is often oriented in space motion is oriented in space time and stereo is oriented in space disparity in these modality 
efficient natural language generation ha been successfully demonstrated using highly compiled knowledge about speech act and their related social action a design and prototype implementation of a parser which utilizes this same pragmatic knowledge to efficiently guide parsing is presented such guidance is shown to prune the search space and thus avoid needle processing of pragmatically unlikely constituent structure 
many machine learning either supervised or unsupervised technique assume that data present themselves in an attribute value form but this formalism is largely insufficient to account for many application therefore much of the ongoing research now focus on first order learning system but complex formalism lead to high computational complexity on the other hand most of the currently installed database have been designed according to a formalism known a entity relationship and usually implemented on a relational database management system this formalism is far le complex than first order logic but much more expressive than attribute value list in that context the database schema defines an abstraction space and learning must occur at each level of abstraction this paper describes a clustering system able to discover useful grouping in structured database it is based in the cobweb algorithm to which it add the ability to cluster structured object 
many work have been carried out to improve search efficiency in csps but few of them treated the semantics of the constraint in this paper we expose some property of two class of constraint functional and bijective constraint we first present condition under which arc and path consistency are sufficient to guarantee the existence of a bactrack free solution we then exhibit class of polynomial problem and finally we propose a new method of decomposition for problem containing functional or bijective constraint an interesting point in this method is that the resolution complexity is known prior to the search 
project listen is developing a novel weapon against illiteracy an automated reading coach that display a story on a computer screen listens to a child read it aloud and help where needed the coach provides a combination of reading and listening in which the child read wherever possible and the coach help wherever necessary we demonstrated a prototype of this coach at the arpa workshop on human language technology in march a short video show the coach in action 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
binocular stereo vision process estimate d surfacesusing a pair of image taken from different pointsof view d surface characteristic are estimated bymatching d image area or feature corresponding tothe projection of same d point the most classicarea based method used cross correlation with a fixedwindow size but this technique present a major drawback the computation of depth is generally prone toerrors close to surface discontinuity in this paper we present our 
i propose a learning algorithm for learning hierarchical mo dels for object recognition the model architecture is a compositional hierarchy that represents part whole relationship part are descr ibed in the local context of substructure of the object the focus of this report is learning hierarchical model from data i e inducing the s tructure of model prototype from observed exemplar of an object at each node in the hierarchy a probability distribution governing it parameter must be learned the connection between node reflects the struc ture of the object the formulation of substructure is encouraged such that their part become conditionally independent the resulting model can be interpreted a a bayesian belief networkand also is in many respect similar to the stochastic visual grammardescribed by mjolsness 
this paper present a new method for producing a dictionary of subcategorization frame from unlabelled text corpus it is shown that statistical filtering of the result of a finite state parser running on the output of a stochastic tagger produce high quality result despite the error rate of the tagger and the parser further it is argued that this method can be used to learn all subcategorization frame whereas previous method are not extensible to a general solution to the problem 
it is fairly common in video sequence that a mostly fixed background scene is imaged with or without object the dominant background change in the image plane mostly due to camera operation and motion zoom pan tilt track etc we address the problem of computation of the dominant image transformation over time and demonstrate how this can be effectively used for efficient video representation through video mosaicing and image registration we formulate the problem of dominant component estimation a that of model based robust estimation using m estimator with direct multi resolution method in addition to d affine and plane projective model that have been used in the past for describing image motion using direct method we also employ a true d model of motion and scene structure imaged with uncalibrated camera this model parameterizes the image motion a that due to a planar component and a parallax component for rigid d scene imaged under camera motion only least square l method with the plane and parallax parameterization are also presented furthermore in the context of robust estimation in contrast with previous approach for similar problem our algorithm employ an automatic computation of a scale parameter that is crucial in rejecting the non dominant component a outlier 
this paper review the concept of metagame and discus the implementation ofmetagamer a program which play metagame in the class of symmetric chess like game which includes chess chinese chess checker draught and shoji the program take asinput just the rule of any game in this class including game unknown to it programmer and play the game against opponent without further human intervention using an evaluationfunction for the entire class of game the program applies 
we estimate the number of training sample required to ensure that the performance of a neural network on it training data match that obtained when fresh data is applied to the network existing estimate are higher by order of magnitude than practice indicates this work seek to narrow the gap between theory and practice by transforming the problem into determining the distribution of the supremum of a random field in the space of weight vector which in turn is attacked by application of a recent technique called the poisson clumping heuristic 
we present a new property called constraint looseness and show how it can be used to estimate thelevel of local consistency of a binary constraint network specifically we present a relationship betweenthe looseness of the constraint the size of the domain and the inherent level of local consistency of aconstraint network the result we present are usefulin two way first a common method for finding solutionsto a constraint network is to first preprocess thenetwork by enforcing 
the brodatz album ha become the de facto standardfor evaluating texture algorithm with hundredsof study having been applied to small set of it image this paper compare two powerful recognition algorithm principal component analysis and multiscaleautoregressive model by evaluating them on a imagedatabase derived from the entire brodatz album the variety of homogeneous and non homogeneous imagesstudied is thus nearly an order of magnitude largerthan ha been compared 
in this paper we present a novel induction algorithmfor bayesian network this selectivebayesian network classifier selects a subset ofattributes that maximizes predictive accuracyprior to the network learning phase therebylearning bayesian network with a bias for small high predictive accuracy network we comparethe performance of this classifier with selectiveand non selective naive bayesian classifier weshow that the selective bayesian network classifierperforms significantly 
we present e cient algorithm for dealing with the problem of missing input incomplete feature vector during training and recall our approach is based on the approximation of the input data distribution using parzen window for recall we obtain closed form solution for arbitrary feedforward network for training we show how the backpropagation step for an incomplete pattern can be approximated by a weighted averaged backpropagation step the complexity of the solution for training and recall is independent of the number of missing feature we verify our theoretical result using one classiflcation and one regression problem 
we compare activation function in term of the approximationpower of their feedforward net we consider the case of analog aswell a boolean input introductionwe consider efficient approximation of a given multivariate function f gamma m r by feedforward neural network we first introduce the notion of a feedforwardnet let gamma be a class of real valued function where each function is defined on somesubset of r a gamma net c is an unbounded fan in circuit whose 
the residue driven architecture presented here is a model of auditory stream segregation from input sound a subsystem to extract auditory stream by using some sound attribute is called an agency and the design of each agency is based on the residue driven architecture this architecture consists of three kind of agent an event detector a tracer generator and tracer the event detector calculates a residue by subtracting the predicted input from the actual input when a residue exceeds a threshold value tracer generator generates a tracerthat extract an auditory stream from the residue and return a predicted input of the next time frame to the event detector this aproach improves the performance of segregation and the resulting system can segregate a woman s voiced stream a man s voiced stream and a noise stream from a mixture of these sound binaural segregation is also designed by the architecture 
in the markov decision process mdp formalization of reinforcement learning a single adaptive agent interacts with an environment defined by a probabilistic transition function in this solipsistic view secondary agent can only be part of the environment and are therefore fixed in their behavior the framework of markov game allows u to widen this view to include multiple adaptive agent with interacting or competing goal this paper considers a step in this direction in which exactly two agent with diametrically opposed goal share an environment it describes a q learning like algorithm for finding optimal policy and demonstrates it application to a simple two player game in which the optimal policy is probabilistic 
in this paper we describe the partially observable markov decision process sc pomdp approach to finding optimal or near optimal control strategy for partially observable stochastic environment given a complete model of the environment the sc pomdp approach wa originally developed in the operation research community and provides a formal basis for planning problem that have been of interest to the ai community we found the existing algorithm for computing optimal control strategy to be highly computationally inefficient and have developed a new algorithm that is empirically more efficient we sketch this algorithm and present preliminary result on several small problem that illustrate important property of the sc pomdp approach 
this paper describes experimental result on using winnow and weighted majority based algorithm on a real world calendar scheduling domain these two algorithm have been highly studied in the theoretical machine learning literature we show here that these algorithm can be quite competitive practically outperforming the decision tree approach currently in use in the calendar apprentice system in term of both accuracy and speed one of the contribution of this paper is a new variant on the 
we present a statistical method that exactly learns the class of constant depth perceptron network with weight taken from f gand arbitrary threshold when the distribution that generates the input example is member of the family of product distribution these network also known a nonoverlapping perceptron network or read once formula over a weighted threshold basis are loop free neural net in which each node ha only one outgoing weight with arbitrary high probability the learner is able to exactly identify the connectivity or skeleton of the target perceptron network by using a new statistical test which exploit the strong unimodality property of sum of independent random variable 
in system which learn a large number of rule production it is important to match therules efficiently in order to avoid the machine learning utility problem if the learned rulesslow down the matcher the quot learning quot can slow the whole system down to a crawl sowe need match algorithm that scale well with the number of production in the system doorenbos introduced right unlinking a a way to improve the scalability of the retematch algorithm in this paper we build on 
an algorithm that performs recursive estimation of ego motion and ambient structure from a stream of monocular perspective image of a number of feature point is presented the algorithm is based on an extended kalman filter ekf that integrates over time the instantaneous motion and structure measurement computed by a two perspective view step the key feature of the author filter are global observability of the model and complete online characterization of the uncertainty of the measurement provided by the two view step the filter is thus guaranteed to be well behaved regardless of the particular motion undergone by the observer region of motion space that do not allow recovery of structure e g pure rotation may be crossed while maintaining good estimate of structure and motion whenever reliable measurement are available they are exploited the algorithm work well for arbitrary motion with minimal smoothness assumption and no ad hoc tuning simulation are presented that illustrate these characteristic 
this paper is in two part in the first part the outline of an emotion reasoning architecture embodied in a simulation program called the affective reasoner is presented and a rudimentary personality representation for simulated agent is introduced in the second part an exercise is reviewed in which the affective reasoner is given the task of representing agent with different personality type in such a way a to allow the user to engage in a simulated interaction with them representational issue pertaining to the unique appraisal and behavioral style of the different personality type are addressed conclusion are drawn about the usefulness of the affective reasoner in such a paradigm 
the statistical basis for sense resolution decision is arrived at by the application of a process to a corpus of instance in general once the process ha been applied to the corpus the system contains both some residual representation of the instance and some explicit augmentation of that representation with information that wa implicit in the corpus for example part of the residual representation of he feel happy on friday might be the word sense pair happy feel a emotion and part of the augmentation might be the probability of happy co occurring with the sense of feel a an emotion we show that for the simple residual representation of word sense pair the existence of such a representation in and of itself capture much of the regularity inherent in the data we also demonstrate that augmenting the residual representation with the actual number of time each pair occurs in the training corpus provides most of the remainder of the power of probabalistic approach finally we show how viewing this residual representation a a form of episodic memory can enable symbolic knowledge rich system to take advantage of this source of regularity in performing sense resolution 
many search domain are non deterministic although real time search method have traditionally been studied in deterministic domain they are well suited for searching nondeterministic domain since they do not have to plan for every contingency they can react to the actual outcome of action in this paper we introduce the min max lrta algorithm a simple extension of korf s learning real time a algorithm lrta to nondeterministic domain we describe which nondeterministic domain min max lrta can solve and analyze it performance for these domain we also give tight bound on it worst case performance and show how this performance depends on property of both the domain and the heuristic function used to encode prior information about the domain 
the hough transform is a class of medium level vision technique generally recognised a a robust way to detect geometric feature from a d image this paper present two related technique first a new hough function is proposed based on a mahalanobis distance measure that incorporates a formal stochastic model for measurement and model noise thus the effect of image and parameter space quantisation can be incorporated directly given a resolution of the parameter space the method provides better result than the standard hough transform sht including under high geometric feature density secondly extended kalman filtering is used a a further refinement process which achieves not only higher accuracy but also better performance than the sht the algorithm are compared with the sht theoretically and experimentally 
present a closed form solution for the determination of d displacement and rotation parameter given a set of d point correspondence the method applies to the general case of arbitrary displacement and rotation with respect to an arbitrary stationary scene the approach is based on the linear subspace method which allows separate linear equation to be extracted for the displacement and rotation this lead to a simple algorithm suitable for real time implementation for the determination of both the d transformation and error estimate for the estimated d displacement preliminary experimental result with range data are presented 
we describe a technique for finding pixelwise correspondence between two image by using model of object of the same class to guide the search the object model are learned from example image also called prototype of an object class the model consist of a linear combination of prototype the flow field giving pixelwise correspondence between a base prototype and each of the other prototype must be given a novel image of an object of the same class is matched to a model by minimizing an error between the novel image and the current guess for the closest model image currently the algorithm applies to line drawing of object an extension to real grey level image is discussed 
this paper describes a policy iteration algorithm for optimizing theperformance of a harmonic function based controller with respectto a user defined index value function are represented a potentialdistributions over the problem domain being control policiesrepresented a gradient field over the same domain all intermediatepolicies are intrinsically safe i e collision are not promotedduring the adaptation process the algorithm ha efficient implementationin parallel simd 
the author propose to replace a set of connected d point or line segment with a d formable curve using weighted least square approximation the parameter of the d energy minimizing curve are updated so that it d projection on two or three stereoscopic image converge toward the corresponding image edge thus a more realistic representation of the real non polygonal world is obtained the case where no initial d information is provided is also considered a novel approach is proposed to match a given d curve in the first image to it corresponding position in the second image the curve is tracked between the two image by a deformable curve constrained by a d motion model the position of the d curve in the second image is then refined by relaxing the motion constraint a a result the correspondence between the curve of both image are known an initial estimate of the d curve can be recovered using the calibration parameter of the stereoscopic system these approach are illustrated by experimental result 
gsat is a randomized local search procedure for solving propositional satisfiability problem gsat can solve hard randomly generated problem that are an order of magnitude larger than those that can be handled by more traditional approach such a the davisputnam procedure this paper present the result of numerous experiment we have performed with gsat in order to improve our understanding of it capability and limitation we first characterize the space traversed by gsat we will see that for nearly all problem class we have encountered the space consists of a steep descent followed by broad flat plateau we then compare gsat with simulated annealing and show how gsat can be viewed a an efficient method for executing the lowtemperature tail of an annealing schedule finally we report on extension to the basic gsat procedure we discus two general domain independent extension that dramatically improve gsat s performance on structured problem the use of clause weight and a way to average in near solution when initializing lhe procedure before each try 
corpus based sense disambiguation method like most other statistical nlp approach suffer from the problem of data sparseness in this paper we describe an approach which overcomes this problem using dictionary definition using the definition based conceptual co occurrence data collected from the relatively small brown corpus our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information 
we develop a formal tool for representing and analyzing informational aspect of robotic task based on the formal concept of knowledge specifically we adopt the notion of knowledge based protocol from distributed system and define the notion of knowledge complexity of a robotic task and knowledge capability of a robot the resulting formalism naturally capture previous work in the area of robot information management but is sufficiently rigorous and natural to allow many extension in this paper we show one novel application the automated distribution of robotic task 
this paper is concerned with three dimensional d analysis of image showing d motion of an observer relative to a scene it present an approach to recovering d motion and structure parameter from multiple feature present in a monocular image sequence such a point region line texture gradient and vanishing line for concreteness the paper focus on flight image of a planar textured surface in this paper a linear integrated estimation method using two view is developed then for robust estimation a nonlinear integrated estimation method using multiple frame is presented the integration of information in these diverse feature is carried out using minimization of image error to reduce computation a sequential batch method is used to compute motion and structure performance is evaluated through simulation and experiment with a real image sequence digitized from a commercially available laserdisc of film taken from flying aircraft 
in this paper it is shown how false operator response due to missing or uncertain data can be signiflcantly reduced or eliminated perhaps the most well knownofsuchefiectsarethevarious edgeefiects which invariably occur at the edge of the input data set further itisshownhowoperatorshavingahigher degreeofselectivityandhighertoleranceagainstnoise can be constructed using simple combination of appropriately chosen convolution the theory is based on linear operation and is general in that it allows for both data and operator to be scalar vector or tensor of higher order threenewmethodsarepresented normalized convolution difierential convolutionand normalized differential convolution all three method are example of the power of the signal certainty philosophy i e the separation of both data and operator into a signal part and a certainty part missing data is simply handled by setting the certainty to zero in the case of uncertain data an estimate of the certainty must accompany the data localization or windowing of operator is done using an applicability function the operator equivalent to certainty not by changing the actual operator coe cients spatially or temporally limited operator are handled by setting the applicability function to zero outside the window consistentwiththephilosophyofthispaperallalgorithms produce a certainty estimate to be used if further processing is needed spectrum analysis is discussed and example of the performance of gradient divergence and curl operator are given 
in this paper we describe an algorithm which exploit the error distribution generatedby a learning algorithm in order to break up the domain which is being approximatedinto piecewise learnable partition traditionally the error distribution ha beenneglected in favor of a lump error measure such a rms by doing this however welose a lot of important information the error distribution tell u where the algorithmis doing badly and if there exists a quot ridge quot of error also tell u 
the result of empirical comparison of existinglearning algorithm illustrate that eachalgorithm ha a selective superiority it is bestfor some but not all task given a data set it is often not clear beforehand which algorithmwill yield the best performance insuch case one must search the space of availablealgorithms to find the one that producesthe best classifier in this paper we presentan approach that applies knowledge aboutthe representational bias of a set of learning 
in consideration of attention a a mean for goal directed behaviorin non stationary environment we argue that the dynamic ofattention should satisfy two opposing demand long term maintenanceand quick transition these two characteristic are contradictorywithin the linear domain we propose the near saddlenodebifurcation behavior of a sigmoidal unit with self connectionas a candidate of dynamical mechanism that satisfies both of thesedemands we further show in simulation of the 
it ha been shown that there exists a transition in the average case complexity of searching a random tree from exponential to polynomial in the search depth we develop a state space transformation method called e transformation that make use of this complexity transition to find a suboptimal solution the expected number of random tree node expanded by branch and bound bnb using e transformation is cubic in the search depth and the relative error of the solution cost compared to the optimal solution cost is bounded by a small constant we also present an iterative version of e transformation that can be used to find both optimal and suboptimal solution depthfirst bnb dfbnb using iterative e transformation significantly improves upon truncated dfbnb on random tree with large branching factor and deep goal node finding better solution sooner on average on the asymmetric traveling salesman problem dfbnb using e transformation outperforms a well known local search method and dfbnb using iterative etransformation is superior to truncated dfbnb 
random error and insufficiency in database limit the performance of any classifier trained from and applied to the database in this paper we propose a method to estimate the limiting performance of classifier imposed by the database we demonstrate this technique on the task of predicting failure in telecommunication path 
most case based reasoning system have used a single best or most similar case a the basis for a solution for many problem however there is no single exact solution rather there is a range of acceptable answer we use case not only a a basis for a solution but also to indicate the boundary within which a solution can be found we solve problem by choosing some point within those boundary in this paper i discus this use of case with illustration from chiron a system t have implemented in the domain of personal income tax planning 
this paper describes a method for the development of dialogue manager for natural language interface a dialogue manager is presented designed on the basis of both a theoretical investigation of model for dialogue management and an analysis of empirical material it is argued that for natural language interface many of the human interaction phenomenon accounted for in for instance plan based model of dialogue do not occur instead for many application dialogue in natural language interface can be managed from information on the functional role of an utterance a conveyed in the linguistic structure this is modelled in a dialogue grammar which control the interaction focus structure is handled using dialogue object recorded in a dialogue tree which can be accessed through a scoreboard by the various module for interpretation generation and background system access a sublanguage approach is proposed for each new application the dialogue manager is customized to meet the need of the application this requires empirical data which are collected through wizard of oz simulation the corpus is used when updating the dierent knowledge source involved in the natural language interface in this paper the customization of the dialogue manager for database information retrieval application is also described 
the optical plume anomaly detection opad program at the nasa marshall space flight center is ubing plume spectroscopy for the diagnosis of the space shuttle main engine a challenging part of this program is matching high resolution spectral data with a physicist s model of spectroscopy to produce estimate of metallic erosion through the plume this paper describes the discovery process used in doing this the physicist s model had to be debugged in order to discover the various instrument characteristic discover critical element of the data and in general perform exploratory analysis to understand the instrument and the data it produce this model give u strong prior knowledge however this need to be incorporated with care we had to use a range of statistical technique in our analysis including onedimensional super resolution to determine the instrument response function the paper concludes with a discussion of the role of discovery in building intelligent instrument that turn real time data into useful information 
constraint relaxation is a frequently used technique for managing over determined constraint satisfaction problem a problem in constraint relaxation is the selection of the appropriate constraint we show that method developed in model based diagnosis solve this problem the resulting method doc an abbreviation for diagnosis of over determined constraint satisfaction problem identifies the set of least important constraint that should be relaxed to solve the remaining constraint satisfaction problem if the solution is not acceptable for a user doc selects next best set of least important constraint until an acceptable solution ha been generated the power of doc is illustrated by a case study of scheduling the dutch major league soccer competition the current schedule is made using human insight and operation research method using doc the schedule ha been improved by reducing the number and importance of the violated constraint by the case study revealed that efficiency improvement is a major issue in order to apply this method to large scale over determined scheduling and constraint satisfaction problem 
many corpus based method for natural language processing are based on supervised training requiring expensive manual annotation of training corpus this paper investigates reducing annotation cost by selective sampling in this approach the learner examines many unlabeled example and selects for labeling only those that are most informative at each stage of training in this way it is possible to avoid redundantly annotating example that contribute little new information the paper first analyzes the issue that need to be addressed when constructing a selective sampling algorithm arguing for the attractiveness of committee based sampling method we then focus on selective sampling for training probabilistic classifier which are commonly applied to problem in statistical natural language processing we report experimental result of applying a specific type of committee based sampling during training of a stochastic part of speech tagger and demonstrate substantially improved learning rate over sequential training using all of the text we are currently implementing and evaluating other variant of committee based sampling a discussed in the paper in order to obtain further insight on optimal design of selective sampling method 
this paper introduces a new algorithm called siao for learning first order logic rule with genetic algorithm siao us the covering principle developed in aq where seed example are generalized into rule using however a genetic search a initially introduced in the sia algorithm for attribute based representation the genetic algorithm us a high level representation for learning rule in first order logic and may deal with numerical data a well a background knowledge such a hierarchy over the predicate or tree structured value the genetic operator may for instance change a predicate into a more general one according to background knowledge or change a constant into a variable the evaluation function may take into account user preference bias 
a general purpose object indexing technique is described that combine the virtue of principal component analysis with the favorable matching property of high dimensional space to achieve high precision recognition an object is represented by a set of high dimensional iconic feature vector comprised of the response of derivative of gaussian filter at a range of orientation and scale since these filter can be shown to form the eigenvectors of arbitrary image containing both natural and man made structure they are well suited for indexing in disparate domain the indexing algorithm us an active vision system in conjunction with a modified form of kanerva s sparse distributed memory which facilitates interpolation between view and provides a convenient platform for learning the association between an object s appearance and it identity the robustness of the indexing method wa experimentally confirmed by subjecting the method to a range of viewing condition and the accuracy wa verified using a well known model database containing a number of complex d object under varying pose 
this paper advocate a microfeature based approach towards developing computational model for metaphor interpretation it is argued that the existing model based on semantic network and mapping of complex symbolic structure are insufficient and inappropriate for modeling metaphor a connectionist model of metaphor interpretation based on microfeatures is presented which try to take into account some important issue such a accurate capturing of similarity automatic formation of feature contextual effect elimination of long path in conceptual hierarchy salience imbalance and feature enhancement some of these issue have broad implication in cognitive modeling 
the evaluation of ranking algorithm for the ranking of term for query expansion is discussed within the context of an investigation of interactive query expansion and relevance feedback in a real operational environment the yardstick for the evaluation wa provided by the user relevance judgement on the list of the candidate term for query expansion the evaluation focus on the similarity in the performance of the different algorithm and how the algorithm with similar performance treat term 
including robotics artificial life and artificial ecosystem we present a new approach to human computer interaction called so z interaction it main characteristic are summarized by the following three point first interaction are realized a multimodal verbal and nonverbal conversation using spoken language facial expression and so on second the conversants are a group of human and social agent that are autonomous and social autonomy is an important property that allows agent to decide how to act in an ever changing environment socialness is also an important property that allows agent to behave both cooperatively and collaboratively generally conversation is a joint work and ill structured it participant are required to be social a well a autonomous third conversants often encounter communication mismatch misunderstanding others intention and belief and fail to achieve their joint goal the social agent therefore are always concerned with detecting communication mismatch we realize a social agent that hears human to human conversation and informs what is causing the misunderstanding it can also interact with human by voice with facial display and head and eye movement however is autonomy itself sufficient for social service although autonomy is vital to survive in the real world it is only concerned with self it is selfish by nature it seems that it doe not work well in human society since it includes socially constructed artifact such a law custom culture social service provided by computer system have to incorporate with these artifact socialness is a higher level concept defined above the concept of an individual and is the style of interaction between the individual in a group socialness can be applied to the interaction between human and computer and possibly to that between multiple computer in this paper we study socialness of conversational interaction between human and computer conversation is no doubt a social activity especially when more than two participant are involved in it however conversation research to date ha been biased to problem solving question answering system are typical example all conversation research based on this view ha the following feature 
edge detector which use a quadratic nonlinearity in the filtering stage are attracting interest in machine vision application because of several advantage they enjoy over linear edge detector however many important property of these quadratic or energy edge detector remain unknown in this paper we investigate the behavior of quadratic edge detector under scaling we consider two case important in practice quadratic detector with constituent filter related by the hilbert transform and with constituent filter related by the first spatial derivative we prove that in one dimension hilbert pair detector with gaussian scaling permit the creation of new feature a scale is increased but such causality failure cannot generically occur with derivative pair detector in addition we report experiment that show the effect of these property in practice thus at least one class of quadratic edge detector can have the same desirable scaling property a detector based on linear differential filtering 
traditional best first search for optimal solution quickly run out of space even for problem instance of moderate size and linear space search ha unnecessarily long running time since it cannot make use of available memory for using available memory effectively we developed a new generic approach to heuristie search it integrates various strategy and includes idea from bidirectional search due to insight into different utilization of available memory it allows the search to use limited memory effectively instantiation of this approach for two different benchmark domain showed excellent result that are statistically significant improvement over previously reported result for finding optimal solution in the puzzle we achieved the fastest search of all those using the manhattan distance heuristic a the only knowledge source and for a scheduling domain our approach can solve much more difficult problem than the best competitor the most important lesson we learned from the experiment are first that also in domain with symmetric graph topology selecting the right search direction can be very important and second that memory can under certain condition be used much more effectively than by traditional best first search 
specular reflection and interreflection produce strong highlight in brightness image these highlight can cause vision algorithm such a segmentation shape from shading binocular stereo and motion detection to produce erroneous result we present an algorithm for separating the specular and difise component of reflection from image the method iis color and polarization simultaneously to obtain strong constraint on the reflection component at each image point polarization is used to locally determine the color oj the specular component constraining the difsuse color at a pixel to a one dimensional linear subspace this subspact is used to find neighboring pixel whose color is consistent with the pixel diffuse color information from consistent neighbor is used to determine the diffuse color of the pixel in contrast to previous separation algorithm the proposed method can handle highlight that have a varying diffuse component a well a highlight that include region with diffkrent reflectance and material property we present several txperimental result obtained by applying the algorithm to complex scene with textured object and strong interrejtrtions 
in this paper a method of theorem proving dual to resolution method is presented in brief the investigated method is called backward dual resolution or bd re solution for short the main idea of bd resolution consists in proving validity of a formula in disjunctive normal form by generating an empty tautology formula from it it is shown that the initial formula is a logical consequence of the obtained tautology an idea of the theorem proving method is outlined and it application to checking completeness of rule based system is investigated a formal definition of completeness and specific completeness are stated and an algorithm for completeness verification is proposed moreover a generalized bd resolution aimed at proving completeness under intended interpretation is defined 
in this paper we discus a nonparametric approach for calibrating a ccd camera a constrained topological mapping ctm approach to analyze the systematic imaging error of an image system and compare it with the parametric approach which are based on optimalization and have been discussed by many other author this nonparametric approach ha several distinct feature in this approach some distortion surface are derived directly from the training sample because no analytical form of these surface is assumed when we modeled the distortion by a nonparametric model the systematic imaging error instead of mere lens distortion are considered this give an new approach to analyze the imaging error of a particular imaging system experimental result are given in detail which indicate that both in image projection and in d reconstruction the accuracy is much improved when the nonparametric approach is employed for calibrating a camera 
a method is presented for identifying the view in an engineering drawing along with their associated view point in preparation for d interpretation of the object shape a formal procedure is developed for constructing a set of view based coordinate system that act a intermediate d coordinate to relate the d drawing based coordinate to the d object based coordinate the method can accommodate auxiliary view in addition to the standard orthogonal set and the number of view and their layout in the drawing need not be known a priori moderate error in line placement and view alignment can also be accommodated 
