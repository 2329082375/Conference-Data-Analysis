we describe a robot control architecture which combine a stimulus response subsystem for rapid reaction with a search based planner for handling unanticipated situation the robot agent continually chooses which action it is to perform using the stimulus response subsystem when possible and falling back on the planning subsystem when necessary whenever it is forced to plan it applies an explanation based learning mechanism to formulate a new stimulus response rule to cover this new situation and others similar to it with experience the agent becomes increasingly reactive a it learning component acquires new stimulus response rule that eliminate the need for planning in similar subsequent situation this theo agent architecture is described and result are presented demonstrating it ability to reduce routine reaction time for a simple mobile robot from minute to under a second 
traditional application of truth maintenance system tm fail to adequately represent heuristic search in which some path are initially preferred what they miss is the idea of switching context rationally based on heuristic preference we show that it is useful especially for plan with contingency to maintain the validity of the reason for context choice and rejection we demonstrate how to do so with a problem solver tm architecture called redux 
most approach to model based diagnosis describe a diagnosis for a system a a set of failing component that explains the symptom in order to characterize the typically very large number of diagnosis usually only the minimal such set of failing component are represented this method of characterizing all diagnosis is inadequate in general in part because not every superset of the faulty component of a diagnosis necessarily provides a diagnosis in this paper we analyze the notion of diagnosis in depth exploiting the notion of implicate implicant and prime implicate implicant we use these notion to propose two alternative approach for addressing the inadequacy of the concept of minimal diagnosis first we propose a new concept that of kernel diagnosis which is free of the problem of minimal diagnosis second we propose to restrict the axiom used to describe the system to ensure that the concept of minimal diagnosis is adequate 
one of the question in understanding the rela tion between circumscription and consistencybased nonmonotonic logic is can default logic be expressed in circumscription while it seems impossible to express default logic in ex isting form of circumscripti on is it neverthe le possible to express default logic in a certain extension of circumscripti on this paper present a construction of default logic in the spirit of circumscripti on it ha been shown that the new formalism circumscriptive exten sion is indeed an extension of circumscripti on the equivalence of the new formalism and default logic is shown to hold under certain con ditions which demonstrates that default logic can be expressed by merely classical logic with a fixed point operator 
although computer are widely used to simulate complex physical system crafting the underlying model that enable computer analysis remains difficult when a model is created for one task it is often impossible to reuse the model for another purpose because each task requires a different set of simplifying assumption by representing modeling assumption explicitly a approximation reformulations we have developed qualitative technique for switching between model we assume that automated reasoning proceeds in three phase model selection quantitative analysis using the model and validation that the assumption underlying the model were appropriate for the task at hand if validation discovers a serious discrepancy between predicted and observed behavior a new model must be chosen we present a domain independent method for performing this model shift when the model are related by an approximation reformulation and describe a common lisp implementation of the theory 
conventional blind search technique generally assume that the goal node for a givenproblem are distributed randomly along the fringe of the search tree we argue that thisis often invalid in practice and suggest that a more reasonable assumption is that decisionsmade at each point in the search carry equal weight we go on to show that a new searchtechnique called iterative broadening lead to order of magnitude saving in the time neededto search a space satisfying this assumption the 
the concept of inductive bias can be broken down into the underlying assumption of the domain the particular implementation choice that restrict or order the space of hypothesis considered by the learning program the bias choice and the inductive policy that link the two we define inductive policy a the strategy used to make bias choice based on the underlying assumption inductive policy decision involve addressing trade offs with respect to different bias choice without addressing these tradeoff bias choice will be made arbitrarily from the standpoint of inductive policy we discus two issue not addressed much in the machine learning literature first we discus batch learning with a strict time constraint and present an initial study with respect to trading off predictive accuracy for speed of learning next we discus the issue of learning in a domain where different type of error have different associated cost risk we show that by using different inductive policy accuracy can be traded off for safety we also show how the value for the latter tradeoff can be represented explicitly in a system that adjusts bias choice with respect to a particular inductive policy 
general purpose truth maintenance system have receivedconsiderable attention in the past few year thispaper discus the functionality of truth maintenancesystems and compare various existing algorithm applicationsand direction for future research are also discussed introductionin jon doyle wrote a master thesis at the mit ailaboratory entitled quot truth maintenance system forproblem solving quot doyle in this thesis doyledescribed an independent module called a 
the purpose of this paper is to expand the syntax and semantics of logic program and deductive database to allow for the correct representation of incomplete information in the presence of multiple extension the language of logic program with classical negation epistemic disjunction and negation by failure is further expanded by a new modal operator k where for the set of rule t and formula f kf stand for f is known to a reasoner with a set of premise t theory containing such an operator will be called strongly introspective we will define the semantics of such theory which expands the semantics of deductive database from gelfond and lifschitz bd and demonstrate the applicability of strongly introspective theory to formalization of some form of commonsense reasoning 
plan recognition requires the construction of possible plan which could explain a set of observed action and then selecting one or more of them a providing the belt explanation in this paper we present a formal model of the latter process based upon probability theory our model consists of a knowledge base of fact about the world expressed in a first order language and rule for using that knowledge base to construct a bayesian network the network is then evaluated to find the plan with the highest probability 
constraint satisfaction problem csps involve finding value for variable subject to constraint on which combination of value are permitted this paper develops a concept of interchangeability of csp value fully interchangeable value can be substituted for one another in solution to the problem removing all but one of a set of fully interchangeable value can simplify the search space for the problem without effectively losing solution refinement of the interchangeability concept extend it applicability basic property of interchangeablity and complexity parameter are established a hierarchy of local interchangeability is defined that permit recognition of some interchangeable value with polynomial time local computation computing local interchangeability at any level in this hierarchy to remove value before backtrack search is guaranteed to be cost effective for some csps several form of weak interchangeability are defined that permit eliminating value without losing all solution interchangeability can be introduced by grouping value or variable and can be recalculated dynamically during search the idea of interchangeability can be abstracted to encompass any mean of recovering the solution involving one value from the solution involving another 
we describe a robot control architecture which combine a stimulus response subsystem for rapid reaction with a search based planner for handling unanticipated situation the robot agent continually chooses which action it is to perform using the stimulus response subsystem when possible and falling back on the planning subsystem when necessary whenever it is forced to plan it applies an explanation based learning mechanism to formulate a new stimulus response rule to cover this new situation and others similar to it with experience the agent becomes increasingly reactive a it learning component acquires new stimulus response rule that eliminate the need for planning in similar subsequent situation this theo agent architecture is described and result are presented demonstrating it ability to reduce routine reaction time for a simple mobile robot from minute to under a second 
thanks to two stronger version of predicate circumscription one of the best known non monotonic reasoning method we give a definitive answer to two old open problem the first one is the problem of expressing do main circumscription in term of predicate cir cumscription the second one is the problem of definability of the circumscribed predicate asked by doyle in and never answered since these two result and the way used to obtain them could help an automatic circumscriptor 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
we develop representation for locative and path specifying preposition emphasizing the implementability of the underlying semantic primitive our primitive pertain to mechanical characteristic such a geometric relationship among object kinematic or motional characteristic implied by preposition the representation along with representation for action verb along similar line have been used to successfully animate the performance of task underlying natural language imperative by human agent 
computer and thought are the two category that together define artificial intelligence a a discipline it is generally accepted that work in artificial intelligence over the last thirty year ha had a strong influence on aspect of computer architecture in this paper we also make the converse claim that the state of computer architecture ha been a strong influence on our model of thought the von neumann model of computation ha lead artificial intelligence in particular direction intelligence in biological system is completely different recent work in behavior based artificial intelligence ha produced new model of intelligence that are much closer in spirit to biological system the non von neumann computational model they use share many characteristic with biological computation 
this paper present an efficient and homogeneous paradigm for automatic acquisition and recognition of nonparametric shape acquisition time varies from linear to cubic in the number of object feature recognition time is linear to cubic in the number of feature in the image and grows slowly with the number of stored model nonparametric shape representation is achieved by spatial autocorrelation transforms both acquisition and recognition are two step process in the first phase spatialautocorrelationoperators are applied to the image data to perform local shape analysis then spatial autocorrelation operator are applied to the local shape descriptor to either create entry acquisition or index recognition into a table containing the distributed shape information the output of the table is used to generate a density function on the space of possible shape with peak corresponding to high confidence in the presence of a particular shape instance the behavior of the system on a set of complex shape is shown with respect to occlusion geometric transformation and cluttered scene 
this paper show how using a nonmonotonic logic to describe the effect of action enables plausible plan to be discovered quickly and then refined if time permit candidate plan are found by allowing them to depend on unproved assumption the nonmonotonic logic make explicit which antecedent of rule have the status of default condition and they are the only one that may be left unproved so only plausible candidate plan are produced these are refined incrementally by trying to justify the assumption on which they depend the new planning strategy ha been implemented with good experimental result 
autoepistemic ae logic is a formal system characterizing agent that have complete introspective access to their own belief ae logic relies on a fixed point definition that ha two significant part the first part is a set of assumption or hypothesis about the content of the fixed point the second part is a set of reflection principle that link sentence with statement about their provability we characterize a family of ideal ae reasoner in term of the minimal hypothesis that they can make and the weakest and strongest reflection principle that they can have while still maintaining the interpretation of ae logic a self belief these result can help in analyzing metatheoretic system in logic programming 
existing approach to text generation fail to consider howinteractions with the user may be managed within a coherentexplanation or description this paper present anapproach to generating such interactive explanation basedon two level of discourse planning content planning anddialogue planning the system developed allows aspect ofthe changing context to be monitored with an explanation and the developing explanation to depend on this changingcontext interruption from the 
an important and readily available source of knowledge for common sense reasoning is partial description of specific experience knowledge base kb containing such information are called episodic knowledge base ekb aggregation of episodic knowledge provide common sense knowledge about the unobserved property of new experience such knowledge is retrieved by applying statistic to a relevant subset of the ekb called the reference class i study a manner in which a corpus of experience can be represented to allow common sense retrieval which is flexible enough to allow the common sense reasoner to deal with new experience and in the simplest case reduces to efficient database look up i define two first order dialect l and ql l is used to represent experience in an episodic knowledge base an extension ql is used for writing query to ekbs 
this paper synthesizes general constraint satisfaction and classical ai planning into a theory of incremental change that account for multiple objective and contingency the hypothesis is that this is a new and useful paradigm for problem solving and re solving a truth maintenance based architecture derived from the theory is useful for contingent assignment problem such a logistics planning 
recently there ha been much criticism in the ai community that knowledge based system are not situated we argue that trying to provide for situatedness in a conventional system will lead to the so called model explosion cycle and that for most application environment adaptivity is needed for situated behavior cooperative system are a solution to the model explosion cycle where adaptivity is delegated to the user a hybrid symbolic connectionist system offer self tuning capability and therefore adaptivity but can t cope with the model explosion cycle integrating both approach into a cooperative hybrid system lead to a much more situated behavior than conventional system can achieve our approach is illustrated using a real life expert system in the domain of technical troubleshooting ongoing practical test indicate that a cooperative hybrid design present an attractive architecture for knowledge based system 
in this paper we define the concept of logical consistency of belief among a group of computational agent that are able to reason nonmonotonically we then provide an algorithm for truth maintenance that guarantee local consistency for each agent and global consistency for data shared by the agent furthermore we show the algorithm to be complete in the sense that if a consistent state exists the algorithm will either find it or report failure the algorithm ha been implemented in the rad distributed expert system shell 
we present an approach to unsupervised concept fonnation based on accumulation of partial regularity using an algorithmic complexity framework we defme regularity a a model that achieves a compressed coding of data we discus induction of model we present induction of finite automaton model for regularity of string and induction of model based on vector translation for set of point the concept we develop are particularly appropriate for natural space structure that accept a decomposition into recurrent recognizable part they are usually hierarchical and suggest that a vocabulary of basic constituent can be learned before focussing on how they are assembled we define and illustrate basic regularity a algorithmically independent building block of structure they are identifiable a local maximum of compression a a function of model complexity stepwise induction consists in finding a model using it to compress the data then applying the same procedure on the code it is a way to induce in polynomial time structure whose basic component have bound complexity library are set of partial regularity a theoretical basis for clustering and concept formation finally we use the above concept to present a new perspective on explanation based generalization we prove it to be a language independent method to specialize the background knowledge 
this paper present a simple sound complete and systematic algorithm for domain independent strip planning simplicity is achieved by starting with a ground procedure and then applying a general and independently verifiable lifting transformation previous planner have been designed directly a lifted procedure our ground procedure is a ground version of tate s nonlin procedure in tate s procedure one is not required to determine whether a prerequisite of a step in an unfinished plan is guaranteed to hold in all linearizations this allows tate s procedure to avoid the use of chapman s modal truth criterion systematicity is the property that the same plan or partial plan is never examined more than once 
this work pertains to the knuth bendix kb algorithm which try to find a complete set of reduction from a given set of equation in the kb algorithm a term ordering is employed and it is required that every equation be orientable in the sense that the left hand side be greater than the right the kb algorithm halt if a non orientable equation is produced a generalization of the kb algorithm ha recently been developed in which every equation is orientable and which halt only when a complete set is generated in the generalization a constraint is added to each equation the constraint governs when the equation can be used a a reduction the constraint is obtained from the equation by solving the term inequality left hand side right hand side to understand what it mean to solve a term inequality consider the analogy with algebra in which solving term equality i e unification is analogous to solving algebraic equality then solving term inequality is analogous to solving algebraic inequality thus the solution of term inequality relates to unification a the solution of algebraic inequality relates to the solution of algebraic equality we show how to solve term inequality when using the lexicographic path ordering 
we present two concept language called pl and pl which are extension of tc we prove that the subsumption problem in these language can be solved in polynomial time both language include a construct for express ing inverse role which ha not been considered up to now in tractable language in addition pl includes number restriction and negation of primitive concept while pl includes role conjunction and role chaining by exploiting recent complexity result we show that none of the construct usually considered in concept language can be added to pl and pl without losing tractabtlity there fore on the assumption that language are characterized by the set of construct they provide the two language pre sented in this paper provide a solution to the problem of singling out an optimal trade off between expressive power and computational complexity 
we propose a new csp formalism that incorporates hard constraint and preference so that the two are easily distinguished both conceptually and for purpose of problem solving preference are represented a a lexicographic order over variable and domain value respectively constraint are treated in the usual manner therefore these problem can be solved with ordinary csp algorithm with the proviso that complete algorithm cannot terminate search after finding a feasible solution except in the important case of heuristic that follow the preference order lexical order we discus the relation of this problem representation to other formalism that have been applied to preference including soft constraint formalism and cp net we show how algorithm selection can be guided by work on phase transition which serve a a useful marker for a reversal in relative efficiency of lexical ordering and ordinary csp heuristic due to reduction in number of feasible solution we also consider branch and bound algorithm and their anytime property finally we consider partitioning strategy that take advantage of the implicit ordering of assignment in these problem to reduce the search space 
temporal projection predicting future state of a changing world ha been studied mainly a a formal problem researcher have been concerned with getting the concept of causality and change right and have ignored the practical issue surrounding projection in planning for example when the effect of a plan s action depend on the prevailing state of the world and that state of the world is not known with certainty projecting the plan may generate an exponential number of possible outcome this problem ha traditionally been eliminated by restricting the domain so the world state is always known and by restricting the action representation so that either the action s intended effect is realized or the action cannot be projected at all we argue against these restriction and instead present a system that represents and reason about an uncertain world support a representation that allows context sensitive action effect and generates projection that reflect only the significant or relevant outcome of the plan where relevance is determined by the planner s query about the resulting world state 
a genetic algorithm is used for learning qualitative model based on the qsim formalism hierarchical representation enables formation of submodels relevant for induction of domain explanation daring the search for better coding of the candidate in parallel with the search for better solution the sise and shape of candidate solution are dynamically created optimisation is based on the maximisation of the number of example covered by a candidate solution combined with the minimisation of the number of constraint used in the solution the result of learning is a set of model of different specificity that explain all given example an experiment in learning a qualitative model of the connected container system u tube is described in detail several solution equivalent to the original model were discovered 
natural language processing nlp is the study of mathematical and computational modeling of various aspect of language and the development of a wide range of system these include spoken language system that integrate speech and natural language cooperative interface to database and knowledge base that model aspect of human human interaction multilingual interface machine translation and message understanding system among others research in nlp is highly interdisciplinary involving concept in computer science linguistics logic and psychology nlp ha a special role in computer science because many aspect of the field deal with linguistic feature of computation and nlp seek to model language computationally 
joint action by a team doe not consist merely of simultaneous and coordinated individual action to act together a team must be aware of and care about the status of the group effort a a whole we present a formal definition of what it could mean for a group to jointly commit to a common goal and explore how these joint commitment relate to the individual commitment of the team member we then consider the case of joint intention where the goal in question involves the team performing some action in both case the theory is formulated in a logical language of belief action and time previously used to characterize individual commitment and intention an important consequence of the theory is the type of communication among the team member that it predicts will often be necessary 
most ai researcher would i believe agree that truly intelligent machine i e machine on a par with human will require at least four order of magnitude more power and memory than are available on any machine today schwartz waltz there is now widespread agreement in the supercomputing community that by the year all supercomputer defined a the most powerful machine available at a given time will be massively parallel fox yet relatively little thought ha been given in ai a to how to utilize such machine with few exception ai s attention ha been limited to workstation minicomputer and pc today s massively parallel machine present ai with a golden opportunity to make an impact especially in the world of commercial application the most striking near term opportunity is in the marriage of research on very large database with case based and memory based ai moreover such application are step on a path that can lead eventually to a class of truly intelligent system 
we consider the case of heuristic search where the location of the goal may change during the course of the search for example the goal may be a target that is actively avoiding the problem solver we present a moving target search algo rithm mt to solve this problem we prove that if the average speed of the target is slower than that of the problem solver then the prob lem solver is guaranteed to eventually reach the target an implementatio n with randomly po sitioned obstacle confirms that the mt algo rithm is highly effective in various situation 
automating proof by induction is important in many computer science and artificial intelligence application in particular in program verification and specification system we present a new method to prove and disprove automatically inductive property given a set of axiom a well suited induction scheme is constructed automatically we call such a scheme a test set then for proving a property we just instanciate it with term from the test set and apply pure algebraic simplification to the result this method avoids completion and explicit induction however it retains their positive feature namely the completeness of the former and the robustness of the latter 
the introduction of null unknown value in the relational database call for an extension of the theoretical foundation of the database model null are alien to classical logic in which the relational database model is rooted this ha led to all sort of counterintuitive ad hoc solution reasonable in one place but awkward in others a sound model theoretical foundation of null in a relational database based on modal logic is presented here the modal interpretation of query is easy to comprehend and intuitively correct partial interpretation which are to be preferred from a computational point of view are inadequate for arbitrary query formula that have an identical partial and modal interpretation are called safe safety guarantee on the one hand that the partial answer is meaningful and on the other hand that the modal interpretation is finitely computable the suitability of modal logic to model null is illustrated by a short discussion of the effect of null on database integrity 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
it ha been widely believed that qualitative analysis guide quantitative analysis while sufficient study ha not been made from technical viewpoint in this paper we present a case study with psx nl a program which autonomously analyzes the behavior of two dimensional nonlinear differential equation by integrating knowledge based method and numerical algorithm psx nl focus on geometric property of solution curve of ordinary differential equation in the phase space psx nl is designed based on a couple of novel idea a a set of flow mapping which is an abstract description of the behavior of solution curve and b a flow grammar which specifies all possible pattern of solution curve enabling psx n l to derive the most plausible interpretation when complete information is not available we describe the algorithm for deriving flow mapping 
this paper introduces the causal reconstruction task the task of reading a causal description of a physical system forming an internal model of the specified behavior and answering question demonstrating comprehension and reasoning on the basis of the input description a representation called transition space is introduced in which event are depicted a path fragment in a space of transition or complex of change in the attribute of participating object by identifying partial match between the transition space representation of event a program called pathfinder is able to perform causal reconstruction on short causal description presented in simplified english simple transformation applied to event representation prior to matching enable the program to bridge discontinuity arising from the writer s use of analogy or abstraction the operation of pathfinder is illustrated in the context of a simple causal description extracted from the encyclopedia americana involving exposure of film in a camera 
the goal in automatic programming is to get a computer to perform a task by telling it what need to be done rather than by explicitly programming it this paper considers the task of automatically generating a computer program to enable an autonomous mobile robot to perform the task of moving a box from the middle of an irregular shaped room to the wall we compare the ability of the recently developed genetic programming paradigm to produce such a program to the reported ability of reinforcement learning technique such a q learning to produce such a program in the style of the subsumption architecture the computational requirement of reinforcement learning necessitates considerable human knowledge and intervention whereas genetic programming come much closer to achieving the goal of getting the computer to perform the task without explicitly programming it the solution produced by genetic programming emerges a a result of darwinian natural selection and genetic crossover sexual recombination in a population of computer program the process is driven by a fitness measure which communicates the nature of the task to the computer and it learning paradigm 
in previous work zlotkin and rosenschein a we have developed a negotiation protocol and offered some negotiation strategy that are in equilibrium this negotiation process can be used only when the negotiation set n is not empty domain in which the negotiation set are never empty are called cooperative domain in general non cooperative domain the negotiation set is sometimes empty in this paper we present a theoretical negotiation model for rational agent in general non cooperative domain necessary and sufficient condition for cooperation are outlined by redefining the concept of utility we are able to enlarge the number of situation that have a cooperative solution an approach is offered for conflict resolution and it is shown that even in a conflict situation partial cooperative step can be taken by interacting agent that is agent in fundamental conflict might still agree to cooperate up to a certain point a unified negotiation protocol is developed that can be used in all case it is shown that in certain borderline cooperative situation a partial cooperative agreement i e one that doe not achieve all agent goal might be preferred by all agent even though there exists a rational agreement that would achieve all their goal 
a major obstacle to the widespread use of expert system in real time domain is the non predictability of response time while some researcher have addressed this issue by optimizing response time through better algorithm or parallel hardware there ha been little research towards run time prediction in order to meet user defined deadline to cope with the latter real time expert system must provide mechanism for estimating run time required to react to external event a a starting point for our investigation we chose the rete algorithm which is widely used for real time production system in spite of rete s combinatorial worst case match behavior we introduce a method forestimating match time in the rete network this paper show that simple profiling method do not work well but by going to a finer granularity we can get much better execution time prediction for basic action a well a for complete right hand side of rule our method is dynamically applied during the run time of the production system by using continuously updated statistical data of individual node in the rete network 
abstract classification method from statistical pattern recognition neural net and machine learning were applied to four real world data set each of these data set ha been previously analyzed and reported in the statistical medical or machine learning literature the data set are characterized by statisucal uncertainty there is no completely accurate solution to these problem training and testing or resampling technique are used to estimate the true error rate of the classification method detailed attention is given to the analysis of performance of the neural net using back propagation for these problem which have relatively few hypothesis and feature the machine learning procedure for rule induction or tree induction clearly performed best 
an original methodology called backward model tracing to model student performance which feature a profitable integration of the bug collection and bug construction technique is presented this methodology ha been used for building the modelling module of a new version of et english tutor an it aimed at supporting the learning of the english verb system backward model tracing is based on the idea of analyzing the reasoning process of the student by reconstructing step by step and in reverse order the chain of reasoning s he ha followed in giving his her answer in order to do this both correct domain specific knowledge and a catalogue of stereotyped error malrules are utilized when the system is unable to explain the student behavior by exploiting it previous knowledge new malrules are generated dynamically by utilizing explanation based learning technique the overall process is based on a deep modelling of the student problem solving and the discrimination among possible explicative hypothesis about the reason underlying the student behavior is carried on nonmonotonically through a truth maintenance system the proposed approach ha been fully implemented in a student modelling module developed in prolog 
merging operator in a plan can yield significant saving in the cost to execute a plan past research in planning ha concentrated on handling harmful interaction among plan but the understanding of positive one ha remained at a qualitative heuristic level this paper provides a quantitative study for plan optimization and present both optimal and approximate algorithm for finding minimum cost merged plan with worst and average case complexity analysis and empirical test we demonstrate that efficient and wellbehaved approximation algorithm are applicable for optimizing general plan with large size 
the purpose of this paper is to characterize a constituent boundary parsing algorithm using an information theoretic measure called generalized mutual information which serf a an alternative to traditional grammar based parsing method this method is based on the hypothesis that constituent boundary can be extracted from a given sentence or word sequence by analyzing the mutual information value of the part of speech n gram within the sentence this hypothesis is supported by the performance of an implementation of this parsing algorithm which determines a recursive unlabeled bracketing of unrestricted english text with a relatively low error rate this paper derives the generalized mutual information statistic describes the parsing algorithm and present result and sample output from the parser 
the candidate elimination algorithm for inductive learning with version space can require both exponential time and space this article describes the incremental non backtracking focusing inbf algorithm which learns strictly tree structured concept in polynomial space and time specifically it learns in time o pnk and space o nk where p is the number of positive n the number of negative and k the number of feature inbf is an extension of an existing batch algorithm avoidance focusing af although af also learns in polynomial time it assumes a convergent set of positive example and handle additional example inefficiently inbf ha neither of these restriction both the af and inbf algorithm assume that the positive example plus the near miss will be sufficient for convergence if the initial set of example is convergent this article formally prof that for treestructured concept this assumption doe in fact hold 
in this paper a resolution method for propositional temporal logic is presented temporal formula incorporating both past time and future time temporal operator are converted to separated normal form snf then both non temporal and temporal resolution rule are applied the resolution method is based on classical resolution but incorporates a temporal resolution rule that can be implemented efficiently using a graph theoretic approach 
this paper report a case study on a large scale and corporate wide case based system unlike most paper for the aaai conference which exclusively focus on algorithm and model executed on computer system this paper heavily involves organizational activity and structure a a part of algorithm in the system it is our claim that successful corporate wide deployment of the case base system must involve organizational effort a a part of an algorithmic loop in the system in a broad sense we have established a corporate wide case acquisition algorithm which is performed by person and developed the squad software quality control advisor system which facilitates sharing and spreading of experience corporate wide the major finding were that the key for the success is not necessary in complex and sophisticated ai theory in fact we use very simple algorithm but the integration of mechanism and algorithm executed by machine and person involved 
all major approach to qualitative reasoning rely on the existence of a model of the physical system however the task of finding a model is usually far from trivial within the area of electrical engineering model building method have been developed to automatically deduce model from measurement in this paper we explicitly show how to incorporate qualitative knowledge in order to apply these method to situation where they do not behave satisfactorily a program ha been developed and applied to a non trivial example the qualitative input in term of an incomplete bond graph and the resulting output can be used to form a more complete bond graph this more informative model is suitable for further reasoning 
we report here on our experiment with post part of speech tagger to address problem of ambiguity and of understanding unknown word part of speech tagging perse is a well understood problem our paper report experiment in three important area handling unknown word limiting the size of the training set and returning a set of the most likely tag for each word rather than a single tag we describe the algorithm that we used and the specific result of our experiment on wall street journal article and on muc terrorist message 
the task of obtaining a line labeling from a greyscale image of trihedral object present difficulty not found in the classical line labeling problem a originally formulated the line labeling problem assumed that each junction wa correctly pre classified a being of a particular junction type e g t y arrow the success of the algorithm proposed have depended critically upon getting this initial junction classification correct in real image however junction of different type may actually look quite similar and this preclassification is often difficult to achieve this issue is addressed by recasting the line labeling problem in term of a coupled probabilistic system which label both line and junction this result in a robust system in which prior knowledge of acceptable configuration can serve to overcome the problem of misleading or ambiguous evidence 
simple indoor navigation subtasks such a moving an autonomousplatform robot in a corridor parallel to a wall or correcting it trajectoryto avoid small obstacle can be accomplished using reflexivebehaviours without the need of a navigation planner we describe adynamic vision module in which reflex are implemented a feedbacksystems measurement on the image data provide the position of thefloor boundary in the coordinate system of the onboard camera thesystem state vector 
traditional application of truth maintenance system tm fail to adequately represent heuristic search in which some path are initially preferred what they miss is the idea of switching context rationally based on heuristic preference we show that it is useful especially for plan with contingency to maintain the validity of the reason for context choice and rejection we demonstrate how to do so with a problem solver tm architecture called redux 
most approach to model based diagnosis describe a diagnosis for a system a a set of failing component that explains the symptom in order to characterize the typically very large number of diagnosis usually only the minimal such set of failing component are represented this method of characterizing all diagnosis is inadequate in general in part because not every superset of the faulty component of a diagnosis necessarily provides a diagnosis in this paper we analyze the notion of diagnosis in depth exploiting the notion of implicate implicant and prime implicate implicant we use these notion to propose two alternative approach for addressing the inadequacy of the concept of minimal diagnosis first we propose a new concept that of kernel diagnosis which is free of the problem of minimal diagnosis second we propose to restrict the axiom used to describe the system to ensure that the concept of minimal diagnosis is adequate 
one of the question in understanding the rela tion between circumscription and consistencybased nonmonotonic logic is can default logic be expressed in circumscription while it seems impossible to express default logic in ex isting form of circumscripti on is it neverthe le possible to express default logic in a certain extension of circumscripti on this paper present a construction of default logic in the spirit of circumscripti on it ha been shown that the new formalism circumscriptive exten sion is indeed an extension of circumscripti on the equivalence of the new formalism and default logic is shown to hold under certain con ditions which demonstrates that default logic can be expressed by merely classical logic with a fixed point operator 
although computer are widely used to simulate complex physical system crafting the underlying model that enable computer analysis remains difficult when a model is created for one task it is often impossible to reuse the model for another purpose because each task requires a different set of simplifying assumption by representing modeling assumption explicitly a approximation reformulations we have developed qualitative technique for switching between model we assume that automated reasoning proceeds in three phase model selection quantitative analysis using the model and validation that the assumption underlying the model were appropriate for the task at hand if validation discovers a serious discrepancy between predicted and observed behavior a new model must be chosen we present a domain independent method for performing this model shift when the model are related by an approximation reformulation and describe a common lisp implementation of the theory 
conventional blind search technique generally assume that the goal node for a givenproblem are distributed randomly along the fringe of the search tree we argue that thisis often invalid in practice and suggest that a more reasonable assumption is that decisionsmade at each point in the search carry equal weight we go on to show that a new searchtechnique called iterative broadening lead to order of magnitude saving in the time neededto search a space satisfying this assumption the 
the concept of inductive bias can be broken down into the underlying assumption of the domain the particular implementation choice that restrict or order the space of hypothesis considered by the learning program the bias choice and the inductive policy that link the two we define inductive policy a the strategy used to make bias choice based on the underlying assumption inductive policy decision involve addressing trade offs with respect to different bias choice without addressing these tradeoff bias choice will be made arbitrarily from the standpoint of inductive policy we discus two issue not addressed much in the machine learning literature first we discus batch learning with a strict time constraint and present an initial study with respect to trading off predictive accuracy for speed of learning next we discus the issue of learning in a domain where different type of error have different associated cost risk we show that by using different inductive policy accuracy can be traded off for safety we also show how the value for the latter tradeoff can be represented explicitly in a system that adjusts bias choice with respect to a particular inductive policy 
general purpose truth maintenance system have receivedconsiderable attention in the past few year thispaper discus the functionality of truth maintenancesystems and compare various existing algorithm applicationsand direction for future research are also discussed introductionin jon doyle wrote a master thesis at the mit ailaboratory entitled quot truth maintenance system forproblem solving quot doyle in this thesis doyledescribed an independent module called a 
the purpose of this paper is to expand the syntax and semantics of logic program and deductive database to allow for the correct representation of incomplete information in the presence of multiple extension the language of logic program with classical negation epistemic disjunction and negation by failure is further expanded by a new modal operator k where for the set of rule t and formula f kf stand for f is known to a reasoner with a set of premise t theory containing such an operator will be called strongly introspective we will define the semantics of such theory which expands the semantics of deductive database from gelfond and lifschitz bd and demonstrate the applicability of strongly introspective theory to formalization of some form of commonsense reasoning 
plan recognition requires the construction of possible plan which could explain a set of observed action and then selecting one or more of them a providing the belt explanation in this paper we present a formal model of the latter process based upon probability theory our model consists of a knowledge base of fact about the world expressed in a first order language and rule for using that knowledge base to construct a bayesian network the network is then evaluated to find the plan with the highest probability 
constraint satisfaction problem csps involve finding value for variable subject to constraint on which combination of value are permitted this paper develops a concept of interchangeability of csp value fully interchangeable value can be substituted for one another in solution to the problem removing all but one of a set of fully interchangeable value can simplify the search space for the problem without effectively losing solution refinement of the interchangeability concept extend it applicability basic property of interchangeablity and complexity parameter are established a hierarchy of local interchangeability is defined that permit recognition of some interchangeable value with polynomial time local computation computing local interchangeability at any level in this hierarchy to remove value before backtrack search is guaranteed to be cost effective for some csps several form of weak interchangeability are defined that permit eliminating value without losing all solution interchangeability can be introduced by grouping value or variable and can be recalculated dynamically during search the idea of interchangeability can be abstracted to encompass any mean of recovering the solution involving one value from the solution involving another 
we describe a robot control architecture which combine a stimulus response subsystem for rapid reaction with a search based planner for handling unanticipated situation the robot agent continually chooses which action it is to perform using the stimulus response subsystem when possible and falling back on the planning subsystem when necessary whenever it is forced to plan it applies an explanation based learning mechanism to formulate a new stimulus response rule to cover this new situation and others similar to it with experience the agent becomes increasingly reactive a it learning component acquires new stimulus response rule that eliminate the need for planning in similar subsequent situation this theo agent architecture is described and result are presented demonstrating it ability to reduce routine reaction time for a simple mobile robot from minute to under a second 
thanks to two stronger version of predicate circumscription one of the best known non monotonic reasoning method we give a definitive answer to two old open problem the first one is the problem of expressing do main circumscription in term of predicate cir cumscription the second one is the problem of definability of the circumscribed predicate asked by doyle in and never answered since these two result and the way used to obtain them could help an automatic circumscriptor 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
we develop representation for locative and path specifying preposition emphasizing the implementability of the underlying semantic primitive our primitive pertain to mechanical characteristic such a geometric relationship among object kinematic or motional characteristic implied by preposition the representation along with representation for action verb along similar line have been used to successfully animate the performance of task underlying natural language imperative by human agent 
computer and thought are the two category that together define artificial intelligence a a discipline it is generally accepted that work in artificial intelligence over the last thirty year ha had a strong influence on aspect of computer architecture in this paper we also make the converse claim that the state of computer architecture ha been a strong influence on our model of thought the von neumann model of computation ha lead artificial intelligence in particular direction intelligence in biological system is completely different recent work in behavior based artificial intelligence ha produced new model of intelligence that are much closer in spirit to biological system the non von neumann computational model they use share many characteristic with biological computation 
this paper present an efficient and homogeneous paradigm for automatic acquisition and recognition of nonparametric shape acquisition time varies from linear to cubic in the number of object feature recognition time is linear to cubic in the number of feature in the image and grows slowly with the number of stored model nonparametric shape representation is achieved by spatial autocorrelation transforms both acquisition and recognition are two step process in the first phase spatialautocorrelationoperators are applied to the image data to perform local shape analysis then spatial autocorrelation operator are applied to the local shape descriptor to either create entry acquisition or index recognition into a table containing the distributed shape information the output of the table is used to generate a density function on the space of possible shape with peak corresponding to high confidence in the presence of a particular shape instance the behavior of the system on a set of complex shape is shown with respect to occlusion geometric transformation and cluttered scene 
this paper show how using a nonmonotonic logic to describe the effect of action enables plausible plan to be discovered quickly and then refined if time permit candidate plan are found by allowing them to depend on unproved assumption the nonmonotonic logic make explicit which antecedent of rule have the status of default condition and they are the only one that may be left unproved so only plausible candidate plan are produced these are refined incrementally by trying to justify the assumption on which they depend the new planning strategy ha been implemented with good experimental result 
autoepistemic ae logic is a formal system characterizing agent that have complete introspective access to their own belief ae logic relies on a fixed point definition that ha two significant part the first part is a set of assumption or hypothesis about the content of the fixed point the second part is a set of reflection principle that link sentence with statement about their provability we characterize a family of ideal ae reasoner in term of the minimal hypothesis that they can make and the weakest and strongest reflection principle that they can have while still maintaining the interpretation of ae logic a self belief these result can help in analyzing metatheoretic system in logic programming 
existing approach to text generation fail to consider howinteractions with the user may be managed within a coherentexplanation or description this paper present anapproach to generating such interactive explanation basedon two level of discourse planning content planning anddialogue planning the system developed allows aspect ofthe changing context to be monitored with an explanation and the developing explanation to depend on this changingcontext interruption from the 
an important and readily available source of knowledge for common sense reasoning is partial description of specific experience knowledge base kb containing such information are called episodic knowledge base ekb aggregation of episodic knowledge provide common sense knowledge about the unobserved property of new experience such knowledge is retrieved by applying statistic to a relevant subset of the ekb called the reference class i study a manner in which a corpus of experience can be represented to allow common sense retrieval which is flexible enough to allow the common sense reasoner to deal with new experience and in the simplest case reduces to efficient database look up i define two first order dialect l and ql l is used to represent experience in an episodic knowledge base an extension ql is used for writing query to ekbs 
this paper synthesizes general constraint satisfaction and classical ai planning into a theory of incremental change that account for multiple objective and contingency the hypothesis is that this is a new and useful paradigm for problem solving and re solving a truth maintenance based architecture derived from the theory is useful for contingent assignment problem such a logistics planning 
recently there ha been much criticism in the ai community that knowledge based system are not situated we argue that trying to provide for situatedness in a conventional system will lead to the so called model explosion cycle and that for most application environment adaptivity is needed for situated behavior cooperative system are a solution to the model explosion cycle where adaptivity is delegated to the user a hybrid symbolic connectionist system offer self tuning capability and therefore adaptivity but can t cope with the model explosion cycle integrating both approach into a cooperative hybrid system lead to a much more situated behavior than conventional system can achieve our approach is illustrated using a real life expert system in the domain of technical troubleshooting ongoing practical test indicate that a cooperative hybrid design present an attractive architecture for knowledge based system 
in this paper we define the concept of logical consistency of belief among a group of computational agent that are able to reason nonmonotonically we then provide an algorithm for truth maintenance that guarantee local consistency for each agent and global consistency for data shared by the agent furthermore we show the algorithm to be complete in the sense that if a consistent state exists the algorithm will either find it or report failure the algorithm ha been implemented in the rad distributed expert system shell 
we present an approach to unsupervised concept fonnation based on accumulation of partial regularity using an algorithmic complexity framework we defme regularity a a model that achieves a compressed coding of data we discus induction of model we present induction of finite automaton model for regularity of string and induction of model based on vector translation for set of point the concept we develop are particularly appropriate for natural space structure that accept a decomposition into recurrent recognizable part they are usually hierarchical and suggest that a vocabulary of basic constituent can be learned before focussing on how they are assembled we define and illustrate basic regularity a algorithmically independent building block of structure they are identifiable a local maximum of compression a a function of model complexity stepwise induction consists in finding a model using it to compress the data then applying the same procedure on the code it is a way to induce in polynomial time structure whose basic component have bound complexity library are set of partial regularity a theoretical basis for clustering and concept formation finally we use the above concept to present a new perspective on explanation based generalization we prove it to be a language independent method to specialize the background knowledge 
this paper present a simple sound complete and systematic algorithm for domain independent strip planning simplicity is achieved by starting with a ground procedure and then applying a general and independently verifiable lifting transformation previous planner have been designed directly a lifted procedure our ground procedure is a ground version of tate s nonlin procedure in tate s procedure one is not required to determine whether a prerequisite of a step in an unfinished plan is guaranteed to hold in all linearizations this allows tate s procedure to avoid the use of chapman s modal truth criterion systematicity is the property that the same plan or partial plan is never examined more than once 
this work pertains to the knuth bendix kb algorithm which try to find a complete set of reduction from a given set of equation in the kb algorithm a term ordering is employed and it is required that every equation be orientable in the sense that the left hand side be greater than the right the kb algorithm halt if a non orientable equation is produced a generalization of the kb algorithm ha recently been developed in which every equation is orientable and which halt only when a complete set is generated in the generalization a constraint is added to each equation the constraint governs when the equation can be used a a reduction the constraint is obtained from the equation by solving the term inequality left hand side right hand side to understand what it mean to solve a term inequality consider the analogy with algebra in which solving term equality i e unification is analogous to solving algebraic equality then solving term inequality is analogous to solving algebraic inequality thus the solution of term inequality relates to unification a the solution of algebraic inequality relates to the solution of algebraic equality we show how to solve term inequality when using the lexicographic path ordering 
we present two concept language called pl and pl which are extension of tc we prove that the subsumption problem in these language can be solved in polynomial time both language include a construct for express ing inverse role which ha not been considered up to now in tractable language in addition pl includes number restriction and negation of primitive concept while pl includes role conjunction and role chaining by exploiting recent complexity result we show that none of the construct usually considered in concept language can be added to pl and pl without losing tractabtlity there fore on the assumption that language are characterized by the set of construct they provide the two language pre sented in this paper provide a solution to the problem of singling out an optimal trade off between expressive power and computational complexity 
we propose a new csp formalism that incorporates hard constraint and preference so that the two are easily distinguished both conceptually and for purpose of problem solving preference are represented a a lexicographic order over variable and domain value respectively constraint are treated in the usual manner therefore these problem can be solved with ordinary csp algorithm with the proviso that complete algorithm cannot terminate search after finding a feasible solution except in the important case of heuristic that follow the preference order lexical order we discus the relation of this problem representation to other formalism that have been applied to preference including soft constraint formalism and cp net we show how algorithm selection can be guided by work on phase transition which serve a a useful marker for a reversal in relative efficiency of lexical ordering and ordinary csp heuristic due to reduction in number of feasible solution we also consider branch and bound algorithm and their anytime property finally we consider partitioning strategy that take advantage of the implicit ordering of assignment in these problem to reduce the search space 
temporal projection predicting future state of a changing world ha been studied mainly a a formal problem researcher have been concerned with getting the concept of causality and change right and have ignored the practical issue surrounding projection in planning for example when the effect of a plan s action depend on the prevailing state of the world and that state of the world is not known with certainty projecting the plan may generate an exponential number of possible outcome this problem ha traditionally been eliminated by restricting the domain so the world state is always known and by restricting the action representation so that either the action s intended effect is realized or the action cannot be projected at all we argue against these restriction and instead present a system that represents and reason about an uncertain world support a representation that allows context sensitive action effect and generates projection that reflect only the significant or relevant outcome of the plan where relevance is determined by the planner s query about the resulting world state 
a genetic algorithm is used for learning qualitative model based on the qsim formalism hierarchical representation enables formation of submodels relevant for induction of domain explanation daring the search for better coding of the candidate in parallel with the search for better solution the sise and shape of candidate solution are dynamically created optimisation is based on the maximisation of the number of example covered by a candidate solution combined with the minimisation of the number of constraint used in the solution the result of learning is a set of model of different specificity that explain all given example an experiment in learning a qualitative model of the connected container system u tube is described in detail several solution equivalent to the original model were discovered 
natural language processing nlp is the study of mathematical and computational modeling of various aspect of language and the development of a wide range of system these include spoken language system that integrate speech and natural language cooperative interface to database and knowledge base that model aspect of human human interaction multilingual interface machine translation and message understanding system among others research in nlp is highly interdisciplinary involving concept in computer science linguistics logic and psychology nlp ha a special role in computer science because many aspect of the field deal with linguistic feature of computation and nlp seek to model language computationally 
joint action by a team doe not consist merely of simultaneous and coordinated individual action to act together a team must be aware of and care about the status of the group effort a a whole we present a formal definition of what it could mean for a group to jointly commit to a common goal and explore how these joint commitment relate to the individual commitment of the team member we then consider the case of joint intention where the goal in question involves the team performing some action in both case the theory is formulated in a logical language of belief action and time previously used to characterize individual commitment and intention an important consequence of the theory is the type of communication among the team member that it predicts will often be necessary 
most ai researcher would i believe agree that truly intelligent machine i e machine on a par with human will require at least four order of magnitude more power and memory than are available on any machine today schwartz waltz there is now widespread agreement in the supercomputing community that by the year all supercomputer defined a the most powerful machine available at a given time will be massively parallel fox yet relatively little thought ha been given in ai a to how to utilize such machine with few exception ai s attention ha been limited to workstation minicomputer and pc today s massively parallel machine present ai with a golden opportunity to make an impact especially in the world of commercial application the most striking near term opportunity is in the marriage of research on very large database with case based and memory based ai moreover such application are step on a path that can lead eventually to a class of truly intelligent system 
we consider the case of heuristic search where the location of the goal may change during the course of the search for example the goal may be a target that is actively avoiding the problem solver we present a moving target search algo rithm mt to solve this problem we prove that if the average speed of the target is slower than that of the problem solver then the prob lem solver is guaranteed to eventually reach the target an implementatio n with randomly po sitioned obstacle confirms that the mt algo rithm is highly effective in various situation 
automating proof by induction is important in many computer science and artificial intelligence application in particular in program verification and specification system we present a new method to prove and disprove automatically inductive property given a set of axiom a well suited induction scheme is constructed automatically we call such a scheme a test set then for proving a property we just instanciate it with term from the test set and apply pure algebraic simplification to the result this method avoids completion and explicit induction however it retains their positive feature namely the completeness of the former and the robustness of the latter 
the introduction of null unknown value in the relational database call for an extension of the theoretical foundation of the database model null are alien to classical logic in which the relational database model is rooted this ha led to all sort of counterintuitive ad hoc solution reasonable in one place but awkward in others a sound model theoretical foundation of null in a relational database based on modal logic is presented here the modal interpretation of query is easy to comprehend and intuitively correct partial interpretation which are to be preferred from a computational point of view are inadequate for arbitrary query formula that have an identical partial and modal interpretation are called safe safety guarantee on the one hand that the partial answer is meaningful and on the other hand that the modal interpretation is finitely computable the suitability of modal logic to model null is illustrated by a short discussion of the effect of null on database integrity 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
it ha been widely believed that qualitative analysis guide quantitative analysis while sufficient study ha not been made from technical viewpoint in this paper we present a case study with psx nl a program which autonomously analyzes the behavior of two dimensional nonlinear differential equation by integrating knowledge based method and numerical algorithm psx nl focus on geometric property of solution curve of ordinary differential equation in the phase space psx nl is designed based on a couple of novel idea a a set of flow mapping which is an abstract description of the behavior of solution curve and b a flow grammar which specifies all possible pattern of solution curve enabling psx n l to derive the most plausible interpretation when complete information is not available we describe the algorithm for deriving flow mapping 
this paper introduces the causal reconstruction task the task of reading a causal description of a physical system forming an internal model of the specified behavior and answering question demonstrating comprehension and reasoning on the basis of the input description a representation called transition space is introduced in which event are depicted a path fragment in a space of transition or complex of change in the attribute of participating object by identifying partial match between the transition space representation of event a program called pathfinder is able to perform causal reconstruction on short causal description presented in simplified english simple transformation applied to event representation prior to matching enable the program to bridge discontinuity arising from the writer s use of analogy or abstraction the operation of pathfinder is illustrated in the context of a simple causal description extracted from the encyclopedia americana involving exposure of film in a camera 
the goal in automatic programming is to get a computer to perform a task by telling it what need to be done rather than by explicitly programming it this paper considers the task of automatically generating a computer program to enable an autonomous mobile robot to perform the task of moving a box from the middle of an irregular shaped room to the wall we compare the ability of the recently developed genetic programming paradigm to produce such a program to the reported ability of reinforcement learning technique such a q learning to produce such a program in the style of the subsumption architecture the computational requirement of reinforcement learning necessitates considerable human knowledge and intervention whereas genetic programming come much closer to achieving the goal of getting the computer to perform the task without explicitly programming it the solution produced by genetic programming emerges a a result of darwinian natural selection and genetic crossover sexual recombination in a population of computer program the process is driven by a fitness measure which communicates the nature of the task to the computer and it learning paradigm 
in previous work zlotkin and rosenschein a we have developed a negotiation protocol and offered some negotiation strategy that are in equilibrium this negotiation process can be used only when the negotiation set n is not empty domain in which the negotiation set are never empty are called cooperative domain in general non cooperative domain the negotiation set is sometimes empty in this paper we present a theoretical negotiation model for rational agent in general non cooperative domain necessary and sufficient condition for cooperation are outlined by redefining the concept of utility we are able to enlarge the number of situation that have a cooperative solution an approach is offered for conflict resolution and it is shown that even in a conflict situation partial cooperative step can be taken by interacting agent that is agent in fundamental conflict might still agree to cooperate up to a certain point a unified negotiation protocol is developed that can be used in all case it is shown that in certain borderline cooperative situation a partial cooperative agreement i e one that doe not achieve all agent goal might be preferred by all agent even though there exists a rational agreement that would achieve all their goal 
a major obstacle to the widespread use of expert system in real time domain is the non predictability of response time while some researcher have addressed this issue by optimizing response time through better algorithm or parallel hardware there ha been little research towards run time prediction in order to meet user defined deadline to cope with the latter real time expert system must provide mechanism for estimating run time required to react to external event a a starting point for our investigation we chose the rete algorithm which is widely used for real time production system in spite of rete s combinatorial worst case match behavior we introduce a method forestimating match time in the rete network this paper show that simple profiling method do not work well but by going to a finer granularity we can get much better execution time prediction for basic action a well a for complete right hand side of rule our method is dynamically applied during the run time of the production system by using continuously updated statistical data of individual node in the rete network 
abstract classification method from statistical pattern recognition neural net and machine learning were applied to four real world data set each of these data set ha been previously analyzed and reported in the statistical medical or machine learning literature the data set are characterized by statisucal uncertainty there is no completely accurate solution to these problem training and testing or resampling technique are used to estimate the true error rate of the classification method detailed attention is given to the analysis of performance of the neural net using back propagation for these problem which have relatively few hypothesis and feature the machine learning procedure for rule induction or tree induction clearly performed best 
an original methodology called backward model tracing to model student performance which feature a profitable integration of the bug collection and bug construction technique is presented this methodology ha been used for building the modelling module of a new version of et english tutor an it aimed at supporting the learning of the english verb system backward model tracing is based on the idea of analyzing the reasoning process of the student by reconstructing step by step and in reverse order the chain of reasoning s he ha followed in giving his her answer in order to do this both correct domain specific knowledge and a catalogue of stereotyped error malrules are utilized when the system is unable to explain the student behavior by exploiting it previous knowledge new malrules are generated dynamically by utilizing explanation based learning technique the overall process is based on a deep modelling of the student problem solving and the discrimination among possible explicative hypothesis about the reason underlying the student behavior is carried on nonmonotonically through a truth maintenance system the proposed approach ha been fully implemented in a student modelling module developed in prolog 
merging operator in a plan can yield significant saving in the cost to execute a plan past research in planning ha concentrated on handling harmful interaction among plan but the understanding of positive one ha remained at a qualitative heuristic level this paper provides a quantitative study for plan optimization and present both optimal and approximate algorithm for finding minimum cost merged plan with worst and average case complexity analysis and empirical test we demonstrate that efficient and wellbehaved approximation algorithm are applicable for optimizing general plan with large size 
the purpose of this paper is to characterize a constituent boundary parsing algorithm using an information theoretic measure called generalized mutual information which serf a an alternative to traditional grammar based parsing method this method is based on the hypothesis that constituent boundary can be extracted from a given sentence or word sequence by analyzing the mutual information value of the part of speech n gram within the sentence this hypothesis is supported by the performance of an implementation of this parsing algorithm which determines a recursive unlabeled bracketing of unrestricted english text with a relatively low error rate this paper derives the generalized mutual information statistic describes the parsing algorithm and present result and sample output from the parser 
the candidate elimination algorithm for inductive learning with version space can require both exponential time and space this article describes the incremental non backtracking focusing inbf algorithm which learns strictly tree structured concept in polynomial space and time specifically it learns in time o pnk and space o nk where p is the number of positive n the number of negative and k the number of feature inbf is an extension of an existing batch algorithm avoidance focusing af although af also learns in polynomial time it assumes a convergent set of positive example and handle additional example inefficiently inbf ha neither of these restriction both the af and inbf algorithm assume that the positive example plus the near miss will be sufficient for convergence if the initial set of example is convergent this article formally prof that for treestructured concept this assumption doe in fact hold 
in this paper a resolution method for propositional temporal logic is presented temporal formula incorporating both past time and future time temporal operator are converted to separated normal form snf then both non temporal and temporal resolution rule are applied the resolution method is based on classical resolution but incorporates a temporal resolution rule that can be implemented efficiently using a graph theoretic approach 
this paper report a case study on a large scale and corporate wide case based system unlike most paper for the aaai conference which exclusively focus on algorithm and model executed on computer system this paper heavily involves organizational activity and structure a a part of algorithm in the system it is our claim that successful corporate wide deployment of the case base system must involve organizational effort a a part of an algorithmic loop in the system in a broad sense we have established a corporate wide case acquisition algorithm which is performed by person and developed the squad software quality control advisor system which facilitates sharing and spreading of experience corporate wide the major finding were that the key for the success is not necessary in complex and sophisticated ai theory in fact we use very simple algorithm but the integration of mechanism and algorithm executed by machine and person involved 
all major approach to qualitative reasoning rely on the existence of a model of the physical system however the task of finding a model is usually far from trivial within the area of electrical engineering model building method have been developed to automatically deduce model from measurement in this paper we explicitly show how to incorporate qualitative knowledge in order to apply these method to situation where they do not behave satisfactorily a program ha been developed and applied to a non trivial example the qualitative input in term of an incomplete bond graph and the resulting output can be used to form a more complete bond graph this more informative model is suitable for further reasoning 
we report here on our experiment with post part of speech tagger to address problem of ambiguity and of understanding unknown word part of speech tagging perse is a well understood problem our paper report experiment in three important area handling unknown word limiting the size of the training set and returning a set of the most likely tag for each word rather than a single tag we describe the algorithm that we used and the specific result of our experiment on wall street journal article and on muc terrorist message 
the task of obtaining a line labeling from a greyscale image of trihedral object present difficulty not found in the classical line labeling problem a originally formulated the line labeling problem assumed that each junction wa correctly pre classified a being of a particular junction type e g t y arrow the success of the algorithm proposed have depended critically upon getting this initial junction classification correct in real image however junction of different type may actually look quite similar and this preclassification is often difficult to achieve this issue is addressed by recasting the line labeling problem in term of a coupled probabilistic system which label both line and junction this result in a robust system in which prior knowledge of acceptable configuration can serve to overcome the problem of misleading or ambiguous evidence 
simple indoor navigation subtasks such a moving an autonomousplatform robot in a corridor parallel to a wall or correcting it trajectoryto avoid small obstacle can be accomplished using reflexivebehaviours without the need of a navigation planner we describe adynamic vision module in which reflex are implemented a feedbacksystems measurement on the image data provide the position of thefloor boundary in the coordinate system of the onboard camera thesystem state vector 
a class of concept learning algorithm cl augments standard similarity based technique by performing feature construction based on the sbl output pagallo and hausslcr s fringe pagallo s extension symmetric fringe sym fringe and a refinement we call dcfringe are all instance of this class using decision tree a their underlying representation these method use pattern at the fringe of the tree to guide their construction but dcfringe us limited construction of conjunction and disjunction experiment with small dnf and cnf concept show that dcfringe outperforms both the purely conjunctive fringe and the le restrictive symfringe in term of accuracy conciseness and efficiency further the gain of these method is linked to the size of the training set we discus the apparent limitation of current method to concept exhibiting a low degree of feature interaction and suggest way to alleviate it this lead to a feature construction approach based on a wider variety of pattern restricted by statistical measure and optional knowledge 
we present a novel object centered formalization of action which allows u to define an interesting class of task called cooking task which can be performed without backtracking since backtracking is unnecessary action can be selected incrementally using a greedy method without having to precompute a plan such an approach is efficient and rapidly adjusts to unforeseen circumstance our argument is that cooking task are widely encountered in everyday life because of the special property of a given culture s artifact in other word culture ha structured the world so a to make it easier to live in we present an implementation of these idea experimental result and control experiment using a standard nonlinear planner 
i describe several computational complexity result for planning some of which identify tractable planning problem the model of planning called propositional planning is simple condition within operator are literal with no variable allowed the different plan ning problem are defined by different restriction on the precondition and postconditions of operator the main result are proposi tional planning is pspace complete even if operator are restricted to two positive nonnegated precondition and two postconditions or if operator are restricted to one postcondi tion with any number of precondition it is np complete if operator are restricted to positive postconditions even if operator are restricted to one precondition and one posi tive postcondition it is tractable in a few re stricted case one of which is if each opera tor is restricted to positive precondition and one postcondition the block world problem slightly modified is a subproblem of this re stricted planning problem 
we present a method to construct real time system using a component anytime algorithm whose quality of result degrades grace fully a computation time decrease introduc ing computation time a a degree of freedom defines a scheduling problem involving the ac tivation and interruption of the anytime com ponents this scheduling problem is especially complicated when trying to construct interruptible algorithm whose total run time is un known in advance we introduce a framework to measure the performance of anytime algo rithms and solve the problem of constructing interruptible algorithm by a mathematical re duction to the problem of constructing con tract algorithm which require the determi nation of the total run time when activated we show how the composition of anytime algo rithms can be mechanized a part of a compiler for a lisp like programming language for realtime system the result is a new approach to the construction of complex real time sys tems that separate the arrangement of the per formance component from the optimization of their scheduling and automates the latter task 
a faulty component that behaves consistently over time is said to behave non intermittently for any given set of input such a component will always generate the same output assuming that component fail non intermittently is a common simplifying strategy used by diagnostician because many real world device often fail this way this strategy remove the need to repeat experiment and this strategy allows information from independent example of system behavior to be combined in relatively simple way this paper extends the formal framework for diagnosis developed in to allow nonintermittency assumption in addition we show how the definition can be easily integrated into atm based diagnosis engine within our formulation component can be individually assumed to be intermittent or nonintermittent 
a new kind of rms based on a close merge of tm and atm is proposed it us the tm graph and interpretation and the atm multiple context labelling procedure in order to fill in the problem of the atm environment in presence of nonmonotonic inference a new kind of environment able to take into account hypothesis that do not hold is defined these environment can inherit formula that hold a in the atm context lattice the dependency graph can be interpreted with regard to these environment so every node can be labelled furthermore this lead to consider several possible interpretation of a query 
constraint satisfaction csp is a powerful and extensively used framework for describing search problem a csp is typically defined a the problem of finding consistent assignment of value to a fixed set of variable given some constraint over these variable however for many synthesis task such a configuration and model composition the set of variable that are relevant to a solution and must be assigned value change dynamically in response to decision made during the course of problem solving in this paper we formalize this notion a a dynamic constraint satisfaction problem that us two type of constraint compatibility constraint correspond to those traditionally found in csps namely constraint over the value of variable activity constraint describe condition under which a variable may or may not be actively considered a a part of a final solution we present a language for expressing four type of activity constraint in term of variable value and variable being considered we then describe an implemented algorithm that enables tight interaction between constraint about variable activity and constraint about variable value the utility of this approach is demonstrated for configuration and model composition task 
causality play an important role in qualitative reasoning about physical system in this paper we show that the bond graph method can be fruitfully applied to represent and generate causal order on a formal basis both physical and computational aspect of bond graph causality are discussed in particular we show that it provides a inner phys ical foundation for a causal order along the line of iwasaki and simon bond graph causality also generates more information than doe the causal ordering theory including better causal resolution an improved definition of exogeneity in term of parameter and source automatic checking of self containment and a more detailed physical treat ment of feedback the bond graph method originates corresponds to the device ontology the topology of the bond graph can be employed to obtain important qualitative physical information top and akkermans ai thus bond graph provide a formal and generic language for modelling and representing physical system in this paper we will deal with the topic of causality according to study of human reasoning about technical device iforbus and gentner causal explanation are based on elementary mechanism that relate individual variable in a directed way we propose that bond graph formalise these intuitive idea in a physically appropriate way and can be fruitfully employed to obtain causal information based on expert knowledge about physical system theory in particular we show that bond graph causality yield a causal ordering method for physical system that is similar to but more powerful than the theory of iwasaki and simon i iwasaki and simon ai 
we argue that current plan based theory of discourse do not by themselves explain prevalent phenomenon in even simple task oriented dialogue the purpose of this paper is to show how one difhcult to explain feature of these dialogue confirmation follows from the joint or team nature of the underlying task specifically we review the concept of a joint intention and we argue that the conversants in a task oriented dialogue jointly intend to accomplish the task from this basis we derive the goal underlying the pervasive use of confirmation observed in a recent experiment we conclude with a discussion on generalizing the analysis presented here to characterize dialogue itself a a joint activity 
we present an efficient method for inferring fact from a propositional knowledge base which is not required to be in conjunctive normal form this logically incomplete method called propositional fact propagation is more powerful and efficient than some form of boolean constraint propagation hence it can be used for tractable deductive reasoning in many ai application including various truth maintenance system we also use propositional fact propagation to define a weak logical entailment relation that is more powerful and efficient than some others presented in the literature among other application this new entailment relation can be used for efficiently answering query posed to a knowledge base and for modeling belief held by a resource limited agent 
in many formalization of a changing world thing do not change all the time but are persistent thoughout a time interval often this persistency is represented by fact refering to interval int which are still valid if int is replaced by any subinterval int approach like episode propagation or penberthy s temporal unification try to employ this property for efficient reasoning however these approach lack formality in this paper their way of reasoning about persistency is reconstructed a inference rule that combine appropriate timeboxes with standard resolution in many case burckert s constrained resolution may be used more complex example may be handled by a new inference rule called persistency resolution an analysis of this rule lead to a more general notion of persistency 
there are two common but quite distinct interpretation of probability they can be interpreted a a measure of the extent to which an agent belief an assertion i e a an agent s degree of belief or they can be interpreted a an assertion of relative frequency i e a a statistical measure used a statistical measure probability can represent various assertion about the objective statistical state of the world while used a degree of belief they can represent various assertion about the subjective state of an agent s belief in this paper we examine how an agent who know certain statistical fact about the world might infer probabilistic degree of belief in other assertion from these statistic for example an agent who know that most bird fly a statistical fact may generate a degree of belief greater than in the assertion that tweety fly given that tweety is a bird this inference of degree of belief from statistical fact is known a direct inference we develop a formal logical mechanism for performing direct inference some of the inference possible via direct inference are closely related to default inference we examine some feature of this relationship 
a significant problem in designing mobile robot control system involves coping with the uncertainty that arises in moving about in an unknown or partially unknown environment and relying on noisy or ambiguous sensor data to acquire knowledge about that environment we describe a control system that chooses what activity to engage in next on the basis of expectation about how the information returned a a result of a given activity will improve it knowledge about the spatial layout of it environment certain of the higher level component of the control system are specified in term of probabilistic decision model whose output is used to mediate the behavior of lower level control component responsible for movement and sensing the control system is capable of directing the behavior of the robot in the exploration and mapping of it environment while attending to the real time requirement of navigation and obstacle avoidance 
the probabilistic network technology is a knowledge based technique which focus on reasoning under uncertainty because of it well defined semantics and solid theoretical foundation the technology is finding increasing application in field such a medical diagnosis machine vision military situation assessment petroleum exploration and information retrieval however like other knowledge based technique acquiring the qualitative and quantitative information needed to build these network can be highly labor intensive constructqr integrates technique and concept from probabilistic network artificial intelligence and statistic in order to induce markov network i e undirected probabilistic network the resulting network are useful both qualitatively for concept organization and quantitatively for the assessment of new data the primary goal of constructor is to find qualitative structure from data constructor find structure by first modeling each feature in a data set a a node in a markov network and secondly by finding the neighbor of each node in the network in markov network the neighbor of a node have the property of being the smallest set of node which shield the node from being affected by other node in the graph this property is used in a heuristic search to identify each node s neighbor the traditional test for independence is used to test if a set of node shield another node cross validation is used to estimate the quality of alternative structure 
in the logical semantics of knowledge base k the handling of contradiction pose a problem not solvable by standard logic an adequate logic for kb must be capable of tolerating inconsistency in a kb without losing it deductive content this is also the bottom line of so called paraconsistent logic but paraconsistent logic doe not address the question whether contradictory information should be accepted or not in the derivation of further information depending on it we propose two computational logic based on the notion of support and acceptance handling contradiction in a conservative resp skeptical manner they neither lead to the break down of the system nor are they accepted a valid piece of information 
this paper identifies two fundamentally different kind of training information for learning search control in term of an evaluation function each kind of training information suggests it own set of method for learning an evaluation function the paper show that one can integrate the method and learn simultaneously from both kind of information 
we present bfl a hybrid logic for representing uncertain knowledge bfl attache a quantified notion of belief based on dempster shafer s theory of belief function to classical first order logic the language of bfl is composed of object of the form f a b where f is a first order sentence and a and b are number in the interval with a b intuitively a measure the strength of our belief in the truth of f and b that in it falseness a number of property of first order logic nicely generalize to bfl in return bfl give u a new perspective on some important point of dempster shafer theory e g the role of dempster s combination rule 
personalized knowledge based system have not yet become widespread despite their potential for valuable assistance in many daily task this is due in part to the high cost of developing and maintaining customized knowledge base the construction of personal assistant a learning apprentice interactive assistant that learn continually from their user is one approach which could dramatically reduce the cost of knowledge based advisor we present one such personal learning apprentice called cap which assist in managing a meeting calendar cap ha been used since june by a secretary in our work place to manage a faculty member s meeting calendar and is the first instance of a fielded learning apprentice in routine use this paper describes the organization of cap it performance in initial field test and more general lesson learned from this effort about learning apprentice system 
psychological evidence indicates that human chess player base their assessment of chess position on structural perceptual pattern learned through experience morph is a computer chess program that ha been developed to be more consistent with the cognitive model the learning mechanism used by morph combine weight updating genetic explanation based and temporal difference learning to create delete generalize and evaluate chess position an associative pattern retrieval system organizes the database for efficient processing the main objective of the project are to demonstrate capacity of the system to learn to deepen our understanding of the interaction of knowledge and search and to build bridge in this area between ai and cognitive science to strengthen connection with the cognitive literature limitation have been place on the system such a restriction to ply search to little domain knowledge and to no supervised training 
in this paper we demonstrate an important role for model based reasoning incase adaptation model based reasoning can allow a case based reasoner to applycases to a wider range of problem than would otherwise be possible we focus on case adaptation in brainstormer a planner that us abstractadvice to help it plan in the domain of political and military policy a it relatesto terrorism we show that by equipping a case adapter with an explicit causalmodel of the planning process case 
massively parallel artificial intelligence is a new and growing area of ai research enabled by the emergence of massively parallel machine it is a new paradigm in ai research a high degree of par allelism not only affect computing performance but also trigger drastic change in the approach to ward building intelligent system memory based reasoning and parallel marker passing are example of new and redefined approach these new ap proaches fostered by massively parallel machine offer a golden opportunity for ai in challenging the vastness and irregularity of real world data that are encountered when a system access and process very large data base and knowledge base this article describes the current status of massively par allel artificial intelligence research and position of each panelist 
this paper descnbes a knowledge representation and reasoning system that performs a limited but interesting class of inference over a restricted class of first order sentence with optimal efficiency the proposed system can answer yes no a well a wh query in time that is only proportional to the length of the shortest derivation of the query and is independent of the size of the knowledge base this work suggests that the expressiveness and the inferential ability of a representation and reasoning system may be limited in unusual way to arrive at extremely efficient yet fairly powerful knowledge based system 
first order learning system e g foil focl forte generally rely on hill climbing heuristic in order to avoid the combinatorial explosion inherent in learning first order concept however hill climbing leaf these system vulnerable to local maximum and local plateau we present a method called relational pathfinding which ha proven highly effective in escaping local maximum and crossing local plateau we present our algorithm and provide learning result in two domain family relationship and qualitative model building 
we describe an algorithm which allows a behavior based robot to learn on the basis of positive and negative feedback when to activate it behavior in accordance with the philosophy of behavior based robot the algorithm is completely distributed each of the behavior independently try to sensor find out i whether it is relevant i e whether it is at all correlated to positive feedback and ii what the condition are under which it becomes reliable i e the condition under which it maximises the probability of receiving positive feedback and minimises the probability of receiving negative feedback the algorithm ha been tested successfully on an autonomous legged robot which had to learn how to coordinate it leg so a to walk forward 
achieving goal despite uncertainty in control and sensing may require robot toperform complicated motion planning and execution monitoring this paper describesa reduced version of the general planning problem in the presence of uncertainty anda complete polynomial algorithm solving it the planner computes a guaranteed plan for given uncertainty bound by backchaining omnidirectional backprojections of thegoal until one fully contains the set of possible initial position of the robot 
feature structure play an important role in linguistic knowledge representation in computational linguistics given the proliferation of different feature structure formalism it is use ful to have a common language to express them in this paper show how a variety of fea ture structure and constraint on them can be expressed in predicate logic except for the use of circumscription for non monotonic device including sorted feature value subsumption constraint and the non monotonic any val ues and constraint equation many feature system can be completely axiomatized in the schonfinkel bernays class of first order formu lae so the decidability of the satisfiability and validity problem for these system follows im mediately 
this paper discus the recent view on knowl edge representation and memory a pre sented by different researcher under the flag of situated cognition the situated view im ply a radical shift of paradigm we argue that there are no strong reason to leave the traditional paradigm of cognitive science and ai four main issue are addressed the role of computational model in theory of cogni tion theory on knowledge and memory the frame of reference problem and implication for learning and instruction the main conclusion of the paper is that situationism is throwing out the baby with the bath water consoli dated achievement of cognitive science and ai still stand even if the architecture that are assumed to underly traditional model of cog nition can be challenged 
this paper describes a method of analysing rule based system which model the procedural semantics of such language through a process of abstract interpretation the program absps derives a description of the mapping between a rule base s input and output in contrast to earlier approach absps can analyse the effect of conflict resolution closed world negation and the retraction of fact this considerably reduces the size of the search space because in the abstract domain absps take advantage of the very same control information which guide the inference engine in the concrete domain absps can detect redundancy which would be missed if the procedural semantics were ignored furthermore the abstract description of a rule base s input output mapping can be used to prove that the rule base meet it specification 
we identify and illustrate five important kind of dialectical example standard configuration of case which enable an arguer to justify rhetorical assertion effectively by example our computer program generates argument context collection of case that instantiate dialectical example from an on line database of case according to a user s general specification the argument context generation program provides a human or automated tutor a stock of dialectical example to teach novice advocate first year law student how to recognize carry out and respond to the associated rhetorical move although generating such example is very hard for human even when dealing with small number of case our program generates and organizes such example quickly and effectively in a preliminary experiment we employed program generated argument context manually to teach basic argument skill to first year law student with good result our ability to define such complex example declaratively in term of logical expression of loom concept and relation affords a number of advantage over previous work 
model based diagnosis is based on first principle reasoning using the behavioral specification of the primitive component of a device unless the computational architecture of the model based reasoning engine is carefully designed combinatorial explosion render the approach useless for device consisting of more than a handful of component this paper analyzes the diverse origin of this combinatorial explosion a nd outline strategy to cope with each one the resulting computational architecture for model based diagnosis provides order of magnitude performance improvement on large example making model based approach practical for device consisting of on the order of component 
an intelligent agent us known fact including statistical knowledge to assign degree of belief to assertion it is uncertain about we investigate three principled technique for doing this all three are application of the principle of indifference because they assign equal degree of belief to all basic situation consistent with the knowledge base they differ because there are competing intuition about what the basic situation are various natural pattern of reasoning such a the preference for the most specific statistical data available turn out to follow from some or all of the technique this is an improvement over earlier theory such a work on direct inference and reference class which arbitrarily postulate these pattern without offering any deeper explanation or guarantee of consistency the three method we investigate have surprising characterization there are connection to the principle of maximum entropy a principle of maximal independence and a center of mass principle there are also unexpected connection between the three that help u understand why the specific language chosen for the knowledge base is much more critical in inductive reasoning of the sort we consider than it is in traditional deductive reasoning 
we describe a planner that work on the description of a multi path environment and generates a conditional plan the resulting plan is guaranteed to fulfill it goal whatever path of the description the environment follows during the plan execution 
this paper present pac learning analysis for instance based learning algorithm for both symbolic and numeric prediction task the algorithm analyzed employ a variant of the k nearest neighbor pattern classifier the main result of these analysis are that the ib instance based learning algorithm can learn using a polynomial number of instance a wide range of symbolic concept and numeric function in addition we show that a bound on the degree of difficulty of predicting symbolic value may be obtained by considering the size of the boundary of the target concept and a bound on the degree of difficulty in predicting numeric value may be obtained by considering the maximum absolute value of the slope between instance in the instance space moreover the number of training instance required by ib is polynomial in these parameter the implication of these result for the practical application of instance based learning algorithm are discussed 
this paper study sorted generalization the generalization with respect to an arbitrary taxonomic theory of atomic formula containing sorted variable it develops an algorithm for the task discus the algorithm and task complexity and present semantic property of sorted generalization based on it semantic property we show how sorted generalization is applicable to such problem a abduction induction knowledge base vivification and analogical reasoning significant distinction between this work and related work with taxonomic information arise from the generality of the taxonomic theory we allow which may be any first order taxonomic theory and the semantic completeness property of sorted generalization 
many problem solving approach are based on the assumption that a problem can be precisely defined before it is solved these approach are inadequate for dealing with ill defined problem which require the coevolution of problem setting and problem solving in this paper we describe integrated domain oriented knowledge based design environment and their underlying multifaceted architecture the environment empower human to cope with ill defined problem such a design by supporting an incremental approach to problem setting and problem solving we focus on the integration of specification construction and a catalog of prestored design object in those environment the synergy of integration enables the environment to make those object relevant to the task at hand taking architectural design a a domain to illustrate our approach we describe an operational prototype system catalogexplorer that assist designer in locating example in the catalog that are relevant to the task at hand a articulated by a partial specification and a partial construction user are thereby relieved of the task of forming query and navigating in information space 
troubleshooting problem in real manufacturing environment impose constraint on admissible solution that make the computational solution offered by troubleshooting from first principle and the conventional experience based expert system approach infeasible in this paper we present a computational theory for a solution to these problem that is based on the principle of locality and exploit the domain specific weak method of troubleshooter and debugging knowledge of the designer the computational theory is evaluated by generating focus of attention heuristic for a moderately complex digital device 
explanation based learning ebl fails to accelerate problem solving in some problem space how do these problem space differ from the one in minton s experiment b can minute modification to problem space encoding drastically alter ebl s performance will prodigy ebl s success scale to real world domain this paper present a formal theory of problem space structure that answer these question the central observation is that prodigy ebl relies on finding nonrecursive explanation of prodigy s problem solving behavior the theory explains and predicts prodigy ebl s performance in a wide range of problem space the theory also predicts that a static program transformer called static can match prodigy ebl s performance in some case the paper report on an array of experiment that confirms this prediction static match prodigy ebl s performance in each of minton s problem space 
the hybrid phenomenon theory hpt isa framework forformalizinghow dynamic state spacemodelsofphysicalsystemsarebuiltfrom firstprinciples the hpt descendsfrom the qualitativeprocesstheory qpt forbus fromwhich itinherits basicconceptslike view phenomenon and influence however thehpt redefinessome oftheseconceptsina more strictmanner inordertorepresentknowledge of physicswith the accuracyneeded to developfullparametricmodels specifically influencesmay specifyquantifiednon linearfunctionsofseveralvariables a mechanism denotedsubsumptionisintroducedtoensureconsistencyintheemergingmodelswhen different simplifyingassumptionsaremade the hpt hasbeen implementedinclos 
using abstraction in planning doe not guarantee an im provement in search efficiency it is possible for an abstract planner to display worse performance than one that doe not use abstraction analysis and experiment have shown that good abstraction hierarchy have or are close to having the downward refinement property whereby given that a concrete level solution exists every abstract solution can be refined to a concrete level solu tion without backtracking across abstract level work ing within a semantics for abstrips style abstraction we provide a characterizati on of the downward refinement property after discussing it effect on search efficiency we develop a semantic condition sufficient for guarantee ing it presence in an abstraction hierarchy using the semantic condition we then provide a set of sufficient and polynomial time checkable syntactic condition that can be used for checking a hierarchy for the downward refinement property 
this is a connected scries of argument concern ing paraconsistent logic it is argued first that paraconsistency is an option worth pursuing in automated reasoning then that the most popular paraconsistent logic fde is inadequate for the reconstruction of essential first order argu ments after a case is made for regarding quan tifiers a dyadic rather than monadic operator it is shown that the addition of such quantifier to fde allows an implication connective to be defined yielding the known logic bn refining the treatment of implication in a manner similar to that found in intuitionist logic lead to the more interesting system bn 
if the back propagation network can produce an inference structure with high and robust performance then it is sensible to extract rule from it the kt algonthm is a novel algonthm for generating rule from an adapted net efficiently the algorithm is able to deal with both single layer and multi layer network and can learn both confirming and disconfirming rule empirically the algorithm is demonstrated in the domain of wind shear detection by infrared sensor with success 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
student modelling is not typically concerned with representing the deep mental model a student employ in dealing with the world around him her in this research we discus an intelligent tutoring system presto whose goal is to understand the mental model a student ha of a physical device and then use this mental model in providing help to overcome misunderstanding related to the functioning of the device the mental model is extracted from the student by asking question about the relationship of variable affecting the physic of the device the mental model is represented using dekleer and brown s qualitative confluence equation the mental model can then be compared to a set of confluence representing a correct perspective on how the device function a variety of pedagogical choice can be made to explain contradiction implicit in the student s understanding of the device to show the student a simpler physical device that by analogy illustrates anomaly in the student s understanding or to let the student witness his her version of the device in operation so the misunderstanding become obvious experiment in running presto with a number of student show this approach to mental modelling to be promising 
finding the configuration of a set of rigid bodiesthat satisfy a set of geometric constraint isa problem traditionally solved by reformulatingthe geometry and constraint a algebraic equationswhich are solved symbolically or numerically but many such problem can be solved by reasoningsymbolically about the geometric bodiesthemselves using a new technique called degreesof freedom analysis in this approach a sequenceof action is devised to satisfy each constraintincrementally 
it is obvious to anyone familiar with the rule of the game of chess that a king on an empty board can reach every square it is true but not obvious that a knight can reach every square why is the first fact obvious but the second fact not this paper present an analytic theory of a class of obviousness judgment of this type whether or not the specific of this analysis are correct it seems that the study of obviousness judgment can be used to construct integrated theory of linguistics knowledge representation and inference 
developing large scale system are major effort which require careful planning and solid methodological foundation this paper describes case method the methodology for building large scale case based system casemethod defines the procedure which manager engineer and domain expert should follow in developing case based system and provides a set of supporting tool an empirical study show that the use of case method attains significant workload reduction in system development and maintenance more than a well a qualitative change in corporate activity 
in this paper we first give a formal semantics of nonmonotonic tm theory with cp justification then we prove that the model of a theory j is also a model of theory j i next we conclude thai for every tm theory j there must be a theory j such that j ha no cp justification and all the model of j is also j s finally we prove that the concept of extension defined by u junker and kun konolige is also correct under our definition 
we provide an algorithm that nd optimal search strategy for and tree and or tree our model includes three outcome when a node is explored nding a solution not nding a solution and realizing that there are no solution beneath the current node pruning and not nding a solution but not pruning the node below the expected cost of examining a node and the probability of the three outcome are given based on this input the algorithm generates an order that minimizes the expected search cost 
the calculus of time interval defined by allen ha been extended in various way in order to accomodate the need for considering other time object than convex interval eg time point and interval non convex interval this paper introduces and investigates the calculus of generalized interval which subsumes these extension in an algebraic setting the set of p q relation which generalizes the set of relation in the sense of allen ha both an order structure and an algebraic structure we show that a an order it is a distributive lattice whose property express the topological property of the set of p q relation we also determine in what sense the algebraic operation of transposition and composition act continuously on this set in allen s algebra the subset of relation which can be translated into conjunctive constraint on the endpoint using only ha special computational significance the constraint propagation algorithm is complete when restricted to such relation we give a geometric characterization of a similar subset in the general case and prove that it is stable under composition a a consequence of this last fact we get a very simple explicit formula for the composition of two element in this subset 
this paper present empirical evidence for five hypothesis about learning from large noisy domain that tree built from very large training set are larger and more accurate than tree built from even large subset that this increased accuracy is only in part due to the extra size of the tree and that the extra training instance allow both better choice of attribute while building the tree and better choice of the subtrees to prune after it ha been built for the practitioner with the common goal of maximising the accuracy and minimising the size of induced tree these conclusion prompt new technique for induction on large training set although building huge tree from huge training set is computational ly expensive pruning smaller tree on them is not yet it improves accuracy where a pruned tree is considered too large for human or machine limitation it can be overpruncd to an acceptable size although this requires far more time than building a tree of that size from a correspondingl y small training set it will usually be more accurate the paper also describes an algorithm for overpruning tree to user specifie d size limit it is evaluated in the course of testing the above hypothesis 
this paper survey some recent theoretical result on the efficiency of machine learning algorithm the main tool described is the notion of probably approximately correct pac learning introduced by valiant we define this learning model and then look at some of the result obtained in it we then consider some criticism of the pac model and the extension proposed to address these criticism finally we look briefly at other model recently proposed in computational learning theory 
we propose a language for programming in autoepistemic logic that extends the standard logic programming and incorporates incomplete information by syntactically distinguishing the true negation from the lack of information we also provide a way to define negative information explicitly a fixpoint semantics can be defined for stratified and conservative program in this paper we investigate definite autoepistemic program we investigate fixpoints of definite autoepistemic program and show that they coincide with the declarative semantics of these program we also define a resolution procedure called slsae resolution for such program slsae resolution is sound and complete for stratified conservative and solvable program 
goal a typically conceived in ai planning provide an insufficient basis for choice of action and hence are deficient a the sole expression of an agent s objective decision theoretic utility offer a more adequate basis yet lack many of the computational advantage of goal we provide a preferential semantics for goal that ground them in decision theory and preserve the validity of some but not all common goal operation performed in planning this semantic account provides a criterion for verifying the design of goal based planning strategy thus providing a new framework for knowledge level analysis of planning system planning to achieve goal in the predominant ai planning paradigm planner construct plan designed to produce state satisfying particular condition called goal each goal represents a partition of possible state of the world into those satisfying and those not satisfying the goal though planner use goal to guide their reasoning the crude binary distinction defined by goal provide no basis for choosing among alternative plan that ensure achievement of goal and no guidance whatever when no such plan can be found these lacuna pose significant problem for planning in all realistic situation where action have uncertain effect or objective can be partially satisfied to overcome these widely recognized expressive limitation of goal many ai planner make ad hoc use of heuristic evaluation function these augment the guidance provided by goal but lack the semantic justification needed to evaluate their true efficacy we believe that heuristic evaluation function should not be viewed a mere second order refinement on the primary goal based representation of objective supporting a separate optimizing phase of planning our thesis is that relative preference over the possible result of a plan constitutes the fundamental concept underlying the objective of planning with goal serv jon doyle is supported by the usaf rome laboratory and darpa under contract f c ing a a computationally useful heuristic approximation to these preference doyle our purpose here is to provide a formal semantics for goal in term of decision theoretic preference that support rational justification for planning principle the grounding in decision theory enables designer to determine whether their planning system act rationally in accord with their goal and provides a principled basis for integrating goal with other type of preference information we begin by summarizing some basic concept of preference we then develop formal decision theoretic semantics for goal and examine some standard planning operation in light of the semantics we conclude by discussing some related work and offering some direction for future investigation preference and utility 
a method is described for deriving rule of inference from relation between probability of sentence in nilsson s probabilistic logic 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
to speed up production system many researcher have turned to parallel implementation we describe a system called par that executes production rule in parallel par is novel because it executes many rule simultaneously run in a highly asynchronous fashion and run on a distributed memory machine item improves available concurrency over system that only perform the match step in parallel item reduces bottleneck over synchronous parallel production system item make the technique more available given the lower cost of distributed versus shared memory machine the two main problem regarding correctness namely serialization and the maintenance of consistent distributed database are addressed and solved estimate of the effectiveness of this approach are also given 
the notion of minimality is widely used in three different area of artificial intelligence nonmonotonic reasoning belief revision and conditional reasoning however it is difficult for the reader of the literature in these area to perceive the similarity clearly because each formalization in those area us it own language sometimes without referring to other formalization we define ordered structure and family of ordered structure a the common ingredient of the semantics of all the work above we also define the logic for ordered structure and family we present a uniform view of how minimality is used in these three area and shed light on deep reciprocal relation among different approach of the area by using the ordered structure and the family of ordered structure 
this work us an alignment approach for classifying object according to their shape similarity previous alignment method were mostly limited to the recognition of specific rigid object allowing only for rigid transformation between the model and the viewed object the current work extends previous alignment scheme in two main direction extending the set of allowed transformation between the model and the viewed object and using structural aspect of the internal model namely their part decomposition the compensating transformation is divided into two part the first rough alignment compensates approximately for change in viewpoint and is derived by matching tangen tial point on the silhouette of the model and the viewed object the second the adjustment transformation is derived by matching local feature discontinuity of the contour orientation and curvature principal aspect of the scheme suggested here are also relevant for the recognition of flexible object 
we present a nonlinear forward search method suitable for planning the reaction of an agent operating in a highly unpredictable environment we show that this method is more efficient than existing linear method we then introduce the notion of safety and liveness rule this make possible a sharper exploitation of the information retrieved when exploring the future of the agent 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
we present a representational format for observed movement the representation ha a temporal structure relating component of a single complex movement we also present oxbow an unsupervised learning system which construct class of these movement empirical result indicate that the system build abstract movement concept with appropriate component structure allowing it to predict the latter portion of a partially observed movement 
in order to navigate autonomously most robot system are provided with some sort of global terrain map to make storage practical these map usually have a high level symbolic representation of the terrain the robot s symbolic map is then used to plan a local path this paper describes a system which us the reverse and perhaps more natural process this system process local sensor data in such a way a to allow efficient reactive local navigation a byproduct of this navigation process is an abstraction of the terrain information which form a global symbolic terrain map of the terrain through which the robot ha passed since this map is in the same format a that used by the local navigation system the map is easy for the system to use augment or correct compared with the data from which the map are created the map are very space efficient and can be modified or used for navigation in real time experiment with this system both in simulation and with a real robot operating in natural terrain are described 
know how is an important concept in artificial intelligence it ha been argued previously that it cannot be successfully reduced to the knowledge of fact in this paper i present sound and complete axiomatizations for two non reductive and intuitively natural formal definition of the knowhow of an agent situated in a complex environment i also present some theorem giving useful property of know how and discus and resolve an interesting paradox which is described within this is done using a new operator in the spirit of dynamic logic that is introduced herein and whose semantics and proof theory are given 
an underlying assumption of research on learning from planning and activity is that agent can exploit regularity they find in the world for agent that interact with a world over an extended period of time there is another possibility the exploited regularity can be created and maintained rather than discovered we explore the way in which agent can actively stabilize the world to increase the predictability and tractability of acting within in it 
we present the system abtweak which extends the precondition elimination abstraction of ab strip to hierarchical planner using the nonlinear plan representation a defined in tweak we show that abtweak satisfies the monotonic property whereby the existence of a lowest level solution ii implies the existence of a highest level solution that is structurally similar to ii this property enables one to prune a considerable amount of the search space without loss of completeness 
we show that the familiar explanation based generalization ebg procedure is applicable to a large family of programming language including three family of importance to ai logic programming such a prolog lambda calculus such a lisp and combinator language such a fp the main application of this result is to extend the algorithm to domain for which predicate calculus is a poor representation in addition many issue in analytical learning become clearer and easier to reason about 
we present a logic for belief revision in which revision of a theory by a sentence is represented using a conditional connective the conditional is not primitive but rather defined using two unary modal operator our approach capture and extends the classic agm model without relying on the limit assumption reasoning about counterfactual or hypothetical situation is also crucial for ai existing logic for such subjunctive query are lacking in several respect however primarily in failing to make explicit the epistemic nature of such query we present a logical model for subjunctive based on our logic of revision that appeal explicitly to the ramsey test we discus a framework for answering subjunctive query and show how integrity constraint on the revision process can be expressed 
in this paper we address the issue of evaluating decision tree generated from training example by a learning algorithm we give a set of performance measure and show how some of them relate to others we derive result suggesting that the number of leaf in a decision tree is the important measure to minimize minimizing this measure will in a probabilistic sense improve performance along the other measure notably it is expected to produce tree whose error rate are le likely to exceed some acceptable limit the motivation for deriving such result is two fold to better understand what constitutes a good measure of performance and to provide guidance when deciding which aspect of a decision tree generation algorithm should be changed in order to improve the quality of the decision tree it generates the result presented in this paper can be used a a basis for a methodology for formally proving that one decision tree generation algorithm is better than another this would provide a more satisfactory alternative to the current empirical evaluation method for comparing algorithm 
this paper address the integration of service for rule based reasoning in knowledge representation server based on term subsumption language a an alternative to previous construction of rule a concept concept link a mechanism is proposed based on intensional role implementing the axiom of comprehension in set theory this ha the benefit of providing both rule a previously defined and set aggregation using a simple mechanism that is of identical computational complexity to that for rule alone the extension proposed have been implemented a part of kr a knowledge representation server written a a class library in c the paper give an example of their application to the ripple down rule technique for large scale knowledge base operation acquisition and maintenance 
struct is a system that learns structural decision tree from positive and negative example the algorithm us a modification of pagallo and haussler s fringe algorithm to construct new feature in a first order representation experiment compare the effect of different hypothesis evaluation strategy domain representation and feature construction struct is also compared with quinlan s foil on two domain the result show that a modified fringe algorithm improves accuracy but that it is sensitive to the distribution of the example 
the kind of resource limitation that is most evident in commonsense reasoner is the passage of time while the reasoner reason there is not necessarily any fixed and final set of consequence with which such a reasoning agent end up in formalizing commonsense reasoner then one must be able to take into account that time is passing a the reasoner is reasoning the reasoner can then make use of such information in subsequent deduction step logic is such a formalism it wa developed in elgot drapldn to model the on going process of deduction conclusion are drawn step by step there is no final state of reasoning the emphasis is on intermediate conclusion in this paper we use step logic to model the three wisemen problem although others have formalized this problem they have ignored the time aspect that is inherent in the problem a correct assessment of the situation is made by recognizing that the reasoning process take time and determining that the other wise men would have concluded such and such by now this is an important aspect of the problem that need to be addressed 
the overfit problem in inductive learning and the utility problem in speedup learning both describe a common behavior of machine learning method the eventual degradation of performance due to increasing amount of learned knowledge plotting the performance of the changing knowledge during execution of a learning method the performance response reveals similar curve for several method the performance response generally indicates an increase to a single peak followed by a more gradual decrease in performance the similarity in performance response suggests a model relating performance to the amount of learned knowledge this paper provides empirical evidence for the existence of a general model by plotting the performance response of several learning program formal model of the performance response are also discussed these model can be used to control the amount of learning and avoid degradation of performance 
agent in multiagent system interact to a large extent by communicating such communication may be fruitfully studied from the point of view of speech act theory in order for multiagent system to be formally and rigorously designed and analyzed a semantics of speech act that give their objective model theoretic condition of satisfaction is needed however most research into multiagent system that deal with communication provides only informal description of the different message 
the aim of this paper is to provide a hasis for a theory of event and process that can be used for reasoning about arbitrarily complex dynamic domain involving multiple agent the approach is based on a model of event that explicitly represents the domain of influence of each event by scoping an event s domain of influence most of the problem that have plagued the more conventional stated transition model of event can be avoided the effect of performing event either in isolation or concurrently with other event is described to represent constraint among event a model of process is developed this allows the modelling of arbitrarily complex behaviour finally a representation of causal influence is provided that allows the ramification of any given event occurrence to be modelled 
this paper describes the integration of abstraction and explanation based learning ebl in the context of the prodigy system prodigy s abstraction module creates a hierarchy of abstract problem space so problem solving can proceed in a more directed fashion the ebl module acquires search control knowledge by analyzing problemsolving trace when the two module are integrated they tend to complement each other s capability resulting in performance improvement that neither system can achieve independently we present empirical result showing the effect of combining the two module and describe the factor that influence the overall performance of the integrated system 
in this paper we address the problem of making correct decision in the context of game playing specifically we address the problem of reducing or eliminating pathology in game tree however the framework used in the paper applies to decision making that depends on evaluating complex boolean expression the main contribution of this paper is in casting general evaluation of game tree a belief propagation in causal tree this allows u to draw several theoretically and practically interesting corollary in the bayesian framework we typically do not want to ignore any evidence even if it may be inaccurate therefore we evaluate the game tree on several level rather than just the deepest one choosing the correct move in a game can be implemented in a straightforward fashion by an efficient linear time algorithm adapted from the procedure for belief propagation in causal tree we propose a probabilistic ally sound heuristic that allows u to reduce the effect of pathology significantly 
we address the problem of selecting an attribute and some of it value for branching during the top down generation of decision tree we study the class of impurity measure member of which are typically used in the literature for selecting attribute during decision tree generation e g entropy in id gid and cart gini index in cart we argue that this class of measure is not particularly suitable for use in classification learning we define a new class of measure called c sep that we argue is better suited for the purpose of class separation a new measure from c sep is formulated and some of it desirable property are shown finally we demonstrate empirically that the new algorithm o btree that us this measure indeed produce better decision tree than algorithm that use impurity measure 
during the past decade knowledge representation research in ai ha generated a class of language called term subsumption language tsl which is a knowledge representation formalism with a well defined logic based semantics due to it formal semantics a term subsumption system can automatically infer the subsumption relationship between concept defined in the system however these system are very limited in handling vague concept in the knowledge base in contrast fuzzy logic directly deal with the notion of vagueness and imprecision using fuzzy predicate fuzzy quantifier linguistic variable and other construct hence fuzzy logic offer an appealing foundation for generalizing the semantics of term subsumption language based on a test score semantics in fuzzy logic this paper first generalizes the semantics of term subsumption language then we discus impact of such a generalization to the reasoning capability of term subsumption system the generalized knowledge representation framework not only alleviates the difficulty of conventional ai knowledge representation scheme in handling imprecise and vague information but also extends the application of fuzzy logic to complex intelligent system that need to perform highlevel analysis using conceptual abstraction 
in spite of the popularity of explanation based learning ebl it theoretical basis is not well understood using a generalization of probably approximately correct pac learning to problem solving domain this paper formalizes two form of explanation based learning of macro operator and prof the sufficient condition for their success these two form of ebl called macro caching and serial parsing respectively exhibit two distinct source of power or bias the sparseness of the solution space and the decomposability of the problem space the analysis show that exponential speedup can be achieved when either of these bias is suitable for a domain somewhat surprisingly it also show that computing the precondition of the macro operator is not necessary to obtain these speedup the the oretical result are confirmed by experiment in the domain of eight puzzle our work suggests that the best way to address the utility problem in ebl is to implement a bias which exploit the problem space structure of the set of domain that one is interested in learning 
interval consistency problem deal with event each of which is assumed to be an interval on the real line or on any other linearly ordered set this paper deal with problem in reasoning about such interval when the precise topological relationship between them is unknown or only partially specified this work unifies notion of interval algebra for temporal reasoning in artificial intelligence with those of interval order and interval graph in combinatorics obtaining new algorithmic and complexity result of interest to both discipline several version of the satisjiability minimum labeling and all consistent solution problem for temporal interval data are investigated the satisfiability question is shown to be np complete even when restricting the possible interval relationship to subset of the relation intersection and precedence only on the other hand we give efficient algorithm for several other restriction of the problem many of these problem are also important in molecular biology archaeology and resolving mutual exclusion constraint in circuit design 
there are many new application field for automated deduction where we have to apply abductive reasoning in these application we have to generate consequence of a given theory having some appropriate property in particular we consider the case where we have to generate the clause containing instance of a given literal l the negation of the other literal in such clause are hypothesis allowing to derive l in this paper we present an inference rule called l inference which wa designed in order to derive those clause and a l strategy the l inference rule is a sort of input hyper resolution the main result of the paper is the proof of the soundness and completeness of the l inference rule the l strategy associated to the l inference rule is a saturation by level with deletion of the tautology and of the subsumed clause we show that the l strategy is also complete 
many learning from experience system use information extracted from problem solving experience to modify a performance element pe forming a new element pe that can solve these and similar problem more efficiently however a transformation that improve performance on one set of problem can degrade performance on other set the new pe is not always better than the original pe this depends on the distribution of problem we therefore seek the performance element whose expected performance over this distribution is optimal unfortunately the actual distribution which is needed to determine which element is optimal is usually not known moreover the task of finding the optimal element even knowing the distribution is intractable for most interesting space of element this paper present a method palo that side step these problem by using a set of sample to estimate the unknown distribution and by using a set of transformation to hill climb to a local optimum this process is based on a mathematically rigorous form of utility analysis in particular it us statistical technique to determine whether the result of a proposed transformation will be better than the original system we also present an efficient way of implementing this learning system in the context of a general class of performance element and include empirical evidence that this approach can work effectively 
although version space provide a useful conceptual tool for inductive concept learning they often face severe computational difficulty when implemented for example the g set of traditional boundary set implementation of version space can have size exponential in the amount of data for even the most simple conjunctive description language haussler this paper present a new representation for version space that is more general than the traditional boundary set representation yet ha worst case time complexity that is polynomial in the amount of data when used for learning from attribute value data with tree structured feature hierarchy which includes language like haussler s the central idea underlying this new representation is to maintain the traditional s boundary set a usual but use a list n of negative data rather than keeping a g set a is typically done 
we present here a new formalization of belief which ha a simpler semantics than the previous formalization and develop an inference method for it by generalizing the resolution method the usual prepositional formula are embedded in our logic a a special type of belief formula one can obtain a non monotonic logic of belief by applying say circumscription to the basic belief logic developed here which is monotonic in nature one can also apply the technique repeatedly to construct a hierarchy of belief logic blk k where blk blk and blkcan handle formula involving up to level k nested application of the belief operator b 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
membership query extended with the meta query concept is proposed a a method to acquire complex classification rule furthermore relevent concept class where a small number of query is sufficient are characterized in this paper we advocate and present the benefit of the use of query in order to learn a target concept efficiently thus providing the foundation for automating the knowledge acquisition process based on these result we developed a knowledge acquisition tool kac z which us query about specific domain object the system usefulness ha been demonstrated by it application in the domain of manufacturing cutting industry 
we investigate the complexity of reasoning with monotonic inheritance hierarchy that contain beside isa edge also role or function edge a role edge is an edge labelled with a name such a spouse of or brother of we call such network isar network given a network with n vertex and m edge we consider two problem p determining whether the network implies an isa relation between two particular node and p determining all isa relation implied by the network a is well known without role edge the time complexity of p is o m and the time complexity of p is o n unfortunately the result do not extend naturally to isar network except in a very restricted case for general isar network we first give an polynomial algorithm by an easy reduction to proposional horn theory a the degree of the polynomial is quite high o m n for p o m n for p we then develop a more direct algorithm for both p and p it complexity is o n m actually a finer analysis of the algorithm reveals a complexity of o nr log r n r n where r is the number of different role label one corolary is that if we fix the number of role label the complexity of our algorithm drop back to o n 
of all the possible way of computing abductive explanation the atm procedure is one of the most popular while this procedure is known to run in exponential time in the worst case the proof actually depends on the existence of query with an exponential number of answer but how much of the difficulty stem from having to return these large set of explanation here we explore abduction task similar to that of the atm but which return relatively small answer the main result is that although it is possible to generate some non trivial explanation quickly deciding if there is an explanation containing a given hypothesis is np hard a is the task of generating even one explanation expressed in term of a given set of assumption letter thus the method of simply listing all explanation a employed by the atm probably cannot be improved upon an interesting result of our analysis is the discovery of a subtask that is at the core of generating explanation and is also at the core of generating extension in reiter s default logic moreover it is this subtask that account for the computational difficulty of both form of reasoning this establishes for the first time a strong connection between computing abductive explanation and computing extension in default logic 
the parameter of the parameterized modal operator p and p usually represent agent in the epistemic interpretation or action in the dynamic logic interpretation or the like in this paper the application of the idea of parametrized modal operator is extended in in two way first of all a modified neighbourhood semantics is defined which permit among others the interpretation of the parameter a probability value a formula f may for example express the fact that in at least of all case world f hold these probability value can be number qualitative description and even arbitrary term secondly a general theory of the parameter and in particular of the characteristic operation on the parameter is developed which unifies for example the multiplication of number in the probabilistic interpretation of the parameter and the sequencing of action in the dynamic logic interpretation 
cost based abduction attempt to find the best explanation for a set of fact by finding a minimal cost proof for the fact the cost are computed by summing the cost of the assumption necessary for the proof plus the cost of the rule we examine existing method for constructing explanation proof a a minimization problem on a dag we then define a probabilistic semantics for the cost and prove the equivalence of the cost minimization problem to the bayesian network map solution of the system 
when autonomous agent attempt to coordinate action it is often necessary that they reach some kind of consensus reaching such a consensus ha traditionally been dealt with in the distributed artificial intelligence literature via the mechanism of negotiation another alternative is to have agent bypass negotiation by using a voting mechanism each agent express it preference and a group choice mechanism is used to select the result some choice mechanism are better than others and ideally we would like one that cannot be manipulated by an untruthful agent one such non manipulable choice mechanism is the clarke tax clarke though theoretically attractive the clarke tax present a number of difficulty when one attempt to use it in a practical implementation this paper examines how the clarke tax could be used a an effective preference revealer in the domain of automated agent reducing the need for explicit negotiation 
universal attachment is a general purpose mechanism for integrating diverse representation structure and their associated inference program into a framework built on logical representation and theorem proving the integration is achieved by link referred to a universal attachment that connect logical expression to these structure and program in this paper we describe a compilation based method for automatically generating new program and new universal attachment to those program given a base set of existing program and universal attachment the generation method provides the mean to obtain large collection of attachment and attached program without the traditional specification overhead a well the method simplifies the task of validating that a collection of attachment is correct 
this paper describes the learning part of a system which ha been developed to provide expert system capability augmented with learning the learning scheme is a hybrid connectionist symbolic one a network representation is used learning may be done incrementally and requires only one pas through the data set to be learned attribute value pair are supported a a variable implementation variable are represented by group of connected cell in the network the learning algorithm is described and an example given current result are discussed which include learning the well known iris data set the result show that the system ha promise 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
in this paper we describe a method for hybridiz ing a genetic algorithm and a k nearest neighbor classification algorithm we use the genetic algo rithm and a training data set to learn real valued weight associated with individual attribute in the data set we use the k nearest neighbor algo rithm to classify new data record based on their weighted distance from the member of the train ing set we applied our hybrid algorithm to three test case classification result obtained with the hybrid algorithm exceed the performance of the k nearest neighbor algorithm in all three case 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
we present an algebraic approach to geometric reasoning and learning the purpose of this research is to avoid the usual difficulty in symbolic handling of geometric concept our system grew is grounded on a reasoning scheme that integrate the symbolic reasoning and algebraic reasoning of wu s method the basic principle of this scheme is to describe mathematical knowledge in term of symbolic logic and to execute the subsidiary reasoning for wu s method the validity of our approach and grew is shown by experiment such a applying to learning by example of computer vision heuristic or solving locus problem 
this paper present an approach to learning from noisy data that view the problem a one of reasoning under uncertainty where prior knowledge of the noise process is applied to compute a posteriori probability over the hypothesis space in preliminary experiment this maximum a posteriori map approach exhibit a learning rate advantage over the c algorithm that is statistically significant 
reliability defined a the guarantee that a program satisfies it specification is an important aspect of many application for which rule based program are suited executing rule based program on a series of test case doe not guarantee correct behavior in all possible test case to show a program is reliable it is desirable to construct formal specification for the program and to prove that it obeys those specification this paper present an assertional approach to the verification of a class of rule based program characterized by the absence of conflict resolution the proof logic needed for verification is already in use by researcher in concurrent programming the approach involves expressing the program in a language called swarm and it specification a assertion over the swarm program among model that employ rule based notation swarm is the first to have an axiomatic proof logic a brief review of swarm and it proof logic is given along with an illustration of the formal verification method used on a simple rule based program 
we present a strategy for restricting the application of the inference rule paramodulation the strategy applies to problem in first order logic with equality and is designed to prevent paramodulation into subterms of skolem expression a weak completeness result is presented the functional reflexive axiom are assumed experimental result on problem in set theory combinatory logic tarski geometry and algebra show that the strategy can be useful when searching for refutation and when applying knuth bendix completion the emphasis of the paper is on the effectiveness of the strategy rather than on it completeness 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
a number of researcher have investigated the use of planbased approach to generate textual explanation e g appelt hovy moore maybury b this paper extends this approach to generate multimedia explanation by defining three type of communicative act linguistic act illocutionary and locutionary speech act visual act e g deictic act and medium independent rhetorical act e g identify describe this paper formalizes several of these communicative act a operator in the library of a hierarchical planner a computational implementation is described which us these plan operator to compose route plan in coordinated natural language and graphic in the context of a cartographic information system 
abstract all major approach to qualitative reasoning rely on the existence of a model of the physical system however the task of finding a model is usually far from trivial within the area of electrical engineering model building method have been developed to automatically deduce model from measurement in this paper we explicitly show how to incorporate qualitative knowledge in order to apply these method to situation where they do not behave satisfac torily a program ha been developed and ap plied to a non trivial example the qualitative input in term of an incomplete bond graph and the resulting output can be used to form a more complete bond graph this more infor mative model is suitable for further reasoning 
appropriate bias is widely viewed a the key to efficient learning and generalization i present a new algorithm the incremental delta bar delta idbd algorithm for the learning of appropriate bias based on previous learning experience the idbd algorithm is developed for the case of a simple linear learning system the lm or delta rule with a separate learning rate parameter for each input the idbd algorithm adjusts the learning rate parameter which are an important form of bias for this system because bias in this approach is adapted based on previous learning experience the appropriate test bed are drifting or non stationary learning task for particular task of this type i show that the idbd algorithm performs better than ordinary lm and in fact find the optimal learning rate the idbd algorithm extends and improves over prior work by jacob and by me in that it is fully incremental and ha only a single free parameter this paper also extends previous work by presenting a derivation of the idbd algorithm a gradient descent in the space of learning rate parameter finally i offer a novel interpretation of the idbd algorithm a an incremental form of hold one out cross validation 
in this paper we show a new approach for reasoning about time and probability that combine a formal declarative language with a graph representation of system of random variable for making inference first we provide a continuous time logic for expressing knowledge about time and probability then we introduce the time net a kind of bayesian network for supporting inference with statement in the logic time net encode the probability of fact and event over time we provide a simulation algorithm to compute probability for answering query about a time net finally we consider an incremental probabilistic temporal database based on the logic and time net to support temporal reasoning and planning application the result is an approach that is semantically well founded expressive and practical 
automatic speech understanding and automatic speech recognition extract different kind of information from the input signal the result of the former must be evaluated on the basis of the response of the system while the result of the latter is the word sequence which best match the input signal in both case search ha to be performed based on score of interpretation hypothesis a scoring method is presented based on stochastic context free grammar the method give optimal upper bound for the computation of the best derivation tree of a sentence this method allows language model to be built based on stochastic context free grammar and their use with an admissible search algorithm that interprets a speech signal with left to right or middle out strategy theoretical and computational aspect are discussed 
this paper describes a simple heuristic method for solving large scale constraint satisfaction and scheduling problem given an initial assignment for the variable in a problem the method operates by searching though the space of possible repair the search is guided by an ordering heuristic the min conflict heuristic that attempt to minimize the number of constraint violation after each step we demonstrate empirically that the method performs order of magnitude better than traditional backtracking technique on certain standard problem for example the one million queen problem can be solved rapidly using our approach we also describe practical scheduling application where the method ha been successfully applied a theoretical analysis is presented to explain why the method work so well on certain type of problem and to predict when it is likely to be most effective 
this paper present a computaional method of calculating the measure of salience in understanding metaphor we mainly treat metaphor in the form of a is like b in which a is called target concept and b is called source concept in understanding a metaphor some property of the source concept are transferred to the target concept in the transfer process we first have to select the property of the source concept that can be more preferably transferred to the target concept the measure of salience represents how typical or prominent the property is and is used to measure the transferability of the property by introducing the measure of salience we have to consider only the high salient property after the selection the measure of salience wa calculated from smith medin s probabilistic concept l according to tversky s two factor l one is intensity which refers to signal to noise ratio this is calculated from the entropy of property the other is diagnostic factor which refers to the classificatory significance of property this is calculated from the distribution of the property s intensity among similar concept finally we briefly outline the whole process of understanding metaphor using the measure of salience 
we present a new approach to model based monitoring and diagnosis of dynamic system the presented diamon algorithm us hierarchical model to monitor and diagnose dynamic system diamon is based on the integration of teleological parameter based monitoring model and repair oriented device based diagnosis model it combine consistency based diagnosis with model based monitoring and us an extension of the qsim language for the representation of qualitative system model furthermore diamon is able to detect and localize a broad range of nonpermanent fault and thus extends traditional diagnosis which exclusively deal with permanent faulty behavior the operation of diamon will be demonstrated on a real world example in a multiple fault scenario 
the analysis of large complex situation pose difficult problem for qualitative reasoning due to the complexity of reasoning from first principle and the proliferation of ambiguity abstraction is a promising bolution to these problem in this paper we study a type of abstraction behavioral aggregation the process of grouping a set of individual entity that collectively behave a a unit in particular we show how to build aggregate model of situation involving dynamic equilibrium and how to reason about their behavior finally we demonstrate through several example the benefit of reasoning at the aggregate level a reduction in the complexity of reasoning and a compact easily interpretable description of the behavior 
we develop the notion that knowledge editing is a cooperative activity that requires knowledge editor to reach consensus a they represent information in a knowledge base we describe an intelligent knowledge editing tool the hit knowledge editor and illustrate how it assist knowledge editor in reaching consensus 
to design a task independent dialogue system we present a task oriented dialogue analysis in term of finding the referent of definite description and we show how this analysis lead to a goal oriented inferential representation of the task this representation provides a logical generic model of the task which is compatible with a belief system then we show how this task model jointly used with the domain specific user model for which we propose a formalization enables a dialogue system to plan request negotiation dialogue 
lqms is a knowledge based system that identifies and explains anomaly in data acquired from multiple sensor the knowledge base wa built by a sequence of domain expert it prototype performed with a high level of accuracy and that performance ha been incrementally and significantly improved during development and field testing several point are developed in this paper the combination of an intuitive model sufficient for the task and powerful graphical development tool allowed the domain expert to build a large high performance system the observation situation relation representation illustrates an intermediate point on the simplicity expressiveness spectrum which is understandable to the domain expert while being expressive enough for the diagnostic task the system wa designed a a workbench for the domain expert this enticed them to become more directly involved and resulted in a better system the use of an integrated knowledge base edit tracking system wa important to the project in several way it reassured computer naive expert that they could not damage the overall system which increased their productivity and it also allowed expert located in various place around the world to compare contrast and integrate change in a structured way 
access limited logic all is a language for knowledge representation which formalizes the access limitation inherent in a network structured knowledge base where a deductive method such a resolution would retrieve all assertion that satisfy a given pattern an access limited logic retrieves all assertion reachable by following an available access path in this paper we extend previous work to include negation disjunction and the ability to make assumption and reason by contradiction we show that the extended allneg remains socratically complete thus guaranteeing that for any fact which is a logical consequence of the knowledge base there exists a series of preliminary query and assumption after which a query of the fact will succeed and computationally tractable we show further that the key factor determining the computational difficulty of finding such a series of preliminary query and assumption is the depth of assumption nesting we thus demonstrate the existence of a family of increasingly powerful inference method parameterized by the depth of assumption nesting ranging from incomplete and tractable to complete and intractable 
we obtain here the complexity of solving a type of prolog problem which genesereth and nilsson have called sequential constraint satisfaction such problem are of direct relevance to relational database retrieval a well a providing a tractable first step in analyzing prolog problem solving in the general case the present paper provides the first analytic expression for the expected complexity of solving sequential constraint satisfaction problem these expression provide a basis for the formal derivation of heuristic for such problem analogous to the theory based heuristic obtained by the author for traditional constraint satisfaction problem solving a first application ha been in providing a formal basis for warren s heuristic for optimally ordering the goal in a conjunctive query due to the incorporation of constraint looseness into the analysis the expected complexity obtained here ha the useful property that it is usually quite accurate even for individual problem instance rather than only for the assumed underlying problem class a a whole heuristic based on these result can be expected to be equally instance specific preliminary result for warren s heuristic have shown this to be the case 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
bandwidth is a fundamental concept in graph theory which ha some surprising application to a class of ai search problem graph bandwidth provides a link between the syntactic structure of a constraint satisfaction problem csp and the complexity of the underlying search task bandwidth can be used to define a new class of easy csp s namely those that have limited constraint graph bandwidth these csp s can be solved in polynomial time essentially by divide and conquer this in turn suggests that bandwidth provides a mathematical measure of the decomposability of a search problem in addition bandwidth supply a measure for comparing different search ordering for a given csp statistical analysis suggests that backtracking with small bandwidth ordering lead to a more efficient search than that obtained under ordering with larger bandwidth small bandwidth ordering also limit the pruning that can be done by intelligent backtracking if small bandwidth ordering are indeed advantageous then a large number of heuristic developed in numerical analysis to find such ordering may find applicability to solving constraint satisfaction problem 
conventional envisioners proposed in qualitative physic have two difficulty in common ambiguity in prediction and inability of reasoning about global behavior we take a geometric approach to overcome these difficulty and have implemented a program psx nl which can reason about global behavior by analyzing geometry and topology of solution curve of ordinary differential equation in the phase space in this paper we highlight a flow grammar which specifies possible pattern of solution curve one may see in the phase space the role of a flow grammar in psx nl is twofold firstly it allows psx nl to reason about complex pattern in a uniform manner secondly it allows psx nl to switch to an approximate top down algorithm when complete geometric clue are not available due to the difficulty of mathematical problem encountered 
we propose a new approach to build a fuzzy inference system of which the parameter can be updated to achieve a desired input output mapping the structure of the proposed fuzzy inference system is called generalized neural network and it learning procedure rule to update parameter is basically composed of a gradient descent algorithm and kalman filter algorithm specifically we first introduce the concept of generalized neural network gnn s and develop a gradient descent based supervised learning procedure to update the gnn s parameter secondly we observe that if the overall output of a gnn is a linear combination of some of it parameter then these parameter can be identified by one time application of kalman filter algorithm to minimize the squared error according to the simulation result it is concluded that the proposedl new fuzzy inference system can not only incorporate prior knowledge about the original system but also fine tune the membership function of the fuzzy rule a the training data set varies 
in the best case using an abstraction hierarchy in problem solving can yield an exponential speed up in search efficiency such a speed up is predicted by various analytical model developed in the literature and efficiency gain of this order have been confirmed empirically however these model assume that the downward refinement property drp hold when this property hold backtracking never need occur across abstraction level when it fails search may have to consider many different abstract solution before finding one that can be refined to a concrete solution in this paper we provide an analysis of the expected search complexity without assuming the drp we find that our model predicts a phase boundary where abstraction provides no benefit if the probability that an abstract solution can be refined is very low or very high search with abstraction yield significant speed up however in the phase boundary area where the probability take on an intermediate value search efficiency is not necessarily improved the phenomenon of a phase boundary where search is hardest agrees with recent empirical study of cheeseman et al ckt 
when rule are executed in a parallel production system the goal of control is to ensure both that a high quality solution is achieved and that processing resource are used effectively we argue that the conventional conflict resolution algorithm is not suitable a a control mechanism for parallel rule firing system the necessity for examining all eligible rule within a system imposes a synchronization delay which limit processor utilization rather than perform conflict resolution we propose that rule should be executed asynchronously a soon a they become enabled however this approach leaf the problem of controlling the computation unsolved we have identified three distinct type of control program sequencing heuristic control and dynamic scheduling which are required for efficient and correct parallel execution of rule we discus the issue involved in implementing each type of control without undue overhead within the context of our system a parallel rule firing system with an augmented agenda manager 
we address the problem of generating adjective in a text generation system we distinguish between usage of adjective informing the hearer of a property of an object and usage expressing an intention of the speaker or an argumentative orientation for such argumentative usage we claim that a generator cannot simply map from information in the knowledge base to adjective instead we identify various knowledge source necessary to decide whether to use an adjective what adjective should be selected and what syntactic function it should have we show how these decision interact with lexical property of adjective and the syntax of the clause we propose a mechanism for adjective selection and illustrate it in the context of the explanation component of the advisor expert system we describe an implementation of adjective selection using a version of functional unification grammar 
evidential reasoning is a body of technique that support automated reasoning from evidence it is based upon the dempster shafer theory of belief function both the formal basis and a framework for the implementation of automated reasoning system based upon these technique are presented the formal and practical approach are divided into four part specifying a set of distinct propositional space each of which delimits a set of possible world situation specifying the interrelationship among these propositional space representing body of evidence a belief distribution over these propositional space and establishing path for the body of evidence to move through these propositional space by mean of evidential operation eventually converging on space where the target question can be answered 
in this paper we describe a formal semantic model that applies to many classical planning system this give a unifying framework in which to study diverse planner and motivates formal logic that can be used to study their property a an example of the model s utility we present a general truth criterion which test for the necessary truth of a proposition at arbitrary point in the planning process 
in this paper we describe the concept of physical impossibility a an alternative to the specification of fault model these axiom can be used to exclude impossible diagnosis similar to fault model we show for horn clause theory while the complexity of finding a first diagnosis is worst case exponential for fault model it is polynomial for physical impossibility axiom even for the case of finding all diagnosis using physical impossibility axiom instead of fault model is more efficient although both are exponential in the worst case these result are used for a polynomial diagnosis and measurement strategy which find a final sufficient diagnosis 
decomposing a difficult problem into simpler subproblems is a classic problem solving technique unfortunately the most difficult subproblems can be a difficult if not more difficult than the original problem this is not an obstacle to problem solving if the difficult subproblems recur in other problem if the difficult subproblems recur often then it solution need only be learned once and reused steppingstone is a learning problem solver that decomposes a problem into simple and difficult but recurring subproblems it solves the simple subproblems with an inexpensive constrained problem solver to solve the difficult subproblems steppingstone us an unconstrained problem solver once it solves a difficult subproblem it us the solution to generate a sequence of subgoals or steppingstones that can be used by the constrained problem solver to solve this difficult subproblem when it occurs again in this paper we provide analytical evidence for steppingstone s capability a well a empirical result from our work with the domain of logic synthesis 
this paper describes a rational reconstruction of einstein s discovery of special relativity validated through an implementation the erlanger program einstein s discovery of special relativity revolutionized both the content of physic and the research strategy used by theoretical physicist this research strategy entail a mutual bootstrapping process between a hypothesis space for bias defined through different postulated symmetry of the universe and a hypothesis space for physical theory the invariance principle mutually constrains these two space the invariance principle enables detecting when an evolving physical theory becomes inconsistent with it bias and also when the bias for theory describing different phenomenon are inconsistent structural property of the invariance principle facilitate generating a new bias when an inconsistency is detected after a new bias is generated this principle facilitates reformulating the old inconsistent theory by treating the latter a a limiting approximation 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
this session will explore the reason for the lack of impact in four important area in which ai ha been expected to significantly affect real world software engineering the panelist each representing one of these area will respond to the conjecture that these failure rest upon a common cause reliance on isolationist technology and approach rather than upon creating additive technology and approach that can be integrated with other existing capability 
we have addressed the problem of analyzing image containing multiple sparse overlapped pattern this problem arises naturally when analyzing the composition of organic macromolecule using data gathered from their nmr spectrum using a neural network approach we have obtained excellent result in using nmr data to analyze the presence of various amino acid in protein molecule we have achieved high correct classification percentage about for image containing a many a five sub stantially distorted overlapping pattern 
although block world planning is well known it complexity ha not previously been analyzed and different planning researcher have expressed conflicting opinion about it difficulty in this paper we present the following result finding optimal plan in a well known formulation of the block world planning domain is np hard even if the goal state is completely specified classical example of deleted condition interaction such a sussman s anomaly and creative destruction are not difficult to handle in this domain provided that the right planning algorithm is used instead the np hardness of the problem result from difficulty in determining which of several different action will best help to achieve multiple goal 
we describe an envisionment based qualitative simulation program the program implement part of an axiomatic first order theory that ha been developed to represent and reason about space and time topological information from the modelled domain is expressed a set of distinct topological relation holding between set of object these form the qualitative state in the underlying theory and simulation process in the theory are represented a path in the envisionment tree the algorithm is illustrated with an example of a simulation of phagocytosis and exocytosis two process used by unicellular organism for garnering food and expelling waste material respectively 
acp is a fully implemented constraint propagation system that computes numeric interval for variable davis along with an atm label de kleer a for each such interval the system is built within a focused atm architecture forbus and de kleer dressler and farquhar and incorporates a variety of technique to improve efficiency 
structural aggreg ation is an inherent aspect of our ability in reasoning about the real world in this paper we present our investigation of structural aggregation to simplify domain model and suppress irrelevant detail of complex physical system we address the role of clustering cluster orientation and creation of black box in qualitative causal reasoning we describe algorithm that automate structural aggregation to achieve efficiency and clarity in planning and explanation of physical system behavior 
the performance of production program can be improved by firing multiple rule in a production cycle although considerable amount of research ha been done on parallel processing of production program the problem of multiple rule firing ha not been thoroughly investigated yet in this paper we begin by identifying the problem associated with multiple rule firing system the compatibility problem and the convergence problem and present three multiple rule firing model which address them the rule dependence model rdm address the compatibility problem using inter rule data dependence analysis the single context multiple rule scmrj model and the multiple context multiple rule mcmr model address both the compatibility and the convergence problem a production program executed under the scmr and the mcmr model is guaranteed to reach a solution which is equivalent to the sequential execution these three multiple rule firing model have been simulated on the rubic simulator and the mcmr model which ha the highest performance ha been implemented on the intel ipsc hypercube the simulation and implementation result are reported 
in this paper we compare the utility of different commitment strategy in planning under a least commitment strategy plan are represented a partial order and operator are ordered only when interaction are detected we investigate claim of the inherent advantage of planning with partial order a compared to planning with total order by focusing our analysis on the issue of operator ordering commitment we are able to carry out a rigorous comparative analysis of two planner we show that partial order planning can be more efficient than total order planning but we also show that this is not necessarily so 
the success of case based reasoning depends on effective retrieval of relevant prior case if retrieval is expensive or if the case retrieved are inappropriate retrieval and adaptation cost will nullify many of the advantage of reasoning from prior experience we propose an indexing vocabulary to facilitate retrieval of explanation in a casebased explanation system the explanation we consider are explanation of anomaly conflict between new situation and prior expectation or belief our vocabulary group anomaly according to the type of information used to generate the expectation or belief that failed and according to how the expectation failed we argue that by using this vocabulary to characterize anomaly and retrieving explanation that were built to account for similarly characterized past anomaly a case based explanation system can restrict retrieval to explanation likely to be relevant in addition the vocabulary can be used to organize general explanation strategy that suggest path for explanation in novel situation 
we extend reiter s general theory of model based diagnosis reiter to a theory of reconfiguration the generality of reiter s theory readily support an extension in which the problem of reconfiguration is viewed a a close analogue of the problem of diagnosis using a reconfiguration predicate rcfg analogous to the abnormality predicate ab we formulate a strategy for reconfiguration by transforming that for diagnosis a benefit of this approach is that algorithm for diagnosis can be exploited a algorithm for reconfiguration thereby promoting an integrated approach to fault detection identification and reconfiguration 
cogin is a system designed for induction of symbolic decision model from pre classed example based on the use of genetic algorithm gas much research in symbolic induction ha focused on technique for reducing classification inaccuracy that arise from inherent limit of underlying incremental search technique genetic algorithm offer an intriguing alternative to stepwise model construction relying instead on model evolution through global competition the difficulty is in providing an effective framework for the ga to be practically applied to complex induction problem cogin merges traditional induction concept with genetic search to provide such a framework and recent experimental result have demonstrated it advantage relative to basic stepwise inductive approach in this paper we describe the essential element of the cogin approach and present a favorable comparison of cogin result with those produced by a more sophisticated stepwise approach with support post processing on standardized multiplexor problem 
we describe an application of the discovery system fahrenheit in a chemistry laboratory our emphasis is on automation of the discovery process a oposed to human intervention and on computer control over real experiment and data collection a opposed to the use of simulation fahrenheit performs automatically many cycle of experimentation data collection and theory formation we report on electrochemistry experiment of several hour duration in which fahrenheit ha developed empirical equation quantitative regularity equivalent to those developed by an analytical chemist working on the same problem the theoretical capability of fahrenheit have been expanded allowing the system to find maximum in a dataset evaluate error for all concept and determine reproducibility of result after minor adjustment fahrenheit ha been able to discover regularity in maximum location and height and to analyse repeatability of measurement by the same mechanism adapted from bacon by which all numerical regularity are detected 
we define fuzzy constraint network and prove a theorem about their relationship to fuzzy logic then we introduce khayyam a fuzzy constraint based programming language in which any sentence in the first order fuzzy predicate calculus is a well formed constraint statement finally using khayyam to address an equipment selection application we illustrate the expressive power of fuzzy constraint based language 
while every shafer belief function corresponds to a set of interval belief on the atom of the frame of di cernment an arbitrarily specified set of interval of belief may not correspond to any belief function even when it doe correspond to bound imposed by set of probability function this paper prof necessary and sufficient condition which must be met by a set of belief interval over atom if a corresponding belief function exists the sufficiency is proved via an an o n algorithm which will always construct an corresponding belief function if one exit for a specific set of interval 
abstract we describe a decision theoretic method that an au tonomous agent can use to model multiagent situ ations and behave rationally based on it model our approach which we call the recursive modeling method explicitly account for the recursive nature of multiagent reasoning our method let an agent recursively model another agent s decision based on probabilistic view of how that agent perceives the multiagent situation which in turn are derived from hypothesizing how that other agent perceives the initial agent s possible decision and so on fur ther we show how the possibility of multiple inter action can affect the decision of agent allowing cooperative behavior to emerge a a rational choice of selfish agent that otherwise might behave uncooperatively 
this paper describes a situated reasoning architecture originally used with ground mobile robot which is shown to easily integrate control theoretic algorithm navigation heuristic and human supervision for semiautonomous robot control in underwater field environment the control architecture produce reaction plan that exploit low level competence a operator the low level competence include both obstacle avoidance heuristic and control theoretic algorithm for generating and following a velocity acceleration trajectory experiment with an undersea remotely piloted robot in a test tank at the deep submergence laboratory at wood hole ma are described the robot performed both pilot aided and autonomous exploration task robustly during normal change in the task environment the architecture wa implemented in the gapps rex situated automaton programming language the guaranteed constant cycle time of the synchronous rex circuit allowed for rapid tuning of the parameter of the control theoretic and heuristic algorithm to obtaln smooth safe motion 
building a large scale system often involves creating a large knowledge store and a these grow and are maintained by a number of individual error are inevitable exploring database a a specialization of knowledge store this paper study the hypothesis that descriptive learned model can be prescriptively used to find error to that end it describes an implemented system called carper applying carper to a real world database demonstrates the viability of the approach and establishes a baseline of performance for future research 
in this article we present a novel hybrid graph spatial representation for robot navigation this representation enables our mobile robot to build a model of it surroundings which it can then use for navigation the model or map that use this representation are hybrid graph the node being analogical local map of landmark location in the robot s environment the arc being the action the robot executes to travel between the location this representation yield a reliable navigation tool one which ensures that the robot can re orient itself to recover from error in path execution and encounter with unexpected obstacle the lognet approach also mesh with human s natural approach of mapping with landmark instead of using angular and translational data 
this paper present an algorithm for first order horn clause abduction that us an atm to avoid redundant computation this algorithm is either more efficient or more general than any other previous abduction algorithm since computing all minimal abductive explanation is intractable we also present a heuristic version of the algorithm that us beam search to compute a subset of the simplest explanation we present empirical result on a broad range of abduction problem from text understanding plan recognition and device diagnosis which demonstrate that our algorithm is at least an order of magnitude faster than an alternative abduction algorithm that doe not use an atm 
the blackboard instructional planner is a blackboard based dynamic planner for intelligent tutoring system it generates a sequence of lesson plan customized to a student s background and adaptively replans to handle student request and unexpected change to the student model or time remaining the planner is designed to be generic to tutor that teach troubleshooting for complex physical device it control the lower hoist tutor a prototype tutor for the mark naval gun mount this tutor teach troubleshooting of the lower hoist a complex hydraulic electronic mechanical assembly of the mark the tutor implementation demonstrates the planner s operation and mean of integration this research contributes to an understanding of dynamic instructional planner planner controlled tutor and it control architecture the planner implementation show precisely how a blackboard architecture can be used to realize a dynamic instructional planner although experimental the tutor implementation demonstrates how such a planner can be embedded in an intelligent tutoring system and what the respective role of the different component of a planner controlled tutor are finally the analysis of the planner s use of the blackboard architecture clarifies requirement for control architecture in intelligent tutoring system and trade offs made in choosing alternative 
this paper explores the problem of learning from example when feature measurement cost are significant it then extends two effective and familiar learning method id and ibl to address this problem the extension c id and c ibl are described in detail and are tested in a natural robot domain and a synthetic domain empirical study support the hypothesis that the extended method are indeed sensitive to feature cost they deal effectively with varying cost distribution and with irrelevant feature 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
a theory of perspectivity is proposed to establish the foundation of the theory of situated agent an account is then given based on the theory of perspectivity of the use of a seemingly perspectivity related expression japanese long range reflexive zibun the theory we proposed for perspectival mental state incorporates two independent notion indexicality and world view the first capture the situatedness of agent within physical environment and the second capture the mode of reasoning adopted by agent in interacting with other agent the relationship between these two notion were also discussed based on the proposed theory of perspectivity we argued that contrary to wide spread belief the usage of zibzsn is not directly related to perspectivity we gave an alternative explication for the interaction of the usage of zibun with perspectivity sensitive expression and the indexical pronoun watashi i in term of the coreference rule for zibun the constraint on the two component of perspectivity and the agent awareness default principle for the world view 
this paper investigates a problem of natural language processing from the perspective of reasoning work on constraint satisfaction we formulate the task of computing singular definite reference to a known contextual entity a a constraint satisfaction problem we argue that such referential constraint problem have a structure which is often simpler than the general case and can therefore often be solved by the sole use of low power network consistency technique to illustrate we define a linguistic fragment which provably generates tree structured constraint problem this enables u to conclude that the limited operation of strong arc consistency is sufficient to resolve the class of noun phrase defined by the fragment 
this paper present a formal relationship for probability ken satoh generation computer technology minato ku tokyo japan ksatoh icot jp for probability theory and a class of nonmonotomc reasoning which we call daxy nonmonotonic reusoning in lazy nonmonotonic reasoning nonmonotonicity emerges only when new added knowledge is contradictory to the previous belief in this paper we consider nonmonotonic reasoning in term of consequence relation a consequence relation is a binary relation over formula which express that a formula is derivable from another formula under inference rule of a considered system a consequence relation which ha lazy nonmonotonicity is called a rutionad consequence relation studied by lehmann and magidor we provide a probabilistic semantics which characterizes a rational consequence relation exactly then we show a relationship between propositional circumscription and consequence relation and apply this semantics to a consequence relation defined by propositional circumscription which ha lazy nonmonotonicity 
recently conditional logic have been developed for application to problem in default reasoning we present a uniform framework for the development and investigation of conditional logic to represent and reason with normality and demonstrate these logic to be equivalent to extension of the modal system s we also show that two conditional logic recently proposed to reason with default knowledge are equivalent to fragment of two logic developed in this framework 
this paper is concerned with making precise the notion that recognizing plan is much like parsing text to this end it establishes a correspondence between kautz plan recognition formalism and existing grammatical framework this mapping help isolate subset of kautz formalism in which plan recognition can be efficiently performed by parsing 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
an algorithm is described which computes stable model of propositional logic program with negation a failure using the assumption based truth maintenance mechanism since stable model of logic program are closely connected to stable expansion of a class of autoepistemic theory this algorithm point to a link between stable expansion of a class of auto epistemic theory and atm structure 
one kind of temporal reasoning is temporal projection the computation of the consequence of a set of event this problem is related to a number of other temporal reasoning task such a story understanding planning and plan validation we show that one particular simple case of temporal projection on partially ordered event turn out to be harder than previously conjectured however given the restriction of this problem story understanding planning and plan validation appear to be easy in fact we show that plan validation one of the intended application of temporal projection is tractable for an even larger class of plan 
constraint satisfaction problem csps provide a model often used in artificial intelligence since the problem of the existence of a solution in a csp is an np complete task many filtering technique have been developed for csps the most used filtering technique are those achieving arc consistency nevertheless many reasoning problem in ai need to be expressed in a dynamic environment and almost all the technique already developed to solve csps deal only with static csps so in this paper we first define what we call a dynamic csp and then give an algorithm achieving arc consistency in a dynamic csp the performance of the algorithm proposed here and of the best algorithm achieving arc consistency in static csps are compared on randomly generated dynamic csps the result show there is an advantage to use our specific algorithm for dynamic csps in almost all the case tested 
we propose a software framework for integrating people and computer system in large geographically dispersed manufacturing enterprise underlying the framework is an enterprise model that is built by dividing complex business process into elementary task or activity each such task is then modeled in cognitive term e g what to look for what to do who to tell and entrusted to an intelligent agent ia for execution the ia interact with each other directly via a message bus or through a shared distributed knowledge base they can also interact with human through personal assistant pa a special type of ia that know how to communicate with people through multi medium interface preliminary experimental result suggest that this model based man machine approach provides a viable path for applying dai to realworld enterprise 
reinforcement learning rl algorithm have traditionally been thought of a trial and error learning method that use actual control experience to incrementally improve a control policy sutton s dyna architecture demonstrated that rl algorithm can work a well using simulated experience from an environment model and that the resulting computation wa similar to doing one step lookahead planning inspired by the literature on hierarchical planning i propose learning a hierarchy of model of the environment that abstract temporal detail a a mean of improving the scalability of rl algorithm i present h dyna hierarchical dyna an extension to sutton s dyna architecture that is able to learn such a hierarchy of abstract model h dyna differs from hierarchical planner in two way first the abstract model are learned using experience gained while learning to solve other task in the same environment and second the abstract model can be used to solve stochastic control task simulation on a set of compositionally structured navigation task show that h dyna can learn to solve them faster than conventional rl algorithm the abstract model also serve a mechanism for achieving transfer of learning across multiple task 
agent collaborating to achieve a goal bring to their joint activity different belief about way in which to achieve the goal and the action necessary for doing so thus a model of collaboration must provide a way of representing and distinguishing among agent belief and of stating the way in which the intention of different agent contribute to achieving their goal furthermore in collaborative activity collaboration occurs in the planning process itself thus rather than modelling plan recognition per se what must be modelled is the augmentation of belief about the action of multiple agent and their intention in this paper we modify and expand the sharedplan model of collaborative behavior grosz sidner we present an algorithm for updating an agent s belief about a partial sharedplan and describe an initial implementation of this algorithm in the domain of network management 
a parallel distributed computational model for reasoning and learning is discussed based on a belief network paradigm issue like reasoning and learning for the proposed model are discussed comparison between our method and other method are also given 
a basic feature of intelligent tutoring system it is their ability to represent domain knowledge that can be attributed to the student at each stage of the learning process in this paper we present a general first order logic framework for the representation of this kind of knowledge acquired by the system through the analysis of the student answer this represantation make it possible to describe the behaviour of well known it and to provide a direct implementation in a logic programming language moreover we point out several improvement that can be easily achieved by exploiting the feature of a declarative approach in particular we address the representation and use of the knowledge that the system know not to be possessed by the student 
an inventorwho isskilledat constructinginnovative designsisdistinguished notjustby thefirstprinciples he know butby theway he usestheseprinciples and how he focussesthesearchfornoveldevicesamong an overwhelmingspaceofpossibilities we proposethatan appropriatefocusfordesignisthenetworkofqualitative interaction betweenquantities calledan interaction topology usedby adevicetoachieveitsdesiredbehavior we presentan approach calledinteraction based invention which viewsdesignas a processof buildinginteraction topology inthispaperdirectly from firstprinciples the programibis whichembodiesthis approach designssimplehydro mechanicalregulators analogoustodevicesthatwerefundamentaltothedevelopmentoffeedbackcontroltheory abstract in highlycompetitivemarkets with rapid technology shiftsa designermust continuallyexploitnew technologiesand existingtechnologiesinnonobviousways currentai designresearchfocuseson heirarchicalrefinement using librariesof design fragment mcdermott roylance mitchelletal ressler mitchelletal mittaletal while librarybased techniqueslikeconfiguration mcdermott have been highlysuccessfulforsome routinetasks they ignoretheseinnovativeaspectsof the designprocess to achievethegeneralitynecessarytomaintainacompetitiveedge the designermight have to consider not justwhat isintheroutinelibrary but any possibledevicestructure evaluatingthebehaviorofany structure requiresreasoningfrom firstprinciples the generality affordedby theseprinciplespresentsthe designerwith an overwhelming space ofpossibilities to avoidbeing losttheinventormust useeverymeans athisdisposalto focusthesearch this isthe abilitytoinnovate thus a robusttheory must capture not only techniquesfor routinedesign but theprocessofinnovationfrom first principle we referto thisas invention discussion withmy advisorrandy davisandcommittee patrickwinstonandtomes lozano perezhad atremendous impacton thisresearch johandekleer brianfalkenhainer leojoskowiczand mark shirleyprovidedvaluablecomments on earlier draft thisresearchwas performedbothatthe mit ai lab and xerox parc mit supportwas provided byan analogdevicesfellowship dec wang and darpa underofficeofnavalresearchcontractn k 
the problem of automatically improving functional program using darlington s unfold fold technique is addressed transformation tactic are formalized a method consisting of preand postconditions expressed within a sorted meta logic predicate and function of this logic induce an abstract program property space within which conventional monotonic planning technique are used to automatically compose method hence tactic into a program improving strategy this meta program reasoning cast the undirected search of the transformation space a a goal directed search of the more abstract space tactic are only weakly specified by method this flexibility is required if they are to be applicable to the class of generalized program that satisfy the pre condition of their method this is achieved by allowing the tactic to generate degenerate script that may require refinement example of tactic and method are given with illustration of their use in automatic program improvement 
this paper describes a model of the complementarity of rule and precedent in the classification task under this model precedent assist rule based reasoning by operationalizing abstract rule antecedent conversely rule assist case based reasoning through case elaboration the process of inferring case fact in order to increase the similarity between case and term reformulation the process of replacing a term whose precedent only weakly match a case with term whose precedent strongly match the case fully exploiting this complementarity requires a control strategy characterized by impartiality the absence of arbitrary ordering restriction on the use of rule and precedent an impartial control strategy wa implemented in grebe in the domain of texas worker s compensation law in a preliminary evaluation grebe s performance wa found to be a good or slightly better than the performance of law student on the same task 
x morf is a language independent morphological component for the recognition and generation of word form based on a lexicon of morphs the approach is based on two level morphology extension are motivated by linguistic data which call into question an underlying assumption of standard two level morphology namely the independence of morphophonology and morphology a exemplified by two level rule and continuation class accordingly i propose a model which allows for interaction between these two part instead of using continuation class word formation is described in a feature based unification grammar two level rule are provided with a morphological context in the form of feature structure information contained in the lexicon and the word formation grammar guide the application of two level rule by matching the morphological context against the morphs i present an efficient implementation of that model where rule are compiled into automaton a in the standard model and where processing of the feature based grammar is enhanced using an automaton derived from that grammar a a filter 
most of today s terminological representation system implement hybrid reasoning architecture wherein a concept classifier is employed to reason about concept definition and a separate recognizer is invoked to compute instantiation relation between concept and instance whereas most of the existing recognizer algorithm designed to maximally exploit the reasoning supplied by the concept classifier loom ha experimented with recognition strategy that place le emphasis on the classifier and rely more on the ability of loom s backward chaining query facility this paper present the result of experiment that test the performance of the loom algorithm these result suggest that at least for some application the loom approach to recognition is likely to outperform the classical approach they also indicate that for some application much better performance can be achieved by eliminating the recognizer entirely in favor of a purely backward chaining architecture we conclude that no single recognition algorithm or strategy is best for all application and that an architecture that offer a choice of inference mode is likely to be more useful than one that offer only a single style of reasoning 
how should opinion of control knowledge source be represented and combined these issue are addressed for the case where control knowledge is used to form an agenda i e a proposed knowledge source execution order a formal model is developed in the dempster shafer belief calculus and computational problem are discussed a well the model is applicable to many other problem where it is desired to order a set of candidate using a knowledge based approach 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
we propose that a planner should be provided with an explicit model of it own planning mechanism and show that linking a planner s expectation about the performance of it plan to such a model by mean of explicit justification structure enables the planner to determine which aspect of it planning are responsible for observed performance failure we have implemented the idea presented in this paper in a computer model applied to the game of chess the model is capable of diagnosing planning failure due to incomplete knowledge of the rule improper or overly optimistic focus of attention faulty projection and insufficient lead time for warning about threat and is therefore able to learn such concept a discovered attack and the fork 
this paper describes dynamic trade off evaluation dte a new technique that ha been developed to improve the performance of real time problem solving system the dte technique is most suitable for automation environment in which the requirement for meeting time constraint is of equal importance to that of providing optimally intelligent solution in such environment the demand of high input data volume and short response time can rapidly overwhelm traditional ai system dte is based on the recognition that in time constrained environment compromise to optimal problem solving in favor of timeliness must often be made in the form of trade offs towards this end dte combine knowledge based technique with decision theory to dynamically modify system behavior and adapt the decision criterion that determine how such modification are made the performance of dte ha been evaluated in the context of several type of real time trade offs in spacecraft monitoring problem one such application ha demonstrated that dte can be used to dynamically vary the data that is monitored making it possible to detect and correctly analyze all anomalous data by examining only a subset of the total input data in carefully structured experimental evaluation that use real spacecraft data and real decision making dte provides the ability to handle a three fold increase in input data in realtime without loss of performance 
it is well known that for many np complete problem such a k sat etc typical case are easy to solve so that computationally hard case must be rare assuming p np this paper show that np complete problem can be summarized by at least one order parameter and that the hard problem occur at a critical value of such a parameter this critical value separate two region of characteristically different property for example for k colorability the critical value separate overconstrained from underconstrained random graph and it mark the value at which the probability of a solution change abruptly from near to near it is the high density of well separated almost solution local minimum at this boundary that cause search algorithm to thrash this boundary is a type of phase transition and we show that it is preserved under mapping between problem we show that for some p problem either there is no phase transition or it occurs for bounded n and so bound the cost these result suggest a way of deciding if a problem is in p or np and why they are different 
this paper describes the integration of a learning mechanism called complementary discrimination learning with a knowledge representation schema called decision list there are two main result of such an integration one is an efficient representation for complementary concept that is crucial for complementary discrimination style learning the other is the first behaviorally incremental algorithm called cdl for learning decision list theoretical analysis and experiment in several domain have shown that cdl is more efficient than many existing symbolic or neural network learning algorithm and can learn multiple concept from noisy and inconsistent data 
this paper describes a parallel associative processor ixm developed mainly for semantic network processing ixm consists of associative processor and network processor having a total of k word of associative memory the large associative memory enables semantic network node to be processed in parallel and reduces the order of algorithmic complexity to o in basic semantic net operation we claim that intensive use of associative memory provides far superior performance in carrying out the basic operation necessary for semantic network processing intersection marker propagation and arithmetic operation 
this paper report experimental result of a high performance real time memory based translation memory based translation is a new approach to machine translation which us example or case of past translation to carry out translation of sentence this idea is counter to traditional machine translation system which rely on extensive use of rule in parsing transfer and generation although there are some preliminary report on the superiority of the memory based translation in term of it scalability quality of translation and easiness of grammar writing we have not seen any report on it performance this is perhaps the first report discussing the feasibility and problem of the approach based on actual massively parallel implementation using real data we also claim that the architecture of the ixm associative processor is highly suitable for memory based translation task parsing performance of the memory based translation system attained a few millisecond per sentence 
existing formalism for default reasoning capture some aspect of the nonmonotonicity of human commonsense reasoning however perlis ha shown that one of these formalism circumscription is subject to certain counterintuitive limitation kraus and perlis suggested a partial solution but significant problem remain in this paper we observe that the unfortunate limitation of circumscription are even broader than perlis originally pointed out moreover these problem are not confined to circumscription they appear to be endemic in current nonmonotonic reasoning formalism we develop a much more general solution than that of kraus and perlis involving restricting the scope of nonmonotonic reasoning and show that it remedy these problem in a variety of formalism 
a central goal of qualitative physic is to provide a framework for organizing and using quantitative knowledge one important use of quantitative knowledge is numerical simulation while current numerical simulator are powerful they are often hard to construct do not reveal the assumption underlying their construction and do not produce explanation of the behavior they predict this paper show how to combine qualitative and quantitative model to produce a new class of self explanatory simulation which combine the advantage of both kind of reasoning self explanatory simulation provide the accuracy of numerical model and the interpretive power of qualitative reasoning we define what self explanatory simulation are and show how to construct them automatically we illustrate their power with some example generated with an implemented system simgen we analyze the limitation of our technique and discus plan for future work 
we have previously argued that the syntactic structure of natural language can be exploited to construct powerful polynomial time inference procedure this paper support the earlier argument by demonstrating that a natural language based polynomial time procedure can solve schubert s steamroller in a single step 
artificial creature are autonomous mobile agent that have to react in real time to sensor and plan and perform action in the real world current effective architecture for artificial creature are behavior based and use variant of the subsumption architecture the paper proposes an extension to these architecture in term of a layer which introduces cognitive capability to artificial creature this extension is frame based and us emergent frame recognition to determine which frame should become active 
tree have played a key role in the study of constraint satisfaction problem because problem with tree structure can be solved efficiently it is shown here that a family of generalized tree k tree can offer increasing representational complexity for constraint satisfaction problem while maintaining a bound on computational complexity linear in the number of variable and exponential in k additional result are obtained for larger class of graph known a partial k tree these method may be helpful even when the original problem doe not have k tree or partial k tree structure specific tradeoff are suggested between representational power and computational complexity 
the power of knowledge acquisition system that employ failure driven learning derives from two main source an effective global credit assignment process that determines when to acquire new knowledge by watching an expert s behavior and an efficient local credit assignment process that determines what new knowledge will be created for completing a failed explanation of an expert s action because an input e g observed action to a failure driven learning system can generate multiple explanation a learning opportunity to extend the incomplete domain theory can go unobserved this paper describes a failure driven learning with a context analysis mechanism a a method to constrain explanation and thereby increase the number of learning opportunity experimentation using a synthetic expert system a the observed expert show that the use of context analysis increase the number of learning opportunity by about and increase the overall amount of improvement to the expert system by around 
admissible heuristic are an important class of heuristic worth discovering they guarantee shortest path solution in search algorithm such a a and they guarantee le expensively produced but boundedly longer solution in search algorithm such a dynamic weighting unfortunately effective accurate and cheap to compute admissible heuristic can take year for people to discover several researcher have suggested that certain transformation of a problem can be used to generate admissible heuristic this article defines a more general class of transformation called abstraction that are guaranteed to generate only admissible heuristic it also describes and evaluates an implemented program absolver io that us a mean end analysis search control strategy to discover abstracted problem that result in effective admissible heuristic absolver i discovered several well known and a few novel admissible heuristic including the first known effective one for rubik s cube thus concretely demonstrating that effective admissible heuristic can be tractably discovered by a machine 
this research attempt to span the gap between the ai in medicine aim and consistency based diagnosis cbd community by applying cbd to physiology the highly regulated nature of physiological system challenge standard cbd algorithm which are not tailored for complex dynamic system to combat this problem we separate static from dynamic analysis so that cbd is performed over the steady state constraint at only a selected set of time slice regulatory model help link static inter slice diagnosis into a complete dynamic account of the physiological progression this provides a simpler approach to cbd in dynamic system that a preserve information reuse capability b extends information theoretic probing and c add a new capability to cbd the detection of dynamic fault i e those that do not necessarily persist throughout diagnosis 
this paper present an approach to retranslation the third and final step of the theory reduction approach to solving theory revision problem retranslation involves putting a modified operationalized or reduced version of the desired revised theory back into the entire language of the original theory this step is desirable for a number of reason not least of which is the need to compress what are generally very large reduced theory into much smaller and thus more efficiently evaluated unreduced theory empirical result for the retranslation method are presented 
this paper provides an account of the representation of default in cyc and their semantics in term of first order logic with reification default reasoning is a complex thing and we have found it beneficial to separate various complex issue whose current best solution is likely to change now and then such a deciding between extension preferring one default to another etc and deal with them explicitly in the knowledge base thus allowing u to adopt a simple and hopefully fixed logical mechanism to handle the basic nonmonotonicity itself we also briefly describe how this default reasoning scheme is implemented in cyc 
analogical planning provides a mean of solving problem where other machine learning method fail because it doe not require numerous previous example or a rich domain theory given a problem in an unfamiliar domain the target case an analogical planning system locates a successful plan in a similar domain the bast case and us the similarity to generate the target plan unfortunately the analogical planning process is expensive and inflexible many of the limiting factor reside in the base selection step which drive the analogy formation process this paper describes two way of increasing the effectiveness and efficiency of analogical planning first a parallel graph match base selection algorithm is presented a parallel implementation on the connection machine is described and shown to substantially decrease the complexity of base selection second a base case merge algorithm is shown to increase the flexibility of analogical planning by combining the benefit of several base case when no single plan contributes enough information to the analogy the effectiveness of this approach is demonstrated with example from the domain of automatic programming 
this paper present a comprehensive approach to automatic theory refinement in contrast to other system the approach is capable of modifying a theory which contains multiple fault and fault which occur at intermediate point in the theory the approach us explanation to focus the correction to the theory with the correction being supplied by an inductive component in this way an attempt is made to preserve the structure of the original theory a much a possible because the approach begin with an approximate theory learning an accurate theory take fewer example than a purely inductive system the approach ha application in expert system development where an initial approximate theory must be refined the approach also applies at any point in the expert system lifecycle when the expert system generates incorrect result the approach ha been applied to the domain of molecular biology and show significantly better result then a purely inductive learner 
to allow efficient parallel processing of prolog program on distributed multiprocessor a nonshared variable binding approach is required such that binding environment can be independently distributed among processor this paper present a binding scheme which realises the independence of a clause s binding environment by eagerly instantiating variable across clause argument the application of the scheme on a prolog virtual machine ha illustrated it feature of efficiency in execution and simplicity in implementation the preliminary performance evaluation ha demonstrated the feasibility of the scheme 
using case to find innovative solution to problem is mainly the result of two process cross contextual rem in ding and composition of multiple case or case part although the ability to use case taken from across contextual boundary is desirable there is a tension between representing and accessing case across context and in using part of multiple case to synthesize a solution one way of alleviating this difficulty is through index transformation in this paper we represent two index transformation technique that facilitate both cross contextual remindings and the access of multiple appropriate case part the mechanism are general and principled based on a qualitative calculus they are also behavior preserving a needed requirement for case synthesis in many domain of interest the transformation technique have been implemented in cadet a case based problem solver mat operates in the domain of mechanical design 
sensor interpret ation involves the determination of high level explalliations of sensor data blackboard based interpretation system have usually been limited to incremental hypothesize and test strategy for resolving uncertainty we have developed a new interpretation framework that support the use of more sophisticated strat egies like differential diagnosis the resun framework ha two key component an evidential represent ation that includes explicit symbolic encoding of the source of uncertainty sou in the evidence for hypothesis and a script based incremental control planner interpretation is viewed a an incremental process of gathering evidence to resolve particular source of uncertainty control plan invoke action that examine the symbolic sou associated with hypothesis and use the resulting information to post goal to resolve uncertadnty these goal direct the system to expand method appropriate for resolving the current source of uncertllinty in the hypothesis the planner s refocusing mechanism make it possible to postpone focusing decision when there is insufficient information to make decision and provides opportunistic control capability the resun framework ha been implemented and experimentally verified using a simulated aircraft monitorilllg application 
expert system in complex domain require rich knowledge representation formalism and problem solving paradigm a typical framework may involve a blackboard architecture and a reason maintenance system rms to guarantee the consistency of the link between the blackboard node however in order to satisfy computational feasibility and become operational the resulting expert system must often be rewritten using le expressive tool we propose an architecture integrating efficiently an ops like inference engine and an assumption based truth maintenance system atm these paradigm have been separately investigated and extended role distribution between an atm and an inference engine integrated in a single framework is one of the major issue to obtain good overall performance two architecture will be studied loose coupling where the atm and the inference engine are clearly separated and tight coupling where the atm is intimately integrated with the match phase of a rete based inference engine the advantage and drawback of both solution are described in detail finally future work is discussed 
plan fail for many reason during planner development failure can often be traced to action of the planner itself failure recovery analysis is a procedure for analyzing execution trace of failure recovery to discover how the planner s action may be causing failure the four step procedure involves statistically analyzing execution data for dependency between action and failure mapping those dependency to plan structure explaining how the structure might produce the observed dependency and recommending modification the procedure is demonstrated by applying it to explain how a particular recovery action may lead to a particular failure in the phoenix planner the planner is modified based on the recommendation of the analysis and the modification are shown to improve the planner s performance by removing a source of failure and so reducing the overall incidence of failure 
this paper characterizes connectionist type architecture that allow a distributed solution for class of constraint satisfaction problem the main issue addressed is whether there exists a uniform model of computation where all node are indistinguishable that guarantee convergence to a solution from every initial state of the system whenever such a solution exists we show that even for relatively simple constraint network such a ring there is no general solution using a completely uniform asynchronous model however some restricted topology like tree can accommodate the uniform asynchronous model and a protocol demonstrating this fact is presented an almost uniform asynchronous network consistency protocol is also presented we show that the algorithm are guaranteed to be self stabilizing which make them suitable for dynamic or error prone environment 
in baader a we have considered different type of semantics for terminologicial cycle in the concept language tlq which allows only conjunction of concept and value restriction it turned out that greatest fixed point semantics gfp semantics seems to be most appropriate for cycle in this language in the present paper we shall show that the concept defining facility of flo with cyclic definition and gfp semantics can also be obtained in a different way one may replace cycle by role definition involving union composition and transitive closure of role this proposes a way of retaining in an extended language the pleasant feature of gfp semantics for flq with cyclic definition without running into the trouble caused by cycle in larger language starting with the language alc of schmidt schau b smolka which allows negation conjunction and disjunction of concept a well a value restric tions and exists in restriction we shall disallow cyclic concept definition but instead shall add the possibility of role definition involving union composition and transitive closure of role in contrast to other terminological kr system which incorporate the transitive closure operator for role we shall be able to give a sound and complete algorithm for concept subsumption 
identifying the regularity underlying speaker decision to emphasize or de emphasize an item intonationally ha long been the subject of speculation and controversy this paper describes a study of accent assignment based upon the analysis of natural recorded read speech result are being incorporated in newspeak an interface to the bell laboratory text to speech system which varies intonational feature based upon syntactic structure and higher level discourse information inferred from unrestricted text in order to generate more natural synthetic speech implication of the work for the evaluation of discourse model for automatic labeling of prosodic feature and for speech synthesis are discussed 
this paper describes a linguistic knowledge representation technique suitable for reducing analysis time and memory requirement in a parser for continuous speech parsing speech having to process a lattice of word hypothesis instead of a string of word involves a tremendous amount of search and the generation of a high number of phrase hypothesis the aim is while using powerful and flexible formalism for syntax and semantics to generate compact phrase hypothesis each one accounting for many syntactic rule simultaneously the proposed method is able to cope with and to take advantage from the fact that short word are often missing from the lattice a detailed example is given to clarify this method finally experimental data arc presented and discussed showing the effectiveness of the proposed technique 
this paper describes an agent architecture and it implementation for situated robot control in field environment the architecture draw from the idea of universal plan and subsumption s layered control producing reaction plan that exploit low level competence a operator the architecture ha been implemented in an extended version of the gapps rex situated automaton programming language this language produce synchronous virtual circuit which have been shown to have formal epistemic property the resulting architecture exhibit robust task execution ha high level goal representation and maintains consistent semantics between agent state and the environment ongoing experiment using the architecture with two land mobile robot and one undersea mobile robot are described the robot perform their task robustly during normal change in the task environment 
this paper report several experimental result on the speed of convergence of neural network training using genetic algorithm and back propagation recent excitement regarding genetic search lead some researcher to apply it to training neural network there are report on both successful and faulty result and unfortunately no systematic evaluation ha been made this paper report result of systematic experiment designed to judge whether use of genetic algorithm provides any gain in neural network training over existing method experimental result indicate that genetic search is at best equally efficient to faster variant of back propagation in very small scale network but far le efficient in larger network 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
explanation requires a dialogue user must be allowed to ask question about previously given explanation however building an interface that allows user to ask follow up question pose a difficult challenge for natural language understanding because such question often intermix meta level reference to the discourse with object level reference to the domain we propose a hypertext like interface that allows user to point to the portion of the system s explanation they would like clarified by allowing user to point many of the difficult referential problem in natural language analysis can be avoided however the feasibility of such an interface rest on the system s ability to understand what the user is pointing at i e the system must understand it own explanation to solve this problem we employ a planning approach to explanation generation which record the design process that produced an explanation so that it can be used in later reasoning in this paper we show how synergy arises from combining a pointing style interface with a text planning generation system making explanation dialogue more feasible 
our eminent researcher including john mccarthy allen newell claude shannon herb simon ken thompson and alan turing put significant effort into computer chess research now that computer have reached the grandmaster level and are beginning to vie for the world championship the ai community should pause to evaluate the significance of chess in the evolving objective of ai evaluate the contribution made to date and ass what can be expected in the future despite the general interest in chess amongst computer scientist and the significant progress in the last twenty year there seems to be a jack of appreciation for the field in the ai community on one hand this is the fruit of success brute force work why study anything else but also the result of a focus on performance above all else in the chess community also chess ha proved to be too challenging for many of the ai technique that have been thrown at it we wish to promote chess a the fundamental test bed recognized by our founding researcher and increase awareness of it contribution to date 
we develop a qualitative method for under standing and representing phase space struc tures of complex system to demonstrate this method a program called map ha been con structed that understands qualitatively differ ent region of a phase space and represents and extract geometric shape information about these region using deep domain knowledge of dynamical system theory given a dynamical system specified a a system of governing equa tions map applies a successive sequence of operation to incrementally extract the qual itative information and generates a complete high level symbolic description of the phase space structure through a combination of nu merical combinatorial and geometric compu tations and spatial reasoning technique the high level description is sensible to human be ings and manipulable by other program we are currently applying the method to a difficult engineering design domain in which controller for complex system are to be automatically synthesized to achieve desired property based on the knowledge of the phase space shape of the system 
this paper give an overview of a natural language dialogue system called pragma this system contains a number of novel and important feature a well a integrating previous work into a unified mechanism the most important advance that pragma represents compared with previous system is the high degree of bidirectionality employed in it design a single grammar is used for interpretation and generation and the same knowledge source are used for plan recognition and response generation the system is also flexible in that it generates useful extended response not only to query which allow the user s plan to be inferred but also to query which do not allow this 
in baader a we have considered different type of semantics for terminologicial cycle in the concept language tlq which allows only conjunction of concept and value restriction it turned out that greatest fixed point semantics gfp semantics seems to be most appropriate for cycle in this language in the present paper we shall show that the concept defining facility of flo with cyclic definition and gfp semantics can also be obtained in a different way one may replace cycle by role definition involving union composition and transitive closure of role this proposes a way of retaining in an extended language the pleasant feature of gfp semantics for flq with cyclic definition without running into the trouble caused by cycle in larger language starting with the language alc of schmidt schau smolka which allows negation conjunction and disjunction of concept a well a value restriction and exists in restriction we shall disallow cyclic concept definition but instead shall add the possibility of role definition involving union composition and transitive closure of role in contrast to other terminological kr system which incorporate the transitive closure operator for role we shall be able to give a sound and complete algorithm for concept subsumption 
we describe an approach to training a statistical parser from a bracketed corpus and demonstrate it use in a software testing application that translates english specification into an automated testing language a grammar is not explicitly specified the rule and contextual probability of occurrence are automatically generated from the corpus the parser is extremely successful at producing and identifying the correct parse and nearly deterministic in the number of par that it produce to compensate for undertraining the parser also us general linguistic subtheories which aid in guessing some type of novel structure 
although generalization and discrimination are commonly used together in machine learning little ha been understood about how these two method are intrinsically related this paper describes the idea of complementary discrimination which exploit semantically the syntactic duality between the two approach discriminating a concept is equivalent to generalizing the complement of the concept and vice versa this relation brings together naturally generalization and discrimination so that learning program may utilize freely the advantage of both approach such a learning by analogy and learning from mistake we will give a detailed description of the complementary discrimination learning cdl algorithm and extend the previous result by considering the effect of noise and analyzing the complexity of the algorithm cdl s performance on both perfect and noisy data and it ability to manage the tradeoff between simplicity and accuracy of concept have provided some evidence that complementary discrimination is a useful and intrinsic relation between generalization and discrimination 
when explanation based learning ebl is used for knowledge level learning kll training example are essential and ebl is not simply reducible to partial evaluation a key enabling factor in this behavior is the use of domain theory in which not every element is believed a priori when used with such domain theory ebl provides a basis for rote learning deductive kll and induction from multiple example nondeductive kll this article lay the groundwork for using ebl in kll by describing how ebl can lead to increased belief and describes new result from using soar s chunking mechanism a variation on ebl a the basis for a task independent rote learning capability and a version space based inductive capability this latter provides a compelling demonstration of nondeductive kll in soar and provides the basis for an integration of conventional ebl with induction however it also reveals how one of soar s key assumption the non penetrable memory assumption make this more complicated than it would otherwise be this complexity may turn out to be appropriate or it may point to where modification of soar are needed 
stadard reinforcement learning method assume they can identify each state distinctly before making an action decision in reality a robot agent only ha a limited sensing capability and identifying each state by extensive sensing can be time consuming this paper describes an approach that learns active perception strategy in reinforcement learning and considers sensing cost explicitly the approach integrates cost sensitive learning with reinforcement learning to learn an efficient internal state representation and a decision policy simultaneously in a finite deterministic environment it not only maximizes the long term discounted reward per action but also reduces the average sensing cost per state the initial experimental result in a simulated robot navigation domain are encouraging 
traditional syntactic model of parsing have been inadequate for task driven processing of extended text because they spend most of their time on misdirected linguistic analysis leading to problem with both efficiency and coverage statistical and domain driven processing offer compelling possibility but only a a complement to syntactic processing for semanticallyoriented task such a data extraction from text the problem is how to combine the coverage of these weaker method with the detail and accuracy of traditional lingusitic analysis a good approach is to focus linguistic analysis on relation that directly impact the semantic result detaching these relation from the complete constituent to which they belong this approach result in a faster more robust and potentially more accurate parser for real text 
many design task have search space that are vague and evaluation criterion that are subjective we present a model of design that can solve such problem using a method of plausible design adaptation in our approach adaptation transformation are used to modify the component and structure of a design and constraint on the design problem this adaptation process play multiple role in design it is used a part of case based reasoning to modify previous design case it accommodates constraint that arrive late in the design process by adapting previous decision rather than by retracting them it resolve impasse in the design process by weakening preference constraint this model of design ha been implemented in a computer program called julia that design the presentation and menu of a meal to satisfy multiple interacting constraint 
this paper describes a series of experiment that were performed on the rocky iii robot rocky iii is a small autonomous rover capable of navigating through rough outdoor terrain to a predesignated area searching that area for soft soil acquiring a soil sample and depositing the sample in a container at it home base the robot is programmed according to a reactive behavior control paradigm using the alfa programming language this style of programming produce robust autonomous performance while requiring significantly le computational resource than more traditional mobile robot control system the code for rocky iii run on an bit processor and us about k of memory 
there exist method in automated theorem proving for non classical logic based on translation of logic from a non classical source logic abbreviated henceforth sl into a classical target logic abbreviated henceforth tl these valuable method do not address the important practical problem of presenting proof in sl we propose a framework applicable at least to s p k t k for presenting proof of theorem of these logic found in a familiar tl order sorted predicate logic abbreviated henceforth ospl the method backward translates lemma in a deduction in tl either a into lemma in a corresponding deduction in sl in the best case or b into formula semantically related to lemma in a corresponding deduction in the worst case a a natural consequence we bring to the fore the fact that this framework can also be used to help in solving another important and very difficult problem the transfer of strategy from one logic to another one conjecture with corresponding theorem which is a particular case of itis stated when b above hold we give sufficient and in general satisfactory condition in order to obtain the lemma in sl two example are treated in full detail the well known problem of the wise man puzzle and another one which show how our method can be used to transfer strategy no additional theoretical result is given in this direction but it is clear from the example how the proposed framework can help to transfer strategy 
designing a user interface is an ill defined problem making cooperative problem solving system a promising approach to support user interface designer cooperative problem solving system are modular system that support the human designer with multiple independent system component we present a system architecture and an implemented system framer that demonstrate the cooperative problem solving approach framer represents design knowledge in formal machine interpretable knowledge source such a critic and dynamic specification sheet and in semi formal knowledge source such a a palette of user interface building block and a checklist each of these component contributes significantly to the overall usefulness of the system while requiring only limited resource to be designed and implemented 
explanation based learning ebl can be used to significantly speed up problem solving is there sufficient structure in the definition of a problem space to enable a static analyzer using ebl style optimization to speed up problem solving without utilizing training example if so will such an analyzer run in reasonable time this paper demonstrates that for a wide range of problem space the answer to both question is yes the static program speed up problem solving for the prodigy problem solver without utilizing training example in minton s problem space static acquires control knowledge from twenty six to seventy seven time faster and speed up prodigy up to three time a much a prodigy ebl this paper present static s algorithm derives a condition under which static is guaranteed to achieve polynomial time problem solving and contrast static with prodigy ebl 
the development of a formal logic for reasoning about change ha proven to be surprisingly difficult furthermore the logic that have been developed have found surprisingly little application in those field such a qualitative reasoning that are concerned with building program that emulate human common sense reasoning about change in this paper we argue that a basic tenet of qualitative reasoning practice the separation of modeling and simulation obviates many of the difficulty faced by previous attempt to formalize reasoning about change our analysis help explain why the qr community ha been nonplussed by some of the problem studied in the nonmonotonic reasoning community further the formalism we present provides both the beginning of a formal foundation for qualitative reasoning and a framework in which to study a number of open problem in qualitative reasoning 
based on psychological study which show that metaphor and other nonliteral construction are comprehended in the same amount of time a comparable literal construction some researcher have concluded that literal meaning is not computed during metaphor comprehension in this paper we suggest that the empirical evidence doe not rule out the possibility that literal meaning is constructed we present a computational model of metaphor comprehension which is consistent with the data but in which literal meaning is computed this model ha been implemented a part of a unification based natural language processing system called link 
function sharing is the simultaneous implementation of several function by a single structural element this article describes how the idea of function sharing can be used in a computational design procedure that produce efficient design from modular design these idea have been implemented a a computer program for the domain of mechanical device that can be described functionally a a network of lumped parameter idealized element 
this paper describes cabot a case based system that is able to adjust it retrieval and adaptation metric in addition to storing case it ha been applied to the game of othello experiment show that cabot save about half a many case a similar system that do not adjust their retrieval and adaptation mechanism it also consistently beat these system these result suggest that existing case based system could save fewer case without reducing their current level of performance they also demonstrate that it is beneficial to distinguish failure due to missing information faulty retrieval and faulty adaptation 
in machine learning there is considerable interest in technique which improve planning ability initial investigation have identified a wide variety of technique to address this issue progress ha been hampered by the utility problem a basic tradeoff between the benefit of learned knowledge and the cost to locate and apply relevant knowledge in this paper we describe the composer system which embodies a probabilistic solution to the utility problem we outline the statistical foundation of our approach and compare it against four other approach which appear in the literature 
a central problem in text understanding research is the indeterminacy of natural language two related issue that arise in confronting this problem are the need to make complex interaction possible among the system component that search for cue and the need to control the amount of reasoning that is done once cue have been discovered we identify a key difficulty iu enabling true interaction among system component and we propose an architectural framework that minimizes this difficulty a concrete example of a reasoning task encountered in an actual text understanding application is used to motivate the design principle of our framework 
the author argue for a generalisation of inference from the standard account in term of truth preservation to one which countenance preservation of other desirable metalinguistic property the development is partly historical and partly analytic a relational account of preservation is then presented and from this the two notion of inferential structure and inferential model are derived to illustrate the generality of the relational conception of inference we show how such structure and model can be realised in the development of a legal advisory system 
one of the grand challenge for machine learning is the problem of learning from textbook this paper address the problem of learning from text including omission and inconsistency that are clarified by illustrative example to avoid problem in natural language understanding we consider a simplification of this problem in which the text ha been manually translated into a logical theory this learning problem is solvable by a technique that we call analogical abductive explanation based learning ana ebl formal evidence and experimental result in the domain of contract bridge show that the learning technique is both efficient and effective 
the development of larger scale natural language system ha been hampered by the need to manually create mapping from syntactic structure into meaning representation a new approach to semantic interpretation is proposed which us partial syntactic structure a the main unit of abstraction for interpretation rule this approach can work for a variety of syntactic representation corresponding to directed acyclic graph it is designed to map into meaning representation based on frame hierarchy with inheritance we define semantic interpretation rule in a compact format the format is suitable for automatic rule extension or rule generalization when existing hand coded rule do not cover the current input furthermore automatic discovery of semantic interpretation rule from input output example is made possible by this new rule format the principle of the approach are validated in a comparison to other method on a separately developed domain instead of relying purely on painstaking human effort this paper combine human expertise with computer learning strategy to successfully overcome the bottleneck of semantic interpretation 
this paper present a new approach for exploiting truth maintenance system tm which make them simpler to use without necessarily incurring a substantial performance penalty the basic intuition behind this approach is to convey the locality of the knowledge representation of the problem solver to the tm the tm then us this locality information to control and restrict it inference the new tm accept arbitrary propositional formula a input and use general boolean constraint propagation bcp to answer query about whether a particular literal follows from the formula our tm exploit the observation that if the set of propositional formula are converted to their prime implicates then bcp is both efficient and logically complete this observation allows the problem solver to influence the degree of completeness of the tm by controlling how many implicates are constructed this control is exerted by using the locality in the original task to guide which combination of formula should be reduced to their prime implicates this approach ha been implemented and tested both within assumption based truth maintenance system and logic based truth maintenance system 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
this paper will present computer model of three robotic motion planning and learning system which use a multi sensory learning strategy for learning and control in these system machine vision input is used to plan and execute movement utilizing an algorithmic controller while at the same time neural network learn the control of those motion using feedback provided by position and velocity sensor in the actuator a specific advantage of this approach is that in addition to the system leaming a more automatic behavior it employ a computationally le costly sensory system more tightly coupled from perception to action 
we evaluate current explanation scheme these are either insufficiently general or suffer from other serious drawback we propose a domain independent explanation system that is based on ignoring irrelevant variable in a probabilistic setting we then prove important property of some specific irrelevance based scheme and discus how to implement them 
knowledge compilation speed inference by creating tractable approximation of a knowledgebase but this advantage is lost if the approximation are too large we show how learningconcept generalization can allow for a more compact representation of the tractabletheory we also give a general induction rule for generating such concept generalization finally we prove that unless np non uniform p not all theory have small horn leastupper bound approximation introductionwork in 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
consider an infinite binary search tree in which the branch have independent random cost suppose that we must find an optimal cheapest or nearly optimal path from the root to a node at depth n karp and pearl show that a bounded lookahead backtracking algorithm a usually find a nearly optimal path in linear expected time when the cost take only the value or from this successful performance one might conclude that similar heuristic should be of more general use but we find here equal success for a simpler non backtracking bounded lookahead algorithm so the search model cannot support this conclusion if however the search tree is generated by a branching process so that there is a possibility of node having no son or branch having prohibitive cost then the non backtracking algorithm is hopeless while the backtracking algorithm still performs very well these result suggest the general guideline that backtracking becomes attractive when there is the possibility of dead end or prohibitively costly outcome 
we present a new algorithm sie for designing lumped parameter model from first principle like the ibis system of williams sle us a qualitative representation of parameter interaction to guide it search and speed the test for working design but sie s interaction set representation is considerably simpler than ibis s space of potential and existing interaction furthermore si is both complete and systematic it explores the space of possible design in an nonredundant manner 
in this paper we examine how the complexity of domain independent planning with strip style operator depends on the nature of the planning operator we show how the time complexity varies depending on a wide variety of condition whether or not delete list are allowed whether or not negative precondition are allowed whether or not the predicate are restricted to be proposition i e ary whether the planning operator are given a part of the input to the planning problem or instead are fixed in advance 
the construction of the semantic representation for a natural language sentence or a piece of discourse cannot be covered by the so called compositional semantics alone in the general case the non compositional construction step of generating quantifier scoping and of anaphora resolution have to be included in order to filter out unnecessary information a soon a possible it is desirable to merge these three phase into one processing step we describe how the rule for extended compositional semantics a presented in pereira can be integrated into a parser for categorial grammar the inspection of the data flow show where concurrency can come into play 
this paper report on an indexing system supporting retrieval of past case a advice about everyday social problem it ha been implemented in the abby lovelorn advising system two point are emphasized d in ice are description of problem and their cause couched in a vocabulary centered on intentional causality and index fit a fixed format that allows reification of identity and thematic relationship a feature abby answer several of the central question that any indexing system must address and ha advantage over le restrictive system 
in this paper we present a novel explanation of the source of indefinite information in common sense reasoning indefinite information arises from report about the world expressed in term of concept that have been defined using only definite rule adopting this point of view we show that first order logic is insufficiently expressive to handle important example of common sense reasoning a a remedy we propose the use of circumscribed definite rule and we then investigate the proof theory of this more expressive framework we consider two approach first prototypical proof a special type of proof by induction which yield a sound proof theory second we describe case in which there exists a decision procedure for answering query a particularly significant result because it show that it is possible to have decidable query processing in circumscribed theory that are not equivalent to any first order theory 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
an equational approach to the synthesis of functional and logic program is taken typically a target program contains equation that are only true in the standard model of the given domain rule to synthesize such program induction is necessary we propose heuristic for generalizing from a sequence of deductive consequence these are combined with rewrite based method of inductive proof to derive provably correct program 
this paper formally present a class of planning problem which allows non binary state variable and parallel execution of action the class is proven to be tractable and we provide a sound and complete polynomial time algorithm for planning within this class this result mean that we are getting closed to tackling realistic planning problem in sequential control where a restricted problem representation is often sufficient but where the size of the problem make tractability an important issue 
robot act upon and perceive the world from a particular perspective it is important to recognize this relativity to perspective if one is not to be overly demanding in specifying what they need to know in order to be able to achieve goal through action in this paper we show how a formal theory of knowledge and action proposed in lesperance can be used to formalize several kind of situation drawn from a robotics domain where indexical knowledge is involved several example treated deal with the fact that ability to act upon an object doe not require de re knowledge of the object or it absolute position knowledge of it relative position is sufficient it is shown how the fact that perception yield indexical knowledge can be captured we also point out the value of being able to relate indexical knowledge and objective knowledge within the same formalism through an example involving the use of a map for navigation finally we discus a problem raised by some higher level parametrized action and propose a solution 
we study what kind of data may ease the computational complexity of learning of horn clause theory in gold s paradigm and boolean function in pac learning paradigm we give several definition of good data basic and generative representative set and develop data driven algorithm that learn faster from good example and degenerate to learn in the limit from the worst possible example we show that horn clause theory k term dnf and general dnf boolean function are polynomially learnable from generative representative presentation 
this paper describes an inductive learning method in probabilistic domain it acquires an appropriate probabilistic model from a small amount of observation data in order to derive an appropriate probabilistic model a presumption tree with least description length is constructed description length of a presumption tree is defined a the sum of it code length and log likelihood using a constructed presumption tree the probabilistic distribution of future event can be presumed appropriately from observation of occurrence in the past this capability enables the efficiency of certain kind of performance system such a diagnostic system that deal with probabilistic problem the experimental result show that a model based diagnostic system performs efficiently by making good use of the learning mechanism in comparison with a simple probability estimation method it is shown that the proposed approach requires fewer observation to acquire an appropriate probabilistic model 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
large case database are numerous and packed with information the largest of them are potentially rich source of domain knowledge the fgp machine is a software architecture that can make this knowledge explicit and bring it to bear on classification and prediction problem the architecture provides much of the functionality of traditional expert system without requiring the system builder to preprocess the database into rule frame or any other fixed abstraction implementation of the fgp machine use similarity based reminding and the case themselves to drive the inference engine by having the system calculate and incorporate a measure of feature salience into it distance calculation the fgp architecture smoothly cope with incomplete data and is particularly well suited to weak theory domain we explain the model describe a particular implementation of it and present test result for a classification task in three application area 
in this paper we explore the use of an adaptive search technique genetic algorithm to construct a system gabel which continually learns and refines concept classification rule from it interaction with the environment the performance of the system is measured on a set of concept learning problem and compared with the performance of two existing system id r and c preliminary result support that despite minimal system bias gabil is an effective concept learner and is quite competitive with id r and c a the target concept increase in complexity 
description logic are a popular formalism for knowledge representation and reasoning this paper introduces a new operation for description logic computing the least common subsumer of a pair of description this operation computes the largest set of commonality between two description after arguing for the usefulness of this operation we analyze it by relating computation of the least common subsumer to the well understood problem of testing subsumption a close connection is shown in the restricted case of structural subsumption we also present a method for computing the least common subsumer of attribute chain equality and analyze the tractability of computing the least common subsumer of a set of description an important operation in inductive learning 
a real time ai problem solver performs a task or a set of task in two phase planning and execution under real time constraint a real time ai problem solver must balance the planning and the execution phase of it operation to comply with deadline this paper provides a methodology for specification and analysis of real time ai problem and problem solver this methodology is demonstrated via domain analysis of the real time path planning problem and via algorithm analysis of dynoraii and rta we provide new result on worst case complexity of the problem we also provide experimental evaluation of dynoraii and rta for deadline compliance 
this paper describes a methodology for the design of shape starting from an initial shape a geometric reasoning kernel is used to generate and control a sequence of numerical optimization subproblems that converges to a final design a topology and associated geometry that can be significantly different from the starting shape a subproblem in the sequence is systematically formulated from a geometric abstraction of current shape and it objective function constraint and bound are dynamically derived the geometric representation of the shape is adaptive and change throughout the problem solving process to accommodate the shape change trend that occur shape evolution take place within each subproblem and between subproblems intrasubproblem evolution is responsible for geometric modification while inter subproblem evolution handle topology modification the combination of geometric reasoning and numerical optimization technique provides a robust and systematic methodology for shape synthesis that can generate new design shape without relying on heuristic or domain spedfic knowledge 
we present a method for approximating the expected number of step required by a heuristic search algorithm to reach a goal from any initial state in a problem space the method is based on a mapping from the original state space to an abstract space in which state are characterized only by a syntactic distance from the nearest goal modeling the search algorithm a a markov process in the abstract space yield a simple system of equation for the solution time for each state we derive some insight into the behavior of search algorithm by examining some closed form solution for these equation we also show that many problem space have a clearly delineated easy zone inside which problem are trivial and outside which problem are impossible the theory is borne out by experiment with both markov and non markov search algorithm our result also bear on recent experimental data suggesting that heuristic repair algorithm can solve large constraint satisfaction problem easily given a preprocessor that generates a sufficiently good initial state 
this paper present fit an intelligent tutoring system it for the domain of addition of fraction it wa developed with the aim of improving on many of the shortcoming of existing tutor in the mathematical domain the paper largely describes it functioning in order to give the reader a better feel of the tutor s capability than obtained from it description an actual student tutor protocol extract is given more significantly the tutor ha also been evaluated in several way with seemingly very encouraging result so far however due to length restriction they are not reported in this paper the paper concludes by briefly highlighting some of fit s improved feature over other existing tutor in the domain a well a some of it shortcoming 
we show that the terminological logic acc comprising boolean operation on concept and value restriction is a notational variant of the propositional modal logic k m to demonstrate the utility of the correspondence we give two of it immediate by product namely we axiomatize acc and give a simple proof that subsumption in acc is pspace complete replacing the original six page one furthermore we consider an extension of acc additionally containing both the identity role and the composition union transitive reflexive closure range restriction and inverse of role it turn out that this language called tsl is a notational variant of the propositional dynamic logic converse pdl using this correspondence we prove that it suffices to consider finite tsl model show that tsl subsumption is decidable and obtain an axiomatization of tsl by discovering that feature correspond to deterministic program in dynamic logic we show that adding them to tsc preserve decidability although violates it finite model property additionally we describe an algorithm for deciding the coherence of inverse free tsc concept with feature finally we prove that universal implication can be expressed within tsc 
i present algorithm for automated long term behavior prediction which can recognize when a simulation ha run long enough to produce a representative behavior sample characterize the behavior and determine whether this behavior will continue forever or eventually terminate or otherwise change it charader i have implemented these algorithm in a working program which doe longterm behavior prediction for mechanical device 
resolution reasoner when applied to set theory problem typically suffer from lack of focus mar is a program that attempt to rectify this difficulty by exploiting the definition like character of the set theory axiom a in the case of it predecessor slim it employ a tableau proof procedure based on binary resolution but mar is enhanced by an equality substitution rule and a device for introducing previously proved theorem a lemma mar s performance compare favorably with that of other existing automated reasoner for this domain mar find proof for many basic fact about function construed a set of ordered pair mar is being used to attack the homomorphism test problem the theorem that the composition of two group homomorphism is a group homomorphism 
we present a heuristic based approach to deep space mission scheduling which is modeled on the approach used by expert human scheduler in producing schedule for planetary encounter new chronological evaluation technique are used to focus the search by using information gained during the scheduling process to locate classify and resolve region of conflict our approach is based on the assumption that during the construction of a schedule there exist several disjunct temporal region where the demand for one resource type or a single temporal constraint dominates bottleneck region if the scheduler can identify these region and classify them based on their dominant constraint then the scheduler can select the scheduling heuristic 
this paper present a parsing method for identifying word in mandarin chinese sentence the identification system is composed of a tomita s parser augmented with test originally a part of the english chinese machine translation system ccl ecmt together with the associated augmented context free grammar for word composition the simple augmented grammar with the score function effectively capture the intuitive idea of longest possible composition of chinese word in sentence and at the same time take into consideration the frequency count of word the identification rate of this system for the corpus taken from book and a newspaper is this identification system is simple but the identification rate is relatively high the minimum element for word composition parsing is down to character a opposed to sentence parsing down to chinese word it ha the potential of incorporating phrase structure and semantic checking into the system in this way word identification syntactic and even semantic analysis can be organized into a single phase the result of testing the word identification on corpus taken from book and a chinese newspaper are also presented 
since knowledge is usually incomplete agent need to introspect on what they know and do not know the best known model of introspective reasoning suffer from intractability or even undecidability if the underlying language is first order to better suit the fact that agent have limited resource we recently proposed a model of decidable introspective reasoning in first order knowledge base kb however this model is deficient in that it doe not allow for quantifying in which is needed to distinguish between knowing that and knowing who in this paper we extend our earlier work by adding quantifying in and equality to a model of limited belief that integrates idea from possible world semantics and relevance logic 
much of the theoretical research on nonmonotonic inheritance ha concentrated on formalism involving only is a link between primitive node however it is hard to imagine a useful network representation of commonsense or expert knowledge that would not involve node representing negative conjunctive or disjunctive property certain node of this kind were included in some of the earliest formalism for defeasible inheritance but were omitted in later work either to secure tractability or to simplify the task of theoretical analysis the purpose of the present paper is to extend the theoretical analysis of defeasible inheritance to network incorporating these expressive enhancement 
recent research in real time artificial intelligence ha focussed upon the design of situated agent and in particular how to achieve effective and robust behaviour with limited computational resource a range of architecture and design principle ha been proposed to solve this problem this ha led to the development of simulated world that can serve a testbeds in which the effectiveness of different agent can be evaluated we report here an experimental program that aimed to investigate how commitment to goal contributes to effective behaviour and to compare the property of different strategy for reacting to change our result demonstrate the feasibility of developing system for empirical measurement of agent performance that are stable sensitive and capable of revealing the effect of high level agent characteristic such a commitment 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
we propose a representation of concurrent action rather than invent a new formalism we model them within the standard situation calculus by introducing the notion of global action and primitive action whose relationship is analogous to that between situation and fluents the result is a framework in which situation and action play quite symmetric role the rich structure of action give rise to a new problem which due to this symmetry between action and situation is analogous to the traditional frame problem in lin and shoham we provided a solution to the frame problem based on a formal adequacy criterion called epistemological completeness here we show how to solve the new problem based on the same adequacy criterion 
strategic planner for robot designed to operate in a dynamic environment must be able to decide i how often a sensory request should be granted and ii how to recover from a detected error this paper derives closed form formula for the appropriate frequency of sensor integration a a function of parameter of the equipment the domain and the type of error from which the system wish to recover 
this paper describes how a competitive tree learning algorithm can be derived from first principle the algorithm approximates the bayesian decision theoretic solution to the learning task comparative experiment with the algorithm and the several mature ai and statistical family of tree learning algorithm currently in use show the derived bayesian algorithm is consistently a good or better although sometimes at computational cost using the same strategy we can design algorithm for many other supervised and model learning task given just a probabilistic representation for the kind of knowledge to be learned a an illustration a second learning algorithm is derived for learning bayesian network from data implication to incremental learning and the use of multiple model are also discussed 
this paper provides a quantitative analysis of domain structure and it effect on the complexity of diagnostic problem solving it introduces a hypothesis about the modular structure of domain and proposes a measured called explanatory power the distribution of explanatory power reveals the inherent structure of domain we conjecture that such structure might facilitate problem solving even when the problem solver doe not exploit it explicitly to test this hypothesis we create a domain without structure by randomizing the distribution of explanatory power we use the structured and randomized knowledge base to study the effect of domain structure on two diagnostic algorithm candidate generation and symptom clustering the result indicate that inherent domain structure even when not encoded explicitly can facilitate problem solving such facilitation occurs for both the candidate generation and symptom clustering algorithm moreover domain structure appears to benefit symptom clustering more than candidate generation suggesting that the efficiency of symptom clustering derives in part from exploiting domain structure 
the goal of this paper is to present a theorem prover where the underlying code ha been written to behave a the procedural metalevel of the object logic we have then defined a logical declarative metatheory mt which can be put in a one to one relation with the code and automatically generated from it mt is proved correct and complete in the sense that for any object level deduction the wff representing it is a theorem of mt and viceversa such theorem can be translated back in the underlying code this open up the possibility of deriving control strategy automatically by metatheoretic theorem proving of mapping them into the code and thus of extending and modifying the system itself this seems a first step towards really self reflective system it system able to reason deductively about and modify their underlying computation mechanism we show that the usual logical reflection rule so called reflection up and down are derived inference rule of the system 
few text longer than a paragraph are written without appropriate formatting to ensure readability automated text generation program must not only plan and generate their text but be able to format them a well we describe how work on the automated planning of multisentence text and on the display of information in a multimedia system led to the insight that text formatting device such a footnote italicized region enumeration etc can be planned automatically by a text structure planning process this is achieved by recognizing that each formatting device fulfills a specific communicative function in a text and that such function can be defined in term of the text structure relation used a plan in a text planning system an example is presented in which a text is planned from a semantic representation to a final form that includes english sentence and latex formatting command intermingled a appropriate 
subtle difference in the method of constructing argument in inheritance system can result in profound difference in both the conclusion reached and the efficiency of inference this paper focus on issue surrounding the defeat of argument in nonmonotonic inheritance looking primarily at skeptical reasoner we analyze several type of defeat that may be encountered especially the defeat of defeaters finally we raise some question specific to network that mix strict and defeasible link 
in this paper we shall discus how to treat the automatic generation of assembly task specification a a constraint satisfaction problem csp over finite and infinite domain conceptually it is straightforward to formulate assembly planning in term of csp however the choice of constraint representation and of the order in which the constraint are applied is nontrivial if a computationally tractable system design is to be achieved this work investigates a subtle interaction between a pair of interleaving constraint namely the kinematic and the spatial occupancy constraint while finding one consistent solution to a general csp is np complete our work show how to reduce the combinatorics in problem arising in assembly using the symmetry of assembly component group theory being the standard mathematical theory of symmetry is used extensively in this work since both robot and assembly component are threedimensional rigid body whose feature have certain symmetry this form part of our high level robot assembly task planner in which geometric solid modelling group theory and csp are combined into one computationally effective framework 
we describe a method of automatically abducing qualitative model from description of behavior we generate from either quantitative or qualitative data model in the form of qualitative differential equation suitable for use by qsim constraint are generated and filtered both by comparison with the input behavior and by dimensional analysis if the user provides complete information on the input behavior and the dimension of the input variable the resulting model is unique maximally constrained and guaranteed to reproduce the input behavior if the user provides incomplete information our method will still generate a model which reproduces the input behavior but the model may no longer be unique incompleteness can take several form missing dimension value of variable or entire variable 
in this paper we present an average case analysisof the bayesian classifier a simple induction algorithmthat fare remarkably well on many learningtasks our analysis assumes a monotone conjunctivetarget concept and independent noise freeboolean attribute we calculate the probabilitythat the algorithm will induce an arbitrary pair ofconcept description and then use this to computethe probability of correct classification over the instancespace the analysis take into account 
most geometric model are quantitative making it difficult to abstract the underlying spatial information needed for task such a planning learning or vision furthermore the precision used in a typical quantitative system often exceeds the actual accuracy of the data in this work we describe a systematic representation that build spatial map based on local qualitative relation between object it derives relation that are more functionally relevant i e those that involve accidental alignment or can be described based on such alignment in one dimension interval logic allen provides a mechanism for representing these type of relation in this work we propose a formalism that enables u to perform alignment based reasoning in two and higher dimension with object at angle the principal advantage of this representation is that a it is free of subjective bias and b it is complete in the qualitative sense of distinguishing all overlap tangency nocontact geometry in addition the model is capable of handling uncertainty in the initial system e g the fuse box is somewhere behind the compressor by constructing bounded inference from disjunctive input data two kind of uncertainty can be handled those arising from deliberate imprecision in the interest of compactness down the road from or those caused by an inadequacy of data sensor spatial description or map 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
we present a theory of plan modification applicable to hierarchical nonlinear planning our theory utilizes the validation structure of the stored plan to yield a flexible and conservative plan modification framework the validation structure which constitutes a hierarchical explanation of correctness of the plan with respect to the planner s own knowledge of the domain is annotated on the plan a a by product of initial planning plan modification is characterized a a process of removing inconsistency in the validation structure of a plan when it is being reused in a new changed planning situation the repair of these inconsistency involves removing unnecessary part of the plan and adding new non primitive task to the plan to establish missing or failing validation the resultant partially reduced plan with a consistent validation structure is sent to the planner for complete reduction we discus the development of this theory in friar system and characterize it completeness coverage efficiency and limitation 
a connectionist unification algorithm is presented it utilizes the fact that the most general unifier of two term corresponds to a finest valid equivalence relation defined on a occurrence label representation of the unification problem the algorithm exploit the maximal parallelism inherent in the computation of such a finest valid equivalence relation while using only computational feature of connectionism it can easily be restricted to solve special form of the unification problem such a the word problem the matching problem or the unification problem over infinite tree 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
research in distributed ai ha led to computational technique for providing ai system with rudimentary social skill this paper give a brief survey of distributed ai describing the work that strives for social skill that a person might acquire in kindergarten and highlighting important unresolved problem facing the field 
the feasibility of derivational analogy a a mechanism for improving problem solving behavior ha been shown for a variety of problem domain by several researcher however most of the implemented system have been empirically evaluated in the restricted context of an already supplied base analog or on a few isolated example in this paper we address the utility of a derivational analogy based approach when the cost of retrie ving analog from a sizable case library and the cost of retrieving inappropriate analog is factored in 
we present a dynamic algorithm for map calculation the algorithm is based upon santos s technique santos b of transforming minimal cost proof problem into linear programming problem the algorithm is dynamic in the sense that it is able to use the result from an earlier near by problem to lessen it search time result are presented which clearly suggest that this is a powerful technique for dynamic abduction problem 
we describe a system called tileworld which consists of a simulated robot agent and a simulated environment which is both dynamic and unpredictable both the agent and the environment are highly parameterized enabling one to control certain characteristic of each we can thus experimentally investigate the behavior of various meta level reasoning strategy by tuning the parameter of the agent and can ass the success of alternative strategy in different environment by tuning the environmental parameter our hypothesis is that the appropriateness of a particular meta level reasoning strategy will depend in large part upon the characteristic of the environment in which the agent incorporating that strategy is situated we describe our initial experiment using tileworld in which we have been evaluating a version of the meta level reasoning strategy proposed in earlier work by one of the author bratman et al 
the ability to generalize remains one of the central issue of concept learning a general generalization algorithm the candidate elimination algorithmexists but practical application of this algorithm are still limited due to it low convergence the issue ha shifted to the design of a useful bias limiting the size of the version space this paper proposes a new kind of bias called empirical bias and a new general algorithm ice for generalization in presence of bias this proposition is founded on the concept of focus set which provides a very flexible way to express expectation or constraint on the space of generalization 
the problem of computing maximally specific generalization mscg s of relational description can be modelled a tree search we describe several transformation and pruning method for reducing the complexity of the problem based on this analysis we have implemented a search program x search for finding the mscg s experiment compare the separate and combined effect of pruning method on search efficiency with effective pruning method full width search appears feasible for moderately sized relational description 
this paper describes a natural language processing system developed for the semantic network array processor snap the goal of our work is to develop a scalable and high performance natural language processing system which utilizes the high degree of parallelism provided by the snap machine we have implemented an experimental machine translation system a a central part of a real time speech to speech dialogue translation system it is a snap version of the dmdiaiog speech to speech translation system memory based natural language processing and syntactic constraint network model ha been incorporated using parallel marker passing which is directly supported from hardware level experimental result demonstrate that the parsing of a sentence is done in the order of millisecond 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
this paper discus the logic lkm which extends circumscription into an epistemic domain this extension will allow u to define circumscription of predicate that appear within the context of a modal operator in fact lkm can be seen a a method of extending any first order nonmonotonic logic whose semantic definition is based on a partial order among model into a new nonmonotonic logic defined for a modal language whose modal operator k follows an undedying s or weak s semantics one interesting use of this nonmonotonic logic is to model nonmonotonic aspect of the communication between agent 
the purpose of this paper are threefold the first is to provide a crisp formalization of abstrips style abstraction since the lack of such formalization ha made it difficult to ascertain the us and value of this type of abstraction in previous research second we define the refinement relationship between solution at different level of the abstraction hierarchy such definition are crucial to developing efficient search strategy with this type of hierarchical planning and third we provide a restriction on the abstraction mapping that provides a criterion for generating useful abstraction 
in this paper we show how to use common knowledge computationally in solving problem involving cooperation of multiple agent when common knowledge is available we will explain why a procedural approach to common knowledge is better suited to solving multiple agent problem than a static one we show even if one can never prove that common knowledge ha been attained halpern and moses that assuming it ha been attained is often safe and efficacious the ability to detect fairly reliably when certain condition are not met suffices a a guideline for when to assume something is common knowledge in principle the problem of when one ha individual knowledge is about a difficult we use the situation oriented programming language prosit by combining reasoning about situation and in situation prosit make possible an especially intuitive and simple solution of hypothetical reasoning problem involving common knowledge nakashima and tutiya 
many classification algorithm require that the training data contain only discrete attribute to use such an algorithm when there are numeric attribute all numeric value must first be converted into discrete value a process called discretization this paper describes chimerge a general robust algorithm that us the statistic to discretize quantize numeric attribute 
verification method and tool developed so far have assumed a very simple model of rule based expert system rbes current rbes often do not comply this model and require more sophisticated verification technique a rbes model including uncertainty and control ha been used to analyze four verification issue inconsistency redundancy circularity and useless rb object identifying a number of new verification problem the concept of label and environment dekleer have been extended to incorporate uncertainty and control information obtaining the construct extended label and extended environment they have been used to express and solve these new verification problem 
we study concept language also called terminological language a mean for both defining a knowledge base and expressing query in particular we investigate on the possibility of using two different concept language one for asserting fact about individual object and the other for querying a set of such assertion contrary to many negative result on the complexity of terminological reasoning our work show that provided that a limited language is used for the assertion it is possible to employ a richer query language while keeping the reasoning process tractable we also show that on the other hand there are construct that make query answering inherently intractable 
one obstacle to wider use of inductive learning algorithm in problem solving system is the sensitivity of the algorithm to the way in which example of the concept are represented human normally decide how the example will be represented so success in incorporating inductive learning algorithm varies from person to person constructive induction reduces but doe not eliminate this sensitivity an ideal solution would eliminate the need for any human intervention in determining how a problem solving system and an inductive learning algorithm are integrated this paper show how a problem solver can use it domain knowledge to automatically create a representation of example that is adequate for learning search control knowledge the resulting representation describes the example in term of how and how well they satisfy the problemsolver s goal experimental evidence from two domain is presented to support the claim that this approach is generally useful 
text processing for complex domain such a terrorism is complicated by the difficulty of being able to reliably distinguish relevant and irrelevant text we have discovered a simple and effective filter the relevancy signature algorithm and demonstrated it performance in the domain of terrorist event description the relevancy signature algorithm is based on the natural language processing technique of selective concept extraction and relies on text representation that reflect predictable pattern of linguistic context this paper describes text classification experiment conducted in the domain of terrorism using the muc text corpus a customized dictionary of about word provides the lexical knowledge base needed to discriminate relevant text and the circus sentence analyzer generates relevancy signature a an effortless side effect of it normal sentence analysis although we suspect that the training base available to u from the muc corpus may not be large enough to provide optimal training we were nevertheless able to attain relevancy discrimination for significant level of recall ranging from to with precision in half of our test run 
causal theory are default theory which explicitly accommodate a distinction between explained and unexplained proposition this is accomplished by mean of an operator c in the language for which proposition are assumed explained when literal of the form c hold the behavior of causal theory is determined by a preference relation on model based on the minimization of unexplained abnormality we show that causal network general logic program and theory for reasoning about change can be all naturally expressed a causal theory we also develop a proof theory for causal theory and discus how they relate to autoepistemic theory prioritized circumscription and pearl s c e calculus 
the key concept of autoepistemic logic introduced by moore is a stable expansion of a set of premise i e a set of belief adopted by an agent with perfect introspection capability on the basis of the premise moore s formalization of a stable expansion however is nonconstructive and produce set of belief which are quite weakly grounded in the premise a new more constructive definition of the set of belief of the agent is proposed it is based on classical logic and enumeration of formula considering only a certain subclass of enumeration l hierarchic enumeration an attractive class of expansion is captured to characterize the set of belief of a fully introspective agent these l hierarchic expansion are stable set minimal very tightly grounded in the premise and independent of the syntactic representation of premise furthermore reiter s default logic is shown to be a special case of autoepistemic logic based on l hierarchic expansion 
knowledge processing is very demanding on computer architecture knowledge processing generates subcomputation path at an exponential rate it is memory intensive and ha high communication requirement marker passing architecture are good candidate to solve knowledge processing problem in this paper we justify the design decision made for the semantic network array processor snap important aspect of snap are the instruction set marker relation propagation rule interconnection network and granularity these feature are compared to those in netl and the connection machine 
the instantaneous image motion field due to a camera moving through a static environment encodes information about ego motion and environmental layout for pure translational motion the motion field ha a unique point termed focus of expansion contraction where the image velocity vanishes we reveal the fact that for an arbitrary d motion the zero velocity point whose number can be large have the regularity of being approximately cocircular more generally all the image point with the same velocity u are located approximately on a circle termed the isovelocity circle ivc determined solely by u and the ego motion except for the pathological case in which the circle degenerate into a straight line while ivcs can be recovered from or more pair of iso velocity point in the motion field using a linear method estimating ego motion reduces to solving system of linear equation constraining iso velocity point pair yang 
korf present a method for learning macro operator and show that the method is applicable to serially decomposable problem in this paper i analyze the computational complexity of serial decomposability assuming that operator take polynomial time it is np complete to determine if an operator or set of operator is not serially decomposable whether or not an ordering of state variable is given in addition to serial decomposability of operator a serially decomposable problem requires that the set of solvable state is closed under the operator it is pspace complete to determine if a given finite state variable problem is serially decomposable in fact every solvable instance of a pspace problem can be converted to a serially decomposable problem furthermore given a bound on the size of the input every problem in pspace can be transformed to a problem that is nearly serially decomposable i e the problem is serially decomposable except for closure of solvable state or a unique goal state 
reasoning about causality is an interesting application area of formal nonmonotonic theory here we focus our attention on a certain aspect of causal reasoning namely causal asymmetry in order to provide a qualitative account of causal asymmetry we present a justification based approach that us circumscription to obtain the minimality of cause we define the notion of causal and evidential support in term of a justification change with respect to a circumscriptive theory and show how the definition provides desirable interaction between causal and evidential support 
this work investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class of control knowledge are defined that use information about the relationship between system goal to schedule task this control knowledge is implemented using bb style control heuristic the usefulness of the heuristic is demonstrated by comparing the effectiveness of problem solving with and without the heuristic problem solving with the new control knowledge resulted in increased processor utilization and decreased total execution time 
a model of the elementary particle of a domain and their rudimentary interaction is essential for sophisticated reasoning about the macroscopic behavior of physical system a microscopic theory can make explicit the deeper mechanism underlying causal model collapse a great variety of macroscopic phenomenon into a few rudimentary interaction elaborate upon or validate macroscopic explanation and so forth this paper describes a qualitative representation for microscopic theory and a method for reasoning with microscopic particle to obtain the macroscopic behavior the representation and reasoning are illustrated using implemented example from the fluid domain 
conventional method for the parametric design of engineering structure rely on the iterative re use of analysis program in order to converge on a satisfactory solution since finite element and other analysis program require considerable computer resource this research proposes a general method to minimize their use by utilizing constraint based reasoning to carry out redesign a problem solver consisting of constraint network which express basic relationship between individual design parameter and variable is attached to the analysis program once an initial design description ha been set out using the conventional analysis program the network can then reason about required adjustment in order to find a consistent set of parameter value we describe how global constraint representing standard design behavioral equation are decomposed to form binary constraint network the network use approximate reasoning to determine dependency between key parameter and after an adjustment ha been made use exact relationship information to update only those part of the design description that are affected by the adjustment we illustrate the idea by taking a an example the design of a continuous prestressed concrete beam 
a system called skordos ha been implemented for model based diagnosis of analog circuit one of the difficulty of model based diagnosis for analog circuit is managing t he tremendous numher of prediction which may he generated by a constraint propagation system fortunately not all of those prediction are valuable for the diagnostic process a process called hibernation which is used in skordos to prevent generation of useless prediction is introduced and described here another technique is introduced and described whcih further assist in controlling the inequality reasoning by exploiting hibernation this technique involves changing the structure in which value are combined it us hibernation a an early filter to reduce the number of interaction resulting from kirchhoff s current law from exponential to quadratic in the number of interacting variable 
an analysis of the property of qualitative differential equation involving feedback structure is presented the topological interpretation of this theory serf a the basis for a simulator of qualitative differential equation quaf quaf predicts the initial trend and final state of each variable thereby elucidating the general character of the response the approach requires that causal differential equation replace algebraic form derived from pseudo steady state moving equilibrium assumption quaf is compared to the qualitative simulator qsim on an example involving interconnected tank and a significant narrowing of the number of interpretation of system behavior is observed 
it is known that perceptual aliasing may significantly diminish the effectiveness of reinforcement learning algorithm whitehead and ballard perceptual aliasing occurs when multiple situation that are indistinguishable from immediate perceptual input require different response from the system for example if a robot can only see forward yet the presence of a battery charger behind it determines whether or not it should backup immediate perception alone is insufficient for determining the most appropriate action it is problematic since reinforcement algorithm typically learn a control policy from immediate perceptual input to the optimal choice of action this paper introduces the predictive distinction approach to compensate for perceptual aliasing caused from incomplete perception of the world an additional component a predictive model is utilized to track aspect of the world that may not be visible at all time in addition to the control policy the model must also be learned and to allow for stochastic action and noisy perception a probabilistic model is learned from experience in the process the system must discover on it own the important distinction in the world experimental result are given for a simple simulated domain and additional issue are discussed 
in order to be taken seriously connectionist natural language processing system must be able to parse syntactically complex sentence current connectionist parser either ignore structure or impose prior restriction on the structural complexity of the sentence they can process either number of phrase or the depth of the sentence structure xeric network presented here are distributed representation connectionist parser which can analyze and represent syntactically varied sentence including one with recursive phrase structure construct no a priori limit are placed on the depth or length of sentence by the architecture xeric network use recurrent network to read word one at a time raam style reduced description and x bar grammar are used to make an economical syntactic representation scheme this is combined with a training technique which allows xeric to use multiple virtual copy of it raam decoder network to learn to parse and represent sentence structure using gradient descent method xeric network also perform number person disambiguation and lexical disambiguation result show that the network train to a few percent error for sentence up to a phrase nesting depth of ten or more and that this performance generalizes well 
we report result from large scale experiment in satisfiability testing a ha been observed by others testing the satisfiability of random formula often appears surprisingly easy here we show that by using the right distribution of instance and appropriate parameter value it is possible to generate random formula that are hard that is for which satisfiability testing is quite difficult our result provide a benchmark for the evaluation of satisfiability testing procedure 
closed world reasoning is a common nonmonotonic technique that allows for dealing with negative information in knowledge and data base we present a detailed analysis of the computational complexity of the different form of closed world reasoning for various fragment of propositional logic the analysis allows u to draw a complete picture of the tractability intractability frontier for such a form of nonmonotonic reasoning we also discus how to use our result in order to characterize the computational complexity of other problem related to nonmonotonic inheritance diagnosis and default reasoning 
we present a comparison of three well known heuristic search algorithm best first search bfs iterative deepening id and depth first branch and bound dfbb we develop a model to analyze the time and space complexity of these three algorithm in term of the heuristic branching factor and solution density our analysis identifies the type of problem on which each of the search algorithm performs better than the other two these analytical result are validated through experiment on different problem we also present a new algorithm dfs which is a hybrid of iterative deepening and depth first branch and bound and show that it outperforms the other three algorithm on some problem 
surface discontinuity are detected in a sequence of image by exploiting physical constraint at early stage in the processing of visual motion to achieve accurate early discontinuity detection we exploit five physical constraint on the presence of discontinuity i the shape of the sum of squared difference ssd error surface in the presence of surface discontinuity ii the change in the shape of the ssd surface due to relative surface motion iii distribution of optic flow in a neighborhood of a discontinuity iv spatial consistency of discontinuity v temporal consistency of discontinuity the constraint are described and experimental result on sequence of real and synthetic image are presented the work ha application in the recovery of environmental structure from motion and in the generation of dense optic flow field 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
we argue for hyper logicism the view hitherto unarticulated that ai can succeed in creating a genuine robot agent by building a symbol system of the appropriate sort which ha no sub symbolic interaction whatsoever with the external world 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
interval and point algebra have been proposed for representing qualitativetemporal information about the relationshipsbetween pair of intervalsand pairsof point respectively in thispaper we address two relatedreasoningtasksthat arisein these algebra given possibly indefinite knowledge of the relationshipsbetween some intervalsor point findone or more scenariosthat are consistentwith the information provided and find allthe feasiblerelationsbetween every pair of intervalsor point solutionsto these problem have application in natural language processing planning and a knowledge representationlanguage we definecomputationallyefficientprocedures forsolvingthese tasksforthe pointalgebraand forum correspondingsubset of the intervalalgebra our algorithm are marked improvement over the previouslyknown algorithm we alsoshow how the resultsfor the point algebrahelp u to design a backtracking algorithmforthe full intervalalgebrathat isusefulinpractice 
a car like indoor mobile robot is a kinematically constrained robot that can be modelled a a d object translating and rotating in the horizontal plane among well defined obstacle the kinematic constraint impose that the linear velocity of the robot point along it main axis no sidewise motion is possible and restrict the range of admissible value for the steering angle in this paperl we describe a fast path planner for such a robot this planner is one to two order of magnitude faster than previously implemented planner for the same type of robot in addition it ha an anytime flavor that allows it to return a path in a short amount of time and to improve that path through iterative optimization according to the amount of time that is devoted to path planning the planner is essentially a combination of preexisting idea it efficiency derives from the good match between these idea and from various technical improvement brought to them 
in we defined the concept of agent oriented programming aop which can be viewed a a specialization of object oriented programming oop aop view object a agent with mental state and in the spirit of speech act theory identifies a number of message type informing requesting offering and so on aop is a general framework in this paper we present a specific and simple language called agento we define it syntax present it interpreter and illustrate both through an example 
since knowledge base kb are usually incomplete they should be able to provide information regarding their own incompleteness which requires them to introspect on what they know and do not know an important area of research is to devise model of introspective reasoning that take into account resource limitation under the view that a kb is completely characterized by the set of belief it represents it epistemic state it seems natural to model kb in term of belief reasoning can then be understood a the problem of computing membership in the epistemic state of a kb the best understood model of belief are based on possible world semantics however their computational property are unacceptable in particular they render reasoning in firstorder kb undecidable in this paper we propose a novel model of belief which preserve many of the advantage of possible world semantics yet at the same time guarantee reasoning to be decidable where a kb may contain sentence in full first order logic moreover such kb have perfect knowledge about their own belief even though their belief about the world are limited 
current explanation based generalization ebg technique can perform badly when the problem being solved involves recursion often an infinite series of learned concept are generated that correspond to the expansion of recursive solution over every finite depth previous attempt to address the problem such a shavlik s generalization to n ebg method are overly reluctant to expand recursion this reluctance can lead to inefficient rule in this paper ebg is viewed a a program transformation technique on logic program within that framework an improved operationality criterion for controlling the expansion of recursion is presented this criterion prevents certain infinite and combinatorially explosive rule class from being generated yet permit expansion in some useful circumstance allowing more efficient rule to be learned 
this paper present a process model of plan inference for use in natural language consultation system it includes a strategy that can both defer unwarranted decision about the relationship of a new action to the user s overall plan and sanction rational default inference the paper describes an implementation of this strategy using the dempster shafer theory of evidential reasoning our process model overcomes a limitation of previous plan recognition system and produce a richer model of the user s plan and goal yet one that can be explained and justified to the user when discrepancy arise between it and what the user is actually trying to accomplish 
there have been many recent attempt to incorporate default into unification based grammar formalism what these attempt have in common is that they all lose one of the most desirable property of feature system namely presentation order independence this paper describes a method of dealing with default that retains order independence the method work by making a strong distinction between strict and default information the addition of nonmonotonic sort allows default information to be carried in the feature structure while retaining a simple deterministic unification operation monotonic feature structure are rederived through a satisfaction relation that is abstract in that it depends only on the ordering information for sort 
three key component of an autonomous intelligent system are planning execution and learning this paper describes how the soar architecture support planning execution and learning in unpredictable and dynamic environment the tight integration of these component provides reactive execution hierarchical execution interruption on demand planning and the conversion of deliberate planning to reaction these capability are demonstrated on two robotic system controlled by soar one using a puma robot arm and an overhead camera the second using a small mobile robot with an arm 
this paper considers the interpretation a a three dimensional velocity field of the changing intensity pattern induced by a smoothly deforming lambertian surface of uniform albedo illuminated by a distant point light source the requisite intensity rate constraint which is derived contains no term relating to the tangential component of surface velocity so the determination of the velocity field is ill posed exhibiting a form of aperture problem a stretch based regulariser is applied to enable estimation of the velocity field and test with synthetic data show a requirement for high accuracy 
many researcher believe that certain aspect of natural language processing such a word sense disambiguation and plan recognition in story constitute abductive inference we have been working with a specific model of abduction called parsimonious covering applied in diagnostic problem solving word sense disambiguation and logical form generation in some restricted setting diagnostic parsimonious covering ha been extended into a dual route model to account for syntactic and semantic aspect of natural language the two route of covering are integrated by defining open class linguistic concept aiding each other the diagnostic model ha dealt with set while the extended version where syntactic consideration dictate word order deal with sequence of linguistic concept here we briefly describe the original model and the extended version and briefly characterize the notion of covering and different criterion of parsimony finally we examine the question of whether parsimonious covering can serve a a general framework for parsing 
a qualitative reasoning planner for determining robot control parameter to drive manipulation action ha been developed integrated into a telerobot system and demonstrated for a match striking task the planner consists of a qualitative reasoner and a numerical execution history which interact to jointly direct and narrow the search for reliable numerical control parameter value the planner algorithm implementation and an execution example are described the relationship to previous qualitative reasoning work is also discussed 
reasoning about one s personal schedule of appointment is a common but surprisingly complex activity motivated by the novel application of planning and temporal reasoning technique to this problem we have extended the formalization of the temporal distance model of dechter meiri and pearl we have developed method for using date a reference interval and for meeting the challenge of repeated activity such a weekly recurring appointment 
causal reasoning is an essential part of a number of task that have been central to many endeavour in ai notably planning and prediction diagnosis and explanation recently it ha become an object of study in it own right drawing inspiration from the work of philosopher and logician a well a more immediately ai oriented concern in this paper i shall examine just one approach to causal reasoning that advocated by yoav shoham in a recent book and article in particular i shall try to lay bare a number of assumption underlying shoham s work all of which i shall call into question key assumption are that causality is an epistemic notion that causal reasoning is inherently non monotonic and that epistemic reasoning should be handled by mean of modal logic while arguing against these assumption i do not offer a specific causal theory of my own but shall conclude with some suggestion a to the general line which i feel such a theory ought to follow 
one usually writes a i program to be used on a range of example which although similar in kind differ in detail this paper show how to predict where in a space of problem instance the hardest problem are to be found and where the fluctuation in difficulty are greatest our key insight is to shift emphasis from modelling sophisticated algorithm directly to modelling a search space which capture their principal effect this allows u to analyze complex a i problem in a simple and intuitive way we present a sample analysis compare our model s quantitative prediction with data obtained independently and describe how to exploit the result to estimate the value of preprocessing finally we circumscribe the kind problem to which the methodology is suited 
investigating the character of scientific discovery using computational model is a growing area in artificial intelligence and cognitive science scientific discovery involves both theory and experiment but existing discovery system have mainly considered the formation and modification of theory this paper focus on the modelling of experiment a general characterization of the nature of experiment is given and more specifically galileo s motion experiment are examined the stern scientific discovery system ha been used to model galileo s investigation of free fall and is introduced here the system ha an extensive representation for experiment and us experiment to i confirm existing hypothesis ii find new hypothesis ii enhance it own performance and iv make intractable hypothesis tractable 
this paper describes a cognitively plausible mechanism for systematically handling complex syntactic construction within a semantic parser more specifically we show how these construction are handled without a global syntactic grammar or syntactic parse tree representation and without sacrificing the benefit of semantically oriented parsing we evaluate the psychological validity of our architecture and conclude that it is a plausible computational model of human processing for an important class of embedded clause construction a a result we achieve robust sentence processing capability not found in other parser of it class 
in many domain of interest to distributed artificial intelligence the problem solving environment may be viewed a a collection of loosely coupled intelligent agent each of which reason based on it own incomplete knowledge of the state of the world no agent ha sufficient knowledge to solve the problem at hand so that coordinated cooperative problem solving is required to satisfy system goal in this paper we present dare a distributed reasoning system in which agent have the ability to focus their attention on selective information interchange to facilitate cooperative problem solving the experimental result we present demonstrate that agent in a loosely coupled network of problem solver can work semi independently yet focus their attention with the aid of relatively simple heuristic when cooperation is appropriate these result suggest that we have developed an effective cooperation strategy which is largely independent of initial knowledge distribution 
in this paper we show how a natural language system can learn to find the antecedent of relative pronoun we use a well known conceptual clustering system to create a case based memory that predicts the antecedent of a wh word given a description of the clause that precedes it our automated approach duplicate the performance of hand coded rule in addition it requires only minimal syntactic parsing capability and a very general semantic feature set for describing noun human intervention is needed only during the training phase thus it is possible to compile relative pronoun disambiguation heuristic tuned to the syntactic and semantic preference of a new domain with relative ease moreover we believe that the technique provides a general approach for the automated acquisition of additional disambiguation heuristic for natural language system especially for problem that require the assimilation of syntactic and semantic knowledge 
one of the most common modification made to the standard strip action representation is the inclusion of filter condition a key function of such filter condition is to distinguish between operator that represent different context dependent effect for the same action we consider how filter condition may be used to provide this functionality in a complete and correct partial order planner we conclude that they are not effective and that in general the use of filter condition is incompatible with the basic assumption that lie behind partial order planning we present an alternative mechanism using the secondary precondition of pednault to represent context dependent effect the use of secondary precondition is effective and preserve completeness and correctness 
there is a need for highly redundant manipulator to work in complex cluttered environment our goal is to plan path for such manipulator efficiently the path planning problem ha been shown to be psp ace complete in term of the number of degree of freedom dof of the manipulator we present a method which overcomes the complexity with a strong heuristic utilizing redundancy by mean of a continuous manipulator model the continuous model allows u to change the complexity of the problem from a function of both the dof of the manipulator believed to be exponential and the complexity of the environment polynomial to a polynomial function of the complexity of the environment only 
decision tree are widely used in machine learning and knowledge acquisition system however there is no optimal or even unanimously accepted strategy of obtaining good such tree and most of the generated tree suffer from impropriety i e inadequacy in representing knowledge the final goal of the research reported here is to formulate a theory for the decision tree domain that is a set of heuristic on which a majority of expert will agree which will describe a good decision tree a well a a set of heuristic specifying how to obtain optimal tree in order to achieve this goal we have designed a recursive architecture learning system which monitor an interactive knowledge acquisition system based on decision tree and driven by explanatory reasoning and incrementally acquires from the expert using it the knowledge used to build the decision tree domain theory this theory is also represented a a set of decision tree and may be domain dependent our system acquires knowledge to define the notion of good bad decision tree and to measure their quality a well a knowledge needed to guide domain expert in constructing good decision tree the partial theory acquired at each moment is also used by the basic knowledge acquisition system in it tree generation process thus constantly improving it performance 
this paper present some complexity result for deductive recognition in the framework of language such a kl one in particular it focus on classification operation that are usually performed in these language through subsumption computation the paper present a simple language that encompasses and extends earlier kl one based recognition framework by relying on parsing algorithm the paper show that a significant class of recognition problem in this language can be performed in polynomial time this is in marked contrast to the exponentiality and undecidability result that have recently been obtained for subsumption in even some of the most restricted variant of kl one 
in this paper we examine the relationship between belief goal and intention in particular we consider the formalization of the asymmetry thesis a proposed by bratman we argue that the semantic characterization of this principle determines if the resulting logic is capable of handling other important problem such a the side effect problem of beliefgoal intention interaction while cohen and levesque s formalization faithfully model some aspect of the asymmetry thesis it doe not solve all the side effect problem on the other hand the formalization provided by rao and georgeff solves all the side effect problem but only model a weak form of the asymmetry thesis in this paper we combine the intuition behind both these approach and provide a semantic account of the asymmetry thesis in both linear time and branching time logic for solving many of these problem 
this paper present a general mathematically rigorous approach to nonlinear planning that handle both complex goal and action with context dependent effect a goal can be any arbitrary well formed formula containing conjunction disjunction negation and quantifier action are likewise not constrained and can have an unrestricted number of complex situation dependent effect the approach presented here can thus be used to solve a wider range of problem than previous approach to nonlinear planning the approach is based on previous work by the author on linear planning the same mathematical framework is used with the result extended to nonlinear plan 
this paper describes the localized search mechanism of the gemplan multiagent planner both formal complexity result and empirical result are provided demonstrating the benefit of localized search a localized domain description is one that decomposes domain activity and requirement into a set of region this description is used to infer how domain requirement are semantically localized and a a result to enable the decomposition of the planning search space into a set of space one for each domain region benefit of localization include a smaller and cheaper overall search space a well a heuristic guidance in controlling search such benefit are critical if current planning technology and other type of reasoning are to be scaled up to large complex domain 
this paper present an explanation based learning strategy for learning general plan for use in an integrated approach to planning the integrated approach augments a classical planner with the ability to defer achievable goal thus preserving the construction of provably correct plan while gaining the ability to utilize runtime information in planning proving achievability is shown to be possible without having to determine the action to achieve the associated goal a learning strategy called contingent explanation based learning us conjectured variable to represent the eventual value of plan parameter with unknown value a priori and completers to determine these value during execution an implemented system demonstrates the use of contingent ebl in learning a general completable reactive plan for spaceship acceleration 
this paper extends yeap s computational theory of cognitive map focusing on the problem of computing a raw cognitive map by an autonomous agent in addition to having a view of the environment a input the agent also maintains a representation of her immediate surroundings this representation is referred to a an mfis a memory for one s immediate surroundings argument for the use of the mfis are presented the main question that we ask in implementing our idea are i what frame of reference is appropriate for the mfis and ii how doe the mfis change a the agent move through the environment a program ha been implemented successfully and the main algorithm used and the result of running the program are presented 
we investigate the utility of explanation based learning in recursive domain theory and examine the cost of using macro rule in these theory the compilation option in a recursive domain theory range from constructing partial unwindings of the recursive rule to converting recursive rule into iterative one we compare these option against using appropriately ordered rule in the original domain theory and demonstrate that unless we make very strong assumption about the nature of the distribution of future problem it is not profitable to form recursive macro rule via explanation based learning in these domain 
most previous work in analytic generalization of plan dealt with totally ordered plan these method cannot be directly applied to generalizing partially ordered plan since they do not capture all interaction among plan operator for all total order of such plan in this paper we introduce a new method for generalizing partially ordered plan this method is based on providing ebg with explanation which systematically capture the interaction among plan operator for all the total order of a partially ordered plan the explanation are based on the modal truth criterion which state the necessary and sufficient condition for ensuring the truth of a proposition at any point in a plan for a class of partially ordered plan the generalization obtained by this method guarantee successful and ineraction free execution of any total order of the generalized plan in addition the systematic derivation of the generalization algorithm from the modal truth criterion obviates the need for carrying out a separate formal proof of correctness of the ebg algorithm 
thematic analysis is best manifested by contrasting collocation such a shipping pacemaker v shipping department while in the first pair the pacemaker are being shipped in the second one the department are probably engaged in some shipping activity but are not being shipped text pre processor intended to inject corpus based intuition into the parsing process must adequately distinguish between such case although statistical tagging church et al meteer et al brill cutting et al ha attained impressive result overall the analysis of multiple contentword string i e collocation ha presented a weakness and caused accuracy degradation to provide acceptable coverage i e of collocation a tagger must have accessible a large database i e pair of individually analyzed collocation consequently training must be based on a corpus ranging well over million word since such a large corpus doe not exist in a tagged form training must be from raw corpus in this paper we present an algorithm for text tagging based on thematic analysis the algorithm yield high accuracy result we provide empirical result the program nlcp nl corpus processing acquired a thematic relation database through the million word wall street journal corpus it wa tested over the tipster word joint venture corpus 
the approach to database query evaluation developed by levesque and reiter treat database a first order theory and query a formula of the language which includes in addition to the language of the database an epistemic modal operator in this epistemic query language one can express question not only about the external world described by the database but also about the database itself about what the database know on the other hand epistemic formula are used in knowledge representation for the purpose of expressing default autoepistemic logic is the best known epistemic nonmonotonic formalism the logic of grounded knowledge proposed recently by lin and shoham is another such system this paper brings these two direction of research together we describe a new version of the lin shoham logic similar in spirit to the levesque reiter theory of epistemic query using this formalism we can give meaning to epistemic query in the context of nonmonotonic database including logic program with negation a failure 
the ease of learning concept from example in empirical machine learning depends on the attribute used for describing the training data we show that decision tree based feature construction can be used to improve the performance of back propagation bp an artificial neural network algorithm both in term of the convergence speed and the number of epoch taken by the bp algorithm to converge we use disjunctive concept to illustrate feature construction and describe a measure of feature quality and concept difficulty we show that a reduction in the difficulty of the concept to be learned by constructing better representation increase the performance of bp considerably 
we previously proposed the moving target search mt algorithm where the location of the goal may change during the course of the search mt is the first search algorithm concerned with problem solving in a dynamically changing environment however since we constructed the algorithm with the minimum operation necessary for guaranteeing it completeness the algorithm a proposed is neither efficient nor intelligent in this paper we introduce innovative notion created in the area of resource bounded planning into the formal search algorithm mt our goal is to improve the efficiency of mt while retaining it completeness notion that are introduced are commitment to goal and deliberation for selecting plan evaluation result demonstrate that the intelligent mt is to time more efficient than the original mt in uncertain situation 
this paper show how a new approach in the use of ai technique ha been successfully used for the design of an effective it in the domain of diagnosis training the originality of this approach wa to take into account three complex problem simultaneously teaching diagnosis method to student giving the mean to the teacher of maintaining the system by themselves and providing a tool easy to insert in the context of university laboratory the architecture of the system is based on a distinct use of two kind of knowledge representation all the knowledge liable to modification is gathered within library under descriptive form easily maintained by the educational staff general diagnosis knowledge independent of hardware circuit and even application field is described with basic production rule and control metarule the development of the system wa based on the precise analysis of the expert s behaviour and of the user s need with the aim of making extensive use of the descriptive form in order to minimize the static knowledge embedded in the rule the system can work on a microcomputer and is used in an engineering school 
the human face is an object that is easily located in complex scene by infant and adult alike yet the development of an automated system to perform this task is extremely challenging this paper is concerned with the development of a computational model for locating human face in newspaper photograph based on cognitive research in human perceptual development in the process of learning to recognize object in the visual world one could assume that natural growth favor the development of the ability to detect the more essential feature first hence a study of the progress of an infant s visual ability can be used to categorize the potential feature in term of their importance the face locator developed by the author take a hypothesis generate and test approach to the task of finding the location of people s face in digitized picture information from the accompanying caption is used in the verification phase the system successfully located all face in of the test newspaper photograph 
this paper discus a radically new scheme of natural language processing called massively parallel memory based parsing most parsing scheme are rule based or principle based which involves extensive serial rule application thus it is a time consuming task which requires a few second or even a few minute to complete the parsing of one sentence also the degree of parallelism attained by mapping such a scheme on parallel computer is at most medium so that the existing scheme can not take advantage of massively parallel computing the massively parallel memory based parsing take a radical departure from the traditional view it view parsing a a memory intensive process which can be sped up by massively parallel computing although we know of some study in this direction we have seen no report regarding implementation strategy on actual massively parallel machine on performance or on practicality accessment based on actual data thus this paper focus on discussion of the feasibility and problem of the approach based on actual massively parallel implementation using real data the degree of parallelism attained in our model reach a few thousand and the performance of a few millisecond per sentence ha been accomplished in addition parsing time grows only linearly or sublincarly to the length of the input sentence the experimental result show the approach is promising for real time parsing and bulk text processing 
qualitative reasoner have been hamstrung by the inability to analyze large model this includes self explanatory simulator which tightly integrate qualitative and numerical model to provide both precision and explanatory power while they have important potential application in training instruction and conceptual design a critical step towards realizing this potential is the ability to build simulator for medium sized system i e on the order of ten to twenty independent parameter this paper describes a new method for developing self explanatory simulator which scale up while our method involves qualitative analysis it doe not rely on envisioning or any other form of qualitative simulation we describe the result of an implemented system which us this method and analyze it limitation and potential 
commitment to an ontological perspective is a primary aspect of reasoning about the physical world for complex analytic task the ability to switch between different ontology to represent the same target system can be critical supplementing the standard device ontology for electronic circuit we outline element of a charge carrier cc ontology for reasoning about electronics having two ontology extends our range of reasoning but raise the issue of how to control their application we propose a set of ontological choice rule to govern the process of ontological shift and demonstrate it effectiveness with example involving the two ontology in reasoning about electronic circuit 
in the paper we study a new and natural modal interpretation of default we show that under this interpretation there are whole family of modal nonmonotonic logic that accurately represent default reasoning one of these logic is used in a definition of possible world semantics for default logic this semantics yield a characterization of default extension similar to the characterization of stable expansion by mean of autoepistemic interpretation we also show that the disjunctive information can easily be handled if disjunction is represented by mean of modal disjunctive default modal formula that we use in our interpretation our result indicate that there is no single modal logic for describing default reasoning on the contrary there exist whole range of modal logic each of which can be used in the embedding a a host logic 
assumption based truth maintenance system have developed into powerful and popular mean for considering multiple context simultaneously during problem solving unfortunately increasing problem complexity can lead to explosive growth of node label in this paper we present a new atm algorithm catms which avoids the problem of label explosion while preserving most of the querytime efficiency resulting from label compilation catms generalizes the standard atm subsumption relation allowing it to compress an entire label into a single assumption this compression of label is balanced by an expansion of environment to include any implied assumption the result is a new dimension of flexibility allowing catms to trade off the query time efficiency of uncompressed label against the cost of computing them to demonstrate the significant computational gain of catms over de kleer s atm we compare the performance of the atm based qpe problem solver using each 
quantitative prediction are typically obtained by characterizing a system in term of algebraic relationship and then using these relationship to compute quantitative prediction from numerical data for real life system such a mainframe operating system an algebraic characterization is often difficult if not intractable this paper proposes a statistical approach to obtaining quantitative prediction from monotone relationship non parametric interpolative prediction for monotone function nimf nimf us monotone relationship to search historical data for bound that provide a desired level of statistical confidence we evaluate nimf by comparing it prediction to those of linear least square regression a widely used statistical technique that requires specifying algebraic relationship for memory contention in an ibm computer system our result suggest that using an accurate monotone relationship can produce better quantitative prediction than using an approximate algebraic relationship 
classical constraint system require that the set of variable whichexist in a problem be known ab initio however there are some applicationsin which the existence of certain variable is dependent on conditionswhose truth or falsity can only be determined dynamically in this paper we show how this conditional existence of variable can be handled in amathematically well founded fashion by viewing a constraint network asa set of sentence in free logic based on these idea we have 
semantic disambiguation is a difficult problem in natural language analysis a better strategy for semantic disambiguation is to accumulate constraint obtained during the analytical process of a sentence and disambiguate a early a possible the meaning incrementally using the constraint we propose such a computational model of natural language analysis and call it the incremental disambiguation model the semantic disambiguation process can be equated with the downward traversal of a discrimination network however the discrimination network ha a problem in that it cannot be traversed unless constraint are entered in an a priori fixed order in general the order in which constraint are obtained cannot be a priori fixed so it is not always possible to traverse the network downward during the analytical process in this paper we propose a method which can traverse the discrimination network according to the order in which constraint are obtained incrementally during the analytical process this order is independent of the a priori fixed order of the network 
in the functionally accurate cooperative fa c distributed problem solving paradigm agent exchange tentative and partial result in order to converge on correct solution the key question for fa c problem solving are how should cooperation among agent be structured and what capability are required in the agent to support the desired cooperation to date the fa c paradigm ha been explored with agent that did not have sophisticated evidential reasoning capability we have implemented a new framework in which agent maintain explicit representation of the reason why their hypothesis are uncertain and explicit representation of the state of the action being taken to meet their goal in this paper we will show that agent with more sophisticated model of their evidence and their problem solving state can support the complex dynamic interaction between agent that are necessary to fully implement the fa c paradigm our framework make it possible for agent to have directed dialogue among agent for distributed differential diagnosis make use of a variety of problem solving method in response to changing situation transmit information at different level of detail and drive local and global problem solving using the notion of the global consistency of local solution these capability have not been part of previous implementation of the fa c paradigm 
since linear resolution with clause ordering is incomplete for consequence finding it ha been used mainly for proof finding in this paper we re evaluate consequence finding firstly consequence finding is generalized to the problem in which only interesting clause having a certain property called characteristic clause should be found then we show how adding a skip rule to ordered linear resolution make it complete for consequence finding in this general sense compared with set of support resolution the proposed method generates fewer clause to find such a subset of consequence in the propositional case this is an elegant tool for computing the prime implicants implicates the importance of the result lie in their applicability to a wide class of ai problem including procedure for nonmonotonic and abductive reasoning and truth maintenance system 
a visual language is defined equivalent in expressive power to term subsumption language expressed in textual form to each knowledge representation primitive there corresponds a visual form expressing it concisely and completely the visual language and textual language are intertranslatable expression in the language are graph of labeled node and directed or undirected arc the node are labeled textually or iconically and their type are denoted by six different outline computer readable expression in the language may be created through a structure editor that ensures that syntactic constraint are obeyed the editor export knowledge structure to a knowledge representation server computing subsumption and recognition and maintaining a hybrid knowledge base of concept definition and individual assertion the server can respond to query graphically displaying the result in the visual language in editable form knowledge structure can be entered directly in the editor or imported from knowledge acquisition tool such a those supporting repertory grid elicitation and empirical induction knowledge structure can be exported to a range of knowledge based system 
when intelligent agent who have different knowledge and capability must work together they must communicate the right information to coordinate their action developing technique for deciding what to communicate however is problematic because it requires an agent to have a model of a message recipient and to infer the impact of a message on the recipient based on that model we have developed a method by which agent build recursive model of each other where the model are probabilistic and decision theoretic in this paper we show how an agent can compute the impact of a message in term of how it increase or decrease it expected utility by treating the imperfect communication channel probabilistically our method allows agent to account for risk in committing to nonintuitive course of action and to compute the utility of acknowledging message 
this paper concern the task of removing redundant information from a given knowledge base and restructuring it in the form of a tree so a to admit efficient problem solving routine we offer a novel approach which guarantee the removal of all redundancy that hide a tree structure we develop a polynomial time algorithm that given an arbitrary constraint network generates a precise tree representation whenever such a tree can be extracted from the input network otherwise the fact that no tree representation exists is acknowledged and the tree generated may serve a a good approximation to the original network 
in this paper we introduce an extension of the probably approximately correct pac learning model to study the problem of learning inclusion hierarchy of concept sometimes called is a hierarchy from random example using only the hypothesis representation output over many different run of a learning algorithm we wish to reconstruct the partial order with respect to generality among the different target concept used to train the algorithm we give an efficient algorithm for this problem with the property that each run is oblivious of all other run each run can take place in isolation without access to any example except those of the current target concept and without access to the current pool of hypothesis representation thus additional mechanism providing shared information between run are not necessary for the inference of some nontrivial hierarchy 
in order to control the motion of a mobile robot it is necessary to have accurate egomotion parameter in addition egomotion parameter are useful in determining environmental depth and structure we present a computationally inexpensive method that rapidly and robustly determines both the translational vector and rotational component of robot motion through the use of an active camera we employ gaze control consisting of two type of camera motion first the camera fixates on an item in the environment while measuring motion parallax based on the measured motion parallax the camera then rapidly saccade to a different fixation point the algorithm iteratively seek out fixation point that are closer to the translational direction of motion rapidly converging so that the active camera will always point in the instantaneous direction of motion at that point the tracking motion of the camera is equal but opposite in sign to the robot s rotational component of motion experiment are carried out both in simulation and in the real world giving result that are close to the actual motion parameter of the robot 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
the problem of finding a set of assumption which explain a given proposition is in general np hard even when the background theory is an acyclic horn theory in this paper it is shown that when the background theory is acyclic horn and it pseudo completion 
motivated by an anomaly in branch and bound bnb search we analyze it average case complexity we first delineate exponential v polynomial average case complexity of bnb when best first bnb is of linear complexity we show that depth first bnb ha polynomial complexity for problem on which best first bnb ha exponential complexity we obtain an expression for the heuristic branching factor next we apply our analysis to explain an anomaly in lookahead search on sliding tile puzzle and to predict the existence of an average case complexity transition of bnb on the asymmetric traveling salesman problem finally by formulating ida a costbounded bnb we show it average case optimality which also implies that rbfs is optimal on average 
reification of proposition expressing state event and property ha been widely advocated a a mean of handling temporal reasoning in ai the author proposes that such reification is both philosophically suspect and technically unnecessary the reified theory of allen and shoham are examined and it is shown how they can be unreified the resulting loss of expressive power can be rectified by adopting davidson s theory in which event token rather than event type are reified this procedure is illustrated by mean of kowalski and sergot s event calculus the additional type reification of the latter system being excised by mean of a general procedure proposed by the author for converting type reification into token reification some example are given to demonstrate the expressive power of the resulting theory 
we present a new approach to developing fast and efficient knowledge representation system previous approach to the problem of tractable inference have used restricted language or incomplete inference mechanism problem include lack of expressive power lack of inferential power and or lack of a formal characterization of what can and cannot be inferred to overcome these disadvantage we introduce a knowledge compilation method we allow the user to enter statement in a general unrestricted representation language which the system compiles into a restricted language that allows for efficient inference since an exact translation into a tractable form is often impossible the system search for the best approximation of the original information we will describe how the approximation can be used to speed up inference without giving up correctness or completeness we illustrate our method by studying the approximation of logical theory by horn theory following the formal definition of horn approximation we present anytime algorithm for generating such approximation we subsequently discus extension to other useful class of approximation 
abstract in the last year many logic of nonmonotonicity have been developedusing various different formalism and axiomatizations whichmakes them very difficult to compare we develop a classificationscheme for these logic using only a few simple concept and axiomsbased on conditional logic property of partial pre order of possibleworlds and centering assumption our framework the p system allows u to discus the similarity main difference and possible extensionsof these 
prime implicates have become a widely used tool in ai the prime implicates of a set of clause can be computed by repeatedly resolving pair of clause adding the resulting resolvent to the set and removing subsumed clause unfortunately this brute force approach performs far more resolution step than necessary tison provided a method to avoid many of the resolution step and kean and tsiknis developed an optimized incremental version unfortunately both these algorithm focus only on reducing the number of resolution step required to compute the prime implicates the actual running time of the algorithm depends critically on the number and expense of the subsumption check they require this paper describes a method based on a simplification of kean and tsiknis algorithm using an entirely different data structure to represent the data base of clause the new algorithm us it form of discrimination net called try to represent the clausal data base which produce an improvement in running time on all known example with a dramatic improvement in running time on larger example 
this paper describes an application of an analytical learning technique plausible explanation based learning pebl that dynamically acquires search control knowledge for a constraint based scheduling system in general the efficiency of a scheduling system suffers because of resource contention among activity our system learns the general condition under which chronic contention occurs and us search control to avoid repeating mistake because it is impossible to prove that a chronic contention will occur with only one example traditional ebl technique are insufficient we extend classical ebl by adding an empirical component that creates search control rule only when the system gain enough confidence in the plausible explanation this extension to ebl wa driven by our observation about the behavior of our scheduling system when applied to the real world problem of scheduling task for nasa space shuttle payload processing we demonstrate the utility of this approach and provide experimental result 
this paper introduces a new approach for computing the exact aspect graph of curved object observed under orthographic projection curve corresponding to various visual event partition the gaussian sphere into region where the image structure is stable a catalogue of these event for piecewise smooth object is available from singularity theory for a solid bounded by rational parametric patch and their intersection curve it is shown that each visual event is characterized by a system of n polynomial in n variable whose solution can be found by numerical curve tracing method combining these method with ray tracing it is possible to characterize the stable image structure within each region result from a preliminary implementation are presented 
diagnosis of multiple disorder can be made efficient using a new representation and algorithm based on symptom clustering the symptom clustering approach partition symptom into causal group in contrast to the existing candidate generation approach which assembles disorder or candidate symptom clustering achieves efficiency by generating aggregate of candidate rather than individual candidate and by representing them implicitly in a cartesian product form search criterion of parsimony subsumption and spanning narrow the symptom clustering search space and a problem reduction search algorithm explores this space efficiently experimental result on a large knowledge base indicate that symptom clustering yield a near exponential increase in performance compared to candidate generation 
the internal representation of the training pattern of multi layer perceptrons wa examined and we demonstrated that the connection weight between layer are effectively transforming the representation format of the information from one layer to another one in a meaningful way the internal code which can be in analog or binary form is found to be dependent on a number of factor including the choice of an appropriate representation of the training pattern the similarity between the pattern a well a the network structure i e the number of hidden layer and the number of hidden unit in each layer 
this paper extends shapiro s model inference system for synthesizing logic program from example of input output behavior a new refinement operator for clause generation based upon the decomposition of prolog program into skeleton basic prolog program with a well understood control flow and technique standard prolog programming practice is described shapiro s original system is introduced skeleton and technique are discussed and simple example are provided to familiarize the reader with the necessary terminology the model inference system equipped with this new refinement operator is compared and contrasted with the original version presented by shapiro the strength and weakness of applying skeleton and technique to synthesizing prolog program is discussed 
this paper present a heterogeneous asynchronous architecture for controlling autonomous mobile robot which is capable of controlling a robot performing multiple task in real time in noisy unpredictable environment the architecture produce behavior which is reliable task directed and taskable and reactive to contingency experiment on real and simulated realworld robot are described the architecture smoothly integrates planning and reacting by performing these two function asynchronously using heterogeneous architectural element and using the result of planning to guide the robot s action but not to control them directly the architecture can thus be viewed a a concrete implementation of agre and chapman s plan ascommunications theory the central result of this work is to show that completely unmodified classical ai programming methodology using centralized world model can be usefully incorporated into real world embedded reactive system 
this paper describes the result of using a reactive control software architecture for a mobile robot retrieval task in an outdoor environment the software architecture draw from the idea of universal plan and subsumption s layered control producing reaction plan that exploit low level competence a operator the retrieval task requires the robot to locate and navigate to a donor agent receive an object from the donor and return the implementation employ the concept of navigation template nats to construct and update an obstacle space from which navigation plan are developed and continually revised selective perception is employed among an infrared beacon detector which determines the bearing to the donor a real time stereo vision system which obtains the range and ultrasonic sensor which monitor for obstacle en route the perception routine achieve a robust controlled switching among sensor mode a defined by the reaction plan of the robot in demonstration run in an outdoor parking lot the robot located the donor object while avoiding obstacle and executed the retrieval task among a variety of moving and stationary object including moving car without stopping it traversal motion the architecture wa previously reported to be effective for simple navigation and pick and place task using ultrasonics thus the result reported herein indicate that the architecture will scale well to more complex task using a variety of sensor 
a knowledge based system for text understanding will incorporate both lexical and encyclopaedic information the lexical information is the basis of the parsing process while the encyclopaedic information form the target representation and is used in the knowledge acquisition process this paper describes twig a text understanding system where these two knowledge base arc integrated into one representation there is some theoretical justification for this and it ha the advantage of reducing duplication of information in the system this integration also ha the advantage of making conceptual information available during the parsing process most of all this integration of diverse information form a natural basis for a blackboard architecture 
in this paper we explore the idea of representing csps using technique from formal language theory the solution set of a csp can be expressed a a regular language we propose the minimized deterministic finite state automaton mdfa recognizing this language a a canonical representation for the csp this representation ha a number of advantage explicit enumerated constraint can be stored in lesser space than traditional technique implicit constraint and network of constraint can be composed from explicit one by using a complete algebra of boolean operator like and or not etc applied in an arbitrary manner such constraint are stored in the same way a explicit constraint by using mdfas this capability allows our technique to construct network of constraint incrementally after constructing this representation answering query like satisfiability validity equivalence etc becomes trivial a this representation is canonical thus mdfas serve a a mean to represent constraint a well a to reason with them while this technique is not a panacea for solving csps experiment demonstrate that it is much better than previously known technique on certain type of problem 
effective monitoring of device supported patient in the intensive care unit icu is complex involving interpretation of many variable comparative evaluation of many therapy option and control of many patient management parameter even skilled clinician make error that limit the quality of care harm patient or cause life threatening situation a growing body of research aim to improve icu monitoring with computer technology most of this research fall in two area a short term engineering of practical solution to narrowly defined immediate problem e g smart alarm system or b basic research on fundamental issue potentially relevant to icu monitoring e g temporal reasoning by contrast our project aim to develop a more comprehensive intelligent agent having a broad range of capability to cooperate on the icu team we do not aim to produce a practical system suitable for near term deployment in the icu but rather a proof of concept an experimental system that a demonstrably performs and coordinate a range of intelligent reasoning task of use in icu monitoring b doe so reliably in a significant range of medical situation and c arguably will scale up to meet the comprehensive set of practical requirement with an appropriate development effort we have developed an experimental system called guardian which exhibit several of the required capability and utilizes an underlying architecture hypothesized to support the full range of required capability in this paper we describe the guardian system it architecture and it current knowledge base we describe it performance and summarize the result of preliminary evaluation finally we discus ongoing and planned research on guardian 
there are still very few system performing a similarity based learning and using a first order logic fol representation this limitation come from the intrinsic complexity of the learning process in fol and from the difficulty to deal with numerical knowledge in this representation in this paper we show that major learning process namely generalizatiorl and clustering can be solved in a homogeneous way by using a similarity measure a this measure is defined the similarity computation come down to a problem of solving a set of equation in several unknown the representation language used to express our example is a subset of fol allowing to express both quantitative knowledge and a relevance scale on the predicate 
dynamic knowledge base are a fact of life in many artificial intelligence application using current technique however it is not always possible to provide the desired level of associative access to them whilst meeting real time or even near real time performance criterion this paper argues the case for a hardware associative storage system that us symbolic pattern matching a it access mechanism a working prototype of such a system designed a a co processor for a workstation host is then described the coprocessor is based on an array of custom designed vlsi smart memory chip these combine storage and search processing logic on the same die parallelism is exploited both on chip and between chip to yield a high system performance the paper concludes with some example of how this hardware can be used to support real application 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
operational definition link scientific attribute to experimental situation prescribing for the experimenter the action and measurement needed to measure or control attribute value while very important in real science operational procedure have been neglected in machine discovery we argue that in the preparatory stage of the empirical discovery process each operational definition must be adjusted to the experimental task at hand this is done in the interest of error reduction and repeatability of measurement both small error and high repeatability are instrumental in theory formation we demonstrate that operational procedure refinement is a discovery process that resembles the discovery of scientific law we demonstrate how the discovery task can be reduced to an application of the fahrenheit discovery system a new type of independent variable the experiment refinement variable have been introduced to make the application of fahrenheit theoretically valid this new extension to fahrenheit us simple operational procedure a well a the system s experimentation and theory formation capability to collect real data in a science laboratory and to build theory of error and repeatability that are used to refine the operational procedure we present the application of fahrenheit in the context of dispensing liquid in a chemistry laboratory 
reinforcement learning algorithm when used to solve multi stage decision problem perform a kind of online incremental search to find an optimal decision policy the time complexity of this search strongly depends upon the size and structure of the state space and upon a priori knowledge encoded in the learner initial parameter value when a priori knowledge is not available search is unbiased and can be excessive cooperative mechanism help reduce search by providing the learner with shorter latency feedback and auxiliary source of experience these mechanism are based on the observation that in nature intelligent agent exist in a cooperative social environment that help structure and guide learning within this context learning involves information transfer a much a it doe discovery by trial and error two cooperative mechanism are described learning with an external critic or lec and learning by watching or lbw the search time complexity of these algorithm along with unbiased q learning are analyzed for problem solving task on a restricted class of state space the result indicate that while unbiased search can be expected to require time moderately exponential in the size of the state space the lec and lbw algorithm require at most time linear in the size of the state space and under appropriate condition are independent of the state space size altogether requiring time proportional to the length of the optimal solution path while these analytic result apply only to a restricted class of task they shed light on the complexity of search in reinforcement learning in general and the utility of cooperative mechanism for reducing search 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
in this paper we describe a design of wafer scale integration for massively parallel memory based reasoning wsi mbr wsi mbr attains about million parallelism on a single inch wafer using the state of the art fabrication technology while wsi mbr is specialized to memory based reasoning which is one of the mainstream approach in massively parallel artificial intelligence research the level of parallelism attained far surpasses any existing massively parallel hardware combination of memory array and analog weight computing circuit enable u to attain super high density implementation with nanosecond order inference time simulation result indicates that inherent robustness of the memory based reasoning paradigm overcomes the possible precision degradation and fabrication defect in the wafer scale integration also the wsi mbr provides a compact desk top size massively parallel computing environment 
we present a model for flexible extruded object such a wire tube or grommet and demonstrate a novel self adjusting seven dimensional hough transform that derives and analyzes their three space curved ax from position and surface normal infor mation the method is purely local and is very cheap to compute the model considers such object a piecewise toroidal and decomposes the seven pa rameters of a torus into three nested subspace the structure of which counteract the error implicit in the analysis of object of great size and or small curvature we believe it is the first example of a parameter space structure designed to cluster ill con ditioned hypothesis together so that they can be easily detected and ignored this work complement existing shape from contour approach for analyz ing torus it us no edge information and it doe not require the solution of high degree non linear equa tions by iterative technique most of the result including the condition for the existence of more that one solution phantom anti torus have been verified using a symbolic mathematical analysis sys tem we present in the environment of the ibm convex system robust result on both synthetic cad cam range data the hasp of a lock and actual range data a knotted piece of coaxial cable and discus several system tuning issue 
consider the problem of exploring a large state space for a goal state although many such state may exist finding anyone state satisfying the requirement is sufficient all method known until now for conducting such search in parallel fail to provide consistent linear speedup over sequential execution the speedup vary between sublinear to superlinear and from run to run further adding processor may sometimes lead to a slow down rather than speedup giving rise to speedup anomaly we present prioritizing strategy which yield consistent linear speedup and requires substantially smaller memory over other method the performance of these strategy is demonstrated on a multiprocessor 
normally constraint network are undirected since constraint merely tell u which set of value are compatible and compatibility is a symmetrical relationship in contrast causal model use directed link conveying cause effect asymmetry in this paper we give a relational semantics to this directionality thus explaining why prediction is easy while diagnosis and planning are hard we use this semantics to show that certain relation posse intrinsic directionality similar to those characterizing causal influence we also use this semantics to decide when and how an unstructured set of symmetrical constraint can be configured so a to form a directed causal theory 
a recent system foil construct horn clause program from numerous example compu tational efficiency is achieved by using greedy search guided by an information based heuristic greedy search tends to be myopic but de terminate term an adaptation of an idea in troduced by another new system golem ha been found to provide many of the benefit of lookahead without substantial increase in computation this paper sketch key idea from foil and golem and discus the use of determinate literal in a greedy search con text the efficacy of this approach is illus trated on the task of learning the quicksort procedure and other small but non trivial listmanipulation function 
ladkin and maddux lama showed how to interpret the calculus of time interval defined by allen all in term of representation of a particular relation algebra and proved that this algebra ha a unique countable representation up to isomorphism in this paper we consider the algebra an of n interval which coincides with allen s algebra for n and prove that an ha a unique countable representation up to isomorphism for all n we get this result which implies that the first order theory of an is decidable by introducing the notion of a weak representation of an interval algebra and by giving a full classification of the connected weak representation of an we also show how the topological property of the set of atom of an can be represented by a n dimensional polytope 
we formulate the dempster shafer formalism of belief function shafer in the spirit of logical inference system our formulation called the belief calculus explicitly avoids the use of set theoretic notation a such it serf a an alternative for the use of the dempster shafer formalism for uncertain reasoning 
this paper present a model of failure recovery from which we have designed and tested set of failure recovery method in the phoenix system we derive the model document it assumption and then test the validity of the assumption and prediction of the model we present three experiment one derives baseline for failure recovery in the phoenix environment the second compare the performance of two strategy for selecting failure recovery method the third compare the performance of an initial set of failure recovery method with a redesigned set that is predicted to have lower expected cost 
constraint satisfaction problem involve finding value for variable subject to constraint on which combination of value are permitted they arise in a wide variety of domain ranging from scene analysis to temporal reasoning we present a new representation for partial solution a cross product of set of value this representation can be used to improve the performance of standard algorithm especially when seeking all solution or discovering that none exist 
ebl can learn justified generalization from only one example when the domain theory is perfect however it doe not work when the domain theory is imperfect imperfectness of the domain theory can be classified into four level i e incomplete intractable inconsistent and non operational one it is necessary to unify ebl and sbl to solve these problem in this paper we propose a framework of an augmented ebl to handle plural example simultaneously we formalize it on logic program and introduce a concept of least ebg to extract similarity from plural example we discus on an approach to solve utility problem with the augmented ebl utility problem is a problem to learn more efficient description under complete tractable consistent but nonoperational domain theory we define operationality criterion with maximizing usage degree and minimizing backtracking number and show they increase partial monotonically by generalization since this partial monotinicity is not preferable to search operational generalization least ebgs are more operational than usual ebgs we design a simple incremental learner based on least ebgs and show it usefulness in recursive domain theory we also discus on other imperfect theory problem 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and partly semantic processing for most input sentence instead using a set of learned rule explanation based learning is used to extract the learned rule automatically from sample sentence submitted by a user and thus tune the system for that particular user by indexing the learned rule efficiently it is possible to achieve dramatic speedup performance measurement were carried out using a training set of sentence and a separate test set of sentence all from the atis corpus a set of learned rule wa derived from the training set these rule covered percent of the test sentence and reduced the total processing time to a third an overall speed up of percent wa accomplished using a set of only learned rule 
in this paper we are interested in using a first order theorem prover to prove theorem that are formulated in some higher order logic to this end we present translation of higher order logic into first order logic with flat sort and equality and give a sufficient criterion for the soundness of these translation in addition translation are introduced that are sound and complete with respect to l henkin s general model semantics our higher order logic are based on a restricted type structure in the sense of a church they have typed function symbol and predicate symbol but no sort 
it is often the case that linguistic and pictorial information are jointly provided to communicate information in situation where the text describes salient aspect of the picture it is possible to use the text to direct the interpretation i e labelling object in the accompanying picture this paper focus on the implementation of a multi stage system piction that us caption to identify human in an accompanying photograph this provides a computationally le expensive alternative to traditional method of face recognition it doe not require a pre stored database of face model for all people to be identified a key component of the system is the utilisation of spatial constraint derived from the caption in order to reduce the number of possible label that could be associated with face candidate generated by a face locator a rule based system is used to further reduce this number and arrive at a unique labelling the rule employ spatial heuristic a well a distinguishing characteristic of face e g male versus female the system is noteworthy since a broad range of ai technique are brought to bear ranging from natural language parsing to constraint satisfaction and computer vision 
we present a mapping from a class of default theory to sentence in propositional logic such that each model of the latter corresponds to an extension of the former using this mapping we show that many property of default theory can be determined by solving propositional satisfiability in particular we show how csp technique can be used to identify analyze and solve tractable subset of reiter s default logic 
this paper describes a general approach for automatically programming a behavior based robot new behavior are learned by trial and error using a performance feedback function a reinforcement two algorithm for behavior learning are described that combine technique for propagating reinforcement value temporally across action and spatially across state a behavior based robot called obelix see figure is described that learns several component behavior in an example task involving pushing box an experimental study using the robot suggests two conclusion one the learning technique are able to learn the individual behavior sometimes outperforming a hand coded program two using a behavior based architecture is better than using a monolithic architecture for learning the box pushing task 
we describe how a behavior hierarchy can be used in a protocol that allows ai agent to discover and resolve interaction flexibly agent that initially do not know with whom they might interact use this hierarchy to exchange abstraction of their anticipated behavior by comparing behavior agent iteratively investigate interaction through more focused exchange of successively detailed information they can also modify their behavior along different dimension to either avoid conflict or promote cooperation we explain why our protocol give agent a richer language for coordination than they get through exchanging plan or goal and we use a prototype implementation to illustrate our protocol we argue that our hierarchical protocol for coordinating behavior provides a powerful representation for negotiation and can act a a common foundation for integrating theory about plan and organization 
we present a general framework for plan recognition whose formulation is motivated by a general purpose algorithm for effective abduction the knowledge representation is a restricted form of first order logic which is made computationally explicit a a graph structure in which plan are manifest a a special kind of graph walk intuitively plan are fabricated by searching an action description graph for relevant connection amongst instance of observed action the class of plan for which our method is applicable is wider than those previously proposed a both recursive and optional plan component can be represented despite the increased generality the proposed message passing algorithm ha an asymptotic upper bound that is an improvement on previous related work 
we consider the approach to game playing where one look ahead in a game tree evaluates heuristically the probability of winning at the leaf and then propagates this evaluation up the tree we show that minimax doe not make optimal use of information contained in the leaf evaluation and in fact misvalues the position associated with all node this occurs because when actually playing a position down the game tree a player would be able to search beyond the boundary of the original search and so ha access to additional information the remark that such extra information will exist allows better use of the information contained in the leaf evaluation even though we do not have access to the extra information itself our analysis implies that while minimax is approximately correct near the top of the game tree near the bottom a formula closer to the probability product formula is better we propose a simple model of how deep search yield extra information about the chance of winning in a position within the context of this model we write down the formula for propagating information up the tree which is correct at all level we generalize our result to the case when the outcome at the leaf are correlated and also to game like chess where there are three possible outcome win lose and draw experiment demonstrate our formula s superiority to minimax and probability product in the game of kalah 
block world cube world ha been one of the most popular model domain in artificial intelligence search and planning the operation and effectiveness of alternative heuristic strategy both basic and complex can be observed easily in this domain we show that finding an optimal solution is np hard in an important variant of the domain and po cr ular extension this enlarges the range of mo el domain whose complexity ha been explored mathematically and it demonstrates that the complexity of search in block world is on the same level a for sliding block problem the traveling salesperson problem binpacking problem and the like these result also support the practice of using block world a a tutorial search domain in course on artificial intelligence to reveal both the value and limitation of heuristic search when seeking optimal solution 
a methodology is presented whereby a nominal trajectory for an assembly operation computed from kinematic constraint alone is augmented with a fine motion strategy synthesized through uncertainty and force analysis insertion clearance and size tolerance are introduced into the assembly part model in parallel with the manual selection of a perturbed nominal trajectory in contact space the selection of small clearance and in turn small insertion angle allows u to linearize contact space about discrete point in the nominal trajectory contact state are represented a affine space in a generalized c space of model error and pose variable the feasibility of proposed command velocity to be executed in the presence of position control and model error is determined through an uncertainty analysis technique based on the forward projection of convex polytopes in contact space our approach further the automates the so called manual method of motion planning with uncertainty 
this paper investigates the semantics of conditional term rewriting system with negation which may not satisfy desirable property like termination it is shown that the approach used by fitting for prolog style logic program is applicable in this context a monotone operator is developed whose fixpoints describe the semantics of conditional rewriting several example illustrate this semantics for non terminating rewrite system which could not be easily handled by previous approach 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
we characterize the complexity of several typical problem in propositional default logic in particular we examine the complexity of extension membership extension existence and extension entailment problem we show that the extension existence problem is p complete even for semi normal theory and that the extension membership and entailment problem are p complete and p complete respectively even when restricted to normal default theory these result contribute to our understanding of the computational relationship between propositional default logic and other formalism for nonmonotonic reasoning e g autoepistemic logic and mcdermott and doyle s nml a well a their relationship to problem outside the realm of nonmonotonic reasoning 
we combine simple retrieval with domain specific validation of retrieved case to produce a useful practical tool for case based reasoning based on real world case we retrieve between three and six case over a wide range of new problem this represents a selectivity ranging from to compared to an average selectivity of only from simple retrieval alone 
two new theorem proving procedure for equational horn clause are presented the largest literal is selected for paramodulation in both strategy except that one method treat positive literal a larger than negative one and result in a unit strategy both use term ordering to restrict paramodulation to potentially maximal side of equation and to increase the amount of allowable simplification demodulation completeness is shown using proof ordering 
we describe a program beatrix that can understand textbook physic problem specified by a combination of english text and a diagram the result of the understanding process is a unified internal model that represents the problem including information derived from both the english text and the diagram the system is implemented a two opportunistic coparsers one for english and one for diagram within a blackboard architecture a central problem is establishing coreference that is determining when part of the text and diagram refer to the same object constraint supplied by the text and diagram mutually reduce ambiguity in interpretation of the other modality 
abstract we present a general theory that capture the relationship between certain domain and negotiation mechanism the analysis make it possible to categorize precisely the kind of do main in which agent find themselves and to use the category to choose appropriate negoti ation mechanism the theory presented here both generalizes previous result and allows agent designer to characterize new domain ac curately the analysis thus serf a a critical step in using the theory of negotiation in realworld application weshow that in certain task oriented do main there exist distributed consensus mech anisms with simple and stable strategy that lead to efficient outcome even when agent have incomplete information about their envi ronment we also present additional novel re sults in particular that in concave domain u ing all or nothing deal no lying by an agent can be beneficial and that in subadditive do main there often exist beneficial decoy lie that do not require full information regarding the other agent s goal 
this paper is a discussion of machine learning theory on empirically learning classification rule the paper proposes six myth in the machine learning community that address issue of bias learning a search computational learning theory occam s razor universal learning algorithm and interactive learning some of the problem raised are also addressed from a bayesian perspective the paper concludes by suggesting question that machine learning researcher should be addressing both theoretically and experimentally 
this paper present two method for adding domain knowledge to similarity based learning through feature construction a form of representation change in which new feature are constructed from relationship detected among existing feature in the first method domain knowledge constraint are used to eliminate le desirable new feature before they are constructed in the second method domain dependent transformation generalize new feature in way meaningful to the current problem these two us of domain knowledge are illustrated in citre where they are shown to improve hypothesis accuracy and conciseness on a tic tat toe classification problem 
for real world concept learning problem feature selection is important to speed up learning and to improve concept quality we review and analyze past approach to feature selection and note their strength and weakness we then introduce and theoretically examine a new algorithm rellef which selects relevant feature using a statistical method relief doe not depend on heuristic is accurate even if feature interact and is noise tolerant it requires only linear time in the number of given feature and the number of training instance regardless of the target concept complexity the algorithm also ha certain limitation such a nonoptimal feature set size way to overcome the limitation are suggested we also report the test result of comparison between relief and other feature selection algorithm the empirical result support the theoretical analysis suggesting a practical approach to feature selection for real world problem 
a case based reasoner can frequently benefit from using piece of multiple previous case in the course of solving a single problem in our model case piece called snippet are organized around the pursuit of a goal and there are link between the piece that preserve the structure of reasoning the advantage of our representational approach include the step taken in a previous case can be followed a long a they are relevant since the connection between step are preserved there is easy access to all part of previous case so they can be directly accessed when appropriate 
program such a bacon abacus coper kepler and others are designed to find functional relationship of scientific significance in numerical data without relying on the deep domain knowledge scientist normally bring to bear in analytic work whether these system actually perform a intended is an open question however to date they have been supported only by anecdotal evidence report that a desirable answer ha been found in one or more handselected and often artificial case in this paper i describe a function finding algorithm which differs radically from previous candidate in three respect first it concentrate rather on reliable identification of a few functional form than on heuristic search of an infinite space of potential relation second it introduces the use of distinction significance and lack of fitthree general concept of value in evaluating apparent functional relationship finally and crucially the algorithm ha been tested prospectively on an extensive collection of real scientific data set though i claim much le than previous investigator about the power of my approach these claim may be considered to a degree quite unfamiliar in function finding research a conclusively proven 
to make informed decision in a multiagent environment an agent need to model itself the world and the other agent including the model that those other agent might be employing we present a framework for recursive modeling that us possible world semantics and is based on extending the kripke structure so that an agent can model the information it think that another agent ha in each of the possible world which in turn can be modeled with kripke structure using recursive nesting we can define the propositional attitude of agent to distinguish between the concept of knowledge and belief through the three wise men example we show how our framework is useful for deductive reasoning and we suggest that it might provide a meeting ground between decision theoretic and deductive method for multiagent reasoning 
we describe a generalization of equivalence between constraint set called weak equivalence this new equivalence relation take into account that not all variable have the same function in a constraint set and therefore distinguishes between restriction variable and intermediate variable we explore the property of weak equivalence and it underlying notion of weak implication with an axiomatic approach in addition a complete set of axiom for weak implication is presented with example derived from the declarative rule language rl we show the applicability of weak equivalence to constraint solving 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
we define a model theoretic reasoning formalism that is naturally implemented on symmetric neural network like hopfield network or boltzman machine we show that every symmetric neural network can be seen a performing a search for a satisfying model of some knowledge that is wired into the network s weight several equivalent language are then shown to describe the knowledge embedded in these network among them is propositional calculus extended by augmenting propositional assumption with penalty the extended calculus is useful in expressing default knowledge preference between argument and reliability of assumption in an inconsistent knowledge base every symmetric network can be described by this language and any sentence in the language is translatable into such a network a sound and complete proof procedure supplement the model theoretic definition and give an intuitive understanding of the nonmonotonic behavior of the reasoning mechanism finally we sketch a connectionist inference engine that implement this reasoning paradigm 
recent research suggests the utility of performing induction over explanation this process identifies commonality across explanation that cannot be extracted solely by explanation based technique this ha important implication for the correctness of learned knowledge flann and dietterich and a we show on the efficiency with which learned knowledge can be reused specifically we illustrate that inductive concept formation can abstract and organize explanatory knowledge for efficient reuse in a domain of algebra story problem 
effective reasoning about complex engineered device requires device model that are both adequate for the task and computationally efficient this paper present a method for constructing simple and adequate device model by selecting appropriate model for each of the device s component appropriate component model are determined by the context in which the device operates we introduce context dependent behavior cdbs a component behavior model representation for encapsulating contextual modeling constraint we show how cdbs are used in the model selection process by exploiting constraint from three source the structural and behavioral context of the component and the expected behavior of the device we describe an implemented program for selecting a simplest adequate model the input are the structure of the device the expected device behavior and a library of cdbs the output is a set of component cdbs forming a structurally and behaviorally consistent device model that achieves the expected behavior 
there is no consensus on how syntax and semantic pragmatic should interact in natural language processing this paper focus on one issue concerning interaction order of processing two approach are compared empirically an interleaved syntax first approach in which semantic interpretation is performed at intermediate point during parsing and a semantics first approach in which semantic consideration drive the rule selection process during parsing the study provides empirical evidence that the semantics first approach is more efficient than the syntax first approach in processing text in narrow domain 
programming robot is a tedious task so there is growing interest in building robot which can learn by themselves self improving which involves trial and error however is often a slow process and could be hazardous in a hostile environment by teaching robot how task can be achieved learning time can be shortened and hazard can be minimized this paper present a general approach to making robot which can improve their performance from experience a well a from being taught based on this proposed approach and other learning speedup technique a simulated learning robot wa developed and could learn three moderately complex behavior which were then integrated in a subsumption style so that the robot could navigate and recharge itself interestingly a real robot could actually use what wa learned in the simulator to operate in the real world quite successfully 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
we describe arachne a concept formation system that us explicit constraint on tree structure and local restructuring operator to produce well formed probabilistic concept tree we also present a quantitative measure of tree quality and compare the system s performance in artificial and natural domain to that of cobweb a well known concept formation algorithm the result suggest that arachne frequently construct higher quality tree than cobweb while still retaining the ability to make accurate prediction 
robot performing complex task in rich environment need very good perception module in order to understand their situation and choose the best action robot planning system have typically assumed that perception wa so good that it could refresh the entire world model whenever the planning system needed it or whenever anything in the world changed unfortunately this assumption is completely unrealistic in many real world domain because perception is far too difficult robot in these domain cannot use the traditional planner paradigm but instead need a new system design that integrates reasoning with perception our research is aimed at showing how a robot can reason about perception how task knowledge can be used to select perceptual target and how this selection dramatically reduces the computational cost of perception 
a primary problem facing real world robot is the question of which sensing action should be performed at any given time it is important that an agent be economical with it allocation of sensing when sensing is expensive or when there are many possible sensing operation available sensing is rational when the expected utility from the information obtained outweighs the execution cost of the sensing operation itself this paper outline an approach to the efficient construction of plan containing explicit sensing operation with the objective of finding nearly optimal cost effective plan with respect to both action and sensing the scheduling of sensing operation in addition to the usual scheduling of physical action potentially result in an enornous increase in the computational complexity of planning our approach avoids this pitfall through strict adherence to a static sensing policy the approach based upon the markov decision process paradigm handle a significant amount of uncertainty in the outcome of action 
in this paper we analyze a particular model of control among intelligent agent that of nonabsolute control non absolute control involves a supervisor agent that issue order to a subordinate agent an example might be a human agent on earth directing the activity of a mar based semi autonomous vehicle both agent operate with essentially the same goal the subordinate agent however is assumed to have access to some information that the supervisor doe not have the agent is thus expected to exercise it judgment in following order i e following the true intent of the supervisor to the best of it ability after presenting our model we discus the planning problem how would a subordinate agent choose among alternative plan our solution focus on evaluating the distance between candidate plan 
in the following paper we view applying default reasoning a a construction of an argument supporting agent s belief this yield a slight reformulation of the notion of an extension for default theory the proposed formalism enjoys a property which we call rational maximization of belief 
this paper present a logically complete assumption based truth maintenance system atm that is part of a complex blast furnace computer aided piloting system this system is built on an efficient and logically complete propositional constraint solver that ha been successfully used for industrial application in computer aided design 
abduction is an important inference process underlying much of human intelligent activity including text understanding plan recognition disease diagnosis and physical device diagnosis in this paper we describe some problem encountered using abduction to understand text and present some solution to overcome these problem the solution we propose center around the use of a different criterion called explanatory coherence a the primary measure to evaluate the quality of an explanation in addition explanatory coherence play an important role in the construction of explanation both in determining the appropriate level of specificity of a preferred explanation and in guiding the heuristic search to efficiently compute explanation of sufficiently high quality 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
in this paper we present a purely semantic view on non monotonic reasoning we follow the direction pointed in and claim that any nonmonotonic logic can be viewed a a result of transforming some base standard logic by a selection strategy defined on model the generalized theory of model preference is shortly outlined here together with it use in modeling non monotonic belief 
understanding a text requires two basic task making inference at several level of knowledge and composing a global interpretation of the given text from those various type of inference since making inference at each level demand an extensive computation there have been several attempt to use parallel inference mechanism such a parallel marker passing pmp to increase the productivity of the inference mecha nism such a mechanism when used with many local processor is capable of making inference in parallel however it often pose a large burden on the task of composing the global interpretation by producing a number of meaningless inference which should be filtered out therefore the increased productivity of the inference mechanism cause the slow down of the task of forming the global interpretation and make it the bottleneck of the whole system our system true effectively solves this problem with the constrained marker passing mechanism the new mechanism not only allows the system to make necessary inference in parallel but also provides a way to compose the global interpretation in parallel therefore the system is truly parallel and doe not suffer from any single bottleneck 
consolidation is inferring the behavioral description of a device by composing the behavioral description of it component e g deriving the qualitative differential equation qdbs of a device from those of it component in previous work dormoy and raiman described the qualitative resolution rule which is a general rule for deriving qdes of combination of component however the qualitative resolution rule is intractable in general a a step toward understanding tractable qualitative reasoning i present a new qde resolution rule the qualitative difference resolution rule that support the tractable consolidation of component in which direction of flow is dependent on the sign of pressure difference pipe and container are general type of component that match this rule the pressure regulator example also match this rule 
this report discus what it mean to claim that a representation is an effective encoding of knowledge we first present dimension of merit for evaluating representation based on the view that usefulness is a behavioral property and is necessarily relative to a specified task we then provide method based on result from mathematical statistic for reliably measuring effectiveness empirically and hence for comparing different representation we also discus weak but guaranteed method of improving inadequate representation our result are an application of the idea of formal learning theory to concrete knowledge representation formalism 
this paper present a strengthened algorithm for temporal reasoning during plan recognition which improves on a straightforward application of allen s reasoning algorithm this is made possible by viewing plan a both hierarchical structure and temporal network a a result we can show how to use a constraint the temporal relation explicitly given in input to improve the result of plan recognition we also discus how to combine the given constraint with those prestored in the system s plan library to make more specific the temporal constraint indicated in the plan being recognised 
we describe a technique for improving problem solving performance by creating concept that allow problem state to be evaluated through an efficient recognition process a temporal difference td method is used to bootstrap a collection of useful concept by backing up evaluation from recognized state to their predecessor this procedure is combined with explanationbased generalization ebg and goal regression to use knowledge of the problem domain to help generalize the new concept definition this maintains the efficiency of using the concept and accelerates the learning process in comparison to knowledge free approach also because the learned definition may describe negative condition it becomes possible to use ebg to explain why some instance is not an example of a concept the learning technique ha been elaborated for minimax gameplaying and tested on a tic tat toe system t given only concept defining the end game state and constrained to a two ply search bound experiment show that t learns concept for achieving near perfect play t s total searching time including concept recognition is within acceptable performance limit while perfect play without the concept requires search taking well over time longer than t s 
case based teaching system like good human teacher tell story in order to help student learn a case based teaching system engages a student in a challenging task and monitor his action looking for opportunity to tell story that will assist the learning process in order to produce story at the appropriate moment a casebased teaching system must have a library of story that are indexed according to how they should be used and a set of reminding strategy to retrieve story when they are relevant in this paper i discus creanimate a biology tutor that us story to help teach elementary school student about animal morphology in particular i discus the reminding strategy and indexing scheme that enable the system to achieve it educational objective these reminding strategy are example remindings similarity based remindings and expectation violation remindings 
knowledge representation kr ha traditionally been thought of a the heart of artificial intelligence anyone who ha ever built an expert system a natural language system almost any ai system at all ha had to tackle the problem of representing it knowledge of the world despite it ubiquity for most of ai s history kr ha been a backstage activity but in the s it emerged a a field unto itself with it own burgeoning literature along with this growth the last decade ha seen major change in kr methodology important technical contribution and challenge to the basic assumption of the field i survey some of these development and then speculate about some of the equally interesting change that appear on the horizon i also look at some of the critical problem facing kr research in the near future both technical and sociological 
we present a new model for the perceptual reasoning involved in hand eye coordination and we show how this model can be developed into a control mechanism for a robot manipulator with a visual sensor this new approach overcomes the hig h computational cost the lack of robustness and the need for precise calibration that plague traditional approach at the heart of our model is the perceptual kinematic map pkm a direct mapping from the control space of the manipulator onto a space defined by a set of measurable image parameter by exploring it workspace the robot learns qualitatively the topology of it pkm and thus acquires the dexterity for future task in a striking parallel to biological system 
a domain model in savile represents the step involved in producing and processing financial data in a company using an ontology appropriate for several reasoning task in accounting and auditing savile is an implemented program that demonstrates the adequacy and appropriateness of this ontology of financial data processing for evaluating internal control designing test and other audit planning related task this paper discus the rationale syntax semantics and implementation of the ontology a it stand today 
belief revision for an intelligent system is usually computationally expensive here we tackle this problem by using focus in belief revision that is revision occurs only in a subset of belief under attention or in focus attention can be shifted within the belief base thus allowing use and revision of other subset of belief this attention shifting belief revision architecture show promise to allow efficient and natural revision of belief base 
most research in computer chess ha focussed on creating an excellent chess player with relatively little concern given to modelling how human play chess the research reported in this paper is aimed at investigating knowledge based chess in the context of building a prototype chess tutor umrao which help student learn how to play bishop pawn endgame in tutoring it is essential to take a knowledge based approach since student must learn how to manipulate strategic concept not how to carry out minimax search umrao us an extension of michic s advice language to represent expert and novice chess plan for any given endgame the system is able to compile the plan into a strategy graph which elaborates strategy both well formed and ill formed that student might use a they solve the endgame problem strategy graph can be compiled off line so that they can be used in real time tutoring we show that the normally rigid model tracing tutoring pardigm can be used in a flexible way in this domain 
temporal reasoning is widely used in ai especially for natural language processing existing method for temporal reasoning are extremely expensive in time and space because complete graph are used we present an approach of temporal reasoning for expert system in technical application that reduces the amount of time and space by using sequence graph a sequence graph consists of one or more sequence chain and other interval that are connected only loosely with these chain sequence chain are based on the observation that in technical application many event occur sequentially the uninterrupted execution of technical process for a long time is characteristic for technical application to relate the first interval in the application with the last one make no sense in sequence graph only these relation are stored that are needed for further propagation in contrast to other algorithm which use incomplete graph no information is lost and the reduction of complexity is significant additionally the representation is more transparent because the flow of time is modelled 
mitchell s version space approach to inductive concept learning ha been highly influential in machine learning a it formalizes inductive concept learning a a search problem to identify some concept definition out of a space of possible definition this paper lay out some theoretical underpinnings of version space it present the condition under which an arbitrary set of concept definition in a concept description language can be represented by boundary set which is a necessary condition for a set of concept definition to be a version space furthermore although version space can be intersected and unioned version space are simply set albeit with special structure the result need not be a version space this paper also present the condition under which such intersection and union of two version space yield a version space i e representable by boundary set finally the paper show how the resulting boundary set after intersection and union can be computed from the initial boundary set and prof the algorithm correct 
multiclass learning problem involve finding a definition for an unknown function f x whose range is a discrete set containing k value i e k class the definition is acquired by studying large collection of training example of the form xi f xi existing approach to this problem include a direct application of multiclass algorithm such a the decision tree algorithm id and cart b application of binary concept learning algorithm to learn individual binary function for each of the k class and c application of binary concept learning algorithm with distributed output code such a those employed by sejnowski and rosenberg in the nettalk system this paper compare these three approach to a new technique in which bch error correcting code are employed a a distributed output representation we show that these output representation improve the performance of id on the nettalk task and of backpropagation on an isolated letter speech recognition task these result demonstrate that error correcting output code provide a general purpose method for improving the performance of inductive learning program on multiclass problem 
we present our research on defining a correct semantics for forward chaining production system p program a correct semantics ensures that the execution of the program will not produce incorrect answer and execution will terminate it also ensures that the answer are consistent we define a class of stratified p program and propose an operational semantics for these program we define an operator tps which computes the operational fixpoint for the production of the stratified p program the fixpoint capture the meaning of the p program the theory that can be derived from the production of the p program may be inconsistent with the constraint that are also derived from the p program we can then view the constraint a modifying the theory so that the modified theory p is consistent with the constraint however the same answer are obtained in the operational semantics of the stratified p program or from the modified theory p 
we have developed and evaluated a set of tutor construction tool which enabled three computer naive educator to build test and modify an intelligent tutoring system the tool constitute a knowledge acquisition interface for representing and rapid prototyping both domain and tutoring knowledge a formative evaluation is described which lasted nearly two year and involved student this research aim to understand and support the knowledge acquisition process in education and to facilitate browsing and modification of knowledge result of a person hour analysis of throughput factor are provided along with knowledge representation and engineering issue for developing knowledge acquisition interface in education 
recently the relationship between several form of default reasoning based on conditional default ha been investigated in particular the system based on e semantics preferential model and fragment of modally defined conditional logic have been shown to be equivalent these system form a plausible core for default inference but are too weak in general failing to deal adequately with irrelevance we propose an extension of the modal conditional logic in which one can express the truth of sentence at inaccessible possible world and show how this logic can be used to axiomatize a simple preference relation on the modal structure of this logic this preferential semantics is shown to be equivalent to entailment and rational closure we suggest that many metalogical system of default inference can be axiomatized within this logic using the notion of inaccessible world 
this paper present an algorithm which make use of tense interpretation to determine the intended temporal ordering between the state and event mentioned in a narrative this is done by maintaining a temporal focus and interpreting the tense of each new statement of the narrative with respect to this focus in particular we propose heuristic for determining the temporal ordering and constraint for characterizing coherent tense sequence the algorithm is further defended through experiment with naturally occurring example 
finding the l most probable explanation mpe of a given evidence se in a bayesian belief network is a process to identify and order a set of composite hypothesis his of which the posterior probability are the l largest i e pr h se pr h se pr hl se a composite hypothesis is defined a an instantiation of all the non evidence variable in the network it could be shown that finding all the probable explanation is a np hard problem previously only the first two best explanation i e l in a singly connected bayesian network could be efficiently derived without restriction on network topology and probability distribution this paper present an efficient algorithm for finding l mpe in singly connected network and the extension of this algorithm for multiply connected network this algorithm is based on a message passing scheme and ha a time complexity o lkn for singly connected network where l is the number of mpe to be derived k the length of the longest path in a network and n the maximum number of node state defined a the product of the size of the conditional probability table of a node and the number of the incoming outgoing arc of the node 
this paper present a new approach to constructive induction discrimination based constructive induction dbc which invents useful predicate in learning relation triggered by failure of selective induction dbc find a minimal set of variable forming a new predicate that discriminates between positive and negative example and induces a definition of the invented predicate if necessary it also induces subpredicates for the definition experimental result show that dbc learns meaningful predicate without any interactive guidance 
finding best explanation is often formalized in ai in term of minimal cost proof finding such proof is naturally characterized a a best first search of the proof tree actually a proof dag unfortunately the only known search heuristic for this task is quite poor in this paper we present a new heuristic a proof that it is admissible for certain successor function and some experimental result suggesting that it is a significant improvement over the currently used heuristic 
in most research on concept formation within machine learning and cognitive psychology the feature from which concept are built are assumed to be provided a elementary vocabulary in this paper we argue that this is an unnecessarily limited paradigm within which to examine concept formation based on evidence from psychology and machine learning we contend that a principled account of the origin of feature can only be given with a grounded model of concept formation i e with a model that incorporates direct access to the world via sensor and manipulator we discus the domain of process control a a suitable framework for research into such model and present a first approach to the problem of developing elementary vocabulary from perceptual sensor data 
we introduce an adaptive search technique that speed up state space search by learning heuristic censor while searching the censor speed up search by pruning away mote and more of the space until a solution is found in the pruned space censor are learned by explaining dead end and other search failure to learn quickly the technique over generalizes by assuming that certain constraint ate preservable i e remain true on at least one solution path a recovery mechanism detects violation of this assumption and selectively relaxes learned censor the technique implemented in an adaptive problem solver named failsafe learns useful heuristic that cannot be learned by other reported method it effectiveness is indicated by a preliminary complexity analysis and by experimental result in three domain including one in which prodigy failed to learn eflective search control rule 
are we justified in inferring a general rule from observation that frequently confirm it this is the usual statement of the problem of induction the present paper argues that this question is relevant for the understanding of machine learning but insufficient research in machine learning ha prompted another more fundamental question the number of possible rule grows exponentially with the size of the example and many of them are somehow confirmed by the data how are we to choose effectively some rule that have good chance of being predictive we analyze if and how this problem is approached in standard account of induction and show the difficulty that are present finally we suggest that the explanation based learning approach and related method of knowledge intensive induction could be a partial solution to some of these problem and help understanding the question of valid induction from a new perspective 
system that discover empirical equation from data require large scale testing to become a reliable research tool in the central part of this paper we discus two convergence test for large scale evaluation of equation finder and we demonstrate that our system which we introduce earlier ha the desired convergence property our system can detect a broad range of equation useful in different science and can be easily expanded by addition of new variable transformation previous system such a bacon or abacus disregarded or oversimplified the problem of error analysis and error propagation leading to paradoxical result and impeding the true world application our system treat experimental error in a systematic and statistically sound manner it propagates error to the transformed variable and assigns error to parameter in equation it us error in weighted least square fitting in the evaluation of equation including their acceptance rejection and ranking and us parameter error to eliminate spurious parameter the system detects equivalent term variable and equation and it remove the repetition this is important for convergence test and system efficiency thanks to the modular structure our system can be easily expanded modified and used to simulate other equation finder 
one of the major focus of research in distributed artificial intelligence dai is the design of automated agent which can interact effectively in order to cooperate in problem solving negotiation is recognized a an important mean by which inter agent cooperation is achieved in this paper we suggest a strategic model of negotiation for n agent n that take the passage of time during the negotiation process itself into consideration change in the agent s preference over time will change their strategy in the negotiation and a a result the agreement they are willing to reach we will show that in this model the delay in reaching such agreement can be shortened and in some case avoided altogether 
we introduce method for identifying operator precondition that need not be expanded further the method are proved to be admissible that is they will not cause a solution to be missed when one exists in certain case the method also identify operator reformulations that increase the number of nonexpandable precondition this approach provides effective loop control in common situation moreover the computation required can be performed during a precompilation of the operator in a domain thus there is no significant additional run time overhead during planning 
delayed reinforcement learning is an attractive framework for the unsupervised learning of action policy for autonomous agent some existing delayed reinforcement learning technique have shown promise in simple domain however a number of hurdle must be passed before they are applicable to realistic problem this paper describes one such difficulty the input generalization problem whereby the system must generalize to produce similar action in similar situation and an implemented solution the g algorithm this algorithm is based on recursive splitting of the state space based on statistical measure of difference in reinforcement received connectionist backpropagation ha previously been used for input generalization in reinforcement learning we compare the two technique analytically and empirically the g algorithm s sound statistical basis make it easy to predict when it should and should not work whereas the behavior of back propagation is unpredictable we found that a previous successful use of backpropagation can be explained by the linearity of the application domain we found that in another domain g reliably found the optimal policy whereas none of a set of run of backpropagation with many combination of parameter did 
in traditional natural language system the channel of communication between the user and the system wa a narrow and constraining device in many area natural ian guage based information access requires the possibility for the user of exploring the domain and a system s comfortable habitability the present work present a rationale for building intelligent interface that combine natural language and hypermedia a new mean for human computer interaction and in particular give an outline of a prototype of this kind built for the exploration of italian fresco the alfresco interactive system 
market price mechanism from economics constitute a well understood framework for coordinating decentralized decision process with minimal communication walras is a general market oriented programming environment for the construction and analysis of distributed planning system based on general equilibrium theory the environment provides basic construct for defining computational market structure and a procedure for deriving their corresponding competitive equilibrium in a particular realization of this approach for a simplified form of distributed transportation planning we see that careful construction of the decision process according to economic principle can lead to effective decentralization and that the behavior of the system can be meaningfully analyzed in economic term 
this paper address the problem of bridging the gap between the field of knowledge renresentation kr and uncertain reasoning ur the proposed solution consists of a framework for representing uncertain knowledge in which two component one dealing with categorical knowledge and one dealing with uncertainty about this knowledge are singled out in this sense the framework is hybrid this framework is characterized in both modeltheoretic and proof theoretic term state of belief is represented by belief set defined in term of the functional approach to knowledge representation suggested by levesque example are given using first order logic and a minimal subset of m krypton for the kr side and a yes no trivial case and dempster shafer theory for the ur side 
this paper describes the development of an architecture and implementation of a graphical tracing system for the parallel logic programming language parlog novel feature of the architecture include a graphical execution model of parlog a range of representational technique that allow the user a choice of perspective and granularity of analysis and ongoing work on graphical tool that provide user defined visualisation of their program either before the program is run or afterwards by demonstration from a textual trace the aim of the architecture are threefold to aid program construction and debugging by providing an informative graphical trace of the program s execution to provide the user with a choice of representational technique at a preferred level of granularity and to allow user to define their own visualisation that more truly map onto their conception of the problem and which support the way they wish to view the execution information 
this paper present an outline of a theory of agency that seek to integrate ongoing understanding planning and activity into a single model of representation and processing our model of agency rise out of three basic piece of work schank s structural model of memory organization schank hammond s work in case based planning and dependency directed repair hammond d and martin s work in direct memory access parsing martin we see this paper a a first step in the production of a memory based theory of agency the active pursuit of goal in the face of a changing environment that can exist within the computational constraint of a computer model 
this dissertation present a model of the human sentence interpretation process which attempt to meet criterion of adequacy imposed by the different paradigm of sentence interpretation these include the need to produce a high level interpretation to embed a linguistically motivated grammar and to be compatible with psycholinguistic result on sentence processing the model includes a theory of grammar called construction based interpretative grammar cig and an interpreter which us the grammar to build an interpretation for single sentence an implementation ofhte interpreter ha been built called sal sal is an on line interpreter reading word one at a time and updating a partial interpretation of the sentence after each constituent this constituent by constituent interpretation is more fine grained and hence more on line than most previous model sal is strongly interactionist in using both bottom up and top down knowledge in an evidential manner to access a set of construction to build interpretation it us a coherence based selection mechanism to choose among these candidate interpretation and allows temporary limited parallelism to handle local ambiguity sal s architecture is consistent with a large number of psycholinguistic result the interpreter embodies a number of strong claim about sentence processing one claim is uniformity with respect to both representation and process in the grammar a single kind of knowledge structure the grammatical construction is used to represent lexical syntactic idiomatic and semantic knowledge cig thus doe not distinguish between the lexicon the idiom dictionary the syntactic rule base and the semantic rule base uniformity in processing mean that there is no distinction between the lexical analyzer the parser and the semantic interpreter because these kind of knowledge are represented uniformly they can be accessed integrated and disambiguated by a single mechanism a second claim the interpreter embodies is that sentence processing is fundamentally knowledge intensive and expectation based the representation and integration of construction us many diverse type of linguistic knowledge similarly the access of construction is sensitive to top down and bottom up syntactic and semantic knowledge and the selection of construction is based on coherence with grammatical knowledge and the interpretation 
work in distributed artificial intelligence dai ha since it earliest year been concerned with negotiation strategy which can be used in building agent that are able to communicate to reach mutually beneficial agreement in this paper we suggest a strategic model of negotiation that take the passage of time during the negotiation process itself into consideration change in the agent s preference over time will change their strategy in the negotiation and a a resuit the agreement they are willing to reach we will show that in this model the delay in reaching agreement can be avoided 
this paper describes uunk a program designed to understand ungrammatical input while most previous work in the field ha relied on syntactic technique or sublanguage analysis to parse grammatical error uunk us a semantics driven algorithm to process such input the paper give a brief overview of link the unification based system upon which ulink is built special attention is given to those aspect of link which allow ulink to use semantics to process ill formed input the detail of ulink s algorithm are then discussed by considering two example the paper concludes with a discussion of related research and problem which remain to be solved 
an approach to generation system design is described which support maximal expression of commonality across language within this approach it becomes natural to represent inherently multilingual grammar and semantics the approach rest on the linguistic notion of functional similarity and difference by capturing the function language need to perform we achieve a level of linguistic description which carry across language far more effectively than account that are structurally based we demonstrate the general principle implementation and benefit of the approach with respect to three unrelated language english chinese and japanese 
iterative sentence such a mary knocked on the door four time john played the sonata every other day and mary wa often busy can be understood a asserting that some situation type is either repeated a certain number of time or with a certain frequency the semantic content of iterative sentence ha been standardly represented by some logical formula which quantifies over instance of a non iterative situation type the principal claim of this paper and the basis of the representation proposed in it is that we also require iterative situation type and instance in order to completely handle the range of possible interpretation of iterative sentence 
the repetitive behavior of a device or system can be described in two way a detailed description of one iteration of the behavior or a summary description of the behavior over many repetition this paper describes an implemented program called ai that transforms the first type of description into the second type ai deal only with behavior where each repetition change parameter by the same amount at present the summary consists of the symbolic average rate of change in parameter value and information on how those rate would be different if various constant and function had been different unlike some other approach ai doe not require that a repeating behavior be described in term of a set of differential equation two example of running ai are given one concern the human heart the other a steam engine 
we present an incremental configuration space c construction algmithm for mechanism described a collection of subassemblies of rigid part the input are the initial subassembly configuration and the subassembly cs partitioned into uniform motion region in which part contact are constant and motion are monotonic the output is a partition of the mechanism c into uniform motion region the algorithm optimizes c construction by incrementally enumerating and testing only the region reachable from the initial configuration we implement the algorithm for subassemblies whose uniform motion region are polyhedral or are of dimension two or lower the program construct the exact c when possible and an approximate c otherwise the approximate c usually is qualitatively correct and in good quantitative agreement with the true c the program cover most mechanism composed of linkage and fixed ax kinematic pair two subassembly type for which c construction program are available 
to speed up production system researcher have developed parallel algorithm that execute multiple instantiation simultaneously unfortunately without special control such system can produce result that could not have been produced by any serial execution we present and compare three different algorithm that guarantee a serializable result in such system our goal is to analyze the overhead that serialization incurs all three algorithm perform synchronization at the level of instantiation not rule and are targeted for shared memory machine one algorithm operates synchronously while the other two operate asynchronously of the latter two one synchronizes instantiation using compiled test that were determined from an omine analysis while the other us a novel locking scheme that requires no such analysis our examination of performance show that asynchronous execution is clearly faster than synchronous execution and that the locking method is somewhat faster than the method using compiled test moreover we predict that the synchronization and or locking needed to guarantee serializability will limit speedup no matter how many processor are used 
overfitting avoidance in induction ha often been treated a if it statistically increase expected predictive accuracy in fact there is no statistical basis for believing it will have this effect overfitting avoidance is simply a form of bias and a such it effect on expected accuracy depends not on statistic but on the degree to which this bias is appropriate to a problem generating domain this paper identifies one important factor that affect the degree to which the bias of overfitting avoidance is appropriate the abundance of training data relative to the complexity of the relationship to be induced and show empirically how it determines whether such method a pessimistic and cross validated cost complexity pruning will increase or decrease predictive accuracy in decision tree induction the effect of sparse data is illustrated first in an artificial domain and then in more realistic example drawn from the uci machine learning database repository 
an approach for introducing default reasoning into first order horn clause theory is described a default theory is expressed a a set of strict implication of the form n and a set of default rule of the form n where the i and are function free literal a partial order of set of formula is obtained from these set of strict and default implication default reasoning is defined with respect to this ordering and a set of contingent ground fact crucially only strict implication appear in this structure consequently the complexity of default reasoning is that of classical reasoning together with an attendant overhead for manipulating the structure this overhead is o n where n is the number of original formula hence for default in propositional horn clause form time complexity is o n m where m is the total length of the original formula the approach is sound in that default reasoning in this structure is proven to conform to that of an extant system for default reasoning 
we present the following result about ida and related algorithm we show that ida is not asymptotically optimal in all of the case where it wa thought to be so in particular there are tree satisfying all of the condition previously thought to guarantee asymptotic optimality for ida such that ida will expand more than o n node where n is the number of node eligible for expansion by a we present a new set of necessary and sufficient condition to guarantee that ida expands o n node on tree on tree not satisfying the above condition there is no best first admissible tree search algorithm that run in s n n where n o memory and always expands o n node there are acyclic graph on which ida expands n node 
recently developed technique have improved the performance of production system several time over however these technique are not yet adequate for continuous problem solving in a dynamically changing environment to achieve adaptive real time performance in such environment we use an organization of distributed production system agent rather than a single monolithic production system to solve problem organization self design is performed to satisfy real time constraint and to adapt to changing resource requirement when overloaded individual agent decompose themselves to increase parallelism and when the load lightens the agent compose with each other to free hardware resource in addition to increased performance generalization of our composition decomposition approach provide several new direction for organization self design a pressing concern in distributed ai 
production system are an established method for encoding knowledge in an expert system the semantics of production system language and the concomitant algorithm for their evaluation rete and treat enumerate the set of rule instantiation and then apply a strategy that selects a single instantiation for firing often rule instantiation are calculated and never fired in a sense the time and space required to eagerly compute these unfired instantiation is wasted this paper present preliminary result about a new match technique lazy matching the lazy match algorithm fold the selection strategy into the search for instantiation such that only one instantiation is computed per cycle the algorithm improves the worst case asymptotic space complexity of incremental matching moreover empirical and analytic result demonstrate that lazy matching can substantially improve the execution time of production system program 
in this paper we argue that a mobile robot s environment can be determined by computing local map surrounding feature point called fixation point these fixation point are obtained by searching the scene for point which present some interesting cue for robot navigation this d computation is based on a monocular active vision system composed of a camera mounted on a rotating table accurately controlled by a computer which gaze the fixation point a the robot move the system then computes the local map and update it with each new observation in order to increase it accuracy and robustness real experimentation in a complex indoor scene illustrates that the d scene coordinate can be obtained with a good accuracy by integrating several observation 
in a precedent bused domain one appeal to previous case to support a solution decision explanation or an argument expert typically use care in choosing case in precedent based domain and apply such criterion a case relevance prototypicality and importance in domain where both case and rule are used expert use an additional case selection criterion the generalization that a particular group of case support domain expert use their knowledge of case to forge the rule learned from those case in this paper we explore inductive learning in a mixed paradigm setting where both rule based and case based reasoning method are used in particular we consider how the technique of casebased reasoning in an adversarial precedent based domain can be used to aid a decision tree based classification algorithm for training set selection branching feature choice and induction policy preference and deliberate exploitation of inductive bias we focus on how precedentbased argumentation may inform the selection of training example used to build classification tree the resulting decision tree may then be reexpressed a rule and incorporated into the mixed paradigm system we discus the heuristic control problem involved in incorporating an inductive learner into cabaret a mixed paradigm reasoner finally we present an empirical study in a legal domain of the classification tree generated by various training set constructed by a case based reasoning module 
in this paper we propose a new approach to intensional semantics of term subsumption language we introduce concept algebra whose signature are given by set of primitive concept role and the operation of the language for a given set of variable standard result give u free algebra we next define for a given set of concept definition a term algebra a the quotient of the free algebra by a congruence generated by the definition the ordering on this algebra is called descriptive subsumption we also construct a universal concept algebra a a non well founded set given by the greatest fixed point of a certain equation the ordering on this algebra is called structural subsumption we prove there are unique mapping from the free algebra to each of these and establish that our method for classifying cycle in a term subsumption language krep consists of constructing accessible pointed graph representing term in the universal concept algebra and checking a simulation relation between term 
pablo is a nonlinear planner that reason hierarchically by generating abstract predicate pablo s abstract search space are generated automatically using predicate relaxation a new technique for defining hierarchy of abstract predicate for some domain this mechanism generates hierarchy that are more useful than those created by previous technique using abstraction can lead to substantial saving in computation time furthermore pablo can achieve a limited form of reactivity when reasoning with relaxed predicate these abstraction can be viewed a small reactive plan and our method a an approach to dynamically combining these into useful nonlinear plan 
this paper is concerned with knowledge representation issue in machine learning in particular it present a representation language that support a hybrid analytical and similarity based classification scheme analytical classification is produced using a kl one like term subsumption strategy while similarity based classification is driven by generalization induced from a training set by an unsupervised learning procedure this approach can be seen a providing an inductive bias to the learning procedure thereby shortening the required training phase and reducing the brittleness of the induced generalization 
effective mapping and retrieval are important issue in successful deployment of plan reuse strategy in this paper we present a domain independent strategy for ranking a set of plausible reuse candidate in the order of cost of modifying them to solve a new planning problem the cost of modification is estimated by measuring the amount of disturbance caused to the validation structure of a reuse candidate if it were to be reused in the new problem situation this strategy is more informed than the typical feature based retrieval strategy and is more efficient than the method which require partial knowledge of the nature of the plan for the new problem situation to guide the retrieval process we discus the implementation of this retrieval strategy in friar a framework for flexible reuse and modification in hierarchical planning 
there ha been recent interest in applying hillclimbing or iterative improvement method to constraint satisfaction problem an important issue for such method is the likelihood of encountering a non solution equilibrium locally optimal point we present analytic technique for determining the relative density of solution and equilibrium point with respect to these algorithm the analysis explains empirically observed data for the n queen problem and provides insight into the potential effectiveness of these method for other problem 
in constructing probabilistic network from human judgment we use causal relationship to convey useful pattern of dependency the converse task that of inferring causal relationship from pattern of dependency is far le understood this paper establishes condition under which the directionality of some interaction can be determined from non temporal probabilistic information an essential prerequisite for attributing a causal interpretation to these interaction an efficient algorithm is developed that given data generated by an undisclosed causal polytree recovers the structure of the underlying polytree a well a the directionality of all it identifiable link 
cyclic definition are often prohibited in terminological knowledge representation language because from a theoretical point of view their semantics is not clear and from a practical point of view existing inference algorithm may go astray in the presence of cycle in this paper we shall consider terminological cycle in a very small kl one based language for this language the effect of the three type of semantics introduced by nebel a can be completely described with the help of finite automaton these description provide a rather intuitive understanding of terminology with cyclic definition and give insight into the essential feature of the respective semantics in addition one obtains algorithm and complexity result for subsumption determination a it stand the greatest fixed point semantics come off best the characterization of this semantics is easy and ha an obvious intuitive interpretation furthermore important construct such a value restriction with respect to the transitive or reflexive transitive closure of a role can easily be expressed 
the economic theory of rationality promise to equal mathematical logic in it importance for the mechaniz tion of reasoning we survey the growing literature on how the basic notion of probability utility and rational choice coupled with practical limitation on information and resource influence the design and analysis of reasoning and representation system 
when explanation include multiple medium such a text and illustration a reference to an object can be made through a combination of medium we call part of a presentation that reference material elsewhere a cross reference we are concerned here with how textual expression can refer to part of accompanying illustration the illustration to which a cross reference refers should also satisfy the specific goal of identifying an object for the user thus producing an effective cross reference not only involves text generation but may also entail modifying or replacing an existing illustration and in some case generating an illustration where previously none wa needed in this paper we describe the different type of cross reference that comet coordinated multimedia explanation testbed generates and show the role that both it text and graphic generator play in this process 
we have developed a conceptual framework and a demonstration system that contextualize or situate learning in the context of real world work situation the conceptual framework is based on the following requirement the choice of task and goal must be under the control of the user not the system the environment must be able to situate learning allow situation to talk back support reflection in action identify the instructional information relevant for task at hand and tum breakdown from disaster into opportunity for learning learning must not disrupt or interfere with solving a problem and new information to be learned must help to accomplish the task at hand our demonstration system janus developed for the domain of architectural design is built on an integrated architecture a knowledge based construction component a hypermedia based argumentation component a set of critic and a catalog of precedent solution contextualized learning is supported by the critic that link construction and argumentation and precedent solution from the catalog that situate argumentation evaluation of janus and the underlying conceptual framework have shown that this approach combine some of the best feature of open ended learning environment and tutoring system 
the construction of a program that generates crossword puzzle is discussed a in a recent paper by dechter and meiri we make an experimental comparison of various search technique the conclusion to which we come differ from theirs in some area although we agree that directional arc consistency is better than path consistency or other form of lookahead and that backjumping is to be preferred to backtracking we disagree in that we believe dynamic ordering of the constraint to be necessary in the solution of more difficult problem 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
a system is described for semi automatically tagging a large body of technical english with domain specific syntactic semantic label these label have been used to disambiguate prepositional phrase attachment for a word body of text containing more than preposition and to provide case role information for about half of the phrase 
a mechanical assembly is usually described by the geometry of it part and the spatial relation defining their position this model doe not directly provide the information needed to reason about assembly and disassembly motion we propose another representation the non directional blocking graph which describes the qualitative internal structure of the assembly this representation make explicit how the part prevent each other from being moved in every possible direction of motion it derives from the observation that the infinite set of motion direction can be partitioned into a finite arrangement of subset such that over each subset the interference among the part remain qualitatively the same we describe how this structure can be efficiently computed from the geometric model of the assembly the dis assembly motion considered include infinitesimal and extended translation in two and three dimension and infinitesimal rigid motion 
much distributed artificial intelligence research on negotiation assumes complete knowledge among the interacting agent and or truthful agent these assumption in many domain will not be realistic and this paper extends previous work to begin dealing with the case of inter agent negotiation with incomplete information a discussion of our existing negotiation framework set out the rule by which agent operate during this phase of their interaction the concept of a solution within this framework is presented the same solution concept serf for interaction between agent with incomplete information a it did for complete information interaction the possibility of incomplete information among agent open up the possibility of deception a part of the negotiation strategy of an agent deception during negotiation among autonomous agent is thus analyzed in the constrained block domain and it is shown that beneficial lie do exist in some scenario the three type of interaction cooperative compromise and conflict are examined an analysis is made of how each affect the possibility of beneficial deception by a negotiating agent 
human explanatory dialogue is an activity in which participant interactively construct explanatory model of the topic phenomenon however current explanation planning technology doe not support such dialogue in this paper we describe contribution in the area of discourse planning architecture heuristic for knowledge communication and user interface design that take step towards addressing this problem first our explanation planning architecture independently applies various constraint on the content and organization of explanation avoiding the inflexibility and contextual assumption of schematic discourse plan second certain planning operator simulate a human explainer s effort to choose and incrementally develop model of the topic phenomenon third dialogue occurs in the medium of a live information interface designed to serve a the representational medium through which the activity of the machine and human are coupled collectively these contribution facilitate interactive model construction in human machine dialogue 
many existing learning method use incre mental algorithm that construct a general ization in one pas through a set of training data and modify it in subsequent pass e g perceptrons neural net and decision tree most of these method do not store the en tire training set in essence employing a limited storage requirement that abstract the notion of a compressed representation the question we address is how much additional processing time is required for method with limited stor age processing time for learning algorithm is equated in this paper with the number of pass necessary through a data set to obtain a correct generalization for instance neural net require many pass through a data set before converging decision tree require fewer pass but precise bound are unknown we consider limited storage algorithm for a particular concept class nested hyperrectangles we prove bound that illustrate the fundamental trade off between storage require ments and processing time required to learn an optimal structure it turn out that our lower bound apply to other algorithm and concept class e g decision tree a well notably imposing storage limitation on the learning task force one to devise a completely different algorithm to reduce the number of pass we also briefly discus parallel learning algorithm 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
the task of inferring a set of class and class description most likely to explain a given data set can be placed on a firm theoretical foundation using bayesian statistic within this framework and using various mathematical and algorithmic approximation the auto class system search for the most probable classification automatically choosing the number of class and complexity of class description simpler version of autoclass have been applied to many large real data set have discovered new independently verified phenomenon and have been released a a robust software package recent extension allow attribute to be selectively correlated within particular class and allow class to inherit or share model parameter though a class hierarchy 
this paper describes an approach to student modeling for intelligent tutoring system based on an explicit representation of the tutor s belief about the student and the argument for and against those belief called endorsement a lexicographic comparison of argument sorted according to evidence reliability provides a principled mean of determining those belief that are considered true false or uncertain each of these belief is ultimately justified by underlying assessment data the endorsement based approach to student modeling is particularly appropriate for tutor controlled by instructional planner these tutor place greater demand on a student model than opportunistic tutor numeric calculus approach are le well suited because it is difficult to correctly assign number for evidence reliability and rule plausibility it may also be difficult to interpret final result and provide suitable combining function when numeric measure of uncertainty are used arbitrary numeric threshold are often required for planning decision such an approach is inappropriate when robust context sensitive planning decision must be made instead the ability to examine belief and justification is required this paper present a tm based implementation of the endorsement based approach to student modeling discus the advantage of this approach for planner controlled tutor and compare this approach to alternative 
a method is presented that cause a to return high quality solution while solving a set of problem using a non admissible heuristic the heuristic guiding the search change a new information is learned during the search and it converges to an admissible heuristic which contains the insight of the original nonadmissible one after a finite number of problem a return only optimal solution experiment on sliding tile problem suggest that learning occurs very fast beginning with hundred of randomly generated problem and an overestimating heuristic the system learned sufficiently fast that only the first problem wa solved non optimally a an application we show how one may construct heuristic for finding high quality solution at lower cost than those returned by a using available admissible heuristic 
the use of abstraction in problem solving is an effective approach to reducing search but finding good abstraction is a difficult problem even for people this paper identifies a criterion for selecting useful abstraction describes a tractable algorithm for generating them and empirically demonstrates that the abstraction reduce search the abstraction learner called alpine is integrated with the prodigy problem solver minton et al b carbonell et al and ha been tested on large problem set in multiple domain 
a novel architecture is presented for combining rule based and case based reasoning the central idea is to apply the rule to a target problem to get a first approximation to the answer but if the problem is judged to be compellingly similar to a known exception of the rule in any aspect of it behavior then that aspect is modelled after the exception rather than the rule the architecture is implemented for the full scale task of pronouncing surname preliminary result suggest that the system performs almost a well a the best commercial system however of more interest than the absolute performance of the system is the result that this performance wa better than what could have been achieved with the rule alone this illustrates the capacity of the architecture to improve on the rule based system it start with the result also demonstrate a beneficial interaction in the system in that improving the rule speed up the case based component 
this paper describes an initial exploration into large learning system i e system that learn a large number of rule given the well known utility problem in learning system efficiency question are a major concern but the question are much broader than just efficiency e g will the effectiveness of the learned rule change with scale this investigation us a single problem solving and learning system dispatcher soar to begin to get answer to these question dispatcher soar ha currently learned new production on top of an initial system of production so it total size is production this represents one of the largest production system in existence and by far the largest number of rule ever learned by an ai system this paper present a variety of data from our experiment with dispatcher soar and raise important question for large learning system 
during the postwar heyday of physic c p snow wrote a short article entitled the two culture there he pointed out the growing division between the science culture and the non science literary culture he observed that scientist basically had no understanding of nay even any concern for literary culture and vice versa he pointed out the profound loss to society that wa resulting from this dichotomy namely creativity often arises in the interchange of idea sadly the two culture were so polarized even then that snow felt that little real dialogue took place between member of the two culture 
in this paper we propose a framework for integrating fault diagnosis and incremental knowledge acquisition in connectionist expert system a new case solved by the diagnostic function is formulated a a new example for the learning function to learn incrementally the diagnostic function is composed of a neural network based example module and a symbolic based rule module while the example module is always first invoked to provide the shortcut solution the rule module provides extensive coverage of case to handle odd case when example module fails two application based on the proposed framework will also be briefly mentioned 
there are many planning application that require an agent to coordinate it activity with process that change continuously over time several proposal have been made for combining a temporal logic of time with the differential and integral calculus to provide a hybrid calculus suitable for planning application we take one proposal and explore some of the issue involved in implementing a practical system that derives conclusion consistent with such a hybrid calculus model for real valued parameter are specified a system of ordinary differential equation and construct are provided for reasoning about how these model change over time for planning problem that require projecting the consequence of a set of event from a set of initial condition and causal rule a combination of numerical approximation and symbolic math routine and a simple default reasoning strategy provide for an efficient inference engine 
it ha long been recognized that hierarchical problem solving can be used to reduce search yet there ha been little analysis of the problem solving method and few experimental result this paper provides the first comprehensive analytical and empirical demonstration of the effectiveness of hierarchical problem solving first the paper show analytically that hierarchical problem solving can reduce the size of the search space from exponential to linear in the solution length and identifies a sufficient set of assumption for such reduction in search second it present empirical result both in a domain that meet all of these assumption a well a in domain in which these assumption do not strictly hold third the paper explores the condition under which hierarchical problem solving will be effective in practice 
we have developed two system fn and andd that use natural language and graphical display respectively to communicate information about object to human user both system must deal with the fundamental problem of ensuring that their output doe not carry unwanted and inappropriate conversational implicatures we describe the type of conversational implicatures that fn and andd can avoid and the computational strategy the two system use to generate output that is free of unwanted implicatures 
many real world planning problem involve substantial amount of domain specific reasoning that is either awkward or inefficient to encode in a general purpose planner previous approach for planning in such domain have either been largely domain specific or have employed shallow model of the domain specific consideration in this paper we investigate a hybrid planning model that utilizes a set of specialist to complement both the overall expressiveness and the reasoning power of a traditional hierarchical planner such a model retains the flexibility and generality of classical planning framework while allowing deeper and more efficient domain specific reasoning through specialist we describe a preliminary implementation of a planning architecture based on this model in a manufacturing planning domain and use it to explore issue regarding the effect of the specialist on the planning and the interaction and interface between them and the planner 
a formal equivalence between propositional expert system and decision table is proved and a practicable procedure given to perform the transformation between propositional expert system and decision table the method gave an order of magnitude speed increase for a well known expert system in routine use the method is very general adaptation are shown for forward and backward chaining inferencing engine inexact reasoning and system where some fact have a high cost and must be determined only if necessary a particular application for the decision table representation is in real time expert system since a simple hardware implementation is available which give further order of magnitude increase in performance finally the decision table representation greatly simplifies the problem of completeness and consistency checking 
we report on the development and application of an efficient unsupervised learning procedure for the classification of an unsegmented datastream given a set of probabilistic binary similarity judgment between region in the stream our method is effective on very large database and tolerates the presence of noise in the similarity judgement and in the extent of similar region we applied this method to the problem of finding the sequence level building block of protein after verifying the effectiveness of the clusterer by testing it on synthetic protein data with known evolutionary history we applied the method to a large protein sequence database a datastream of more than element and found about protein sequence class the motif defined by these class are of biological interest and have the potential to supplement or replace the existing manual annotation of protein sequence database 
we develop a formalism for reasoning with default that are expressed with different level of firmness necessary and sufficient condition for consistency are established and a unique ranking of the rule is found called z which render model a normal a possible subject to the consistency condition we provide the necessary machinery for testing consistency computing the z ranking and drawing the set of plausible conclusion it entail 
speedup learning seek to improve the efficiency of search based problem solver in this paper we propose a new theoretical model of speedup learning which capture system that improve problem solving performance by solving a user given set of problem we also use this model to motivate the notion of batch problem solving and argue that it is more congenial to learning than sequential problem solving our theoretical result are applicable to all serially decomposable domain we empirically validate our result in the domain of eight puzzle 
symmetric network that are based on energy minimization such a boltzmann machine or hopfield net are used extensively for optimization constraint satisfaction and approximation of np hard problem nevertheless finding a global minimum for the energy function is not guaranteed and even a local minimum may take an exponential number of step we propose an improvement to the standard activation function used for such network the improved algorithm guarantee that a global minimum is found in linear time for tree like subnetworks the algorithm is uniform and doe not assume that the network is a tree it performs no worse than the standard algorithm for any network topology in the case where there are tree growing from a cyclic subnetwork the new algorithm performs better than the standard algorithm by avoiding local minimum along the tree and by optimizing the free energy of these tree in linear time the algorithm is self stabilizing for tree cycle free undirected graph and remains correct under various scheduling demon however no uniform protocol exists to optimize tree under a pure distributed demon and no such protocol exists for cyclic network under central demon 
this paper investigates the effect of parallelism on blackboard system scheduling a parallel blackboard system is described that allows multiple knowledge source instantiation to execute in parallel using a shared memory blackboard approach new class 
numerical simulation phase space analysis and analytic techmques are three method used to solve quantitative differential equation most work in qualitative reasoning ha dealt with analog of the first two technique producing capability applicable to a wide range of system although potentially of benefit little ha been done to provide closed form analytic solution technique for qualitative differential equation qdes this paper present one such technique for the solution of a class of ordinary linear and nonlinear differential equation the technique is capable of deriving closed form description of the qualitative temporal behavior represented by such equation a language qfl for describing qualitative temporal behavior is presented and procedure and an implementation qdiff that solves equation in this form are demonstrated 
one of the main reason why computer generated proof are not widely accepted is often their complexity and incomprehensibility especially proof of mathematical theorem with equation are normally presented in an inadequate and not intuitive way this is even more of a problem for the presentation of inference drawn by automated reasoning component in other ai system for first order logic proof transformation procedure have been designed in order to structure proof and state them in a formalism that is more familiar to human mathematician in this report we generalize these approach so that proof involving equational reasoning can also be handled to this end extended refutation graph are introduced to represent combined resolution and paramodulation proof in the process of transforming these proof into natural deduction proof with equality the inherent structure can also be extracted by exploiting topological property of refutation graph 
this paper investigates design issue associated with representing relation in binary network augmented with hidden variable the trade off between the number of variable required and the size of their domain is discussed we show that if the number of value available to each variable is just two then hidden variable cannot improve the expressional power of the network regardless of their number however for k we can always find a layered network using k valued hidden variable that represent an arbitrary relation we then provide a scheme for decomposing an arbitrary relation using k hidden variable each having k value k 
the symbolic probabilistic inference spi algorithm d ambrosio provides an efficient framework for resolving general query on a belief network it applies the concept of dependency directed backward search to probabilistic inference and is incremental with respect to both query and observation unlike most belief network algorithm spi is goal directed performing only those calculation that are required to respond to query the directed graph of the underlying belief network is used to develop a tree structure for recursive query processing this allows effective caching of intermediate result and significant opportunity for parallel computation a simple preprocessing step ensures that given the search tree the algorithm will include no unnecessary distribution the preprocessing step eliminates dimension from the intermediate result and prune the search path 
island driven parsing is of great relevance for speech recognition understanding and other natural language processing application a bidirectional algorithm is presented that efficiently solves this problem allowing both any possible determination of the starting word in the input sentence and flexible control in particular a mixed bottom to top and top down approach is followed without leading to redundant partial analysis the algorithm performance is discussed 
this paper discus discovery of mathematical model from engineering data set keds a knowledge based equation discovery system identifies several potentially overlapping region in the problem space each associated with an equation of different complexity and accuracy the minimum description length principle together with the keds algorithm is used to guide the partitioning of the problem space the kedsmdl algorithm ha been tested on discovering model for predicting the performance efficiency of an internal combustion engine 
object recognition requires complicated domain specific rule for many problem domain it is impractical for a programmer to generate these rule a method for automatically generating the required object class description is needed this paper present a method to accomplish this goal in our approach the supervisor provides a series of example scene description to the system with accompanying object class assignment generalization rule then produce object class description these rule manipulate non symbolic descriptor in a symbolic framework the resulting class description are useful both for object recognition and for providing clear explanation of the decision process we present a simple method for maintaining an optimal description set a new example possibly of previously unseen class become available providing needed update to the description set finally the system s performance is shown a it learns object class description from realistic scene video image of electronic component 
a touted advantage of symbolic representation is the ease of transferring learned information from one intelligent agent to another this paper investigates an analogous problem how to use information from one neural network to help a second network learn a related task rather than translate such information into symbolic form in which it may not be readily expressible we investigate the direct transfer of information encoded a weight here we focus on how transfer can be used to address the important problem of improving neural network learning speed first we present an exploratory study of the somewhat surprising effect of pre setting network weight on subsequent learning guided by hypothesis from this study we sped up back propagation learning for two speech recognition task by transferring weight from smaller network trained on subtasks we achieved speedup of up to an order of magnitude compared with training starting with random weight even taking into account the time to train the smaller network we include result on how transfer scale to a large phoneme recognition problem 
real time constraint on ai system require guaranteeing bound on these system performance however in the presence of source of uncontrolled combinatorics it is extremely difficult to guarantee such bound on their performance in production system the prirnary source of uncontrolled combinatorics is the production match to eliminate these combinatorics the unique attribute formulation wa introduced in tambe and rosenbloom which achieved a linear bound on the production match this formulation lead to several question is this unique attribute formulation the best conceivable production system formulation in fact are there other alternative production system formulation if there are other formulation how should these alternative be compared with the unique attribute formulation this paper attempt to address these question in the context of soar it identifies independent dimension along which alternative production system formulation can be specified these dimension are based on the fiied class of match algorithm currently employed in production system these dimension create a framework for systematically generating alternative formulation using this framework we show that the unique attribute formulation is the best one within the dimension investigated however if a new class of match algorithm is admitted by relaxing certain constraint other competitor fonnulations emerge the paper indicates which competitor formulation are promising and why although some of the concept such a unique attribute are introduced in the context of soar they should also be relevant to other rule based system 
research in artificial intelligence on constraint based representation for temporal reasoning ha largely concentrated on two kind of formalism system of simple linear inequality to encode metric relation between time point and system of binary constraint in allen s temporal calculus to encode qualitative relation between time interval each formalism ha certain advantage linear inequality can represent date duration and other quantitive information allen s qualitative calculus can express relation between time interval such a disjointedness that are useful for constraint based approach to planning in this paper we demonstrate how metric and allenstyle constraint network can be integrated in a constraint based reasoning system the highlight of the work include a simple but powerful logical language for expressing both quantitative and qualitative information translation algorithm between the metric and allen sublanguages that entail minimal loss of information and a constraint propagation procedure for problem expressed in a combination of metric and allen constraint 
in this paper we develop a proof procedure for autoepistemic ael and default logic dl based on translating them into a truth maintenance system tm the translation is decidable if the theory consists of a finite number of default and premise and classical derivability for the base language is decidable to determine all extension of a network we develop variant of doyle s labelling algorithm 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
the virtue of the strip assumption for planning is that it bound the information relevant to determining the effect of action viewing the assumption a a statement about belief we find that it doe not actually assume anything about the world itself we can characterize the assertion about belief in term of probabilistic independence thereby facilitating analysis of representation for planning under uncertainty this interpretation separate the strip assumption from other necessary feature of a planning architecture such a it model of persistence and it inferential policy by isolating these factor we can understand the role of dependence across a wide range of planner and action representation graphical model of dependence developed for probabilistic analysis provide a convenient tool for verifying the strip assumption for a variety of planning system investigation of a few representative system reveals a markovian event structure common to these planning model 
we describe an algorithm for tracking an unknown object in natural scene we require that the object s approximate initial location be available and further assume that the it ean be distinguished from the background by motion or stereo no constraint is placed on the object s shape other than that it not change too rapidly from frame to frame object are tracked and segmented cross temporally using a massively parallel bottom up approach our algorithm ha been implemented on a connection machine and run in real time frame per second 
we introduce a greedy local search procedure called gsat for solving propositional satisfiability problem our experiment show that this procedure can be used to solve hard randomly generated problem that are an order of magnitude larger than those that can be handled by more traditional approach such a the davis putnam procedure or resolution we also show that gsat can solve structured satisfiability problem quickly in particular we solve encoding of graph coloring problem n queen and boolean induction general application strategy and limitation of the approach are also discussed gsat is best viewed a a model finding procedure it good performance suggests that it may be advantageous to reformulate reasoning task that have traditionally been viewed a theorem proving problem a model finding task 
this paper present a projection algorithm for incremental control rule synthesis the algorithm synthesizes an initial set of goal achieving control rule using a combination of situation probability and estimated remaining work a a search heuristic this set of control rule ha a certain probability of satisfying the given goal the probability is incrementally increased by synthesizing additional control rule to handle error situation the execution system is likely to encounter when following the initial control rule by using situation probability the algorithm achieves a computationally effective balance between the limited robustness of triangle table and the absolute robustness of universal plan 
the previously described kbann system integrates existing knowledge into neural network by defining the network topology and setting initial link weight standard neural learning technique can then be used to train such network thereby refining the information upon which the network is based however standard neural learning technique are reputed to have difficulty training network with multiple layer of hidden unit kbann commonly creates such network in addition standard neural learning technique ignore some of the information contained in the network created by kbann this paper describes a symbolic inductive learning algorithm for training such network that us this previously ignored information and which help to address the problem of training deep network empirical evidence show that this method improves not only learning speed but also the ability of network to generalize correctly to testing example 
choosing between multiple ontological perspective is crucial for reasoning about the physical world choosing the wrong perspective can make a reasoning task impossible this paper introduces a lagrangian plug flow ontology pf for reasoning about thermodynamic fluid flow we show that this ontology capture continuously changing behavior of flowing fluid not represented in currently implemented ontology these behavior are essential for understanding thermodynamic application such a power cycle refrigeration liquefaction throttling and flow through nozzle we express the ontology within the framework of qualitative process qp theory to derive our qp theory for plug flow we use the method of causal clustering to find causal interpretation of thermodynamic equation we also incorporate qualitative version of standard thermodynamic relation including the second law of thermodynamics and clapeyron s equation 
generalized clause differ from ordinary clause by allowing conjunction of literal in the role of ordinary literal i e they are disjunction of conjunction of simple literal an advantage of this clausal form is that implication with conjunctive conclusion or disjunctive premise are not split into multiple clause an extension of lovelands model elimination calculus loveland a loveland is presented able to deal with such generalized clause furthermore we describe a method for generating lemma that correspond to valid instance of conjunctive conclusion using these lemma it is possible to avoid multiple proof of the premise of implication with conjunctive conclusion 
best first search is a general search algorithm that always expands next a frontier node of lowest cost it applicability however is limited by it exponential memory requirement iterative deepening a previous approach to this problem doe not expand node in best first order if the cost function can decrease along a path we present a linear space best first search algorithm rbfs that always explores new node in best first order regardless of the cost function and expands fewer node than iterative deepening with a nondecreasing cost function on the sliding tile puzzle rbfs with a weighted evaluation function dramatically reduces computation time with only a small penalty in solution cost in general rbfs reduces the space complexity of best first search from exponential to linear at the cost of only a constant factor in time complexity in our experiment 
two important computational approach to problem solving are model based reasoning mbr and case based reasoning cbr mbr since it reason from first principle is especially suited for solving novel problem cbr since it reason from previous experience is especially suited for solving frequently encountered problem however large novel problem pose difficulty for both approach mbr rapidly grows intractable and cbr fails to find a relevant previous case in this paper we describe an approach called prototype based reasoning that integrates both approach to solve such problem prototype based reasoning treat a large novel problem a a novel combination of several familiar subproblems it us cbr to find and solve the subproblems formulates a new problem by combining these individual solution and us mbr to solve this new problem we demonstrate the effectiveness of this method on several example involving the causal simulation of complex electronic circuit 
this paper discus unsupervised learning of orthogonal concept on relational data relational predicate while formally equivalent to the feature of the concept learning literature are not a good basis for defining concept hence the current task demand a much larger search space than traditional concept learning algorithm the sort of space explored by connectionist algorithm however the intended application using the discovered concept in the cyc knowledge base requires that the concept be interpretable by a human an ability not yet realized with connectionist algorithm interpretability is aided by including a characterization of simplicity in the evaluation function for hinton s family relation data we do find cleaner more intuitive feature yet when the solution are not known in advance the difficulty of interpreting even feature meeting the simplicity criterion call into question the usefulness of any reformulation algorithm that creates radically new primitive in a knowledge based setting at the very least much more sophisticated explanation tool are needed 
constraint network have been shown to be useful in formulating such diverse problem a scene labeling natural language parsing and temporal reasoning given a constraint network we often wish to i find a solution that satisfies the constraint and ii find the corresponding minimal network where the constraint are a explicit a possible both task are known to be np complete in the general case task i is usually solved using a backtracking algorithm and task ii is often solved only approximately by enforcing various level of local consistency in this paper we identify a property of binary constraint called row convexity and show it usefulness in deciding when a form of local consistency called path consistency is sufficient to guarantee a network is both minimal and decomposable decomposable network have the property that a solution can be found without backtracking we show that the row convexity property can be tested for efficiently and we show by examining application of constraint network discussed in the literature that our result are useful in practice thus we identify a large class of constraint network for which we can solve both task i and ii efficiently 
the performance of production program can be improved by firing multiple rule in a production cycle in this paper we present the multiple context multiple rule mcmr model which speed up production program execution by firing multiple rule concurrently and guarantee the correctness of the solution the mcmr model is implemented using the rubic parallel inference model on the intel ipsc hypercube the intel ipsc hypercube is chosen because it is a cost effective solution to large scale application to avoid unnecessary synchronization and improve performance rule are executed asynchronously and message are used to update the database preliminary implementation result for the rubic parallel inference environment on the intel ipsc hypercube are reported 
this paper describes the application of explanation based learning a machine learning technique to the sri core language engine a large scale general purpose natural language analysis system the idea is to bypass normal morphological syntactic and 
this paper present a methodology which enables the derivation of goal ordering rule from the analysis of problem failure we examine all the planning action that lead to failure if there are restriction imposed by a problem state on taking possible action the restriction manifest themselves in the form of a restricted set of possible binding our method make use of this observation to derive general control rule which are guaranteed to be correct the overhead involved in learning is low because our method examines only the goal stack retrieved from the leaf node of a failure search tree rather than the whole tree empirical test show that the rule derived by our system pal after sufficient training performs a well a or better than those derived by system such a prodigy ebl and static 
the intelligent database interface idi is a cache based interface that is designed to provide artificial intelligence system with efficient access to one or more database on one or more remote database management system dbms it can be used to interface with a wide variety of different dbms with little or no modification since sql is used to communicate with remote dbms and the implementation of the idi provides a high degree of portability the query language of the idi is a restricted subset of function free horn clause which is translated into sql result from the idi are returned one tuple at a time and the idi manages a cache of result relation to improve efficiency the idi is one of the key component of the intelligent system server i knowledge representation and reasoning system and is also being used to provide database service for the unisys spoken language system program 
we investigate the computational complexity of membership problem in a number of propositional default logic we introduce a hierarchy of class of propositional default rule that extends that described in kautz and selman and characterize the complexity of membership problem in these class under various simplifying assumption about the underlying propositional theory our work significantly extends both that presented in kautz and selman and in stillman a 
although most design replay technique have been empirically tested against some performance program there ha been very little empirical evidence published that compare various approach on the same problem to determine the source of power six different design replay algorithm based on approach in the literature are implemented and tested on different design replay problem the resulting data indicate that there is a trade off between efficiency and autonomy for certain type of adaptation strategy based on some of the lesson drawn from this data a new algorithm remaid ha been developed this algorithm recognizes two different type of mi match between previous experience and current problem detour and pretours the remaid strategy take advantage of it knowledge of mi match to improve replay autonomy without sacrificing efficiency the success of the remaid algorithm is empirically verified 
one goal of machine discovery is to automate creative task from human scientific practice this paper describes a project to automate in a general manner the theory driven discovery of reaction pathway in chemistry and biology we have designed a system called mechem that proposes credible pathway hypothesis from data ordinarily available to the chemist mechem ha been applied to reaction drawn from the history of biochemistry from recent industrial chemistry a reported in journal and from organic chemistry textbook the paper first explains the chemical problem and discus previous ai treatment then are presented the architecture of the system the key algorithmic idea and the heuristic used to explore the very large space of chemical pathway the system s efficacy is demonstrated on a biochemical reaction studied earlier by kulkarni and simon in the kekada system and on another reaction from industrial chemistry our project ha also resulted in separate novel contribution to chemical knowledge demonstrating that we have not simplified the task for our convenience but have addressed it full complexity 
a new generation of knowledge database is emerging these system contain thousand of object densely interconnected and heterogeneously organized entered from many source both human and automated such system present tremendous challenge to their user who must locate relevant information quickly and add new information effectively our research aim to understand and support the knowledge editing task the hit knowledge editor hke is an interface that support browsing and modifying the cyc knowledge base guha lenat hke ha been designed to be a collaborative interface following a set of principle for sharing task between system and user we describe these principle and illustrate how hke provides resource built according to those principle that collaborate with it user on a variety of knowledge editing task 
this paper present a formal theory of multiple agent non monotonic reasoning we introduce the subject of multiple agent non monotonic reasoning for inquiry and motivate the field in term of it application for commonsense reasoning we extend moore s autoepistemic logic to the multiple agent case and show that the resulting logic is too weak for most application of commonsense reasoning we then suggest some possible set of principle for a logic of multipleagent non monotonic reasoning based on the concept of an agent s arrogance towards his knowledge of another agent s ignorance while the principle of arrogance are in general too strong we demonstrate that restricted version of these principle can work quite well for commonsense reasoning in particular we show that a restricted form of the principle of arrogance yield result that are equivalent to emat morgenstern a non monotonic logic which wa designed to reason about temporal projection in epistemic context 
this paper describes an adaptive model based diagnostic mechanism although model based system are more robust than heuristic based expert system they generally require more computation time time consumption can be significantly reduced by using a hierarchical model scheme which present view of the device at several different level of detail we argue that in order to employ hierarchical model effectively it is necessary to make economically rational choice concerning the trade off between the cost of a diagnosis and it precision the mechanism presented here make these choice using a model diagnosability criterion which estimate how much information could be gained by using a candidate model it take into account several important parameter including the level of diagnosis precision required by the user the computational resource available the cost of observation and the phase of the diagnosis experimental result demonstrate the effectiveness of the proposed mechanism 
consistency technique have been studied extensively in the past a a way of tackling constraint satisfaction problem csp in particular various arc consistency algorithm have been proposed originating from waltz s filtering algorithm and culminating in the optimal algorithm ac of mohr and henderson ac run in o ed in the worst case where e is the number of arc or constraint and d is the site of the largest domain being applicable to the whole class of binary csp these algorithm do not take into account the semantics of constraint in this paper we present a new generic arc consistency algorithm ac the algorithm is parametrised on two specified procedure and can be instantiated to reduce to ac and ac more important ac can be instantiated to produce an o ed algorithm for two important class of constraint functional and monotonic constraint we also show that ac ha an important application in constraint logic programming over finite domain the kernel of the constraint solver for such a programming language is an arc consistency algorithm for a set of basic constraint we prove that ac in conjunction with node consistency provides a decision procedure for these constraint running in time o ed 
standard algorithm for explanation based learning require complete and correct knowledge base the kbann system relaxes this constraint through the use of empirical learning method to refine approximately correct knowledge this knowledge is used to determine the structure of an artificial neural network and the weight on it link thereby making the knowledge accessible for modification by neural learning kbann is evaluated by empirical test in the domain of molecular biology network created by kbann are shown to be superior in term of their ability to correctly classify unseen example to randomly initialized neural network decision tree nearest neighbor matching and standard technique reported in the biological literature in addition kbann s network improve the initial knowledge in biologically interesting way 
the mating paradigm for automated theorem provers wa proposed by andrew to avoid some of the shortcoming in resolution it facilitates automated deduction in higher order and non classical logic moreover there are procedure which translate back and forth between refutation by the mating method and proof in a natural deduction system we describe a search procedure called path focused duplication for finding refutation by the mating method this procedure which is a complete strategy for the mating method address two crucial issue inadequately handled in current implementation that arise in the search for refutation when and how to expand the search space it focus on a particular path that seems to cause an impasse in the search and expands the search space relative to this path in a way that allows the search to immediately resolve the impasse the search space grows and shrink dynamically to respond to the requirement that have arisen or have been met in the search process thus avoiding an explosion in the size of the search space we have implemented a prototype of this procedure and have been able to easily solve many problem that an earlier program found difficult 
utterance that include rationale clause and mean clause display a variety of feature that affect their interpretation a well a the subsequent discourse of particular importance it the information that is conveyed about agent belief and intention with respect to the action they talk about or perform hence for a language interpretation system to handle these utterance it must identify the relevant feature of each construction and draw appropriate inference about the agent mental state with respect to the action and action relation that are involved this paper describes an interpretation model that satisfies this need by providing a set of interpretation rule and showing how these rule allow for the derivation of the appropriate set of belief and intention associated with each construction 
a common problem in robotic assembly is that of mating tightly fitting part when the location and the dimension of the part are somewhat uncertain it is neeessary to be able to reason about these uncertainty in conjunction with the geometry of the part involved in order to develop motion plan for assembly operation in this paper we will present a method for the treatment of three type of uncertainty usually prevalent in robotic assembly system uncertainty in the initial location of part uncertainty in the control of the robot used to assemble these part and uncertainty in the dimension of these part the method we will present used by a cad based planning system we have developed discovers which portion of an assembly operation must be carried out using force torque guided motion because the composite uncertainty exceed the clearance during these portion of the operation the method further suggests the type of force torque guided motion that need to be used for these portion with this knowledge our planning system formulates motion plan for assembly operation plan for a variety of assembly have been produced by our planning system and have been experimentally verified on both a cincinnati mi lacron t robot and a puma robot 
